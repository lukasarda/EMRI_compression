no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 1400
20240628_002429
Epoch 1:
batch 5 loss: 0.018958589900285004
batch 10 loss: 0.0037794905714690686
batch 15 loss: 0.0020835301838815212
batch 20 loss: 0.0017168887658044695
batch 25 loss: 0.0014490473084151744
batch 30 loss: 0.001307660131715238
batch 35 loss: 0.0011665476253256203
batch 40 loss: 0.001251071784645319
batch 45 loss: 0.0009545764769427478
batch 50 loss: 0.0008853473933413625
batch 55 loss: 0.0008303568116389215
batch 60 loss: 0.0007759708561934531
batch 65 loss: 0.0008155868505127728
batch 70 loss: 0.0007129012956283987
batch 75 loss: 0.000687247549649328
batch 80 loss: 0.000670579809229821
batch 85 loss: 0.0006804822362028063
batch 90 loss: 0.0006599446758627891
batch 95 loss: 0.0006438821088522673
batch 100 loss: 0.0006309137679636478
batch 105 loss: 0.000616613496094942
batch 110 loss: 0.0006288141943514347
batch 115 loss: 0.0006375482538715005
batch 120 loss: 0.0006204059347510338
batch 125 loss: 0.0006154282251372934
batch 130 loss: 0.0006146400235593319
batch 135 loss: 0.0006140008103102445
batch 140 loss: 0.0006130113964900374
batch 145 loss: 0.0006278080632910133
batch 150 loss: 0.0007776724058203399
batch 155 loss: 0.0006436129915527999
batch 160 loss: 0.0006210055551491677
batch 165 loss: 0.0006198948714882136
batch 170 loss: 0.0006193720269948244
batch 175 loss: 0.0006182789453305304
batch 180 loss: 0.0006163239711895585
batch 185 loss: 0.000614798697642982
batch 190 loss: 0.0006133397342637181
batch 195 loss: 0.0006180098280310631
batch 200 loss: 0.0006154915201477707
batch 205 loss: 0.0006135169416666031
batch 210 loss: 0.0006128183216787874
batch 215 loss: 0.0006122949183918535
batch 220 loss: 0.0006118257530033589
batch 225 loss: 0.0006113237584941089
batch 230 loss: 0.0006107740453444421
batch 235 loss: 0.0006098902551457286
batch 240 loss: 0.0006084173335693776
Training Loss: 0.0012107822578400374
Validation Loss: 0.0005930557750010242
Epoch 2:
batch 5 loss: 0.0006273558363318443
batch 10 loss: 0.0006919676088728011
batch 15 loss: 0.0006320198765024543
batch 20 loss: 0.0006140039884485304
batch 25 loss: 0.0006131795584224164
batch 30 loss: 0.0006118854507803917
batch 35 loss: 0.0006103807361796498
batch 40 loss: 0.0006089058937504887
batch 45 loss: 0.0006070986273698509
batch 50 loss: 0.0006065579713322222
batch 55 loss: 0.0006054113269783557
batch 60 loss: 0.0006045676651410759
batch 65 loss: 0.0006033796118572355
batch 70 loss: 0.0006024381145834923
batch 75 loss: 0.0006018919288180768
batch 80 loss: 0.0006012075464241207
batch 85 loss: 0.0006003918242640793
batch 90 loss: 0.0005996633670292794
batch 95 loss: 0.0005981379305012524
batch 100 loss: 0.0005980666959658265
batch 105 loss: 0.0005956447566859424
batch 110 loss: 0.0005932891159318388
batch 115 loss: 0.0005922031006775797
batch 120 loss: 0.0005918017472140491
batch 125 loss: 0.0005914296838454902
batch 130 loss: 0.0005907196318730712
batch 135 loss: 0.0005940102855674922
batch 140 loss: 0.0005937122157774865
batch 145 loss: 0.0005877366987988353
batch 150 loss: 0.0026547991903498767
batch 155 loss: 0.0016271381871774792
batch 160 loss: 0.0014204007340595127
batch 165 loss: 0.0012888740748167038
batch 170 loss: 0.0011570680188015103
batch 175 loss: 0.0010168621549382806
batch 180 loss: 0.0009543634601868689
batch 185 loss: 0.0008747041807509959
batch 190 loss: 0.0008450560737401247
batch 195 loss: 0.0008054868085309863
batch 200 loss: 0.000771244231145829
batch 205 loss: 0.0007497070473618805
batch 210 loss: 0.0007319889613427222
batch 215 loss: 0.000715127494186163
batch 220 loss: 0.0006996474228799343
batch 225 loss: 0.0006868310272693634
batch 230 loss: 0.0006771329557523131
batch 235 loss: 0.0006686760578304529
batch 240 loss: 0.0006604783120565116
Training Loss: 0.0007619717747729737
Validation Loss: 0.0006173024206267049
Epoch 3:
batch 5 loss: 0.0006521237199194729
batch 10 loss: 0.0006451293476857245
batch 15 loss: 0.0006383829400874675
batch 20 loss: 0.0006314071244560183
batch 25 loss: 0.0006259199697524309
batch 30 loss: 0.0006244718562811614
batch 35 loss: 0.0006287845666520297
batch 40 loss: 0.0006198517978191376
batch 45 loss: 0.000614260055590421
batch 50 loss: 0.0006089904927648604
batch 55 loss: 0.0006051637232303619
batch 60 loss: 0.0006015173392370343
batch 65 loss: 0.0005980947054922581
batch 70 loss: 0.0005938197369687259
batch 75 loss: 0.0005896107759326696
batch 80 loss: 0.0005872485227882863
batch 85 loss: 0.000585234141908586
batch 90 loss: 0.0005828291992656887
batch 95 loss: 0.0005813285824842751
batch 100 loss: 0.000580023298971355
batch 105 loss: 0.0005780987325124442
batch 110 loss: 0.0005769343697465956
batch 115 loss: 0.0005764120258390903
batch 120 loss: 0.0005761155392974615
batch 125 loss: 0.0005755353369750083
batch 130 loss: 0.0005753125762566925
batch 135 loss: 0.0005749118281528354
batch 140 loss: 0.0005745762260630727
batch 145 loss: 0.0005743695423007011
batch 150 loss: 0.0005739689222536981
batch 155 loss: 0.000573750352486968
batch 160 loss: 0.0005733811063691974
batch 165 loss: 0.0005730781587772072
batch 170 loss: 0.0005726687493734061
batch 175 loss: 0.0005717224325053394
batch 180 loss: 0.0005698221852071583
batch 185 loss: 0.000568953505717218
batch 190 loss: 0.0005687661934643984
batch 195 loss: 0.0005684945615939796
batch 200 loss: 0.0005683490890078247
batch 205 loss: 0.0005682330578565597
batch 210 loss: 0.0005682918126694858
batch 215 loss: 0.000568101613316685
batch 220 loss: 0.0005676053115166724
batch 225 loss: 0.0005674524465575815
batch 230 loss: 0.0005660693743266166
batch 235 loss: 0.0005645949626341462
batch 240 loss: 0.0005639023962430656
Training Loss: 0.000587368006381439
Validation Loss: 0.000563341886542427
Epoch 4:
batch 5 loss: 0.0005637837224639952
batch 10 loss: 0.0005637344089336694
batch 15 loss: 0.0005634359200485051
batch 20 loss: 0.0005635233828797936
batch 25 loss: 0.0005634316126815975
batch 30 loss: 0.0005634411587379873
batch 35 loss: 0.0005634035682305693
batch 40 loss: 0.0005633033229969442
batch 45 loss: 0.0005633034743368626
batch 50 loss: 0.0005632810876704753
batch 55 loss: 0.000563197280280292
batch 60 loss: 0.0005632874788716435
batch 65 loss: 0.0005632483516819775
batch 70 loss: 0.0005631859530694783
batch 75 loss: 0.0005631023785099387
batch 80 loss: 0.0005631616106256843
batch 85 loss: 0.0005631317966617644
batch 90 loss: 0.0005630973493680358
batch 95 loss: 0.0005632015876471996
batch 100 loss: 0.0005630690488032996
batch 105 loss: 0.0005630585132166743
batch 110 loss: 0.0005630892585031689
batch 115 loss: 0.0005629997933283449
batch 120 loss: 0.0005630529136396945
batch 125 loss: 0.0005630504339933396
batch 130 loss: 0.0005629630875773727
batch 135 loss: 0.000562993180938065
batch 140 loss: 0.0005628886283375323
batch 145 loss: 0.0005630245665088296
batch 150 loss: 0.0005630887462757528
batch 155 loss: 0.0005629969411529601
batch 160 loss: 0.0005628131562843919
batch 165 loss: 0.0005628278478980065
batch 170 loss: 0.000562806858215481
batch 175 loss: 0.0005627914913929998
batch 180 loss: 0.0005627198144793511
batch 185 loss: 0.0005627350183203816
batch 190 loss: 0.0005627621081657708
batch 195 loss: 0.0005626763100735843
batch 200 loss: 0.0005626491387374699
batch 205 loss: 0.0005626861937344074
batch 210 loss: 0.0005626823171041906
batch 215 loss: 0.000562710640951991
batch 220 loss: 0.0005626039346680045
batch 225 loss: 0.0005625442601740361
batch 230 loss: 0.0005625911639072001
batch 235 loss: 0.0005625737481750548
batch 240 loss: 0.000562581152189523
Training Loss: 0.0005630267856759019
Validation Loss: 0.0005621085758320988
Epoch 5:
batch 5 loss: 0.0005626491387374699
batch 10 loss: 0.0005624456098303199
batch 15 loss: 0.0005625722580589354
batch 20 loss: 0.0005625293008051813
batch 25 loss: 0.0005626266589388252
batch 30 loss: 0.0005623637931421399
batch 35 loss: 0.0005626588594168424
batch 40 loss: 0.0005622355150990188
batch 45 loss: 0.0005622337106615305
batch 50 loss: 0.0005624907673336566
batch 55 loss: 0.00056237883400172
batch 60 loss: 0.0005624504992738366
batch 65 loss: 0.0005624218261800707
batch 70 loss: 0.0005625089979730546
batch 75 loss: 0.0005622685886919499
batch 80 loss: 0.0005624225712381303
batch 85 loss: 0.000562297459691763
batch 90 loss: 0.0005622224300168455
batch 95 loss: 0.0005622136290185153
batch 100 loss: 0.0005623134085908533
batch 105 loss: 0.0005624185083433986
batch 110 loss: 0.0005622294615022838
batch 115 loss: 0.000562393816653639
batch 120 loss: 0.0017850781325250864
batch 125 loss: 0.002032700995914638
batch 130 loss: 0.001354978745803237
batch 135 loss: 0.0011076432187110186
batch 140 loss: 0.0009738973574712872
batch 145 loss: 0.0008702327264472842
batch 150 loss: 0.0007687753764912486
batch 155 loss: 0.0006972509203478694
batch 160 loss: 0.0006604681024327874
batch 165 loss: 0.0007114085718058049
batch 170 loss: 0.0007215131423436105
batch 175 loss: 0.0006291702506132424
batch 180 loss: 0.0006194594549015165
batch 185 loss: 0.0006161344121210277
batch 190 loss: 0.0006141834077425301
batch 195 loss: 0.0006099552265368402
batch 200 loss: 0.0006032328121364117
batch 205 loss: 0.0005974423256702721
batch 210 loss: 0.000594878604169935
batch 215 loss: 0.0005925871315412224
batch 220 loss: 0.0005883455974981189
batch 225 loss: 0.0005839415942318737
batch 230 loss: 0.0005838551907800138
batch 235 loss: 0.0005837630131281912
batch 240 loss: 0.0005837873788550497
Training Loss: 0.0006879172777795854
Validation Loss: 0.0005797171606294189
Epoch 6:
batch 5 loss: 0.0005836182506754994
batch 10 loss: 0.0005835542688146234
batch 15 loss: 0.0005834110081195832
batch 20 loss: 0.0005834068520925939
batch 25 loss: 0.000583223276771605
batch 30 loss: 0.0005832226481288672
batch 35 loss: 0.0005830997251905501
batch 40 loss: 0.000583089143037796
batch 45 loss: 0.0005830610636621714
batch 50 loss: 0.0005828791297972202
batch 55 loss: 0.0005826968466863036
batch 60 loss: 0.0005827777204103768
batch 65 loss: 0.0005824777297675609
batch 70 loss: 0.0005824067513458431
batch 75 loss: 0.0005823709769174456
batch 80 loss: 0.0005823865300044418
batch 85 loss: 0.0005821936530992389
batch 90 loss: 0.0005819977261126042
batch 95 loss: 0.0005819805199280381
batch 100 loss: 0.0005819075740873814
batch 105 loss: 0.0005815994809381664
batch 110 loss: 0.0005817273166030645
batch 115 loss: 0.0005815075826831162
batch 120 loss: 0.0005813647410832345
batch 125 loss: 0.0005813675117678941
batch 130 loss: 0.0005809834226965904
batch 135 loss: 0.0005809788010083139
batch 140 loss: 0.0005809278460219503
batch 145 loss: 0.0005807462730444968
batch 150 loss: 0.0005805652821436524
batch 155 loss: 0.000580422196071595
batch 160 loss: 0.0005802263156510889
batch 165 loss: 0.0005801239749416709
batch 170 loss: 0.0005800091312266886
batch 175 loss: 0.0005798271507956087
batch 180 loss: 0.0005796164507046342
batch 185 loss: 0.0005794591270387173
batch 190 loss: 0.0005790875642560422
batch 195 loss: 0.0005789225804619491
batch 200 loss: 0.0005787807400338352
batch 205 loss: 0.000578480027616024
batch 210 loss: 0.00057846320560202
batch 215 loss: 0.0005780605133622885
batch 220 loss: 0.0005779312690719962
batch 225 loss: 0.000577786355279386
batch 230 loss: 0.0005775251192972064
batch 235 loss: 0.0005772800068370998
batch 240 loss: 0.000577283138409257
Training Loss: 0.000580975302485361
Validation Loss: 0.0005757029226515442
Epoch 7:
batch 5 loss: 0.0005770827061496675
batch 10 loss: 0.0005768547183834016
batch 15 loss: 0.0005768023547716439
batch 20 loss: 0.0005765189067460597
batch 25 loss: 0.0005763191962614656
batch 30 loss: 0.0005761364940553903
batch 35 loss: 0.0005759928375482559
batch 40 loss: 0.0005757913691923022
batch 45 loss: 0.0005755069199949503
batch 50 loss: 0.0005750998272560537
batch 55 loss: 0.000575030199252069
batch 60 loss: 0.00057468106970191
batch 65 loss: 0.0005743217887356877
batch 70 loss: 0.0005740770837292075
batch 75 loss: 0.000573932658880949
batch 80 loss: 0.0005735983955673874
batch 85 loss: 0.0005735728656873107
batch 90 loss: 0.000573277601506561
batch 95 loss: 0.0005731097189709544
batch 100 loss: 0.0005729359108954668
batch 105 loss: 0.0005727918702177704
batch 110 loss: 0.0005725324503146112
batch 115 loss: 0.000572302145883441
batch 120 loss: 0.0005721676861867309
batch 125 loss: 0.0005718714208342135
batch 130 loss: 0.0005717892665416003
batch 135 loss: 0.0005713503109291196
batch 140 loss: 0.0005710471421480178
batch 145 loss: 0.0005707880714908242
batch 150 loss: 0.0005707881762646138
batch 155 loss: 0.0005700849113054573
batch 160 loss: 0.000570085016079247
batch 165 loss: 0.0005696249892935157
batch 170 loss: 0.0005694693885743618
batch 175 loss: 0.0005693110055290162
batch 180 loss: 0.0005691472091712057
batch 185 loss: 0.0005688324919901789
batch 190 loss: 0.0005688877776265144
batch 195 loss: 0.0005684440373443067
batch 200 loss: 0.0005685401614755392
batch 205 loss: 0.0005686933291144669
batch 210 loss: 0.0005684941657818854
batch 215 loss: 0.0005682957242242991
batch 220 loss: 0.0005682563758455216
batch 225 loss: 0.0005680149770341814
batch 230 loss: 0.0005680182948708534
batch 235 loss: 0.0005675276857800782
batch 240 loss: 0.0005675075692124665
Training Loss: 0.0005719855473823069
Validation Loss: 0.0005661992977062861
Epoch 8:
batch 5 loss: 0.0005673122126609087
batch 10 loss: 0.000567033200059086
batch 15 loss: 0.0005666442681103945
batch 20 loss: 0.0005665812990628183
batch 25 loss: 0.0005653680069372058
batch 30 loss: 0.0005647470825351775
batch 35 loss: 0.0005643785698339343
batch 40 loss: 0.0005638599279336632
batch 45 loss: 0.0005641760304570198
batch 50 loss: 0.0005639537353999913
batch 55 loss: 0.0005638314643874765
batch 60 loss: 0.0005635849782265722
batch 65 loss: 0.0005635896814055741
batch 70 loss: 0.0005635666311718524
batch 75 loss: 0.0005634080735035241
batch 80 loss: 0.0005633284221403301
batch 85 loss: 0.0005634568398818374
batch 90 loss: 0.0005632976535707712
batch 95 loss: 0.0005632403306663036
batch 100 loss: 0.00056334970286116
batch 105 loss: 0.0005632933345623315
batch 110 loss: 0.0005633504944853485
batch 115 loss: 0.0005631614243611693
batch 120 loss: 0.0005632197484374046
batch 125 loss: 0.0005632313783280552
batch 130 loss: 0.0005631330772303045
batch 135 loss: 0.0005631159525364638
batch 140 loss: 0.0005630556144751609
batch 145 loss: 0.0005630527739413083
batch 150 loss: 0.0005632157204672694
batch 155 loss: 0.0005633204476907849
batch 160 loss: 0.0005628907005302608
batch 165 loss: 0.0005633168737404048
batch 170 loss: 0.0005634616245515645
batch 175 loss: 0.0005632572458125651
batch 180 loss: 0.0005635917186737061
batch 185 loss: 0.0005634539062157274
batch 190 loss: 0.0005640892428345978
batch 195 loss: 0.0005633728927932679
batch 200 loss: 0.0005640138522721827
batch 205 loss: 0.0005634017754346133
batch 210 loss: 0.000564076832961291
batch 215 loss: 0.0005636107409372926
batch 220 loss: 0.0005628766142763197
batch 225 loss: 0.0005633056513033807
batch 230 loss: 0.0005635619279928505
batch 235 loss: 0.0005636293673887849
batch 240 loss: 0.0005633266060613096
Training Loss: 0.0005638144927312775
Validation Loss: 0.0005627176743776848
Epoch 9:
batch 5 loss: 0.0005634423578158021
batch 10 loss: 0.0005636520916596055
batch 15 loss: 0.0005638847942464054
batch 20 loss: 0.0005632274318486452
batch 25 loss: 0.0005629180348478257
batch 30 loss: 0.0005634366651065647
batch 35 loss: 0.0005632208194583654
batch 40 loss: 0.0005629883147776127
batch 45 loss: 0.0005634218454360962
batch 50 loss: 0.000563425524160266
batch 55 loss: 0.0005638003698550164
batch 60 loss: 0.0005645369063131511
batch 65 loss: 0.0005650172592140734
batch 70 loss: 0.0005640044575557113
batch 75 loss: 0.0005638189963065088
batch 80 loss: 0.0005628742277622223
batch 85 loss: 0.0005627577775157988
batch 90 loss: 0.0005624541779980063
batch 95 loss: 0.0005626526195555926
batch 100 loss: 0.0005628447863273323
batch 105 loss: 0.0005631702370010316
batch 110 loss: 0.0005627399194054305
batch 115 loss: 0.0005631256266497075
batch 120 loss: 0.0005628284183330834
batch 125 loss: 0.0005633134045638144
batch 130 loss: 0.0005635536974295974
batch 135 loss: 0.0005634170025587082
batch 140 loss: 0.0005628156126476824
batch 145 loss: 0.0005622766097076237
batch 150 loss: 0.0005622951895929873
batch 155 loss: 0.0005622602882795035
batch 160 loss: 0.000561966048553586
batch 165 loss: 0.0005623705685138703
batch 170 loss: 0.0005630924366414547
batch 175 loss: 0.0005633717402815819
batch 180 loss: 0.0005635255598463118
batch 185 loss: 0.0005633568158373237
batch 190 loss: 0.0005627735401503742
batch 195 loss: 0.000562906707637012
batch 200 loss: 0.0005629878141917288
batch 205 loss: 0.0005638905684463679
batch 210 loss: 0.0005627805483527481
batch 215 loss: 0.0005622164811939001
batch 220 loss: 0.0005623189848847687
batch 225 loss: 0.0005627801176160574
batch 230 loss: 0.0005629134248010814
batch 235 loss: 0.0005629964056424796
batch 240 loss: 0.0005636061541736126
Training Loss: 0.0005631256120977923
Validation Loss: 0.0005625670203395809
Epoch 10:
batch 5 loss: 0.00056273911613971
batch 10 loss: 0.000562702864408493
batch 15 loss: 0.0005626189755275846
batch 20 loss: 0.0005626398604363203
batch 25 loss: 0.0005621821619570255
batch 30 loss: 0.0005619615665636956
batch 35 loss: 0.0005625448888167739
batch 40 loss: 0.0005627225618809462
batch 45 loss: 0.0005621701711788774
batch 50 loss: 0.0005620613577775657
batch 55 loss: 0.0005615479312837124
batch 60 loss: 0.0005613185930997133
batch 65 loss: 0.000562341557815671
batch 70 loss: 0.0005623166565783322
batch 75 loss: 0.0005622489843517542
batch 80 loss: 0.0005619664327241481
batch 85 loss: 0.0005617084680125117
batch 90 loss: 0.0005617594113573432
batch 95 loss: 0.0005619868985377252
batch 100 loss: 0.0005619725910946727
batch 105 loss: 0.0005621831049211323
batch 110 loss: 0.0005624451441690326
batch 115 loss: 0.000563698320183903
batch 120 loss: 0.0005632199347019196
batch 125 loss: 0.0005628412240184843
batch 130 loss: 0.0005630627274513245
batch 135 loss: 0.0005627815378829836
batch 140 loss: 0.0005622047930955887
batch 145 loss: 0.0005617306334897876
batch 150 loss: 0.0005616430193185806
batch 155 loss: 0.0005610372987575829
batch 160 loss: 0.0005622364929877222
batch 165 loss: 0.0005619705305434763
batch 170 loss: 0.0005616975831799209
batch 175 loss: 0.0005618885508738459
batch 180 loss: 0.0005620173877105116
batch 185 loss: 0.0005625108256936074
batch 190 loss: 0.0005621707765385509
batch 195 loss: 0.0005617099814116955
batch 200 loss: 0.0005622053635306656
batch 205 loss: 0.0005616403534077108
batch 210 loss: 0.0005616031354293227
batch 215 loss: 0.0005612313048914075
batch 220 loss: 0.0005618212977424264
batch 225 loss: 0.0005617291433736682
batch 230 loss: 0.0005612924927845597
batch 235 loss: 0.0005614705733023584
batch 240 loss: 0.0005622801138088108
Training Loss: 0.0005621215561404825
Validation Loss: 0.000561457064274388
Epoch 11:
batch 5 loss: 0.000563617201987654
batch 10 loss: 0.0005635202745907008
batch 15 loss: 0.0005634700995869934
batch 20 loss: 0.0005633734283037484
batch 25 loss: 0.0005628892453387379
batch 30 loss: 0.0005631924024783075
batch 35 loss: 0.0005633894237689674
batch 40 loss: 0.0005628930637612938
batch 45 loss: 0.0005628455546684563
batch 50 loss: 0.0005623103934340179
batch 55 loss: 0.0005619734525680542
batch 60 loss: 0.00056170147145167
batch 65 loss: 0.0005620774696581066
batch 70 loss: 0.0005618664785288274
batch 75 loss: 0.0005616147886030376
batch 80 loss: 0.0005615034606307745
batch 85 loss: 0.000561560585629195
batch 90 loss: 0.0005622189724817872
batch 95 loss: 0.0005619133240543306
batch 100 loss: 0.0005618612165562808
batch 105 loss: 0.00056192526826635
batch 110 loss: 0.0005617631250061095
batch 115 loss: 0.0005619331728667021
batch 120 loss: 0.0005620856769382954
batch 125 loss: 0.0005623740726150572
batch 130 loss: 0.0005624476121738553
batch 135 loss: 0.0005621666554361582
batch 140 loss: 0.0005626995698548853
batch 145 loss: 0.0005623271805234254
batch 150 loss: 0.0005622348980978131
batch 155 loss: 0.0005622406373731792
batch 160 loss: 0.0005620997748337686
batch 165 loss: 0.0005620165378786623
batch 170 loss: 0.0005625115940347314
batch 175 loss: 0.0005629559862427413
batch 180 loss: 0.0005628523416817188
batch 185 loss: 0.0005627162638120354
batch 190 loss: 0.000562350603286177
batch 195 loss: 0.0005619584931991995
batch 200 loss: 0.0005617744405753911
batch 205 loss: 0.0005618892493657768
batch 210 loss: 0.0005616737762466073
batch 215 loss: 0.0005614744732156396
batch 220 loss: 0.0005615894217044116
batch 225 loss: 0.0005614408059045672
batch 230 loss: 0.0005617536837235094
batch 235 loss: 0.0005613686284050345
batch 240 loss: 0.0005614867084659636
Training Loss: 0.0005622479783293481
Validation Loss: 0.0005615328816929832
Epoch 12:
batch 5 loss: 0.0005624261335469783
batch 10 loss: 0.0005616914480924606
batch 15 loss: 0.0005623318953439594
batch 20 loss: 0.0005621821386739611
batch 25 loss: 0.000562598486430943
batch 30 loss: 0.0005619351170025766
batch 35 loss: 0.0005621025571599603
batch 40 loss: 0.0005623867036774755
batch 45 loss: 0.0005627476493827999
batch 50 loss: 0.0005627500475384295
batch 55 loss: 0.0005622030701488256
batch 60 loss: 0.000562298612203449
batch 65 loss: 0.0005626050638966263
batch 70 loss: 0.000562746764626354
batch 75 loss: 0.00056275415699929
batch 80 loss: 0.0005628969054669141
batch 85 loss: 0.0005629700724966824
batch 90 loss: 0.0005635100533254445
batch 95 loss: 0.0005627332488074899
batch 100 loss: 0.0005624315585009753
batch 105 loss: 0.0005620690528303385
batch 110 loss: 0.0005632584099657834
batch 115 loss: 0.0005632758606225252
batch 120 loss: 0.0005637633497826755
batch 125 loss: 0.0005633395165205001
batch 130 loss: 0.0005632003769278527
batch 135 loss: 0.0005633724969811738
batch 140 loss: 0.0005621922085992992
batch 145 loss: 0.0005617513321340084
batch 150 loss: 0.0005617302260361612
batch 155 loss: 0.0005617613904178142
batch 160 loss: 0.0005615220870822668
batch 165 loss: 0.0005613700137473643
batch 170 loss: 0.0005616888985969127
batch 175 loss: 0.0005617422517389059
batch 180 loss: 0.000561960507184267
batch 185 loss: 0.0005616523558273911
batch 190 loss: 0.0005617958609946072
batch 195 loss: 0.0005619394942186773
batch 200 loss: 0.0005617561168037355
batch 205 loss: 0.0005611599772237241
batch 210 loss: 0.0005618259077891707
batch 215 loss: 0.0005619244766421616
batch 220 loss: 0.0005614779889583588
batch 225 loss: 0.0005613493616692722
batch 230 loss: 0.000561526243109256
batch 235 loss: 0.0005611391505226492
batch 240 loss: 0.0005613352521322667
Training Loss: 0.0005622329551745982
Validation Loss: 0.0005609789436372618
Epoch 13:
batch 5 loss: 0.0005615290021523834
batch 10 loss: 0.0005615965579636395
batch 15 loss: 0.0005618033581413328
batch 20 loss: 0.0005621564574539662
batch 25 loss: 0.0005617872695438564
batch 30 loss: 0.0005619129980914295
batch 35 loss: 0.0005619708448648453
batch 40 loss: 0.0005616579088382423
batch 45 loss: 0.0005619495408609509
batch 50 loss: 0.0005618246621452272
batch 55 loss: 0.0005617386195808649
batch 60 loss: 0.0005614352296106518
batch 65 loss: 0.0005619912291876972
batch 70 loss: 0.0005611828644759953
batch 75 loss: 0.0005611017579212785
batch 80 loss: 0.0005618347320705652
batch 85 loss: 0.0005646049859933556
batch 90 loss: 0.0005639010574668646
batch 95 loss: 0.0005630391300655901
batch 100 loss: 0.0005628223880194128
batch 105 loss: 0.0005622792523354292
batch 110 loss: 0.0005616976763121784
batch 115 loss: 0.0005622377968393267
batch 120 loss: 0.0005632019601762295
batch 125 loss: 0.0005645368946716189
batch 130 loss: 0.0005630748579278589
batch 135 loss: 0.000562265224289149
batch 140 loss: 0.0005626099067740142
batch 145 loss: 0.0005626754253171384
batch 150 loss: 0.0005616642301902175
batch 155 loss: 0.0005616808426566422
batch 160 loss: 0.0005617586662992835
batch 165 loss: 0.0005617170012556016
batch 170 loss: 0.000561883533373475
batch 175 loss: 0.0005617832532152534
batch 180 loss: 0.000561206869315356
batch 185 loss: 0.0005616930546239018
batch 190 loss: 0.0005614607594907284
batch 195 loss: 0.0005611161468550563
batch 200 loss: 0.0005611257394775749
batch 205 loss: 0.0005615747417323291
batch 210 loss: 0.0005614663125015796
batch 215 loss: 0.0005612677778117359
batch 220 loss: 0.0005609195563010871
batch 225 loss: 0.0005611724685877561
batch 230 loss: 0.0005611267872154713
batch 235 loss: 0.0005616654409095645
batch 240 loss: 0.000561721611302346
Training Loss: 0.0005619671746292927
Validation Loss: 0.000561243782673652
Epoch 14:
batch 5 loss: 0.0005613605375401675
batch 10 loss: 0.00056113894097507
batch 15 loss: 0.0005611947271972895
batch 20 loss: 0.0005611257045529782
batch 25 loss: 0.0005608665756881237
batch 30 loss: 0.0005609846673905849
batch 35 loss: 0.0005610839813016355
batch 40 loss: 0.0005612807930447161
batch 45 loss: 0.0005613284534774721
batch 50 loss: 0.0005611786502413452
batch 55 loss: 0.0005611817352473736
batch 60 loss: 0.0005616214708425105
batch 65 loss: 0.0005619747214950621
batch 70 loss: 0.0005612861015833915
batch 75 loss: 0.0005611956818029285
batch 80 loss: 0.0005609748768620193
batch 85 loss: 0.0005610056221485138
batch 90 loss: 0.0005608671344816685
batch 95 loss: 0.000561083119828254
batch 100 loss: 0.0005607705563306809
batch 105 loss: 0.0005606581224128604
batch 110 loss: 0.0005611144937574863
batch 115 loss: 0.000560854107607156
batch 120 loss: 0.000560723536182195
batch 125 loss: 0.0005611171713098883
batch 130 loss: 0.0005611756816506386
batch 135 loss: 0.0005610835505649447
batch 140 loss: 0.0005609719082713127
batch 145 loss: 0.0005608057021163404
batch 150 loss: 0.0005609875777736306
batch 155 loss: 0.0007984154974110425
batch 160 loss: 0.0010548989870585502
batch 165 loss: 0.0007559122517704963
batch 170 loss: 0.0006867194315418601
batch 175 loss: 0.0006693136761896312
batch 180 loss: 0.0006532898056320846
batch 185 loss: 0.000644810835365206
batch 190 loss: 0.0006368139060214162
batch 195 loss: 0.0006315478472970426
batch 200 loss: 0.0006264527793973685
batch 205 loss: 0.0006217128364369273
batch 210 loss: 0.0006178069277666509
batch 215 loss: 0.000615328038111329
batch 220 loss: 0.0006121844402514398
batch 225 loss: 0.000609014369547367
batch 230 loss: 0.0006056455662474036
batch 235 loss: 0.0006017451523803174
batch 240 loss: 0.0005981202819384634
Training Loss: 0.000601515177792559
Validation Loss: 0.000592866888230977
Epoch 15:
batch 5 loss: 0.0005960431299172342
batch 10 loss: 0.000593773031141609
batch 15 loss: 0.0005918301525525749
batch 20 loss: 0.0005899892072193325
batch 25 loss: 0.000588036421686411
batch 30 loss: 0.0005864619859494268
batch 35 loss: 0.0005850181332789361
batch 40 loss: 0.000583330588415265
batch 45 loss: 0.0005818508914671838
batch 50 loss: 0.0005806569126434625
batch 55 loss: 0.0005797620862722397
batch 60 loss: 0.0005787295172922313
batch 65 loss: 0.0005776565172709525
batch 70 loss: 0.0005765335517935455
batch 75 loss: 0.0005755722057074308
batch 80 loss: 0.0005742361536249518
batch 85 loss: 0.0005734523991122841
batch 90 loss: 0.0005729010212235153
batch 95 loss: 0.000572362751699984
batch 100 loss: 0.0005720710731111466
batch 105 loss: 0.0005716146435588599
batch 110 loss: 0.000571215630043298
batch 115 loss: 0.0005710147903300822
batch 120 loss: 0.0005708635086193681
batch 125 loss: 0.0005704899551346899
batch 130 loss: 0.0005703787552192807
batch 135 loss: 0.0005700400914065539
batch 140 loss: 0.0005698100663721562
batch 145 loss: 0.000569798843935132
batch 150 loss: 0.0005696169449947774
batch 155 loss: 0.0005696123349480331
batch 160 loss: 0.0005691501311957837
batch 165 loss: 0.0005688199424184858
batch 170 loss: 0.0005685424548573792
batch 175 loss: 0.0005683428724296391
batch 180 loss: 0.0005680944886989892
batch 185 loss: 0.0005676749977283179
batch 190 loss: 0.0005675568012520671
batch 195 loss: 0.0005670454353094101
batch 200 loss: 0.0005670195678249002
batch 205 loss: 0.0005665697040967643
batch 210 loss: 0.0005664650350809097
batch 215 loss: 0.0005663549061864614
batch 220 loss: 0.0005661894218064844
batch 225 loss: 0.0005661200382746756
batch 230 loss: 0.00056586058344692
batch 235 loss: 0.0005657683941535652
batch 240 loss: 0.0005656460765749216
Training Loss: 0.0005738738364016172
Validation Loss: 0.0005646480101859197
Epoch 16:
batch 5 loss: 0.0005653198692016304
batch 10 loss: 0.000565450347494334
batch 15 loss: 0.0005653161206282675
batch 20 loss: 0.0005649425904266537
batch 25 loss: 0.0005649148370139301
batch 30 loss: 0.0005649427534081042
batch 35 loss: 0.0005646552657708525
batch 40 loss: 0.0005649060709401965
batch 45 loss: 0.0005645738914608955
batch 50 loss: 0.0005645420984365046
batch 55 loss: 0.0005644901539199054
batch 60 loss: 0.0005644253687933087
batch 65 loss: 0.0005641647265292704
batch 70 loss: 0.0005643994780257344
batch 75 loss: 0.000564002152532339
batch 80 loss: 0.0005639770650304853
batch 85 loss: 0.0005640476243570447
batch 90 loss: 0.0005640157265588641
batch 95 loss: 0.0005641980213113129
batch 100 loss: 0.0005639433627948165
batch 105 loss: 0.0005640144110657275
batch 110 loss: 0.000563794793561101
batch 115 loss: 0.0005637712893076241
batch 120 loss: 0.00056368054356426
batch 125 loss: 0.0005635281559079885
batch 130 loss: 0.0005636033951304853
batch 135 loss: 0.0005635182373225689
batch 140 loss: 0.0005634384811855853
batch 145 loss: 0.0005634568398818374
batch 150 loss: 0.0005633916240185499
batch 155 loss: 0.0005634297034703195
batch 160 loss: 0.0005632474785670638
batch 165 loss: 0.0005632671643979847
batch 170 loss: 0.0005633119610138238
batch 175 loss: 0.0005632029264234006
batch 180 loss: 0.0005632246960885823
batch 185 loss: 0.0005630753934383392
batch 190 loss: 0.0005632309475913644
batch 195 loss: 0.0005632238811813295
batch 200 loss: 0.0005630419938825071
batch 205 loss: 0.0005630127619951963
batch 210 loss: 0.0005629479419440031
batch 215 loss: 0.000562993437051773
batch 220 loss: 0.0005638369708321989
batch 225 loss: 0.0005631080246530473
batch 230 loss: 0.0005629145074635744
batch 235 loss: 0.0005634169443510473
batch 240 loss: 0.000563229201361537
Training Loss: 0.0005638571089851515
Validation Loss: 0.0005627372428231562
Epoch 17:
batch 5 loss: 0.0005625832942314446
batch 10 loss: 0.0005625280202366412
batch 15 loss: 0.0005622333032079041
batch 20 loss: 0.000562181300483644
batch 25 loss: 0.0005623058998025954
batch 30 loss: 0.0005621657473966479
batch 35 loss: 0.0005624052137136459
batch 40 loss: 0.0005624524084851146
batch 45 loss: 0.0005624272860586643
batch 50 loss: 0.000562179065309465
batch 55 loss: 0.0005625106976367533
batch 60 loss: 0.0005621866323053837
batch 65 loss: 0.0005627035745419562
batch 70 loss: 0.0005626041092909873
batch 75 loss: 0.0005625695106573403
batch 80 loss: 0.0005622200784273446
batch 85 loss: 0.0005619544070214033
batch 90 loss: 0.000562012696173042
batch 95 loss: 0.0005620756652206183
batch 100 loss: 0.0005628749961033464
batch 105 loss: 0.00056398652959615
batch 110 loss: 0.0005658504320308566
batch 115 loss: 0.0005657048663124442
batch 120 loss: 0.0005656221765093505
batch 125 loss: 0.0005652718828059733
batch 130 loss: 0.0005646764882840216
batch 135 loss: 0.0005638499045744538
batch 140 loss: 0.0005627373582683503
batch 145 loss: 0.0005618657683953643
batch 150 loss: 0.0005620220443233847
batch 155 loss: 0.0005622590426355601
batch 160 loss: 0.0005617318791337311
batch 165 loss: 0.000561869761440903
batch 170 loss: 0.0005616384209133684
batch 175 loss: 0.0005619862931780517
batch 180 loss: 0.0005619060597382486
batch 185 loss: 0.0005615045200102031
batch 190 loss: 0.0005618272931315005
batch 195 loss: 0.0005617600050754845
batch 200 loss: 0.0005612356588244438
batch 205 loss: 0.0005615621572360397
batch 210 loss: 0.0005617008893750608
batch 215 loss: 0.0005616948008537292
batch 220 loss: 0.000561396824195981
batch 225 loss: 0.0005614101071842014
batch 230 loss: 0.0005615273141302169
batch 235 loss: 0.0005619175266474485
batch 240 loss: 0.0005618725786916912
Training Loss: 0.0005624908852041699
Validation Loss: 0.0005615778800953801
Epoch 18:
batch 5 loss: 0.0005617080489173532
batch 10 loss: 0.0005616245092824101
batch 15 loss: 0.0005617607734166086
batch 20 loss: 0.0005615313653834164
batch 25 loss: 0.0005612220847979188
batch 30 loss: 0.0005610985914245248
batch 35 loss: 0.0005614370573312044
batch 40 loss: 0.0005613199318759143
batch 45 loss: 0.0005612387554720044
batch 50 loss: 0.0005614352994598448
batch 55 loss: 0.0005614094901829958
batch 60 loss: 0.000561430724337697
batch 65 loss: 0.0005612873239442707
batch 70 loss: 0.0005615168833173811
batch 75 loss: 0.0005614672438241542
batch 80 loss: 0.0005613102694042027
batch 85 loss: 0.0005613531917333603
batch 90 loss: 0.0005613094312138855
batch 95 loss: 0.0005612198146991432
batch 100 loss: 0.0005610222578980029
batch 105 loss: 0.0005610274733044207
batch 110 loss: 0.0005609705345705152
batch 115 loss: 0.0005610603955574333
batch 120 loss: 0.0005617632181383669
batch 125 loss: 0.0005614789319224655
batch 130 loss: 0.0005610477644950151
batch 135 loss: 0.0005608591367490589
batch 140 loss: 0.0005610153311863542
batch 145 loss: 0.0005610905471257865
batch 150 loss: 0.0005614516325294971
batch 155 loss: 0.000561709247995168
batch 160 loss: 0.0005612355656921863
batch 165 loss: 0.0005615768721327185
batch 170 loss: 0.0005616742069832981
batch 175 loss: 0.0005614171386696398
batch 180 loss: 0.0005614369292743504
batch 185 loss: 0.0005613955319859088
batch 190 loss: 0.0005616619135253131
batch 195 loss: 0.0005614880006760359
batch 200 loss: 0.0005615902482531965
batch 205 loss: 0.0005616623093374074
batch 210 loss: 0.0005614409339614213
batch 215 loss: 0.0005613849032670259
batch 220 loss: 0.0005613590707071126
batch 225 loss: 0.0005611347733065486
batch 230 loss: 0.0005613860907033086
batch 235 loss: 0.0005613059271126986
batch 240 loss: 0.0005611947854049504
Training Loss: 0.0005613650513017395
Validation Loss: 0.0005607673403574154
Epoch 19:
batch 5 loss: 0.000561410270165652
batch 10 loss: 0.000561215914785862
batch 15 loss: 0.0005612825159914791
batch 20 loss: 0.000561205402482301
batch 25 loss: 0.0005613895133137703
batch 30 loss: 0.0005613340996205807
batch 35 loss: 0.0005614324123598635
batch 40 loss: 0.0005612457287497818
batch 45 loss: 0.0005611136904917658
batch 50 loss: 0.0005613023764453829
batch 55 loss: 0.0005612916429527104
batch 60 loss: 0.0005616881651803851
batch 65 loss: 0.0005614047986455262
batch 70 loss: 0.0005611077416688204
batch 75 loss: 0.000561222352553159
batch 80 loss: 0.0005611477536149323
batch 85 loss: 0.0005611394182778895
batch 90 loss: 0.0005614788853563369
batch 95 loss: 0.000562243047170341
batch 100 loss: 0.000562427076511085
batch 105 loss: 0.0005623932811431587
batch 110 loss: 0.0005627891048789024
batch 115 loss: 0.0005626340047456324
batch 120 loss: 0.0005626021418720483
batch 125 loss: 0.0005617303191684187
batch 130 loss: 0.0005612489767372608
batch 135 loss: 0.0005611795000731945
batch 140 loss: 0.0005609756917692721
batch 145 loss: 0.0005610927590169012
batch 150 loss: 0.0005611976026557386
batch 155 loss: 0.0005610483116470277
batch 160 loss: 0.0005611200002022087
batch 165 loss: 0.0005610193242318928
batch 170 loss: 0.0005608257721178234
batch 175 loss: 0.0005608760286122561
batch 180 loss: 0.0005610857391729951
batch 185 loss: 0.0005608993582427501
batch 190 loss: 0.0005610779509879649
batch 195 loss: 0.0005612712237052619
batch 200 loss: 0.0005614738212898374
batch 205 loss: 0.0005626828991807997
batch 210 loss: 0.0005749529227614403
batch 215 loss: 0.0005814624251797796
batch 220 loss: 0.0005734230158850551
batch 225 loss: 0.0005697850603610278
batch 230 loss: 0.0005658623878844083
batch 235 loss: 0.0005643692100420594
batch 240 loss: 0.0005631038919091224
Training Loss: 0.0005627555319127471
Validation Loss: 0.0005618967979292695
Epoch 20:
batch 5 loss: 0.0005619962466880679
batch 10 loss: 0.0005614459048956632
batch 15 loss: 0.0005608530598692596
batch 20 loss: 0.0005606781924143434
batch 25 loss: 0.0005605884245596827
batch 30 loss: 0.0005604715319350362
batch 35 loss: 0.0005604126723483205
batch 40 loss: 0.0005606346065178513
batch 45 loss: 0.0005607852479442954
batch 50 loss: 0.0005609026062302292
batch 55 loss: 0.0005608513369224965
batch 60 loss: 0.0005610552383586764
batch 65 loss: 0.0005608015228062869
batch 70 loss: 0.0005606634309515357
batch 75 loss: 0.0005605898913927376
batch 80 loss: 0.000560794782359153
batch 85 loss: 0.0005607423605397344
batch 90 loss: 0.0005608361447229981
batch 95 loss: 0.0005606190650723874
batch 100 loss: 0.0005606417777016759
batch 105 loss: 0.0005607006838545203
batch 110 loss: 0.0005606014980003238
batch 115 loss: 0.0005604955716989934
batch 120 loss: 0.0005603203899227083
batch 125 loss: 0.0005603728350251913
batch 130 loss: 0.0005603326600976289
batch 135 loss: 0.0005606458289548755
batch 140 loss: 0.0005608445848338306
batch 145 loss: 0.0005606814520433545
batch 150 loss: 0.0005608742823824287
batch 155 loss: 0.0005608801962807775
batch 160 loss: 0.0005605887854471803
batch 165 loss: 0.0005607276456430554
batch 170 loss: 0.0005605587502941489
batch 175 loss: 0.0005604543373920023
batch 180 loss: 0.0005605751764960587
batch 185 loss: 0.0005604800535365939
batch 190 loss: 0.0005605725105851889
batch 195 loss: 0.0005607429309748113
batch 200 loss: 0.0005607219529338181
batch 205 loss: 0.0005607999279163778
batch 210 loss: 0.0005608285660855472
batch 215 loss: 0.0005607061670161783
batch 220 loss: 0.0005606652703136206
batch 225 loss: 0.0005610040738247335
batch 230 loss: 0.0005606597289443016
batch 235 loss: 0.000560679379850626
batch 240 loss: 0.0005605142912827432
Training Loss: 0.0005607165328304594
Validation Loss: 0.0005603505647741258
Epoch 21:
batch 5 loss: 0.0005606004386208951
batch 10 loss: 0.0005605880636721849
batch 15 loss: 0.0005605107173323632
batch 20 loss: 0.0005606253631412983
batch 25 loss: 0.0005608260282315314
batch 30 loss: 0.0005606357241049409
batch 35 loss: 0.0005607427563518285
batch 40 loss: 0.0005606457125395536
batch 45 loss: 0.0005608420236967504
batch 50 loss: 0.0005607595550827682
batch 55 loss: 0.0005603026365861297
batch 60 loss: 0.0005604220787063241
batch 65 loss: 0.000560483371373266
batch 70 loss: 0.0005608824430964887
batch 75 loss: 0.0005611772183328867
batch 80 loss: 0.0005611652857623994
batch 85 loss: 0.0005607404862530529
batch 90 loss: 0.0005607043043710291
batch 95 loss: 0.0005607096129097045
batch 100 loss: 0.0005606470047496259
batch 105 loss: 0.0005603950936347246
batch 110 loss: 0.0005605957936495543
batch 115 loss: 0.0005604242207482458
batch 120 loss: 0.0005605824757367372
batch 125 loss: 0.0005604591802693904
batch 130 loss: 0.0005605559796094895
batch 135 loss: 0.0005608682637102902
batch 140 loss: 0.0005610526422969997
batch 145 loss: 0.0005619359202682972
batch 150 loss: 0.0005628695129416883
batch 155 loss: 0.0005639377050101757
batch 160 loss: 0.000564241549000144
batch 165 loss: 0.0005630591535009444
batch 170 loss: 0.0005625968566164375
batch 175 loss: 0.0005615941947326065
batch 180 loss: 0.0005616686772555113
batch 185 loss: 0.0005614087567664683
batch 190 loss: 0.0005610542255453765
batch 195 loss: 0.0005611690692603588
batch 200 loss: 0.0005606500199064612
batch 205 loss: 0.0005605215555988252
batch 210 loss: 0.0005603339523077011
batch 215 loss: 0.0005605213926173746
batch 220 loss: 0.0005606692982837557
batch 225 loss: 0.0005606345017440617
batch 230 loss: 0.0005606204271316528
batch 235 loss: 0.0005605031619779766
batch 240 loss: 0.0005603248137049377
Training Loss: 0.0005610262337237752
Validation Loss: 0.00056015223187084
Epoch 22:
batch 5 loss: 0.0005604050005786121
batch 10 loss: 0.0005604790057986975
batch 15 loss: 0.0005605106591247023
batch 20 loss: 0.0005602682475000619
batch 25 loss: 0.0005600602948106825
batch 30 loss: 0.0005605005193501711
batch 35 loss: 0.0005604431848041713
batch 40 loss: 0.0005604441394098103
batch 45 loss: 0.0005605696118436754
batch 50 loss: 0.0005606773891486227
batch 55 loss: 0.0005606301361694932
batch 60 loss: 0.0005608904990367592
batch 65 loss: 0.0005989265977405011
batch 70 loss: 0.0010531786712817848
batch 75 loss: 0.0008657733094878494
batch 80 loss: 0.0007183538516983389
batch 85 loss: 0.0006326308590359986
batch 90 loss: 0.0006030180142261088
batch 95 loss: 0.0005865434184670448
batch 100 loss: 0.0005798567552119493
batch 105 loss: 0.0005768325296230614
batch 110 loss: 0.0005744030931964517
batch 115 loss: 0.0005726068280637264
batch 120 loss: 0.000571185804437846
batch 125 loss: 0.0005705253919586539
batch 130 loss: 0.0005700581124983728
batch 135 loss: 0.000569620169699192
batch 140 loss: 0.0005692011094652116
batch 145 loss: 0.0005689395824447274
batch 150 loss: 0.0005686448770575225
batch 155 loss: 0.0005681255483068526
batch 160 loss: 0.0005678036948665976
batch 165 loss: 0.0005671168095432222
batch 170 loss: 0.0005667019286192954
batch 175 loss: 0.0005665360135026276
batch 180 loss: 0.0005662716459482909
batch 185 loss: 0.0005662694456987083
batch 190 loss: 0.0005661383620463312
batch 195 loss: 0.0005659959744662047
batch 200 loss: 0.0005658494075760246
batch 205 loss: 0.0005656393594108522
batch 210 loss: 0.00056547736749053
batch 215 loss: 0.0005653736763633788
batch 220 loss: 0.0005653795320540667
batch 225 loss: 0.0005652924883179367
batch 230 loss: 0.0005649529746733605
batch 235 loss: 0.0005646343808621169
batch 240 loss: 0.0005645678145810961
Training Loss: 0.0005888396684895269
Validation Loss: 0.0005642511464733009
Epoch 23:
batch 5 loss: 0.0005643519456498325
batch 10 loss: 0.0005641465657390654
batch 15 loss: 0.0005640764604322613
batch 20 loss: 0.000563849275931716
batch 25 loss: 0.000563633581623435
batch 30 loss: 0.0005633917753584683
batch 35 loss: 0.0005632565706036984
batch 40 loss: 0.0005630826577544213
batch 45 loss: 0.0005628886981867253
batch 50 loss: 0.0005630028317682445
batch 55 loss: 0.0005627782084047794
batch 60 loss: 0.000562701909802854
batch 65 loss: 0.0005624523269943893
batch 70 loss: 0.0005623183329589665
batch 75 loss: 0.0005621945369057357
batch 80 loss: 0.0005619849427603186
batch 85 loss: 0.0005619400879368186
batch 90 loss: 0.000561679678503424
batch 95 loss: 0.0005616378271952272
batch 100 loss: 0.0005614706315100193
batch 105 loss: 0.0005614324705675245
batch 110 loss: 0.0005612981854937971
batch 115 loss: 0.0005612183711491525
batch 120 loss: 0.0005612025619484484
batch 125 loss: 0.0005609632469713687
batch 130 loss: 0.0005609799525700509
batch 135 loss: 0.0005608096718788147
batch 140 loss: 0.000560778088402003
batch 145 loss: 0.0005620918003842234
batch 150 loss: 0.0005633214488625527
batch 155 loss: 0.0005671496270224452
batch 160 loss: 0.0005650989129208029
batch 165 loss: 0.0005627689533866942
batch 170 loss: 0.0005619654664769768
batch 175 loss: 0.0005614066147245466
batch 180 loss: 0.000561064516659826
batch 185 loss: 0.0005612059030681849
batch 190 loss: 0.000561248289886862
batch 195 loss: 0.0005609444808214903
batch 200 loss: 0.0005603171070106327
batch 205 loss: 0.0005603500409051776
batch 210 loss: 0.0005614831927232445
batch 215 loss: 0.0005618304014205933
batch 220 loss: 0.000561374076642096
batch 225 loss: 0.0005605090991593897
batch 230 loss: 0.000564504147041589
batch 235 loss: 0.0005679301335476339
batch 240 loss: 0.000566423824056983
Training Loss: 0.0005624689464942396
Validation Loss: 0.0005670318477010975
Epoch 24:
batch 5 loss: 0.0005644919932819903
batch 10 loss: 0.0005628938437439501
batch 15 loss: 0.0005619619856588543
batch 20 loss: 0.0005610710708424449
batch 25 loss: 0.0005604158504866064
batch 30 loss: 0.0005601088865660131
batch 35 loss: 0.0005599543452262879
batch 40 loss: 0.000560418190434575
batch 45 loss: 0.0005600831471383571
batch 50 loss: 0.0005599672556854784
batch 55 loss: 0.0005598459276370705
batch 60 loss: 0.0005598838324658573
batch 65 loss: 0.0005598756950348615
batch 70 loss: 0.0005599786411039532
batch 75 loss: 0.0005598668940365314
batch 80 loss: 0.0005595436552539468
batch 85 loss: 0.0005595331895165145
batch 90 loss: 0.0005597727023996413
batch 95 loss: 0.000559762550983578
batch 100 loss: 0.0005598540417850017
batch 105 loss: 0.0005597707466222345
batch 110 loss: 0.0005595590686425566
batch 115 loss: 0.0005597402923740447
batch 120 loss: 0.0005597620271146298
batch 125 loss: 0.0005597961368039251
batch 130 loss: 0.0005596060073003173
batch 135 loss: 0.0005595667520537972
batch 140 loss: 0.0005594949005171656
batch 145 loss: 0.000559875275939703
batch 150 loss: 0.0005607171100564301
batch 155 loss: 0.0005603788653388619
batch 160 loss: 0.0005598697229288518
batch 165 loss: 0.0005611512460745871
batch 170 loss: 0.0005623390083201229
batch 175 loss: 0.0005609113140963018
batch 180 loss: 0.0005603034747764468
batch 185 loss: 0.0005602602497674525
batch 190 loss: 0.0005600942298769951
batch 195 loss: 0.0005596472881734371
batch 200 loss: 0.0005595772061496973
batch 205 loss: 0.0005600303295068443
batch 210 loss: 0.000559667800553143
batch 215 loss: 0.0005598537158221006
batch 220 loss: 0.000559845264069736
batch 225 loss: 0.0005596862290985882
batch 230 loss: 0.0005599539843387901
batch 235 loss: 0.0005598918301984668
batch 240 loss: 0.0005599146708846093
Training Loss: 0.0005602197593058615
Validation Loss: 0.0005594862663807969
Epoch 25:
batch 5 loss: 0.0005594174843281507
batch 10 loss: 0.0005595289054326713
batch 15 loss: 0.0005595162860117852
batch 20 loss: 0.0005595238879323005
batch 25 loss: 0.0005594745394773782
batch 30 loss: 0.0005592825473286211
batch 35 loss: 0.0005594330141320825
batch 40 loss: 0.0005592884263023734
batch 45 loss: 0.0005591547000221908
batch 50 loss: 0.0005594501737505198
batch 55 loss: 0.0005594865302555263
batch 60 loss: 0.0005595606518909336
batch 65 loss: 0.0005596258910372853
batch 70 loss: 0.0005595305934548378
batch 75 loss: 0.0005596773000434041
batch 80 loss: 0.0005596056929789483
batch 85 loss: 0.0005595318274572491
batch 90 loss: 0.0005597623763605952
batch 95 loss: 0.000559767778031528
batch 100 loss: 0.0005594481714069844
batch 105 loss: 0.0005595857393927873
batch 110 loss: 0.0005593950278125704
batch 115 loss: 0.0005594815127551555
batch 120 loss: 0.0005595254944637418
batch 125 loss: 0.0005597015842795372
batch 130 loss: 0.0005596017115749418
batch 135 loss: 0.0005594545160420239
batch 140 loss: 0.0005593989044427872
batch 145 loss: 0.000559666205663234
batch 150 loss: 0.000559442664962262
batch 155 loss: 0.0005594305461272597
batch 160 loss: 0.0005593415233306586
batch 165 loss: 0.0005595750175416469
batch 170 loss: 0.0005596094415523112
batch 175 loss: 0.0005596566130407155
batch 180 loss: 0.0005595897324383259
batch 185 loss: 0.0005600434495136141
batch 190 loss: 0.0005596069153398275
batch 195 loss: 0.0005596077302470803
batch 200 loss: 0.0005593935027718544
batch 205 loss: 0.0005594987189397216
batch 210 loss: 0.0005596609204076231
batch 215 loss: 0.0005595077527686954
batch 220 loss: 0.0005595251801423729
batch 225 loss: 0.0005595945753157139
batch 230 loss: 0.0005596706876531243
batch 235 loss: 0.0005595858558081091
batch 240 loss: 0.0005595335853286088
Training Loss: 0.0005595364976519098
Validation Loss: 0.0005592107792229702
Epoch 26:
batch 5 loss: 0.0005593000678345561
batch 10 loss: 0.0005592256318777799
batch 15 loss: 0.0005593445850536228
batch 20 loss: 0.0005595953320153057
batch 25 loss: 0.0005687304306775332
batch 30 loss: 0.0005934698856435716
batch 35 loss: 0.0006085800705477595
batch 40 loss: 0.0006069840514101088
batch 45 loss: 0.0005969106452539564
batch 50 loss: 0.0005861257552169264
batch 55 loss: 0.0005784475011751056
batch 60 loss: 0.0005740149994380772
batch 65 loss: 0.0005714956670999527
batch 70 loss: 0.0005699985194951296
batch 75 loss: 0.0005689795361831784
batch 80 loss: 0.0005680624512024224
batch 85 loss: 0.0005679031484760344
batch 90 loss: 0.0005674186395481229
batch 95 loss: 0.0005671154474839568
batch 100 loss: 0.0005666513578034937
batch 105 loss: 0.0005660438910126686
batch 110 loss: 0.0005657534929923713
batch 115 loss: 0.0005654662614688277
batch 120 loss: 0.0005652355845086277
batch 125 loss: 0.0005645722849294543
batch 130 loss: 0.000565982109401375
batch 135 loss: 0.0005672958446666599
batch 140 loss: 0.0005677695735357702
batch 145 loss: 0.000565350113902241
batch 150 loss: 0.000563740509096533
batch 155 loss: 0.0005626900121569634
batch 160 loss: 0.0005618402501568198
batch 165 loss: 0.0005596719798631966
batch 170 loss: 0.0005577722680754959
batch 175 loss: 0.0005576550494879485
batch 180 loss: 0.0005576165160164237
batch 185 loss: 0.0005576339899562299
batch 190 loss: 0.0005577080766670406
batch 195 loss: 0.0005576709983870388
batch 200 loss: 0.000557591870892793
batch 205 loss: 0.0005577968433499337
batch 210 loss: 0.0005576246883720159
batch 215 loss: 0.000557701988145709
batch 220 loss: 0.0005576923955231905
batch 225 loss: 0.000557735119946301
batch 230 loss: 0.0005576819996349514
batch 235 loss: 0.0005576599505729973
batch 240 loss: 0.0005577423144131899
Training Loss: 0.0005667718687618617
Validation Loss: 0.0005576934005754689
Epoch 27:
batch 5 loss: 0.0005578182404860854
batch 10 loss: 0.000557728426065296
batch 15 loss: 0.0005576114635914564
batch 20 loss: 0.0005576592753641308
batch 25 loss: 0.0005576430587098003
batch 30 loss: 0.0005576025578193367
batch 35 loss: 0.0005576432915404439
batch 40 loss: 0.0005576911848038435
batch 45 loss: 0.0005576310912147164
batch 50 loss: 0.000557637878227979
batch 55 loss: 0.0005577365984208882
batch 60 loss: 0.0005576235591433943
batch 65 loss: 0.0005577501724474132
batch 70 loss: 0.0005578611046075821
batch 75 loss: 0.0005577417439781129
batch 80 loss: 0.0005576239782385528
batch 85 loss: 0.0005576939438469708
batch 90 loss: 0.0005575692164711654
batch 95 loss: 0.0005577768082730472
batch 100 loss: 0.0005577862728387117
batch 105 loss: 0.0005576052702963352
batch 110 loss: 0.0005576171563006938
batch 115 loss: 0.0005577481351792812
batch 120 loss: 0.0005576442810706795
batch 125 loss: 0.0005576829542405903
batch 130 loss: 0.0005575385177507997
batch 135 loss: 0.0005577374831773341
batch 140 loss: 0.0005577345029450953
batch 145 loss: 0.000557794445194304
batch 150 loss: 0.0005577030009590089
batch 155 loss: 0.0005577077274210751
batch 160 loss: 0.0005576760857366025
batch 165 loss: 0.0005576706724241375
batch 170 loss: 0.0005576525931246578
batch 175 loss: 0.0005576860625296831
batch 180 loss: 0.0005577088566496968
batch 185 loss: 0.0005576577736064791
batch 190 loss: 0.0005576205090619624
batch 195 loss: 0.0005576348630711436
batch 200 loss: 0.0005577674019150436
batch 205 loss: 0.0005577754927799106
batch 210 loss: 0.0005577179021202028
batch 215 loss: 0.0005577321513555944
batch 220 loss: 0.0005575461429543794
batch 225 loss: 0.0005577080766670406
batch 230 loss: 0.0005576311959885061
batch 235 loss: 0.0005576371564529836
batch 240 loss: 0.0005576361087150872
Training Loss: 0.0005576833830370257
Validation Loss: 0.0005576934044559796
Epoch 28:
batch 5 loss: 0.0005577538046054542
batch 10 loss: 0.0005576492752879858
batch 15 loss: 0.0005577229429036378
batch 20 loss: 0.0005576745257712901
batch 25 loss: 0.000557630939874798
batch 30 loss: 0.0005576890427619219
batch 35 loss: 0.0005576224066317081
batch 40 loss: 0.0005576384253799915
batch 45 loss: 0.0005576925002969801
batch 50 loss: 0.0005578335840255022
batch 55 loss: 0.0005577777395956218
batch 60 loss: 0.0005576848518103362
batch 65 loss: 0.0005576336290687323
batch 70 loss: 0.0005577018950134516
batch 75 loss: 0.000557768193539232
batch 80 loss: 0.000557654129806906
batch 85 loss: 0.0005576929543167353
batch 90 loss: 0.0005576305440627039
batch 95 loss: 0.0005576015450060367
batch 100 loss: 0.0005577481584623456
batch 105 loss: 0.0005576848518103362
batch 110 loss: 0.000557779602240771
batch 115 loss: 0.0005576382391154766
batch 120 loss: 0.0005575710791163146
batch 125 loss: 0.0005576333380304277
batch 130 loss: 0.0005576423485763371
batch 135 loss: 0.0005577885429374873
batch 140 loss: 0.0005576566560193897
batch 145 loss: 0.0005577185656875372
batch 150 loss: 0.000557736773043871
batch 155 loss: 0.0005576294614002108
batch 160 loss: 0.0005576070980168879
batch 165 loss: 0.0005577265983447433
batch 170 loss: 0.0005576709983870388
batch 175 loss: 0.000557711988221854
batch 180 loss: 0.0005577565752901137
batch 185 loss: 0.0005577131872996688
batch 190 loss: 0.0005577370640821755
batch 195 loss: 0.0005577206262387336
batch 200 loss: 0.0005576175288297236
batch 205 loss: 0.0005576430587098003
batch 210 loss: 0.0005575553281232714
batch 215 loss: 0.0005576715688221156
batch 220 loss: 0.0005577889503911137
batch 225 loss: 0.0005576655152253806
batch 230 loss: 0.0005576463765464724
batch 235 loss: 0.0005575973074883223
batch 240 loss: 0.0005576920928433537
Training Loss: 0.0005576833835220895
Validation Loss: 0.0005576934102767458
Epoch 29:
batch 5 loss: 0.000557711604051292
batch 10 loss: 0.0005577532225288451
batch 15 loss: 0.0005575982504524291
batch 20 loss: 0.0005577453994192183
batch 25 loss: 0.0005577099160291255
batch 30 loss: 0.0005576571449637413
batch 35 loss: 0.0005576601717621088
batch 40 loss: 0.0005576939904130996
batch 45 loss: 0.0005577065283432602
batch 50 loss: 0.0005575799499638378
batch 55 loss: 0.0005576664349064231
batch 60 loss: 0.0005576596595346928
batch 65 loss: 0.0005576598923653364
batch 70 loss: 0.0005575485876761377
batch 75 loss: 0.0005577177624218165
batch 80 loss: 0.0005577288218773901
batch 85 loss: 0.0005577988922595978
batch 90 loss: 0.000557673058938235
batch 95 loss: 0.0005578076583333313
batch 100 loss: 0.0005576692055910826
batch 105 loss: 0.0005576541763730348
batch 110 loss: 0.0005577146308496595
batch 115 loss: 0.0005576338386163116
batch 120 loss: 0.0005576733266934753
batch 125 loss: 0.0005576897063292563
batch 130 loss: 0.0005576311727054417
batch 135 loss: 0.0005576686351560056
batch 140 loss: 0.0005576404160819948
batch 145 loss: 0.0005575070972554386
batch 150 loss: 0.000557779218070209
batch 155 loss: 0.0005576742812991142
batch 160 loss: 0.0005577046773396433
batch 165 loss: 0.0005575703107751906
batch 170 loss: 0.0005576815456151962
batch 175 loss: 0.0005577257368713617
batch 180 loss: 0.0005576721858233213
batch 185 loss: 0.0005576290655881167
batch 190 loss: 0.0005576288211159408
batch 195 loss: 0.0005576571566052735
batch 200 loss: 0.0005577441304922103
batch 205 loss: 0.0005576790077611804
batch 210 loss: 0.0005577012081630528
batch 215 loss: 0.0005577582400292158
batch 220 loss: 0.0005576964002102613
batch 225 loss: 0.00055774818174541
batch 230 loss: 0.0005576600204221904
batch 235 loss: 0.0005577434436418116
batch 240 loss: 0.0005577896838076413
Training Loss: 0.0005576833847347492
Validation Loss: 0.0005576934151273841
Epoch 30:
batch 5 loss: 0.0005576930940151215
batch 10 loss: 0.000557691475842148
batch 15 loss: 0.0005578299053013325
batch 20 loss: 0.0005576760973781347
batch 25 loss: 0.0005577140138484538
batch 30 loss: 0.0005576501484028995
batch 35 loss: 0.0005577142932452261
batch 40 loss: 0.000557767041027546
batch 45 loss: 0.0005576582625508308
batch 50 loss: 0.000557597924489528
batch 55 loss: 0.0005576280178502202
batch 60 loss: 0.0005575784831307829
batch 65 loss: 0.0005576737225055694
batch 70 loss: 0.0005576498224399984
batch 75 loss: 0.0005576246767304838
batch 80 loss: 0.000561885058414191
batch 85 loss: 0.0005577013711445033
batch 90 loss: 0.0005576173425652087
batch 95 loss: 0.000557687331456691
batch 100 loss: 0.000557622779160738
batch 105 loss: 0.0005576734431087971
batch 110 loss: 0.0005575618124566972
batch 115 loss: 0.0005577882518991828
batch 120 loss: 0.000557745574042201
batch 125 loss: 0.0005576840252615511
batch 130 loss: 0.000557637435849756
batch 135 loss: 0.0005576150841079652
batch 140 loss: 0.000557708041742444
batch 145 loss: 0.0005575989955104888
batch 150 loss: 0.0005577700678259135
batch 155 loss: 0.0005578073672950268
batch 160 loss: 0.0005577454576268792
batch 165 loss: 0.0005576854455284774
batch 170 loss: 0.0005576645606197416
batch 175 loss: 0.0005576214985921979
batch 180 loss: 0.000557720148935914
batch 185 loss: 0.0005576517782174051
batch 190 loss: 0.0005577890202403069
batch 195 loss: 0.0005576851894147694
batch 200 loss: 0.0005575548275373876
batch 205 loss: 0.0005576918832957744
batch 210 loss: 0.0005576661555096507
batch 215 loss: 0.0005576820112764835
batch 220 loss: 0.0005575987976044417
batch 225 loss: 0.0005577705334872008
batch 230 loss: 0.0005577310686931014
batch 235 loss: 0.0005576964700594545
batch 240 loss: 0.0005578077398240566
Training Loss: 0.0005577711155638099
Validation Loss: 0.0005576934122170011
Epoch 31:
batch 5 loss: 0.0005576697993092238
batch 10 loss: 0.0005576162599027157
batch 15 loss: 0.0005576709285378456
batch 20 loss: 0.0005575790186412632
batch 25 loss: 0.0005575867253355682
batch 30 loss: 0.0005576433148235082
batch 35 loss: 0.0005578860989771783
batch 40 loss: 0.0005576697993092238
batch 45 loss: 0.0005576795432716608
batch 50 loss: 0.000557726388797164
batch 55 loss: 0.0005576879484578967
batch 60 loss: 0.0005575820337980986
batch 65 loss: 0.0005577025236561895
batch 70 loss: 0.000557726458646357
batch 75 loss: 0.0005577494390308857
batch 80 loss: 0.0005577796953730285
batch 85 loss: 0.0005575507530011237
batch 90 loss: 0.0005577510455623269
batch 95 loss: 0.0005577041651122272
batch 100 loss: 0.0005576504860073328
batch 105 loss: 0.0005576433846727014
batch 110 loss: 0.0005576282157562674
batch 115 loss: 0.0005577895906753838
batch 120 loss: 0.0005577548407018184
batch 125 loss: 0.0005577594391070306
batch 130 loss: 0.0005577424075454473
batch 135 loss: 0.0005576750147156418
batch 140 loss: 0.0005575710791163146
batch 145 loss: 0.0005576767143793404
batch 150 loss: 0.0005576159572228789
batch 155 loss: 0.0005576149909757077
batch 160 loss: 0.0005575870629400015
batch 165 loss: 0.0005576301831752062
batch 170 loss: 0.0005576897994615138
batch 175 loss: 0.0005577587173320353
batch 180 loss: 0.0005577646428719163
batch 185 loss: 0.0005576804862357676
batch 190 loss: 0.0005577014992013574
batch 195 loss: 0.0005577568663284183
batch 200 loss: 0.0005576730472967029
batch 205 loss: 0.0005577350384555757
batch 210 loss: 0.0005576826981268824
batch 215 loss: 0.000557568424846977
batch 220 loss: 0.0005577915464527905
batch 225 loss: 0.0005576667492277921
batch 230 loss: 0.0005576718598604202
batch 235 loss: 0.0005577213480137289
batch 240 loss: 0.0005576385417953133
Training Loss: 0.0005576833869175364
Validation Loss: 0.0005576934267689164
Epoch 32:
batch 5 loss: 0.0005576556082814932
batch 10 loss: 0.0005576246418058872
batch 15 loss: 0.0005577410338446498
batch 20 loss: 0.0005576680763624608
batch 25 loss: 0.0005575474351644516
batch 30 loss: 0.0005577303003519773
batch 35 loss: 0.0005576329305768013
batch 40 loss: 0.0005576374474912882
batch 45 loss: 0.000557650881819427
batch 50 loss: 0.0005575775750912726
batch 55 loss: 0.0005577034666202962
batch 60 loss: 0.0005577272037044168
batch 65 loss: 0.0005577240372076631
batch 70 loss: 0.0005577498115599156
batch 75 loss: 0.000557669170666486
batch 80 loss: 0.0005577962612733245
batch 85 loss: 0.0005575713701546192
batch 90 loss: 0.0005578120122663677
batch 95 loss: 0.000557701219804585
batch 100 loss: 0.0005576631403528154
batch 105 loss: 0.0005577612901106477
batch 110 loss: 0.0005577242816798389
batch 115 loss: 0.0005577031057327986
batch 120 loss: 0.0005577180767431855
batch 125 loss: 0.000557691405992955
batch 130 loss: 0.0005577306612394751
batch 135 loss: 0.0005576132098212838
batch 140 loss: 0.0005577514646574854
batch 145 loss: 0.0005576518713496625
batch 150 loss: 0.0005577335134148598
batch 155 loss: 0.0005578037933446466
batch 160 loss: 0.0005576670519076288
batch 165 loss: 0.0005578223266638815
batch 170 loss: 0.0005575996940024198
batch 175 loss: 0.0005576593917794526
batch 180 loss: 0.000557707843836397
batch 185 loss: 0.0005577650968916714
batch 190 loss: 0.0005577105330303311
batch 195 loss: 0.0005576158757321537
batch 200 loss: 0.0005576828960329294
batch 205 loss: 0.0005575964925810695
batch 210 loss: 0.0005576851079240441
batch 215 loss: 0.0005575917311944067
batch 220 loss: 0.0005576492636464536
batch 225 loss: 0.0005575187155045569
batch 230 loss: 0.0005577181465923787
batch 235 loss: 0.0005576019175350666
batch 240 loss: 0.0005577213480137289
Training Loss: 0.0005576829110699085
Validation Loss: 0.0005576898751314729
Epoch 33:
batch 5 loss: 0.0005577655974775553
batch 10 loss: 0.0005578049924224616
batch 15 loss: 0.0005577113013714551
batch 20 loss: 0.0005575927556492388
batch 25 loss: 0.0005577864241786302
batch 30 loss: 0.0005577047239057719
batch 35 loss: 0.0005576091003604233
batch 40 loss: 0.0005576934199780226
batch 45 loss: 0.0005576851195655763
batch 50 loss: 0.0005575739545747638
batch 55 loss: 0.0005575572140514851
batch 60 loss: 0.0005577679490670562
batch 65 loss: 0.0005577470525167883
batch 70 loss: 0.0005576417664997279
batch 75 loss: 0.0005575958057306707
batch 80 loss: 0.0005576961324550211
batch 85 loss: 0.0005576467840000987
batch 90 loss: 0.0005576340365223587
batch 95 loss: 0.0005577421747148037
batch 100 loss: 0.0005577526288107037
batch 105 loss: 0.0005577129777520895
batch 110 loss: 0.0005576595081947744
batch 115 loss: 0.0005577343632467091
batch 120 loss: 0.0005577332689426839
batch 125 loss: 0.0005576576688326896
batch 130 loss: 0.0005577007425017655
batch 135 loss: 0.0005577550153248012
batch 140 loss: 0.000557690137065947
batch 145 loss: 0.0005577377509325743
batch 150 loss: 0.0005577009986154735
batch 155 loss: 0.0005575769930146635
batch 160 loss: 0.0005576865631155669
batch 165 loss: 0.0005577419884502888
batch 170 loss: 0.0005577489384450019
batch 175 loss: 0.0005577042466029525
batch 180 loss: 0.0005576153984293341
batch 185 loss: 0.0005577022559009493
batch 190 loss: 0.0005576109513640403
batch 195 loss: 0.0005577273317612707
batch 200 loss: 0.0005575193674303592
batch 205 loss: 0.0005576840601861476
batch 210 loss: 0.0005575947812758386
batch 215 loss: 0.0005576056428253651
batch 220 loss: 0.0005577691830694675
batch 225 loss: 0.0005576854920946062
batch 230 loss: 0.0005576701718382537
batch 235 loss: 0.0005577372154220939
batch 240 loss: 0.0005577618372626603
Training Loss: 0.0005576861204948121
Validation Loss: 0.0005576934452013423
Epoch 34:
batch 5 loss: 0.000557723781093955
batch 10 loss: 0.0005577147123403848
batch 15 loss: 0.0005576334195211529
batch 20 loss: 0.0005577827454544604
batch 25 loss: 0.0005575465853326023
batch 30 loss: 0.0005577148171141743
batch 35 loss: 0.0005576838389970362
batch 40 loss: 0.000557792792096734
batch 45 loss: 0.0005576001363806427
batch 50 loss: 0.0005577181582339108
batch 55 loss: 0.000557669042609632
batch 60 loss: 0.0005576450261287391
batch 65 loss: 0.0005577015224844217
batch 70 loss: 0.0005575838382355869
batch 75 loss: 0.0005577499279752374
batch 80 loss: 0.0005576363299041986
batch 85 loss: 0.0005577173666097224
batch 90 loss: 0.000557632592972368
batch 95 loss: 0.0005576238268986345
batch 100 loss: 0.0005575889023020864
batch 105 loss: 0.0005577430478297174
batch 110 loss: 0.0005575637449510395
batch 115 loss: 0.0005577271338552236
batch 120 loss: 0.000557684968225658
batch 125 loss: 0.0005577006260864437
batch 130 loss: 0.0005577346426434814
batch 135 loss: 0.0005577812436968089
batch 140 loss: 0.0005576988332904875
batch 145 loss: 0.000557678600307554
batch 150 loss: 0.0005576586467213928
batch 155 loss: 0.0005576406489126384
batch 160 loss: 0.0005576263647526503
batch 165 loss: 0.0005576077615842223
batch 170 loss: 0.0005577247589826584
batch 175 loss: 0.0005575673305429519
batch 180 loss: 0.000557717913761735
batch 185 loss: 0.0005577142466790975
batch 190 loss: 0.0005576402763836086
batch 195 loss: 0.0005576828843913972
batch 200 loss: 0.0005578185082413256
batch 205 loss: 0.0005576813826337456
batch 210 loss: 0.0005576566443778574
batch 215 loss: 0.0005577906384132802
batch 220 loss: 0.0005576806957833469
batch 225 loss: 0.0005577224772423505
batch 230 loss: 0.000557675096206367
batch 235 loss: 0.0005577545263804496
batch 240 loss: 0.0005576696014031768
Training Loss: 0.0005576833876451323
Validation Loss: 0.0005576934452013423
Epoch 35:
batch 5 loss: 0.0005576633033342659
batch 10 loss: 0.0005577090894803404
batch 15 loss: 0.0005576305207796395
batch 20 loss: 0.0005576518131420016
batch 25 loss: 0.0005577061441726982
batch 30 loss: 0.0005576589377596974
batch 35 loss: 0.0005576392286457121
batch 40 loss: 0.0005576678086072206
batch 45 loss: 0.0005575682735070586
batch 50 loss: 0.0005576926399953663
batch 55 loss: 0.0005576761672273278
batch 60 loss: 0.0005575705436058342
batch 65 loss: 0.0005578128388151526
batch 70 loss: 0.0005576658644713461
batch 75 loss: 0.0005577062489464879
batch 80 loss: 0.0005576756084337831
batch 85 loss: 0.0005575999850407243
batch 90 loss: 0.0005576810333877801
batch 95 loss: 0.000557743466924876
batch 100 loss: 0.0005576206371188164
batch 105 loss: 0.0005576048279181123
batch 110 loss: 0.0005576791008934379
batch 115 loss: 0.0005577884730882943
batch 120 loss: 0.0005577517906203866
batch 125 loss: 0.0005577037343755364
batch 130 loss: 0.0005576236173510552
batch 135 loss: 0.0005577192641794682
batch 140 loss: 0.0005575690302066505
batch 145 loss: 0.0005576888564974069
batch 150 loss: 0.0005577573669143021
batch 155 loss: 0.0005575879476964474
batch 160 loss: 0.0005576922325417399
batch 165 loss: 0.0005576672730967403
batch 170 loss: 0.0005577284842729569
batch 175 loss: 0.0005577347707003355
batch 180 loss: 0.0005577825009822846
batch 185 loss: 0.000557674199808389
batch 190 loss: 0.0005576893105171621
batch 195 loss: 0.0005576360155828298
batch 200 loss: 0.0005576879600994288
batch 205 loss: 0.0005576945259235799
batch 210 loss: 0.0005576046416535974
batch 215 loss: 0.0005577135249041021
batch 220 loss: 0.0005578004056587815
batch 225 loss: 0.0005576387513428927
batch 230 loss: 0.000557732698507607
batch 235 loss: 0.0005577770061790943
batch 240 loss: 0.0005577335483394563
Training Loss: 0.0005576833752760043
Validation Loss: 0.0005576927893950293
Epoch 36:
batch 5 loss: 0.0005576590658165515
batch 10 loss: 0.0005576691473834217
batch 15 loss: 0.0005576343974098563
batch 20 loss: 0.0005576421739533543
batch 25 loss: 0.0005577016272582114
batch 30 loss: 0.0005576632916927338
batch 35 loss: 0.0005576610798016191
batch 40 loss: 0.0005576624884270131
batch 45 loss: 0.0005577315809205174
batch 50 loss: 0.0005575946881435812
batch 55 loss: 0.0005576834664680064
batch 60 loss: 0.0005576993688009679
batch 65 loss: 0.0005576746654696763
batch 70 loss: 0.0005577786825597287
batch 75 loss: 0.0005577487987466157
batch 80 loss: 0.0005577243398874998
batch 85 loss: 0.0005577127682045102
batch 90 loss: 0.0005575751652941108
batch 95 loss: 0.0005577556206844747
batch 100 loss: 0.0005575988907366991
batch 105 loss: 0.0005577482050284743
batch 110 loss: 0.0005576794967055321
batch 115 loss: 0.0005577284377068281
batch 120 loss: 0.0005576759576797485
batch 125 loss: 0.000557664968073368
batch 130 loss: 0.0005578026524744928
batch 135 loss: 0.0005575869116000831
batch 140 loss: 0.000557722628582269
batch 145 loss: 0.0005576472845859826
batch 150 loss: 0.0005576686351560056
batch 155 loss: 0.0005576604278758168
batch 160 loss: 0.0005575786926783621
batch 165 loss: 0.0005576700204983354
batch 170 loss: 0.0005575904157012701
batch 175 loss: 0.0005577735835686326
batch 180 loss: 0.0005576863186433911
batch 185 loss: 0.0005577290197834372
batch 190 loss: 0.0005576410098001361
batch 195 loss: 0.0005576835479587317
batch 200 loss: 0.0005575545248575509
batch 205 loss: 0.000557625328656286
batch 210 loss: 0.0005576891941018403
batch 215 loss: 0.000557772780302912
batch 220 loss: 0.0005578775657340884
batch 225 loss: 0.0005576448864303529
batch 230 loss: 0.0005577203934080899
batch 235 loss: 0.000557670125272125
batch 240 loss: 0.0005576837691478431
Training Loss: 0.0005576822518681486
Validation Loss: 0.0005576934102767458
Epoch 37:
batch 5 loss: 0.0005576114170253276
batch 10 loss: 0.0005577569827437401
batch 15 loss: 0.0005576976807788014
batch 20 loss: 0.0005576772149652242
batch 25 loss: 0.0005576340248808265
batch 30 loss: 0.0005577277042903006
batch 35 loss: 0.0005577487754635513
batch 40 loss: 0.0005576660623773932
batch 45 loss: 0.0005576596013270319
batch 50 loss: 0.0005577836185693741
batch 55 loss: 0.0005577020579949022
batch 60 loss: 0.0005577616393566132
batch 65 loss: 0.0005576598108746111
batch 70 loss: 0.0005576822091825307
batch 75 loss: 0.0005577249918133021
batch 80 loss: 0.0005577067611739039
batch 85 loss: 0.0005576835712417961
batch 90 loss: 0.0005577026400715113
batch 95 loss: 0.00055767095182091
batch 100 loss: 0.0005577060976065696
batch 105 loss: 0.000557691475842148
batch 110 loss: 0.0005576466326601803
batch 115 loss: 0.0005575426970608532
batch 120 loss: 0.0005576890893280506
batch 125 loss: 0.0005576014402322472
batch 130 loss: 0.0005577675648964942
batch 135 loss: 0.0005576851894147694
batch 140 loss: 0.0005577069940045476
batch 145 loss: 0.0005576426163315773
batch 150 loss: 0.0005576297640800476
batch 155 loss: 0.0005576502997428178
batch 160 loss: 0.0005577544216066599
batch 165 loss: 0.0005577286588959396
batch 170 loss: 0.0005577807081863284
batch 175 loss: 0.0005577357951551676
batch 180 loss: 0.0005576647352427244
batch 185 loss: 0.0005576873198151588
batch 190 loss: 0.0005576764582656324
batch 195 loss: 0.0005574807408265769
batch 200 loss: 0.0005576502997428178
batch 205 loss: 0.0005575844901613891
batch 210 loss: 0.0005577024887315929
batch 215 loss: 0.0005577132804319262
batch 220 loss: 0.0005576514289714396
batch 225 loss: 0.00055764737771824
batch 230 loss: 0.0005576941999606788
batch 235 loss: 0.0005577319883741438
batch 240 loss: 0.000557701219804585
Training Loss: 0.0005576833997717282
Validation Loss: 0.000557693427739044
Epoch 38:
batch 5 loss: 0.0005576209514401853
batch 10 loss: 0.0005577396135777235
batch 15 loss: 0.0005576118477620184
batch 20 loss: 0.0005577457486651838
batch 25 loss: 0.0005576805328018963
batch 30 loss: 0.0005575897288508713
batch 35 loss: 0.0005577543284744024
batch 40 loss: 0.0005575759802013636
batch 45 loss: 0.0005577650619670749
batch 50 loss: 0.0005577202304266393
batch 55 loss: 0.0005577020347118378
batch 60 loss: 0.0005576839321292937
batch 65 loss: 0.000557708682026714
batch 70 loss: 0.000557559181470424
batch 75 loss: 0.0005577023839578032
batch 80 loss: 0.0005575528368353844
batch 85 loss: 0.0005577172967605292
batch 90 loss: 0.0005576843279413879
batch 95 loss: 0.0005576479365117848
batch 100 loss: 0.0005577092757448554
batch 105 loss: 0.0005576359108090401
batch 110 loss: 0.0005576829542405903
batch 115 loss: 0.0005577185424044728
batch 120 loss: 0.000557712058071047
batch 125 loss: 0.0005578379961661995
batch 130 loss: 0.0005577423726208508
batch 135 loss: 0.0005576173425652087
batch 140 loss: 0.0005577195202931762
batch 145 loss: 0.0005577102536335587
batch 150 loss: 0.000557800370734185
batch 155 loss: 0.0005576246767304838
batch 160 loss: 0.0005576625932008028
batch 165 loss: 0.0005577141186222434
batch 170 loss: 0.0005576641764491796
batch 175 loss: 0.000557610800024122
batch 180 loss: 0.0005576869123615324
batch 185 loss: 0.0005577484029345214
batch 190 loss: 0.000557626795489341
batch 195 loss: 0.0005577352014370263
batch 200 loss: 0.0005577044561505317
batch 205 loss: 0.0005576254450716078
batch 210 loss: 0.0005576129537075758
batch 215 loss: 0.0005577587755396963
batch 220 loss: 0.0005576461087912321
batch 225 loss: 0.0005576328840106726
batch 230 loss: 0.0005577234784141183
batch 235 loss: 0.0005577039555646479
batch 240 loss: 0.000557672674767673
Training Loss: 0.0005576834092304732
Validation Loss: 0.0005576933986352135
Epoch 39:
batch 5 loss: 0.0005576995783485473
batch 10 loss: 0.000557707843836397
batch 15 loss: 0.0005576933501288295
batch 20 loss: 0.0005576296243816614
batch 25 loss: 0.0005577069707214833
batch 30 loss: 0.0005575939547270536
batch 35 loss: 0.0005576818832196296
batch 40 loss: 0.0005577012547291815
batch 45 loss: 0.0005575420218519866
batch 50 loss: 0.000557757739443332
batch 55 loss: 0.0005576270050369203
batch 60 loss: 0.0005577737465500832
batch 65 loss: 0.0005576660623773932
batch 70 loss: 0.0005577319418080152
batch 75 loss: 0.000557768577709794
batch 80 loss: 0.0005576275871135294
batch 85 loss: 0.0005576512776315212
batch 90 loss: 0.0005576165858656168
batch 95 loss: 0.0005575713468715549
batch 100 loss: 0.0005577286123298108
batch 105 loss: 0.0005576932104304433
batch 110 loss: 0.0005576600204221904
batch 115 loss: 0.0005576601484790445
batch 120 loss: 0.0005577105563133955
batch 125 loss: 0.0005576992174610495
batch 130 loss: 0.0005577252362854779
batch 135 loss: 0.0005577578558586538
batch 140 loss: 0.0005576094496063888
batch 145 loss: 0.0005576710333116353
batch 150 loss: 0.0005577244213782251
batch 155 loss: 0.0005576505791395903
batch 160 loss: 0.0005577048170380295
batch 165 loss: 0.0005577990668825805
batch 170 loss: 0.0005576837109401822
batch 175 loss: 0.0005577314761467278
batch 180 loss: 0.0005577346193604172
batch 185 loss: 0.0005576815688982606
batch 190 loss: 0.0005577375995926559
batch 195 loss: 0.0005576918367296458
batch 200 loss: 0.0005576997180469334
batch 205 loss: 0.0005576108931563794
batch 210 loss: 0.0005576244206167757
batch 215 loss: 0.0005576115567237139
batch 220 loss: 0.0005576203810051083
batch 225 loss: 0.0005578033393248916
batch 230 loss: 0.0005575911607593298
batch 235 loss: 0.0005577430012635887
batch 240 loss: 0.0005576956667937338
Training Loss: 0.0005576834068051539
Validation Loss: 0.0005576934422909592
Epoch 40:
batch 5 loss: 0.000557711860165
batch 10 loss: 0.0005577276577241718
batch 15 loss: 0.0005577534902840853
batch 20 loss: 0.000557692814618349
batch 25 loss: 0.0005576736759394408
batch 30 loss: 0.0005576828611083329
batch 35 loss: 0.0005577152711339295
batch 40 loss: 0.0005576915224082768
batch 45 loss: 0.0005577131756581366
batch 50 loss: 0.0005576796713285149
batch 55 loss: 0.000557603093329817
batch 60 loss: 0.0005576924188062549
batch 65 loss: 0.0005575886578299105
batch 70 loss: 0.0005577399278990924
batch 75 loss: 0.0005576863419264555
batch 80 loss: 0.0005577209638431668
batch 85 loss: 0.0005576625582762063
batch 90 loss: 0.0005577208125032484
batch 95 loss: 0.0005577101837843657
batch 100 loss: 0.0005576876341365278
batch 105 loss: 0.0005577196949161589
batch 110 loss: 0.0005576336989179254
batch 115 loss: 0.0005577714066021145
batch 120 loss: 0.0005575857358053327
batch 125 loss: 0.0005576575989834964
batch 130 loss: 0.0005576199851930142
batch 135 loss: 0.0005576308118179441
batch 140 loss: 0.000557750347070396
batch 145 loss: 0.0005576449912041426
batch 150 loss: 0.0005576758179813623
batch 155 loss: 0.000557635840959847
batch 160 loss: 0.0005576020339503884
batch 165 loss: 0.0005576369469054044
batch 170 loss: 0.0005576913594268262
batch 175 loss: 0.0005575673189014196
batch 180 loss: 0.0005577429430559278
batch 185 loss: 0.0005576777854003012
batch 190 loss: 0.0005575624061748385
batch 195 loss: 0.0005577239790000021
batch 200 loss: 0.0005577848991379142
batch 205 loss: 0.0005576208117417991
batch 210 loss: 0.0005577531293965877
batch 215 loss: 0.0005577154923230409
batch 220 loss: 0.0005576452938839793
batch 225 loss: 0.0005577266565524042
batch 230 loss: 0.0005577384377829731
batch 235 loss: 0.0005577561794780194
batch 240 loss: 0.0005576774827204644
Training Loss: 0.0005576834099580689
Validation Loss: 0.0005576934907973434
Epoch 41:
batch 5 loss: 0.0005577158415690064
batch 10 loss: 0.0005577436531893909
batch 15 loss: 0.0005577485542744398
batch 20 loss: 0.0005576786585152149
batch 25 loss: 0.0005577492876909674
batch 30 loss: 0.0005576458177529275
batch 35 loss: 0.0005575866904109717
batch 40 loss: 0.0005578043404966593
batch 45 loss: 0.0005577526637353003
batch 50 loss: 0.000557809614110738
batch 55 loss: 0.0005577020114287734
batch 60 loss: 0.0005576118477620184
batch 65 loss: 0.0005577713949605822
batch 70 loss: 0.0005576979485340417
batch 75 loss: 0.0005575893330387772
batch 80 loss: 0.0005576915689744055
batch 85 loss: 0.0005577903124503791
batch 90 loss: 0.0005577515927143395
batch 95 loss: 0.0005576336639933288
batch 100 loss: 0.0005575862945988774
batch 105 loss: 0.0005576598807238043
batch 110 loss: 0.000557594676502049
batch 115 loss: 0.0005576608702540398
batch 120 loss: 0.0005576517665758729
batch 125 loss: 0.0005576930823735892
batch 130 loss: 0.0005576445488259196
batch 135 loss: 0.0005575920804403722
batch 140 loss: 0.0005575836985372007
batch 145 loss: 0.0005576729192398489
batch 150 loss: 0.000557655154261738
batch 155 loss: 0.0005577894626185298
batch 160 loss: 0.0005576987517997622
batch 165 loss: 0.0005576958763413131
batch 170 loss: 0.000557725306134671
batch 175 loss: 0.0005576988216489554
batch 180 loss: 0.000557646038942039
batch 185 loss: 0.0005575969233177602
batch 190 loss: 0.0005577751668170095
batch 195 loss: 0.0005576832219958306
batch 200 loss: 0.0005577230243943631
batch 205 loss: 0.0005576132563874126
batch 210 loss: 0.0005575883551500738
batch 215 loss: 0.0005575992399826646
batch 220 loss: 0.0005576427676714957
batch 225 loss: 0.0005577003117650748
batch 230 loss: 0.0005576419527642429
batch 235 loss: 0.000557726772967726
batch 240 loss: 0.0005577886593528091
Training Loss: 0.0005576834099580689
Validation Loss: 0.0005576933996053413
Epoch 42:
batch 5 loss: 0.0005577362840995193
batch 10 loss: 0.0005577621865086258
batch 15 loss: 0.0005577743519097567
batch 20 loss: 0.0005576665746048093
batch 25 loss: 0.0005576677387580276
batch 30 loss: 0.0005575949791818858
batch 35 loss: 0.0005577018018811941
batch 40 loss: 0.0005577756557613612
batch 45 loss: 0.0005576498573645949
batch 50 loss: 0.0005576916504651308
batch 55 loss: 0.0005576386000029742
batch 60 loss: 0.0005576167954131961
batch 65 loss: 0.000557667319662869
batch 70 loss: 0.0005577498464845121
batch 75 loss: 0.0005576504277996719
batch 80 loss: 0.0005577639327384532
batch 85 loss: 0.0005575711606070399
batch 90 loss: 0.0005577605799771846
batch 95 loss: 0.0005576930707320571
batch 100 loss: 0.0005576787865720689
batch 105 loss: 0.0005576326977461577
batch 110 loss: 0.0005575560964643956
batch 115 loss: 0.0005576441413722933
batch 120 loss: 0.000557630939874798
batch 125 loss: 0.0005577177973464132
batch 130 loss: 0.00055767388548702
batch 135 loss: 0.0005576390540227294
batch 140 loss: 0.0005576455150730908
batch 145 loss: 0.0005576474475674331
batch 150 loss: 0.0005577195202931762
batch 155 loss: 0.0005576972849667073
batch 160 loss: 0.0005577237694524229
batch 165 loss: 0.0005575708230026067
batch 170 loss: 0.0005577303469181061
batch 175 loss: 0.000557791255414486
batch 180 loss: 0.0005576968076638877
batch 185 loss: 0.0005576594732701779
batch 190 loss: 0.0005576141411438584
batch 195 loss: 0.000557788647711277
batch 200 loss: 0.0005576982977800072
batch 205 loss: 0.0005576552241109312
batch 210 loss: 0.0005577174481004477
batch 215 loss: 0.0005575663642957806
batch 220 loss: 0.0005579143879003822
batch 225 loss: 0.0005576337338425219
batch 230 loss: 0.000557697773911059
batch 235 loss: 0.000557678914628923
batch 240 loss: 0.0005576516501605511
Training Loss: 0.0005576834383343036
Validation Loss: 0.0005576934655740236
Epoch 43:
batch 5 loss: 0.0005577555275522172
batch 10 loss: 0.000557710649445653
batch 15 loss: 0.0005576313473284244
batch 20 loss: 0.0005577538977377117
batch 25 loss: 0.0005577572504989803
batch 30 loss: 0.0005578166455961764
batch 35 loss: 0.0005576824070885778
batch 40 loss: 0.0005576172145083547
batch 45 loss: 0.0005576203460805118
batch 50 loss: 0.0005576330819167197
batch 55 loss: 0.0005576138035394251
batch 60 loss: 0.0005577150732278824
batch 65 loss: 0.000557671522255987
batch 70 loss: 0.0005577012663707138
batch 75 loss: 0.0005577291711233556
batch 80 loss: 0.0005577016971074044
batch 85 loss: 0.0005577179254032671
batch 90 loss: 0.0005576496361754835
batch 95 loss: 0.0005576495197601617
batch 100 loss: 0.0005576486815698445
batch 105 loss: 0.0005577657837420702
batch 110 loss: 0.0005577107658609748
batch 115 loss: 0.0005576966097578407
batch 120 loss: 0.0005576006136834621
batch 125 loss: 0.0005577739444561303
batch 130 loss: 0.0005576883908361196
batch 135 loss: 0.0005576686700806022
batch 140 loss: 0.0005577216390520335
batch 145 loss: 0.0005577536998316645
batch 150 loss: 0.0005576008930802345
batch 155 loss: 0.0005576153635047376
batch 160 loss: 0.0005576507654041051
batch 165 loss: 0.0005576698225922882
batch 170 loss: 0.0005576416151598096
batch 175 loss: 0.0005577146308496595
batch 180 loss: 0.000557625771034509
batch 185 loss: 0.0005576580064371228
batch 190 loss: 0.0005576902069151402
batch 195 loss: 0.0005576994502916932
batch 200 loss: 0.0005576171795837581
batch 205 loss: 0.0005577280302532017
batch 210 loss: 0.000557754433248192
batch 215 loss: 0.0005575732444413007
batch 220 loss: 0.0005576142459176481
batch 225 loss: 0.0005576683674007654
batch 230 loss: 0.0005576979252509773
batch 235 loss: 0.0005576903582550585
batch 240 loss: 0.0005577368545345962
Training Loss: 0.0005576834155363031
Validation Loss: 0.0005576934345299378
Epoch 44:
batch 5 loss: 0.0005577133153565228
batch 10 loss: 0.0005576579947955907
batch 15 loss: 0.0005576120340265334
batch 20 loss: 0.0005576191819272935
batch 25 loss: 0.0005576860276050866
batch 30 loss: 0.0005576337571255863
batch 35 loss: 0.0005577674019150436
batch 40 loss: 0.0005576592171564698
batch 45 loss: 0.0005578459240496159
batch 50 loss: 0.0005576380528509617
batch 55 loss: 0.0005576682044193149
batch 60 loss: 0.0005576455616392196
batch 65 loss: 0.0005576460738666355
batch 70 loss: 0.0005577056668698788
batch 75 loss: 0.0005576161900535225
batch 80 loss: 0.0005577247124165296
batch 85 loss: 0.0005577723262831569
batch 90 loss: 0.000557571742683649
batch 95 loss: 0.0005575929535552859
batch 100 loss: 0.0005577542819082737
batch 105 loss: 0.0005576476687565446
batch 110 loss: 0.0005575673072598875
batch 115 loss: 0.00055768700549379
batch 120 loss: 0.0005577025352977216
batch 125 loss: 0.0005576752359047532
batch 130 loss: 0.0005576873780228197
batch 135 loss: 0.000557716318871826
batch 140 loss: 0.0005576116614975035
batch 145 loss: 0.0005576211377047002
batch 150 loss: 0.0005576885188929737
batch 155 loss: 0.0005577005445957184
batch 160 loss: 0.000557746912818402
batch 165 loss: 0.0005577526637353003
batch 170 loss: 0.0005576540366746485
batch 175 loss: 0.0005577386473305523
batch 180 loss: 0.000557704595848918
batch 185 loss: 0.0005577363423071802
batch 190 loss: 0.0005576316267251969
batch 195 loss: 0.0005576467257924378
batch 200 loss: 0.000557617680169642
batch 205 loss: 0.0005577426636591554
batch 210 loss: 0.0005577339907176793
batch 215 loss: 0.0005576076335273683
batch 220 loss: 0.0005578301032073795
batch 225 loss: 0.0005577072035521269
batch 230 loss: 0.0005577038624323905
batch 235 loss: 0.0005576826515607536
batch 240 loss: 0.0005577314412221312
Training Loss: 0.0005576834315434098
Validation Loss: 0.0005576933976650859
Epoch 45:
batch 5 loss: 0.0005577118834480643
batch 10 loss: 0.0005576115334406495
batch 15 loss: 0.0005576493451371789
batch 20 loss: 0.0005578128970228135
batch 25 loss: 0.0005577442236244679
batch 30 loss: 0.0005576036055572331
batch 35 loss: 0.0005578321288339794
batch 40 loss: 0.000557690137065947
batch 45 loss: 0.0005578031530603766
batch 50 loss: 0.0005577890551649034
batch 55 loss: 0.0005576916970312596
batch 60 loss: 0.0005577506381087005
batch 65 loss: 0.0005576774477958679
batch 70 loss: 0.0005576577386818826
batch 75 loss: 0.0005576633731834591
batch 80 loss: 0.0005576195544563234
batch 85 loss: 0.0005575456074438989
batch 90 loss: 0.0005576343741267919
batch 95 loss: 0.0005576718482188881
batch 100 loss: 0.0005577717907726765
batch 105 loss: 0.000557714409660548
batch 110 loss: 0.0005576952942647039
batch 115 loss: 0.0005576717900112272
batch 120 loss: 0.0005575631861574948
batch 125 loss: 0.0005577376927249133
batch 130 loss: 0.0005578100797720253
batch 135 loss: 0.0005577214295044541
batch 140 loss: 0.0005575202172622085
batch 145 loss: 0.0005576036055572331
batch 150 loss: 0.0005577886127866805
batch 155 loss: 0.0005577416159212589
batch 160 loss: 0.0005576672730967403
batch 165 loss: 0.0005577349686063827
batch 170 loss: 0.0005576791823841632
batch 175 loss: 0.0005576164228841662
batch 180 loss: 0.0005576680763624608
batch 185 loss: 0.0005576415220275521
batch 190 loss: 0.0005577231175266206
batch 195 loss: 0.0005577412666752934
batch 200 loss: 0.0005576178315095604
batch 205 loss: 0.0005577384028583765
batch 210 loss: 0.0005576662137173116
batch 215 loss: 0.0005576845607720316
batch 220 loss: 0.0005575702176429332
batch 225 loss: 0.0005575726856477559
batch 230 loss: 0.0005576784489676357
batch 235 loss: 0.0005576928961090744
batch 240 loss: 0.0005576113471761346
Training Loss: 0.000557683424995048
Validation Loss: 0.0005576934093066181
Epoch 46:
batch 5 loss: 0.0005576727678999304
batch 10 loss: 0.0005576952244155109
batch 15 loss: 0.0005576689611189068
batch 20 loss: 0.0005576478899456561
batch 25 loss: 0.00055763233685866
batch 30 loss: 0.0005577053874731064
batch 35 loss: 0.0005577182862907648
batch 40 loss: 0.0005576991010457277
batch 45 loss: 0.0005576987052336336
batch 50 loss: 0.0005577043280936778
batch 55 loss: 0.0005577371921390295
batch 60 loss: 0.0005576706025749445
batch 65 loss: 0.0005576633382588625
batch 70 loss: 0.0005576232331804931
batch 75 loss: 0.0005577471223659813
batch 80 loss: 0.0005577422329224646
batch 85 loss: 0.0005576626863330603
batch 90 loss: 0.0005575938732363284
batch 95 loss: 0.0005576835130341351
batch 100 loss: 0.0005577170522883534
batch 105 loss: 0.0005577107076533139
batch 110 loss: 0.0005576606839895248
batch 115 loss: 0.0005577112548053265
batch 120 loss: 0.0005577428615652025
batch 125 loss: 0.0005576879018917679
batch 130 loss: 0.0005576364812441171
batch 135 loss: 0.000557716772891581
batch 140 loss: 0.0005577271804213524
batch 145 loss: 0.0005575661780312657
batch 150 loss: 0.0005576444673351943
batch 155 loss: 0.0005576316267251969
batch 160 loss: 0.0005576597526669502
batch 165 loss: 0.0005577172967605292
batch 170 loss: 0.0005576473660767079
batch 175 loss: 0.0005576812429353595
batch 180 loss: 0.0005576199386268854
batch 185 loss: 0.0005577989970333875
batch 190 loss: 0.000557704537641257
batch 195 loss: 0.0005576872499659657
batch 200 loss: 0.0005576768657192588
batch 205 loss: 0.0005576460156589746
batch 210 loss: 0.0005576676921918988
batch 215 loss: 0.000557625270448625
batch 220 loss: 0.000557638646569103
batch 225 loss: 0.0005576448515057564
batch 230 loss: 0.0005578101146966219
batch 235 loss: 0.0005577842355705797
batch 240 loss: 0.0005576746189035475
Training Loss: 0.0005576834300882183
Validation Loss: 0.0005576934325896824
Epoch 47:
batch 5 loss: 0.0005576364463195204
batch 10 loss: 0.0005577124422416091
batch 15 loss: 0.0005576614174060523
batch 20 loss: 0.0005577418254688382
batch 25 loss: 0.0005576545489020645
batch 30 loss: 0.0005576833966188133
batch 35 loss: 0.0005576717318035662
batch 40 loss: 0.00055779495742172
batch 45 loss: 0.0005576266325078904
batch 50 loss: 0.0005577221629209817
batch 55 loss: 0.0005576893338002265
batch 60 loss: 0.0005577000789344311
batch 65 loss: 0.0005577822565101087
batch 70 loss: 0.0005576848750934004
batch 75 loss: 0.0005576589261181653
batch 80 loss: 0.0005577035597525537
batch 85 loss: 0.0005576512194238603
batch 90 loss: 0.000557583209592849
batch 95 loss: 0.0005577707663178444
batch 100 loss: 0.0005576921859756113
batch 105 loss: 0.0005576637573540211
batch 110 loss: 0.000557731103617698
batch 115 loss: 0.000557604432106018
batch 120 loss: 0.0005576587980613112
batch 125 loss: 0.000557770614977926
batch 130 loss: 0.0005576169118285179
batch 135 loss: 0.0005576764699071646
batch 140 loss: 0.0005576652591116726
batch 145 loss: 0.000557731743901968
batch 150 loss: 0.000557722127996385
batch 155 loss: 0.0005577099742367864
batch 160 loss: 0.0005576797411777079
batch 165 loss: 0.0005576731637120247
batch 170 loss: 0.0005577837000600994
batch 175 loss: 0.0005576925002969801
batch 180 loss: 0.0005576750379987061
batch 185 loss: 0.0005577321397140622
batch 190 loss: 0.0005575947114266455
batch 195 loss: 0.000557626225054264
batch 200 loss: 0.0005576819879934191
batch 205 loss: 0.0005576953175477683
batch 210 loss: 0.0005577303585596382
batch 215 loss: 0.0005575312534347177
batch 220 loss: 0.0005577108473517001
batch 225 loss: 0.0005575964460149407
batch 230 loss: 0.0005576705094426871
batch 235 loss: 0.0005577158997766673
batch 240 loss: 0.0005576437222771346
Training Loss: 0.0005576834735014321
Validation Loss: 0.0005576934160975119
Epoch 48:
batch 5 loss: 0.0005576635943725705
batch 10 loss: 0.0005576579947955907
batch 15 loss: 0.000557637691963464
batch 20 loss: 0.0005576340714469552
batch 25 loss: 0.0005576589610427618
batch 30 loss: 0.0005576246068812907
batch 35 loss: 0.0005576099967584014
batch 40 loss: 0.0005576281575486064
batch 45 loss: 0.0005576175288297236
batch 50 loss: 0.0005575741291977465
batch 55 loss: 0.000557657063473016
batch 60 loss: 0.0005577208707109093
batch 65 loss: 0.0005578134441748261
batch 70 loss: 0.0005575475515797734
batch 75 loss: 0.0005577388452365995
batch 80 loss: 0.0005575954332016408
batch 85 loss: 0.0005576474475674331
batch 90 loss: 0.0005576704163104296
batch 95 loss: 0.000557721417862922
batch 100 loss: 0.0005577844916842878
batch 105 loss: 0.0005577684729360044
batch 110 loss: 0.000557691918220371
batch 115 loss: 0.000557711988221854
batch 120 loss: 0.000557670381385833
batch 125 loss: 0.0005577112780883909
batch 130 loss: 0.0005576197407208383
batch 135 loss: 0.0005577702773734927
batch 140 loss: 0.000557629147078842
batch 145 loss: 0.000557689880952239
batch 150 loss: 0.0005577064934186637
batch 155 loss: 0.0005576598457992076
batch 160 loss: 0.0005577543401159346
batch 165 loss: 0.0005577336531132459
batch 170 loss: 0.000557700521312654
batch 175 loss: 0.0005576825584284961
batch 180 loss: 0.0005578032461926341
batch 185 loss: 0.0005577567499130964
batch 190 loss: 0.0005577167379669845
batch 195 loss: 0.0005575550952926278
batch 200 loss: 0.0005577710922807455
batch 205 loss: 0.0005577333038672804
batch 210 loss: 0.0005576041061431169
batch 215 loss: 0.0005577460047788918
batch 220 loss: 0.0005576666444540024
batch 225 loss: 0.0005577111151069403
batch 230 loss: 0.000557729450520128
batch 235 loss: 0.0005576665280386806
batch 240 loss: 0.0005576419178396463
Training Loss: 0.0005576834625874957
Validation Loss: 0.0005576934636337683
Epoch 49:
batch 5 loss: 0.0005576455383561552
batch 10 loss: 0.0005578408483415842
batch 15 loss: 0.0005576483090408146
batch 20 loss: 0.0005578177515417337
batch 25 loss: 0.0005577261908911169
batch 30 loss: 0.0005576105206273497
batch 35 loss: 0.0005577646777965128
batch 40 loss: 0.0005575194372795522
batch 45 loss: 0.0005576171213760972
batch 50 loss: 0.0005576589261181653
batch 55 loss: 0.0005576691473834217
batch 60 loss: 0.0005577375181019306
batch 65 loss: 0.0005575962015427649
batch 70 loss: 0.0005577070522122085
batch 75 loss: 0.000557777809444815
batch 80 loss: 0.000557730218861252
batch 85 loss: 0.0005576260504312813
batch 90 loss: 0.0005576772731728852
batch 95 loss: 0.0005576125113293529
batch 100 loss: 0.0005576720461249352
batch 105 loss: 0.0005576520459726452
batch 110 loss: 0.0005576968309469521
batch 115 loss: 0.0005575847695581615
batch 120 loss: 0.000557582569308579
batch 125 loss: 0.0005576789728365839
batch 130 loss: 0.0005576766678132116
batch 135 loss: 0.0005577182164415717
batch 140 loss: 0.0005576079012826086
batch 145 loss: 0.0005577907897531987
batch 150 loss: 0.0005576548865064979
batch 155 loss: 0.0005577817442826927
batch 160 loss: 0.0005576941301114857
batch 165 loss: 0.0005577554693445563
batch 170 loss: 0.0005577242118306458
batch 175 loss: 0.0005578022100962698
batch 180 loss: 0.0005576927098445595
batch 185 loss: 0.0005576597410254181
batch 190 loss: 0.000557651452254504
batch 195 loss: 0.000557711673900485
batch 200 loss: 0.0005576043156906962
batch 205 loss: 0.000557681790087372
batch 210 loss: 0.0005576964817009866
batch 215 loss: 0.0005577165051363408
batch 220 loss: 0.0005576304160058498
batch 225 loss: 0.0005576461786404252
batch 230 loss: 0.0005575959337875247
batch 235 loss: 0.0005576917668804526
batch 240 loss: 0.0005577705334872008
Training Loss: 0.0005576834596771126
Validation Loss: 0.0005576934219182779
Epoch 50:
batch 5 loss: 0.000557766854763031
batch 10 loss: 0.0005576794850639999
batch 15 loss: 0.0005576626164838672
batch 20 loss: 0.0005575065617449582
batch 25 loss: 0.0005576382158324122
batch 30 loss: 0.0005576685653068125
batch 35 loss: 0.0005577562144026161
batch 40 loss: 0.0005576992291025818
batch 45 loss: 0.0005576082156039774
batch 50 loss: 0.0005577466450631619
batch 55 loss: 0.0005575331742875278
batch 60 loss: 0.000557671335991472
batch 65 loss: 0.0005575570627115667
batch 70 loss: 0.0005575562245212495
batch 75 loss: 0.0005575563292950392
batch 80 loss: 0.0005577265983447433
batch 85 loss: 0.0005577221512794494
batch 90 loss: 0.0005577319068834185
batch 95 loss: 0.0005577310919761657
batch 100 loss: 0.0005577261792495847
batch 105 loss: 0.0005575844319537282
batch 110 loss: 0.0005576710449531674
batch 115 loss: 0.000557687203399837
batch 120 loss: 0.0005577494972385466
batch 125 loss: 0.000557839113753289
batch 130 loss: 0.0005576699390076101
batch 135 loss: 0.0005576946190558374
batch 140 loss: 0.0005578353418968617
batch 145 loss: 0.0005577142350375652
batch 150 loss: 0.000557679298799485
batch 155 loss: 0.0005577252246439457
batch 160 loss: 0.0005577243398874998
batch 165 loss: 0.0005576428724452853
batch 170 loss: 0.0005576725583523512
batch 175 loss: 0.0005577905219979584
batch 180 loss: 0.0005576680530793964
batch 185 loss: 0.000557821593247354
batch 190 loss: 0.0005577508825808763
batch 195 loss: 0.0005577333737164736
batch 200 loss: 0.000557639985345304
batch 205 loss: 0.0005576549679972232
batch 210 loss: 0.0005576963070780039
batch 215 loss: 0.0005577079369686544
batch 220 loss: 0.0005576344905421138
batch 225 loss: 0.000557552243117243
batch 230 loss: 0.0005576737690716982
batch 235 loss: 0.0005575692863203585
batch 240 loss: 0.0005577790085226298
Training Loss: 0.0005576834749566236
Validation Loss: 0.000557693403485852
Epoch 51:
batch 5 loss: 0.0005577425239607692
batch 10 loss: 0.0005576172494329512
batch 15 loss: 0.0005577676929533482
batch 20 loss: 0.0005576609051786363
batch 25 loss: 0.0005576119991019368
batch 30 loss: 0.000557594164274633
batch 35 loss: 0.0005577397299930453
batch 40 loss: 0.0005575916613452137
batch 45 loss: 0.00055766279110685
batch 50 loss: 0.000557704211678356
batch 55 loss: 0.0005577520816586912
batch 60 loss: 0.0005575593910180032
batch 65 loss: 0.0005576136754825711
batch 70 loss: 0.0005576344090513885
batch 75 loss: 0.0005576784024015069
batch 80 loss: 0.0005576218478381634
batch 85 loss: 0.0005576895899139344
batch 90 loss: 0.0005576501484028995
batch 95 loss: 0.0005576012306846678
batch 100 loss: 0.0005576752126216888
batch 105 loss: 0.0005576827330514789
batch 110 loss: 0.0005577484611421823
batch 115 loss: 0.0005577729549258948
batch 120 loss: 0.0005575769697315991
batch 125 loss: 0.0005576664698310196
batch 130 loss: 0.000557729066349566
batch 135 loss: 0.000557745061814785
batch 140 loss: 0.0005577810807153583
batch 145 loss: 0.0005576153984293341
batch 150 loss: 0.0005576845840550959
batch 155 loss: 0.0005577199975959956
batch 160 loss: 0.0005577511503361166
batch 165 loss: 0.0005576915922574699
batch 170 loss: 0.0005576740368269384
batch 175 loss: 0.0005577696254476904
batch 180 loss: 0.0005577357485890388
batch 185 loss: 0.0005577176227234304
batch 190 loss: 0.0005576902301982045
batch 195 loss: 0.0005576345720328391
batch 200 loss: 0.0005576583906076848
batch 205 loss: 0.0005576883326284587
batch 210 loss: 0.0005576145718805492
batch 215 loss: 0.000557827774900943
batch 220 loss: 0.0005577952251769602
batch 225 loss: 0.0005576422321610153
batch 230 loss: 0.0005575962481088936
batch 235 loss: 0.0005576527561061084
batch 240 loss: 0.0005577746429480612
Training Loss: 0.000557683467680666
Validation Loss: 0.0005576934141572565
Epoch 52:
batch 5 loss: 0.000557661906350404
batch 10 loss: 0.0005576881696470082
batch 15 loss: 0.000557654770091176
batch 20 loss: 0.000557751301676035
batch 25 loss: 0.0005575337680056691
batch 30 loss: 0.0005575462244451046
batch 35 loss: 0.000557683629449457
batch 40 loss: 0.0005576357012614608
batch 45 loss: 0.0005576526047661901
batch 50 loss: 0.0005578057025559247
batch 55 loss: 0.0005577043397352099
batch 60 loss: 0.0005576853407546877
batch 65 loss: 0.0005577756906859577
batch 70 loss: 0.0005575833609327673
batch 75 loss: 0.00055765068391338
batch 80 loss: 0.0005577101954258978
batch 85 loss: 0.0005576728028245271
batch 90 loss: 0.0005576319294050336
batch 95 loss: 0.0005577587173320353
batch 100 loss: 0.0005576553521677852
batch 105 loss: 0.0005576725234277546
batch 110 loss: 0.0005577274248935282
batch 115 loss: 0.0005577810807153583
batch 120 loss: 0.0005577013012953103
batch 125 loss: 0.0005577067960985005
batch 130 loss: 0.0005577280069701373
batch 135 loss: 0.0005577479605562985
batch 140 loss: 0.0005576506606303156
batch 145 loss: 0.00055767388548702
batch 150 loss: 0.0005577069008722901
batch 155 loss: 0.0005577242001891137
batch 160 loss: 0.0005577137810178101
batch 165 loss: 0.0005576448747888207
batch 170 loss: 0.0005577100906521082
batch 175 loss: 0.0005575738032348454
batch 180 loss: 0.0005576598807238043
batch 185 loss: 0.0005576590425334871
batch 190 loss: 0.0005577251082286239
batch 195 loss: 0.0005576435825787485
batch 200 loss: 0.0005576582043431699
batch 205 loss: 0.000557715876493603
batch 210 loss: 0.0005575562361627817
batch 215 loss: 0.0005576904397457838
batch 220 loss: 0.0005576488678343594
batch 225 loss: 0.0005577840143814683
batch 230 loss: 0.0005577458068728447
batch 235 loss: 0.0005576764000579715
batch 240 loss: 0.0005577365169301629
Training Loss: 0.0005576834470654527
Validation Loss: 0.0005576934141572565
Epoch 53:
batch 5 loss: 0.0005575510673224926
batch 10 loss: 0.0005576161784119904
batch 15 loss: 0.0005575831746682525
batch 20 loss: 0.0005577792064286769
batch 25 loss: 0.0005576407187618315
batch 30 loss: 0.0005578120122663677
batch 35 loss: 0.0005577702191658318
batch 40 loss: 0.0005576408235356212
batch 45 loss: 0.0005577298230491579
batch 50 loss: 0.0005577750387601554
batch 55 loss: 0.0005577365169301629
batch 60 loss: 0.0005576484487392009
batch 65 loss: 0.0005576544092036784
batch 70 loss: 0.0005577435134910047
batch 75 loss: 0.0005575692630372941
batch 80 loss: 0.0005576132563874126
batch 85 loss: 0.0005576377850957214
batch 90 loss: 0.0005577076924964785
batch 95 loss: 0.0005577755277045071
batch 100 loss: 0.0005577675183303654
batch 105 loss: 0.00055772002087906
batch 110 loss: 0.0005576144787482918
batch 115 loss: 0.0005576040362939239
batch 120 loss: 0.0005576541647315025
batch 125 loss: 0.0005576333263888955
batch 130 loss: 0.0005575393442995846
batch 135 loss: 0.0005577642703428864
batch 140 loss: 0.0005577225121669472
batch 145 loss: 0.0005576868657954037
batch 150 loss: 0.0005576743395067751
batch 155 loss: 0.000557649414986372
batch 160 loss: 0.0005577510455623269
batch 165 loss: 0.0005577754811383784
batch 170 loss: 0.0005577668664045632
batch 175 loss: 0.0005576886469498277
batch 180 loss: 0.0005577383446507156
batch 185 loss: 0.0005577848758548498
batch 190 loss: 0.0005576189490966499
batch 195 loss: 0.0005577785079367459
batch 200 loss: 0.0005577107775025069
batch 205 loss: 0.0005575871444307267
batch 210 loss: 0.0005576963070780039
batch 215 loss: 0.0005575892515480519
batch 220 loss: 0.0005576979834586382
batch 225 loss: 0.0005577099393121899
batch 230 loss: 0.0005576212191954255
batch 235 loss: 0.0005576288676820695
batch 240 loss: 0.0005576472613029182
Training Loss: 0.0005576834674381341
Validation Loss: 0.0005576934374403209
Epoch 54:
batch 5 loss: 0.0005576216266490519
batch 10 loss: 0.0005577616626396775
batch 15 loss: 0.0005576588329859078
batch 20 loss: 0.0005576175753958523
batch 25 loss: 0.0005577084375545382
batch 30 loss: 0.0005576698342338205
batch 35 loss: 0.0005576701369136572
batch 40 loss: 0.0005577584845013917
batch 45 loss: 0.0005577771458774805
batch 50 loss: 0.0005577564821578562
batch 55 loss: 0.0005578368320129812
batch 60 loss: 0.0005576151655986905
batch 65 loss: 0.0005576091702096164
batch 70 loss: 0.000557607423979789
batch 75 loss: 0.0005576270865276456
batch 80 loss: 0.0005575683666393161
batch 85 loss: 0.0005576753290370106
batch 90 loss: 0.0005578037584200501
batch 95 loss: 0.0005577738978900015
batch 100 loss: 0.0005577428382821381
batch 105 loss: 0.0005576343741267919
batch 110 loss: 0.000557649799156934
batch 115 loss: 0.0005576778668910265
batch 120 loss: 0.0005576787167228759
batch 125 loss: 0.000557611824478954
batch 130 loss: 0.0005576429422944785
batch 135 loss: 0.0005576909519731998
batch 140 loss: 0.0005578304873779417
batch 145 loss: 0.0005576152936555446
batch 150 loss: 0.0005577005562372505
batch 155 loss: 0.0005575519055128098
batch 160 loss: 0.0005577437579631806
batch 165 loss: 0.0005576788447797298
batch 170 loss: 0.0005577296367846429
batch 175 loss: 0.0005576442461460828
batch 180 loss: 0.0005577117903158068
batch 185 loss: 0.0005576791591010988
batch 190 loss: 0.0005576505558565259
batch 195 loss: 0.0005575665039941668
batch 200 loss: 0.0005576173309236765
batch 205 loss: 0.0005575393442995846
batch 210 loss: 0.000557648716494441
batch 215 loss: 0.0005576989729888737
batch 220 loss: 0.0005576983094215393
batch 225 loss: 0.0005577676114626229
batch 230 loss: 0.0005577642936259508
batch 235 loss: 0.0005577553296461701
batch 240 loss: 0.0005577672622166574
Training Loss: 0.0005576834681657298
Validation Loss: 0.0005576935965412607
Epoch 55:
batch 5 loss: 0.0005576587282121181
batch 10 loss: 0.0005576538853347302
batch 15 loss: 0.0005577160976827144
batch 20 loss: 0.0005576778552494943
batch 25 loss: 0.0005576677620410919
batch 30 loss: 0.0005577312549576163
batch 35 loss: 0.0005576795083470643
batch 40 loss: 0.0005576020921580493
batch 45 loss: 0.0005576503928750754
batch 50 loss: 0.0005577550386078656
batch 55 loss: 0.0005577488569542766
batch 60 loss: 0.0005576848750934004
batch 65 loss: 0.0005577135016210377
batch 70 loss: 0.0005577022209763526
batch 75 loss: 0.0005576221272349357
batch 80 loss: 0.0005577499163337052
batch 85 loss: 0.0005575583432801068
batch 90 loss: 0.0005576912080869079
batch 95 loss: 0.0005578052834607661
batch 100 loss: 0.0005577589850872755
batch 105 loss: 0.0005576689960435033
batch 110 loss: 0.0005576366907916963
batch 115 loss: 0.0005577067146077752
batch 120 loss: 0.0005577893578447401
batch 125 loss: 0.0005575335351750254
batch 130 loss: 0.0005576092749834061
batch 135 loss: 0.0005576180294156075
batch 140 loss: 0.0005575934308581054
batch 145 loss: 0.0005576984607614577
batch 150 loss: 0.0005577739211730659
batch 155 loss: 0.0005576178315095604
batch 160 loss: 0.0005576510797254741
batch 165 loss: 0.0005576390074566006
batch 170 loss: 0.0005577120464295149
batch 175 loss: 0.0005576717550866306
batch 180 loss: 0.0005576403113082051
batch 185 loss: 0.0005577869014814496
batch 190 loss: 0.000557613733690232
batch 195 loss: 0.0005576610798016191
batch 200 loss: 0.0005576612195000053
batch 205 loss: 0.0005577694857493043
batch 210 loss: 0.0005577833275310695
batch 215 loss: 0.0005576518946327269
batch 220 loss: 0.0005576365278102457
batch 225 loss: 0.0005577347590588033
batch 230 loss: 0.0005577856325544416
batch 235 loss: 0.0005576687282882631
batch 240 loss: 0.0005576656199991703
Training Loss: 0.0005576834851429642
Validation Loss: 0.0005576935616166641
Epoch 56:
batch 5 loss: 0.0005577011499553919
batch 10 loss: 0.0005576829775236547
batch 15 loss: 0.0005575766670517623
batch 20 loss: 0.0005576317897066474
batch 25 loss: 0.0005577868665568531
batch 30 loss: 0.0005575800780206919
batch 35 loss: 0.0005577035481110215
batch 40 loss: 0.0005576945492066443
batch 45 loss: 0.0005577302421443164
batch 50 loss: 0.0005576894036494195
batch 55 loss: 0.0005576726631261409
batch 60 loss: 0.0005576440948061645
batch 65 loss: 0.0005577131290920079
batch 70 loss: 0.0005575796705670655
batch 75 loss: 0.0005577170639298856
batch 80 loss: 0.0005577450501732528
batch 85 loss: 0.000557720789220184
batch 90 loss: 0.0005575579591095448
batch 95 loss: 0.0005577155272476376
batch 100 loss: 0.0005576552823185921
batch 105 loss: 0.0005577709292992949
batch 110 loss: 0.0005577400792390108
batch 115 loss: 0.0005576741532422602
batch 120 loss: 0.0005577336880378426
batch 125 loss: 0.0005576450028456747
batch 130 loss: 0.00055778285022825
batch 135 loss: 0.0005576236289925873
batch 140 loss: 0.0005576786235906184
batch 145 loss: 0.0005575734190642834
batch 150 loss: 0.000557768577709794
batch 155 loss: 0.0005576867028139531
batch 160 loss: 0.0005577164469286799
batch 165 loss: 0.0005576957715675235
batch 170 loss: 0.0005576276918873191
batch 175 loss: 0.0005576764582656324
batch 180 loss: 0.0005576537572778761
batch 185 loss: 0.000557591684628278
batch 190 loss: 0.0005576942930929363
batch 195 loss: 0.0005577686126343906
batch 200 loss: 0.000557684141676873
batch 205 loss: 0.0005576806725002825
batch 210 loss: 0.0005577894975431263
batch 215 loss: 0.0005576372961513699
batch 220 loss: 0.000557630555704236
batch 225 loss: 0.0005576710100285709
batch 230 loss: 0.0005577839212492109
batch 235 loss: 0.000557608949020505
batch 240 loss: 0.00055771938059479
Training Loss: 0.000557683464527751
Validation Loss: 0.0005576935509452596
Epoch 57:
batch 5 loss: 0.0005577610805630684
batch 10 loss: 0.0005577995791099966
batch 15 loss: 0.000557690137065947
batch 20 loss: 0.0005577429197728634
batch 25 loss: 0.0005576966213993728
batch 30 loss: 0.0005577196250669658
batch 35 loss: 0.0005576865165494383
batch 40 loss: 0.000557704851962626
batch 45 loss: 0.000557666877284646
batch 50 loss: 0.0005576223717071116
batch 55 loss: 0.0005576798692345619
batch 60 loss: 0.0005577539559453726
batch 65 loss: 0.00055758620146662
batch 70 loss: 0.0005576411494985223
batch 75 loss: 0.0005576915689744055
batch 80 loss: 0.0005576559924520552
batch 85 loss: 0.0005576899973675608
batch 90 loss: 0.0005577416624873877
batch 95 loss: 0.0005577037110924721
batch 100 loss: 0.0005576347233727574
batch 105 loss: 0.0005577369476668537
batch 110 loss: 0.0005577564472332597
batch 115 loss: 0.0005576730356551707
batch 120 loss: 0.0005576319876126945
batch 125 loss: 0.0005577845498919487
batch 130 loss: 0.0005576498806476593
batch 135 loss: 0.0005576768424361944
batch 140 loss: 0.000557587156072259
batch 145 loss: 0.0005576580879278481
batch 150 loss: 0.000557606271468103
batch 155 loss: 0.0005576834781095385
batch 160 loss: 0.0005576211726292968
batch 165 loss: 0.0005577548174187541
batch 170 loss: 0.0005577115691266954
batch 175 loss: 0.0005577213014476001
batch 180 loss: 0.0005577531293965877
batch 185 loss: 0.0005576523835770786
batch 190 loss: 0.0005576521274633705
batch 195 loss: 0.0005576192517764867
batch 200 loss: 0.00055762481642887
batch 205 loss: 0.0005576854455284774
batch 210 loss: 0.0005576333263888955
batch 215 loss: 0.0005577746662311256
batch 220 loss: 0.00055768700549379
batch 225 loss: 0.0005575511022470891
batch 230 loss: 0.0005577639793045818
batch 235 loss: 0.0005577098112553358
batch 240 loss: 0.0005575751303695143
Training Loss: 0.000557683440274559
Validation Loss: 0.0005576935169907907
Epoch 58:
batch 5 loss: 0.0005576470633968711
batch 10 loss: 0.0005576343275606633
batch 15 loss: 0.0005576244671829045
batch 20 loss: 0.0005577777279540897
batch 25 loss: 0.0005577872041612863
batch 30 loss: 0.0005576716153882444
batch 35 loss: 0.0005577122210524976
batch 40 loss: 0.0005577044328674674
batch 45 loss: 0.0005576606490649283
batch 50 loss: 0.0005576946306973696
batch 55 loss: 0.0005576305091381073
batch 60 loss: 0.0005576454219408334
batch 65 loss: 0.0005577106261625886
batch 70 loss: 0.0005575948860496282
batch 75 loss: 0.0005576152238063514
batch 80 loss: 0.0005577510339207948
batch 85 loss: 0.0005577470874413848
batch 90 loss: 0.0005576900555752217
batch 95 loss: 0.000557639414910227
batch 100 loss: 0.0005577449803240598
batch 105 loss: 0.00055759601527825
batch 110 loss: 0.0005576892988756299
batch 115 loss: 0.0005576505325734615
batch 120 loss: 0.0005577535834163427
batch 125 loss: 0.0005577024887315929
batch 130 loss: 0.000557669170666486
batch 135 loss: 0.0005576574243605137
batch 140 loss: 0.0005576910101808607
batch 145 loss: 0.000557590217795223
batch 150 loss: 0.0005576854920946062
batch 155 loss: 0.0005577260279096663
batch 160 loss: 0.00055775634245947
batch 165 loss: 0.0005576167604885995
batch 170 loss: 0.000557702628429979
batch 175 loss: 0.0005577330943197012
batch 180 loss: 0.0005577315110713244
batch 185 loss: 0.0005576736060902476
batch 190 loss: 0.0005576238152571022
batch 195 loss: 0.0005575934774242342
batch 200 loss: 0.0005576712777838111
batch 205 loss: 0.0005576109047979117
batch 210 loss: 0.0005577200441621244
batch 215 loss: 0.0005578120704740286
batch 220 loss: 0.0005576461204327643
batch 225 loss: 0.0005577121512033046
batch 230 loss: 0.0005576588679105044
batch 235 loss: 0.000557708612177521
batch 240 loss: 0.0005577416624873877
Training Loss: 0.0005576834955718368
Validation Loss: 0.0005576938206407552
Epoch 59:
batch 5 loss: 0.0005576732801273465
batch 10 loss: 0.0005576736410148441
batch 15 loss: 0.0005576870986260474
batch 20 loss: 0.0005577256437391042
batch 25 loss: 0.0005576241877861321
batch 30 loss: 0.0005576614756137132
batch 35 loss: 0.0005577239790000021
batch 40 loss: 0.000557577412109822
batch 45 loss: 0.0005577064235694707
batch 50 loss: 0.0005577784264460206
batch 55 loss: 0.0005577094387263059
batch 60 loss: 0.0005576843046583235
batch 65 loss: 0.0005576625233516097
batch 70 loss: 0.0005577267264015972
batch 75 loss: 0.0005576627212576568
batch 80 loss: 0.0005576151888817549
batch 85 loss: 0.0005577449919655919
batch 90 loss: 0.0005576723255217075
batch 95 loss: 0.0005576158291660249
batch 100 loss: 0.0005577293573878706
batch 105 loss: 0.0005576264229603112
batch 110 loss: 0.0005577100091613829
batch 115 loss: 0.0005577813601121307
batch 120 loss: 0.0005576909636147321
batch 125 loss: 0.0005576104740612209
batch 130 loss: 0.0005576860392466187
batch 135 loss: 0.0005575939663685858
batch 140 loss: 0.0005575465969741344
batch 145 loss: 0.000557697913609445
batch 150 loss: 0.0005577831878326833
batch 155 loss: 0.0005577825591899455
batch 160 loss: 0.000557631952688098
batch 165 loss: 0.0005577347474172711
batch 170 loss: 0.0005575954099185764
batch 175 loss: 0.000557708356063813
batch 180 loss: 0.0005576597875915467
batch 185 loss: 0.000557706702966243
batch 190 loss: 0.0005577352712862194
batch 195 loss: 0.0005577017669565976
batch 200 loss: 0.0005577846663072705
batch 205 loss: 0.0005576628143899142
batch 210 loss: 0.0005576296709477902
batch 215 loss: 0.0005576583091169596
batch 220 loss: 0.0005576855153776705
batch 225 loss: 0.0005577176692895591
batch 230 loss: 0.0005578123498708009
batch 235 loss: 0.0005575210321694613
batch 240 loss: 0.0005576956784352661
Training Loss: 0.0005576834618598998
Validation Loss: 0.0005576934063962351
Epoch 60:
batch 5 loss: 0.0005576413706876338
batch 10 loss: 0.0005577027797698975
batch 15 loss: 0.0005577511736191809
batch 20 loss: 0.0005577269592322409
batch 25 loss: 0.0005576128023676574
batch 30 loss: 0.0005577038740739226
batch 35 loss: 0.0005577513831667602
batch 40 loss: 0.0005576458526775241
batch 45 loss: 0.0005577410804107786
batch 50 loss: 0.0005576739669777453
batch 55 loss: 0.0005575910792686046
batch 60 loss: 0.0005577066331170499
batch 65 loss: 0.0005576352123171091
batch 70 loss: 0.0005577729200012982
batch 75 loss: 0.0005575869581662118
batch 80 loss: 0.0005575903807766736
batch 85 loss: 0.0005576366791501641
batch 90 loss: 0.0005577093339525163
batch 95 loss: 0.0005577929667197167
batch 100 loss: 0.0005577547359280288
batch 105 loss: 0.0005576990195550024
batch 110 loss: 0.000557642022613436
batch 115 loss: 0.0005577789037488401
batch 120 loss: 0.000557754433248192
batch 125 loss: 0.0005576272960752249
batch 130 loss: 0.00055763985728845
batch 135 loss: 0.000557676237076521
batch 140 loss: 0.0005576986353844404
batch 145 loss: 0.0005576507770456374
batch 150 loss: 0.0005576821858994663
batch 155 loss: 0.0005577691830694675
batch 160 loss: 0.0005576937808655202
batch 165 loss: 0.0005576857132837176
batch 170 loss: 0.0005577082163654267
batch 175 loss: 0.0005576648749411106
batch 180 loss: 0.0005576387513428927
batch 185 loss: 0.0005575831048190594
batch 190 loss: 0.0005576940602622926
batch 195 loss: 0.0005576654570177197
batch 200 loss: 0.0005576935480348765
batch 205 loss: 0.0005577446892857552
batch 210 loss: 0.0005576330935582518
batch 215 loss: 0.0005576251307502389
batch 220 loss: 0.0005576417781412602
batch 225 loss: 0.0005577397649176418
batch 230 loss: 0.0005576662952080369
batch 235 loss: 0.0005577725358307361
batch 240 loss: 0.0005576081573963165
Training Loss: 0.0005576834509459634
Validation Loss: 0.0005576934238585333
Epoch 61:
batch 5 loss: 0.0005576370400376618
batch 10 loss: 0.0005577316507697106
batch 15 loss: 0.0005576479365117848
batch 20 loss: 0.0005576803465373814
batch 25 loss: 0.0005576648749411106
batch 30 loss: 0.0005578086827881634
batch 35 loss: 0.0005577040836215019
batch 40 loss: 0.0005575684714131057
batch 45 loss: 0.0005575684597715735
batch 50 loss: 0.0005575523828156292
batch 55 loss: 0.0005575792049057782
batch 60 loss: 0.0005577357369475067
batch 65 loss: 0.0005577313248068094
batch 70 loss: 0.0005576707655563951
batch 75 loss: 0.0005577091244049371
batch 80 loss: 0.0005576410330832005
batch 85 loss: 0.0005576291121542453
batch 90 loss: 0.000557656807359308
batch 95 loss: 0.0005576465395279228
batch 100 loss: 0.0005578475771471858
batch 105 loss: 0.0005577027681283653
batch 110 loss: 0.0005576770636253059
batch 115 loss: 0.0005577780772000551
batch 120 loss: 0.0005577763775363564
batch 125 loss: 0.0005577620817348361
batch 130 loss: 0.000557773478794843
batch 135 loss: 0.0005576248397119344
batch 140 loss: 0.0005576890311203897
batch 145 loss: 0.0005575264221988618
batch 150 loss: 0.0005576225812546909
batch 155 loss: 0.0005577454576268792
batch 160 loss: 0.0005577082047238946
batch 165 loss: 0.0005578286247327924
batch 170 loss: 0.0005575891933403909
batch 175 loss: 0.0005577685544267297
batch 180 loss: 0.0005575874703936279
batch 185 loss: 0.0005575788673013449
batch 190 loss: 0.0005577017553150653
batch 195 loss: 0.0005578106152825058
batch 200 loss: 0.0005577035597525537
batch 205 loss: 0.0005576400784775615
batch 210 loss: 0.0005576659343205393
batch 215 loss: 0.0005577659467235208
batch 220 loss: 0.0005576203111559153
batch 225 loss: 0.0005577015923336148
batch 230 loss: 0.00055765884462744
batch 235 loss: 0.0005577273084782064
batch 240 loss: 0.0005576595314778388
Training Loss: 0.0005576834526436869
Validation Loss: 0.000557693427739044
Epoch 62:
batch 5 loss: 0.0005576354800723493
batch 10 loss: 0.0005576223367825151
batch 15 loss: 0.0005576220341026783
batch 20 loss: 0.0005577748059295117
batch 25 loss: 0.0005577255506068468
batch 30 loss: 0.0005576087278313935
batch 35 loss: 0.0005576711031608283
batch 40 loss: 0.0005577252013608813
batch 45 loss: 0.0005577164469286799
batch 50 loss: 0.0005575817544013262
batch 55 loss: 0.0005577639094553888
batch 60 loss: 0.0005576532916165888
batch 65 loss: 0.0005577890085987746
batch 70 loss: 0.0005576249794103205
batch 75 loss: 0.0005576813244260847
batch 80 loss: 0.0005577206145972013
batch 85 loss: 0.0005578117328695953
batch 90 loss: 0.0005576838739216328
batch 95 loss: 0.0005577005795203149
batch 100 loss: 0.0005577803705818952
batch 105 loss: 0.0005576959811151028
batch 110 loss: 0.0005576744326390326
batch 115 loss: 0.000557819230016321
batch 120 loss: 0.0005576845840550959
batch 125 loss: 0.0005577173898927868
batch 130 loss: 0.0005577728152275085
batch 135 loss: 0.000557674712035805
batch 140 loss: 0.0005576476338319481
batch 145 loss: 0.0005577011266723275
batch 150 loss: 0.0005577130359597504
batch 155 loss: 0.0005575228831730783
batch 160 loss: 0.0005576889612711966
batch 165 loss: 0.0005577247124165296
batch 170 loss: 0.0005576210794970393
batch 175 loss: 0.0005576508003287018
batch 180 loss: 0.0005577136063948274
batch 185 loss: 0.0005575622082687915
batch 190 loss: 0.0005577805917710066
batch 195 loss: 0.0005576685187406838
batch 200 loss: 0.0005576724302954972
batch 205 loss: 0.000557629531249404
batch 210 loss: 0.0005576618015766144
batch 215 loss: 0.0005576348630711436
batch 220 loss: 0.0005575412069447339
batch 225 loss: 0.0005577237927354873
batch 230 loss: 0.0005577253527007997
batch 235 loss: 0.0005576777970418334
batch 240 loss: 0.0005576106370426715
Training Loss: 0.0005576834342112609
Validation Loss: 0.000557693568407558
Epoch 63:
batch 5 loss: 0.0005576545489020645
batch 10 loss: 0.0005575917894020677
batch 15 loss: 0.0005576170748099685
batch 20 loss: 0.0005577116273343564
batch 25 loss: 0.0005576826049946248
batch 30 loss: 0.000557619717437774
batch 35 loss: 0.0005576909286901354
batch 40 loss: 0.0005576038733124733
batch 45 loss: 0.000557630485855043
batch 50 loss: 0.0005577731062658131
batch 55 loss: 0.0005577071802690625
batch 60 loss: 0.0005576834199018776
batch 65 loss: 0.0005576572963036597
batch 70 loss: 0.0005577328382059931
batch 75 loss: 0.000557742826640606
batch 80 loss: 0.0005576890776865185
batch 85 loss: 0.0005576655035838485
batch 90 loss: 0.0005576600087806582
batch 95 loss: 0.0005575903807766736
batch 100 loss: 0.0005578062147833407
batch 105 loss: 0.0005577360047027468
batch 110 loss: 0.000557679811026901
batch 115 loss: 0.000557689112611115
batch 120 loss: 0.000557526876218617
batch 125 loss: 0.0005577650503255427
batch 130 loss: 0.0005577091360464692
batch 135 loss: 0.0005576888681389392
batch 140 loss: 0.00055755281355232
batch 145 loss: 0.0005577583215199411
batch 150 loss: 0.0005576321738772095
batch 155 loss: 0.0005577071337029338
batch 160 loss: 0.0005575890536420047
batch 165 loss: 0.0005577066214755178
batch 170 loss: 0.0005576438736170531
batch 175 loss: 0.0005577053758315742
batch 180 loss: 0.0005576804280281067
batch 185 loss: 0.000557760486844927
batch 190 loss: 0.0005577483214437961
batch 195 loss: 0.0005577494041062892
batch 200 loss: 0.0005576126044616103
batch 205 loss: 0.0005576634081080556
batch 210 loss: 0.000557620485778898
batch 215 loss: 0.0005577391711995006
batch 220 loss: 0.000557580788154155
batch 225 loss: 0.0005577160278335214
batch 230 loss: 0.0005577607313171029
batch 235 loss: 0.0005578005919232964
batch 240 loss: 0.0005577721865847706
Training Loss: 0.0005576834451251973
Validation Loss: 0.0005576934384104485
Epoch 64:
batch 5 loss: 0.0005575289949774742
batch 10 loss: 0.0005577640258707106
batch 15 loss: 0.0005578730488196015
batch 20 loss: 0.0005577193340286612
batch 25 loss: 0.0005577148869633675
batch 30 loss: 0.0005577871692366898
batch 35 loss: 0.0005576917785219849
batch 40 loss: 0.0005577110219746828
batch 45 loss: 0.0005577307543717325
batch 50 loss: 0.0005575074232183397
batch 55 loss: 0.0005576349329203367
batch 60 loss: 0.000557702942751348
batch 65 loss: 0.0005576638272032141
batch 70 loss: 0.0005577178322710097
batch 75 loss: 0.0005577190429903566
batch 80 loss: 0.0005576848518103362
batch 85 loss: 0.000557767937425524
batch 90 loss: 0.0005576893570832908
batch 95 loss: 0.0005577062838710845
batch 100 loss: 0.0005576518015004694
batch 105 loss: 0.000557650113478303
batch 110 loss: 0.0005576490773819387
batch 115 loss: 0.0005575591349042952
batch 120 loss: 0.0005576511612161994
batch 125 loss: 0.0005576591938734055
batch 130 loss: 0.0005576389492489397
batch 135 loss: 0.0005576757714152337
batch 140 loss: 0.0005577037460170686
batch 145 loss: 0.0005577285308390855
batch 150 loss: 0.0005577452830038965
batch 155 loss: 0.0005577388452365995
batch 160 loss: 0.0005577529780566692
batch 165 loss: 0.0005577117903158068
batch 170 loss: 0.0005577119765803218
batch 175 loss: 0.0005575581802986562
batch 180 loss: 0.0005577174131758511
batch 185 loss: 0.0005577586241997778
batch 190 loss: 0.0005576352588832379
batch 195 loss: 0.0005577242933213711
batch 200 loss: 0.0005576312076300382
batch 205 loss: 0.000557559821754694
batch 210 loss: 0.000557665154337883
batch 215 loss: 0.0005577666335739196
batch 220 loss: 0.000557625514920801
batch 225 loss: 0.0005575968534685671
batch 230 loss: 0.0005576774594374001
batch 235 loss: 0.00055770268663764
batch 240 loss: 0.0005576440831646323
Training Loss: 0.0005576834788371343
Validation Loss: 0.0005576937643733496
Epoch 65:
batch 5 loss: 0.0005576413008384406
batch 10 loss: 0.0005576231516897679
batch 15 loss: 0.0005578000913374126
batch 20 loss: 0.0005575181683525443
batch 25 loss: 0.0005577157717198133
batch 30 loss: 0.0005577614647336304
batch 35 loss: 0.000557667890097946
batch 40 loss: 0.0005575604969635606
batch 45 loss: 0.0005577060161158443
batch 50 loss: 0.0005577150615863502
batch 55 loss: 0.0005576924188062549
batch 60 loss: 0.0005577380303293467
batch 65 loss: 0.0005577697535045445
batch 70 loss: 0.0005577010102570057
batch 75 loss: 0.0005576846655458212
batch 80 loss: 0.000557664327789098
batch 85 loss: 0.0005577238742262125
batch 90 loss: 0.0005576440133154392
batch 95 loss: 0.0005576812895014882
batch 100 loss: 0.0005577113362960518
batch 105 loss: 0.0005576610681600868
batch 110 loss: 0.0005576128838583827
batch 115 loss: 0.0005577663076110184
batch 120 loss: 0.0005577093455940485
batch 125 loss: 0.0005576330819167197
batch 130 loss: 0.0005577389616519213
batch 135 loss: 0.0005576217430643737
batch 140 loss: 0.0005575613118708134
batch 145 loss: 0.0005577352014370263
batch 150 loss: 0.0005576859344728291
batch 155 loss: 0.0005576890078373253
batch 160 loss: 0.0005576174356974661
batch 165 loss: 0.0005576902534812689
batch 170 loss: 0.000557727727573365
batch 175 loss: 0.0005576919531449676
batch 180 loss: 0.0005577122210524976
batch 185 loss: 0.0005577194155193865
batch 190 loss: 0.0005576418712735176
batch 195 loss: 0.000557724351529032
batch 200 loss: 0.0005577286239713431
batch 205 loss: 0.000557794631458819
batch 210 loss: 0.000557665922679007
batch 215 loss: 0.000557720405049622
batch 220 loss: 0.0005576339550316334
batch 225 loss: 0.0005576805444434286
batch 230 loss: 0.0005574653390794992
batch 235 loss: 0.0005576923256739974
batch 240 loss: 0.0005577626987360417
Training Loss: 0.0005576834303307502
Validation Loss: 0.0005576935858698562
Epoch 66:
batch 5 loss: 0.0005576695431955159
batch 10 loss: 0.000557795912027359
batch 15 loss: 0.0005577827803790569
batch 20 loss: 0.0005576517432928085
batch 25 loss: 0.0005576432566158473
batch 30 loss: 0.0005577738280408085
batch 35 loss: 0.0005576395778916776
batch 40 loss: 0.0005576946074143052
batch 45 loss: 0.0005576771683990956
batch 50 loss: 0.0005576267023570836
batch 55 loss: 0.0005576898925937712
batch 60 loss: 0.0005576038151048124
batch 65 loss: 0.0005577976116910577
batch 70 loss: 0.0005576894385740161
batch 75 loss: 0.0005576969706453383
batch 80 loss: 0.0005576926749199628
batch 85 loss: 0.0005577936884947121
batch 90 loss: 0.0005576614057645202
batch 95 loss: 0.000557703257072717
batch 100 loss: 0.0005577139090746641
batch 105 loss: 0.0005577091244049371
batch 110 loss: 0.0005576965166255831
batch 115 loss: 0.0005576966796070337
batch 120 loss: 0.0005576549796387553
batch 125 loss: 0.0005576368188485503
batch 130 loss: 0.0005576929659582674
batch 135 loss: 0.0005576096009463072
batch 140 loss: 0.0005577486706897616
batch 145 loss: 0.0005576802184805274
batch 150 loss: 0.0005577033967711032
batch 155 loss: 0.0005577565752901137
batch 160 loss: 0.0005576735478825867
batch 165 loss: 0.000557665538508445
batch 170 loss: 0.000557753664907068
batch 175 loss: 0.0005576208583079279
batch 180 loss: 0.0005576578550972044
batch 185 loss: 0.0005576317314989864
batch 190 loss: 0.0005577928968705237
batch 195 loss: 0.0005576453055255115
batch 200 loss: 0.0005576184950768948
batch 205 loss: 0.0005576967378146946
batch 210 loss: 0.0005576860392466187
batch 215 loss: 0.0005576545023359359
batch 220 loss: 0.0005577051779255271
batch 225 loss: 0.0005576232098974288
batch 230 loss: 0.0005575939430855215
batch 235 loss: 0.0005576410796493292
batch 240 loss: 0.0005575612653046846
Training Loss: 0.0005576834412446866
Validation Loss: 0.0005576934112468734
Epoch 67:
batch 5 loss: 0.000557634059805423
batch 10 loss: 0.0005576613824814558
batch 15 loss: 0.0005577003234066069
batch 20 loss: 0.0005576929426752031
batch 25 loss: 0.0005576543509960175
batch 30 loss: 0.0005576934898272156
batch 35 loss: 0.0005576011026278138
batch 40 loss: 0.0005576820112764835
batch 45 loss: 0.0005576903349719942
batch 50 loss: 0.0005577148287557066
batch 55 loss: 0.0005577062838710845
batch 60 loss: 0.0005576196359470487
batch 65 loss: 0.0005576045950874686
batch 70 loss: 0.0005576775991357863
batch 75 loss: 0.0005578135955147445
batch 80 loss: 0.0005577644566074014
batch 85 loss: 0.0005575894843786955
batch 90 loss: 0.0005578106502071023
batch 95 loss: 0.0005576925468631089
batch 100 loss: 0.0005577151081524789
batch 105 loss: 0.0005575817776843905
batch 110 loss: 0.0005576929310336709
batch 115 loss: 0.0005575758521445095
batch 120 loss: 0.000557555363047868
batch 125 loss: 0.0005577393458224833
batch 130 loss: 0.0005577945034019649
batch 135 loss: 0.0005576554103754461
batch 140 loss: 0.000557713583111763
batch 145 loss: 0.0005577548290602863
batch 150 loss: 0.0005576197290793061
batch 155 loss: 0.0005577369127422571
batch 160 loss: 0.0005577837815508247
batch 165 loss: 0.0005576720344834029
batch 170 loss: 0.0005576955969445408
batch 175 loss: 0.0005576039897277951
batch 180 loss: 0.0005576560972258449
batch 185 loss: 0.00055768535239622
batch 190 loss: 0.0005577134666964412
batch 195 loss: 0.0005577348289079964
batch 200 loss: 0.0005576953059062362
batch 205 loss: 0.0005577070638537407
batch 210 loss: 0.0005576769472099841
batch 215 loss: 0.0005576280527748168
batch 220 loss: 0.000557697273325175
batch 225 loss: 0.0005577587056905031
batch 230 loss: 0.0005576699273660779
batch 235 loss: 0.0005576484254561365
batch 240 loss: 0.0005576387629844248
Training Loss: 0.0005576834298456864
Validation Loss: 0.0005576934015455966
Epoch 68:
batch 5 loss: 0.0005576137686148286
batch 10 loss: 0.0005577384261414408
batch 15 loss: 0.0005577538046054542
batch 20 loss: 0.0005576353054493665
batch 25 loss: 0.0005577576579526067
batch 30 loss: 0.0005578416283242404
batch 35 loss: 0.000557610101532191
batch 40 loss: 0.0005577140720561147
batch 45 loss: 0.0005576372146606446
batch 50 loss: 0.0005576708004809916
batch 55 loss: 0.000557752640452236
batch 60 loss: 0.0005576312309131026
batch 65 loss: 0.0005576819996349514
batch 70 loss: 0.0005577543983235955
batch 75 loss: 0.0005576357245445252
batch 80 loss: 0.0005576088442467153
batch 85 loss: 0.0005577353527769447
batch 90 loss: 0.0005577377742156386
batch 95 loss: 0.0005577787291258573
batch 100 loss: 0.0005577287054620683
batch 105 loss: 0.0005576334195211529
batch 110 loss: 0.0005576745257712901
batch 115 loss: 0.0005576940369792282
batch 120 loss: 0.0005576303927227854
batch 125 loss: 0.0005577292176894844
batch 130 loss: 0.0005576273426413537
batch 135 loss: 0.0005577204283326864
batch 140 loss: 0.0005576810683123768
batch 145 loss: 0.000557642092462629
batch 150 loss: 0.0005576826282776892
batch 155 loss: 0.0005575269460678101
batch 160 loss: 0.0005575987510383129
batch 165 loss: 0.0005576546303927898
batch 170 loss: 0.0005577249219641089
batch 175 loss: 0.0005576291005127132
batch 180 loss: 0.0005577527335844934
batch 185 loss: 0.0005576302530243993
batch 190 loss: 0.0005575600313022733
batch 195 loss: 0.0005576872616074979
batch 200 loss: 0.0005576400551944971
batch 205 loss: 0.0005576597643084825
batch 210 loss: 0.0005576964118517935
batch 215 loss: 0.000557703827507794
batch 220 loss: 0.0005576946423389018
batch 225 loss: 0.0005577299045398832
batch 230 loss: 0.0005577590432949364
batch 235 loss: 0.0005577351548708975
batch 240 loss: 0.0005576886236667633
Training Loss: 0.0005576834456102613
Validation Loss: 0.0005576935616166641
Epoch 69:
batch 5 loss: 0.0005576710798777639
batch 10 loss: 0.0005575901479460299
batch 15 loss: 0.0005576305440627039
batch 20 loss: 0.00055755969369784
batch 25 loss: 0.0005577388103120029
batch 30 loss: 0.0005577071569859982
batch 35 loss: 0.0005577549571171402
batch 40 loss: 0.0005575863644480706
batch 45 loss: 0.0005576973315328359
batch 50 loss: 0.0005576752941124141
batch 55 loss: 0.0005576872965320945
batch 60 loss: 0.0005576166906394065
batch 65 loss: 0.0005576817085966468
batch 70 loss: 0.0005577408475801349
batch 75 loss: 0.00055774652864784
batch 80 loss: 0.0005576433148235082
batch 85 loss: 0.0005577395088039339
batch 90 loss: 0.000557823560666293
batch 95 loss: 0.0005578166921623052
batch 100 loss: 0.0005577080883085728
batch 105 loss: 0.0005577822215855121
batch 110 loss: 0.0005576397059485316
batch 115 loss: 0.0005576408002525568
batch 120 loss: 0.000557762454263866
batch 125 loss: 0.0005576292751356959
batch 130 loss: 0.0005577573669143021
batch 135 loss: 0.0005576984491199255
batch 140 loss: 0.0005577194155193865
batch 145 loss: 0.0005577987642027438
batch 150 loss: 0.000557611824478954
batch 155 loss: 0.0005576684721745551
batch 160 loss: 0.0005576098337769508
batch 165 loss: 0.000557688984554261
batch 170 loss: 0.0005576193798333406
batch 175 loss: 0.000557686691172421
batch 180 loss: 0.0005576894152909517
batch 185 loss: 0.000557666935492307
batch 190 loss: 0.0005577180651016534
batch 195 loss: 0.0005576079827733337
batch 200 loss: 0.0005577202304266393
batch 205 loss: 0.0005577195901423692
batch 210 loss: 0.0005576060037128627
batch 215 loss: 0.0005576734081842005
batch 220 loss: 0.0005575688323006034
batch 225 loss: 0.000557642593048513
batch 230 loss: 0.0005577064235694707
batch 235 loss: 0.0005576165276579559
batch 240 loss: 0.0005577388685196639
Training Loss: 0.0005576834194168138
Validation Loss: 0.0005576934306494271
Epoch 70:
batch 5 loss: 0.000557626539375633
batch 10 loss: 0.0005576395429670811
batch 15 loss: 0.0005576468771323562
batch 20 loss: 0.0005576704745180905
batch 25 loss: 0.0005578194395639002
batch 30 loss: 0.0005575721617788077
batch 35 loss: 0.0005576470284722745
batch 40 loss: 0.0005577145260758698
batch 45 loss: 0.0005577888805419207
batch 50 loss: 0.0005577385076321661
batch 55 loss: 0.0005577083094976843
batch 60 loss: 0.0005575860501267016
batch 65 loss: 0.0005578088923357428
batch 70 loss: 0.0005575901712290942
batch 75 loss: 0.0005577075062319636
batch 80 loss: 0.0005576187861151994
batch 85 loss: 0.0005576757714152337
batch 90 loss: 0.000557679811026901
batch 95 loss: 0.0005575641989707947
batch 100 loss: 0.0005577062489464879
batch 105 loss: 0.0005576855386607349
batch 110 loss: 0.0005576122086495161
batch 115 loss: 0.0005577748292125761
batch 120 loss: 0.000557641254272312
batch 125 loss: 0.0005577303585596382
batch 130 loss: 0.0005578067735768855
batch 135 loss: 0.0005577559000812471
batch 140 loss: 0.0005577352247200906
batch 145 loss: 0.0005576379597187042
batch 150 loss: 0.0005576868541538715
batch 155 loss: 0.0005576786934398115
batch 160 loss: 0.0005576358060352504
batch 165 loss: 0.0005576235009357334
batch 170 loss: 0.0005576296825893223
batch 175 loss: 0.0005577616975642741
batch 180 loss: 0.0005577180651016534
batch 185 loss: 0.0005576208583079279
batch 190 loss: 0.0005576417897827923
batch 195 loss: 0.000557682104408741
batch 200 loss: 0.0005576649098657071
batch 205 loss: 0.0005576855037361383
batch 210 loss: 0.0005576358060352504
batch 215 loss: 0.0005576676689088344
batch 220 loss: 0.0005577623494900763
batch 225 loss: 0.0005577122094109655
batch 230 loss: 0.0005577188450843096
batch 235 loss: 0.0005576669704169035
batch 240 loss: 0.0005577213014476001
Training Loss: 0.000557683424752516
Validation Loss: 0.000557693403485852
Epoch 71:
batch 5 loss: 0.0005577268195338548
batch 10 loss: 0.0005576694500632584
batch 15 loss: 0.0005577102303504944
batch 20 loss: 0.0005577077390626073
batch 25 loss: 0.0005576355149969458
batch 30 loss: 0.0005576628493145109
batch 35 loss: 0.0005576590425334871
batch 40 loss: 0.000557617109734565
batch 45 loss: 0.0005577709176577628
batch 50 loss: 0.0005576075520366431
batch 55 loss: 0.0005576576804742217
batch 60 loss: 0.0005576354917138815
batch 65 loss: 0.0005576774710789323
batch 70 loss: 0.0005577272037044168
batch 75 loss: 0.0005577139556407929
batch 80 loss: 0.0005577045027166605
batch 85 loss: 0.0005578280193731189
batch 90 loss: 0.0005577072501182556
batch 95 loss: 0.0005576558061875403
batch 100 loss: 0.0005577579606324435
batch 105 loss: 0.0005576481809839607
batch 110 loss: 0.0005576286232098937
batch 115 loss: 0.0005576942930929363
batch 120 loss: 0.0005576811148785054
batch 125 loss: 0.0005576757830567658
batch 130 loss: 0.0005576334078796208
batch 135 loss: 0.0005577658885158598
batch 140 loss: 0.0005576189490966499
batch 145 loss: 0.0005576442112214863
batch 150 loss: 0.0005577782052569091
batch 155 loss: 0.0005577648873440922
batch 160 loss: 0.0005576048395596444
batch 165 loss: 0.0005576981115154922
batch 170 loss: 0.0005576320341788233
batch 175 loss: 0.0005576970870606601
batch 180 loss: 0.0005578298238106072
batch 185 loss: 0.0005575933842919767
batch 190 loss: 0.0005577050149440765
batch 195 loss: 0.0005576273426413537
batch 200 loss: 0.0005576615571044385
batch 205 loss: 0.0005576505209319293
batch 210 loss: 0.0005576086463406682
batch 215 loss: 0.0005577404168434441
batch 220 loss: 0.000557672360446304
batch 225 loss: 0.0005576396593824029
batch 230 loss: 0.0005577543284744024
batch 235 loss: 0.000557712372392416
batch 240 loss: 0.0005576104507781565
Training Loss: 0.0005576834179616222
Validation Loss: 0.0005576934238585333
Epoch 72:
batch 5 loss: 0.0005576456780545414
batch 10 loss: 0.0005576111259870231
batch 15 loss: 0.0005576093913987279
batch 20 loss: 0.0005577461677603423
batch 25 loss: 0.0005576225696131587
batch 30 loss: 0.000557755387853831
batch 35 loss: 0.0005576814524829388
batch 40 loss: 0.0005577680887654424
batch 45 loss: 0.0005576262716203928
batch 50 loss: 0.0005576708703301847
batch 55 loss: 0.0005576209165155888
batch 60 loss: 0.0005577573669143021
batch 65 loss: 0.000557720591314137
batch 70 loss: 0.0005576487863436341
batch 75 loss: 0.0005576319992542267
batch 80 loss: 0.0005577144795097411
batch 85 loss: 0.0005576387047767639
batch 90 loss: 0.0005576236755587161
batch 95 loss: 0.000557631382253021
batch 100 loss: 0.0005578064941801131
batch 105 loss: 0.0005576532916165888
batch 110 loss: 0.0005576074356213212
batch 115 loss: 0.0005577211151830852
batch 120 loss: 0.0005577786942012608
batch 125 loss: 0.0005577097646892071
batch 130 loss: 0.000557597225997597
batch 135 loss: 0.0005576780531555414
batch 140 loss: 0.0005577540723606944
batch 145 loss: 0.0005575623828917742
batch 150 loss: 0.0005577118718065321
batch 155 loss: 0.0005576811148785054
batch 160 loss: 0.0005577591364271939
batch 165 loss: 0.0005578039446845651
batch 170 loss: 0.0005576453520916402
batch 175 loss: 0.0005576844094321131
batch 180 loss: 0.0005577430012635887
batch 185 loss: 0.0005575945600867271
batch 190 loss: 0.0005577308707870543
batch 195 loss: 0.0005576115567237139
batch 200 loss: 0.0005576113355346024
batch 205 loss: 0.0005576509749516845
batch 210 loss: 0.0005576742463745177
batch 215 loss: 0.0005577181349508464
batch 220 loss: 0.000557747739367187
batch 225 loss: 0.0005576793802902103
batch 230 loss: 0.0005576987052336336
batch 235 loss: 0.000557721802033484
batch 240 loss: 0.0005577429081313312
Training Loss: 0.0005576834266927714
Validation Loss: 0.0005576934713947897
Epoch 73:
batch 5 loss: 0.0005575885181315244
batch 10 loss: 0.0005576348281465471
batch 15 loss: 0.0005577004747465252
batch 20 loss: 0.0005576931056566536
batch 25 loss: 0.00055777239613235
batch 30 loss: 0.0005576354102231562
batch 35 loss: 0.0005576469004154206
batch 40 loss: 0.0005576984258368611
batch 45 loss: 0.0005577541189268232
batch 50 loss: 0.0005577450850978493
batch 55 loss: 0.0005576144554652274
batch 60 loss: 0.0005577666684985161
batch 65 loss: 0.0005577625590376556
batch 70 loss: 0.0005575770977884531
batch 75 loss: 0.0005576462252065539
batch 80 loss: 0.0005576620693318546
batch 85 loss: 0.0005576813127845526
batch 90 loss: 0.000557684781961143
batch 95 loss: 0.0005576874827966094
batch 100 loss: 0.0005576899275183678
batch 105 loss: 0.0005578010343015194
batch 110 loss: 0.0005576817551627755
batch 115 loss: 0.0005577305448241532
batch 120 loss: 0.0005576729075983166
batch 125 loss: 0.0005576804978772998
batch 130 loss: 0.0005577139207161963
batch 135 loss: 0.0005576252471655608
batch 140 loss: 0.000557627622038126
batch 145 loss: 0.0005577429197728634
batch 150 loss: 0.0005575311952270567
batch 155 loss: 0.0005576625233516097
batch 160 loss: 0.0005577190895564854
batch 165 loss: 0.0005577135947532952
batch 170 loss: 0.0005576771101914347
batch 175 loss: 0.0005576708470471203
batch 180 loss: 0.0005577047355473042
batch 185 loss: 0.0005576381576247513
batch 190 loss: 0.0005576610215939582
batch 195 loss: 0.0005577258765697479
batch 200 loss: 0.0005577450734563172
batch 205 loss: 0.0005575547693297267
batch 210 loss: 0.000557656493037939
batch 215 loss: 0.0005577460979111493
batch 220 loss: 0.0005576969240792095
batch 225 loss: 0.0005576489260420203
batch 230 loss: 0.0005576508468948304
batch 235 loss: 0.00055782834533602
batch 240 loss: 0.0005576545372605324
Training Loss: 0.0005576834262077076
Validation Loss: 0.0005576935247518123
Epoch 74:
batch 5 loss: 0.0005577448871918022
batch 10 loss: 0.0005577409639954567
batch 15 loss: 0.0005576077615842223
batch 20 loss: 0.0005576820578426123
batch 25 loss: 0.0005576861905865372
batch 30 loss: 0.000557686504907906
batch 35 loss: 0.0005577783449552953
batch 40 loss: 0.0005577428732067346
batch 45 loss: 0.0005577191244810819
batch 50 loss: 0.0005576605210080743
batch 55 loss: 0.000557624502107501
batch 60 loss: 0.0005577096715569496
batch 65 loss: 0.0005576332448981702
batch 70 loss: 0.0005577172501944006
batch 75 loss: 0.0005576168419793248
batch 80 loss: 0.0005577317089773715
batch 85 loss: 0.0005577331525273621
batch 90 loss: 0.0005577424657531082
batch 95 loss: 0.000557732256129384
batch 100 loss: 0.0005575730581767857
batch 105 loss: 0.0005577059579081834
batch 110 loss: 0.0005576546420343221
batch 115 loss: 0.0005575728602707386
batch 120 loss: 0.0005576655152253806
batch 125 loss: 0.0005576489376835525
batch 130 loss: 0.0005576735129579902
batch 135 loss: 0.0005576153635047376
batch 140 loss: 0.0005577598116360605
batch 145 loss: 0.0005576803698204458
batch 150 loss: 0.0005576030118390918
batch 155 loss: 0.0005577366217039525
batch 160 loss: 0.0005576197290793061
batch 165 loss: 0.0005577434087172151
batch 170 loss: 0.0005577279604040086
batch 175 loss: 0.0005577071802690625
batch 180 loss: 0.0005578133277595043
batch 185 loss: 0.0005577124306000769
batch 190 loss: 0.0005575816612690687
batch 195 loss: 0.0005575012532062828
batch 200 loss: 0.0005578103708103299
batch 205 loss: 0.0005575784482061863
batch 210 loss: 0.0005577235715463758
batch 215 loss: 0.0005576806957833469
batch 220 loss: 0.0005577050847932697
batch 225 loss: 0.0005577337462455035
batch 230 loss: 0.0005576587864197791
batch 235 loss: 0.0005576745606958866
batch 240 loss: 0.0005576514755375684
Training Loss: 0.0005576834099580689
Validation Loss: 0.0005576934112468734
Epoch 75:
batch 5 loss: 0.000557672861032188
batch 10 loss: 0.000557874352671206
batch 15 loss: 0.0005577343981713057
batch 20 loss: 0.0005576036521233619
batch 25 loss: 0.0005576113821007311
batch 30 loss: 0.0005577192991040647
batch 35 loss: 0.0005576715455390513
batch 40 loss: 0.0005575737450271845
batch 45 loss: 0.0005578495911322534
batch 50 loss: 0.0005576556897722185
batch 55 loss: 0.000557703897356987
batch 60 loss: 0.0005576549447141588
batch 65 loss: 0.0005575828603468835
batch 70 loss: 0.0005576669820584357
batch 75 loss: 0.0005576113238930702
batch 80 loss: 0.0005576427443884313
batch 85 loss: 0.0005576065741479397
batch 90 loss: 0.0005577412783168256
batch 95 loss: 0.0005578181589953601
batch 100 loss: 0.0005577528150752187
batch 105 loss: 0.0005577190313488245
batch 110 loss: 0.0005576846306212246
batch 115 loss: 0.0005575377494096756
batch 120 loss: 0.0005576895899139344
batch 125 loss: 0.0005577756790444254
batch 130 loss: 0.0005576628260314465
batch 135 loss: 0.0005577295320108533
batch 140 loss: 0.0005576500319875777
batch 145 loss: 0.0005576834897510708
batch 150 loss: 0.0005576860974542796
batch 155 loss: 0.0005576763767749071
batch 160 loss: 0.0005576465395279228
batch 165 loss: 0.0005577170639298856
batch 170 loss: 0.0005576075287535787
batch 175 loss: 0.0005577496951445938
batch 180 loss: 0.0005576969240792095
batch 185 loss: 0.0005576282739639283
batch 190 loss: 0.0005576771683990956
batch 195 loss: 0.0005576904746703804
batch 200 loss: 0.0005577050149440765
batch 205 loss: 0.0005577771808020771
batch 210 loss: 0.000557544594630599
batch 215 loss: 0.0005577073665335774
batch 220 loss: 0.0005577937117777765
batch 225 loss: 0.0005577140836976469
batch 230 loss: 0.0005576848401688039
batch 235 loss: 0.0005575640592724085
batch 240 loss: 0.0005576572148129344
Training Loss: 0.0005576834346963248
Validation Loss: 0.0005576934500519808
Epoch 76:
batch 5 loss: 0.0005577771342359483
batch 10 loss: 0.0005578017793595791
batch 15 loss: 0.0005576485767960548
batch 20 loss: 0.0005576405208557844
batch 25 loss: 0.0005576596013270319
batch 30 loss: 0.0005576834315434098
batch 35 loss: 0.000557668216060847
batch 40 loss: 0.000557581102475524
batch 45 loss: 0.0005577109404839575
batch 50 loss: 0.0005575735471211374
batch 55 loss: 0.0005576536641456187
batch 60 loss: 0.0005576211959123611
batch 65 loss: 0.000557715620379895
batch 70 loss: 0.0005576317082159222
batch 75 loss: 0.0005577706964686513
batch 80 loss: 0.0005577924544923008
batch 85 loss: 0.0005578388459980488
batch 90 loss: 0.0005576578550972044
batch 95 loss: 0.0005576926283538342
batch 100 loss: 0.0005577229079790413
batch 105 loss: 0.0005577213363721967
batch 110 loss: 0.0005576093913987279
batch 115 loss: 0.0005576474242843688
batch 120 loss: 0.000557694211602211
batch 125 loss: 0.0005576480412855745
batch 130 loss: 0.0005576753173954785
batch 135 loss: 0.0005576584837399424
batch 140 loss: 0.0005577007657848298
batch 145 loss: 0.000557655596639961
batch 150 loss: 0.0005576922558248043
batch 155 loss: 0.0005577385891228914
batch 160 loss: 0.0005577037110924721
batch 165 loss: 0.0005576711497269571
batch 170 loss: 0.0005576169234700501
batch 175 loss: 0.0005577111267484724
batch 180 loss: 0.0005577252246439457
batch 185 loss: 0.0005576395778916776
batch 190 loss: 0.0005576940136961638
batch 195 loss: 0.0005577332456596195
batch 200 loss: 0.0005577150033786893
batch 205 loss: 0.0005576432100497186
batch 210 loss: 0.0005576357827521861
batch 215 loss: 0.0005576690658926964
batch 220 loss: 0.000557634886354208
batch 225 loss: 0.0005576538038440049
batch 230 loss: 0.0005576738389208913
batch 235 loss: 0.0005576672381721437
batch 240 loss: 0.0005577318021096289
Training Loss: 0.0005576834051074305
Validation Loss: 0.0005576934015455966
Epoch 77:
batch 5 loss: 0.0005577446077950299
batch 10 loss: 0.0005577997071668506
batch 15 loss: 0.0005577973672188819
batch 20 loss: 0.0005576020223088563
batch 25 loss: 0.0005577701842412353
batch 30 loss: 0.0005577681818976999
batch 35 loss: 0.0005576285999268294
batch 40 loss: 0.000557811523322016
batch 45 loss: 0.0005576375056989491
batch 50 loss: 0.0005577044095844031
batch 55 loss: 0.000557758507784456
batch 60 loss: 0.0005576583789661527
batch 65 loss: 0.0005576390074566006
batch 70 loss: 0.0005576636758632958
batch 75 loss: 0.0005577126750722528
batch 80 loss: 0.0005576869356445968
batch 85 loss: 0.000557575048878789
batch 90 loss: 0.0005575986811891198
batch 95 loss: 0.000557700137142092
batch 100 loss: 0.0005577275995165109
batch 105 loss: 0.0005576459807343781
batch 110 loss: 0.0005575768882408738
batch 115 loss: 0.0005576416850090026
batch 120 loss: 0.0005577228497713804
batch 125 loss: 0.0005576126510277391
batch 130 loss: 0.0005575814633630216
batch 135 loss: 0.0005576809984631836
batch 140 loss: 0.0005576150841079652
batch 145 loss: 0.0005577468546107411
batch 150 loss: 0.0005577518022619188
batch 155 loss: 0.0005577305448241532
batch 160 loss: 0.0005576660390943289
batch 165 loss: 0.0005576021387241781
batch 170 loss: 0.0005576143739745021
batch 175 loss: 0.0005578008946031332
batch 180 loss: 0.0005577846080996096
batch 185 loss: 0.0005576737225055694
batch 190 loss: 0.0005577427335083484
batch 195 loss: 0.0005576929892413318
batch 200 loss: 0.0005577000090852379
batch 205 loss: 0.0005576174706220626
batch 210 loss: 0.0005576509051024914
batch 215 loss: 0.000557680707424879
batch 220 loss: 0.000557625072542578
batch 225 loss: 0.0005576835363171994
batch 230 loss: 0.0005576693685725332
batch 235 loss: 0.0005575910443440079
batch 240 loss: 0.0005577164003625513
Training Loss: 0.0005576834077752816
Validation Loss: 0.0005576934170676395
Epoch 78:
batch 5 loss: 0.0005577420000918209
batch 10 loss: 0.0005577107076533139
batch 15 loss: 0.0005575988092459738
batch 20 loss: 0.0005576617666520178
batch 25 loss: 0.000557494314853102
batch 30 loss: 0.0005576038965955377
batch 35 loss: 0.0005577141302637756
batch 40 loss: 0.0005577823030762374
batch 45 loss: 0.0005577546660788357
batch 50 loss: 0.0005576415453106165
batch 55 loss: 0.0005578011041507125
batch 60 loss: 0.000557634059805423
batch 65 loss: 0.0005576326046139001
batch 70 loss: 0.0005575904389843345
batch 75 loss: 0.0005576996132731438
batch 80 loss: 0.0005577033851295709
batch 85 loss: 0.0005576963187195361
batch 90 loss: 0.0005576820462010801
batch 95 loss: 0.0005577096482738853
batch 100 loss: 0.0005577466799877584
batch 105 loss: 0.0005578079377301038
batch 110 loss: 0.0005577171104960144
batch 115 loss: 0.0005576970055699348
batch 120 loss: 0.0005576555151492358
batch 125 loss: 0.0005576585070230066
batch 130 loss: 0.000557700265198946
batch 135 loss: 0.0005576780997216702
batch 140 loss: 0.0005577179952524602
batch 145 loss: 0.0005576321389526128
batch 150 loss: 0.0005576624185778201
batch 155 loss: 0.0005577543983235955
batch 160 loss: 0.0005577429430559278
batch 165 loss: 0.0005576782859861851
batch 170 loss: 0.0005577048403210938
batch 175 loss: 0.000557646609377116
batch 180 loss: 0.0005577035364694894
batch 185 loss: 0.0005577540723606944
batch 190 loss: 0.0005577489035204052
batch 195 loss: 0.000557614746503532
batch 200 loss: 0.0005576589494012297
batch 205 loss: 0.0005575860152021051
batch 210 loss: 0.0005576589843258262
batch 215 loss: 0.0005576832918450236
batch 220 loss: 0.0005577065516263246
batch 225 loss: 0.0005576339201070369
batch 230 loss: 0.0005577742354944349
batch 235 loss: 0.0005576824420131743
batch 240 loss: 0.0005575437797233462
Training Loss: 0.0005576834070476858
Validation Loss: 0.0005576933928144475
Epoch 79:
batch 5 loss: 0.0005577068543061614
batch 10 loss: 0.0005576219875365496
batch 15 loss: 0.0005576562951318919
batch 20 loss: 0.0005576774710789323
batch 25 loss: 0.0005576864467002451
batch 30 loss: 0.0005576146766543388
batch 35 loss: 0.0005576889263466001
batch 40 loss: 0.0005577746662311256
batch 45 loss: 0.0005577836418524384
batch 50 loss: 0.0005577689385972917
batch 55 loss: 0.0005575958639383316
batch 60 loss: 0.0005576899857260287
batch 65 loss: 0.0005576475872658193
batch 70 loss: 0.0005575755727477372
batch 75 loss: 0.0005576050607487559
batch 80 loss: 0.0005577083444222808
batch 85 loss: 0.0005575914517976343
batch 90 loss: 0.0005575993913225829
batch 95 loss: 0.0005576853058300912
batch 100 loss: 0.0005577197414822876
batch 105 loss: 0.0005576152936555446
batch 110 loss: 0.0005576090305112302
batch 115 loss: 0.0005577644682489335
batch 120 loss: 0.0005576585070230066
batch 125 loss: 0.000557688798289746
batch 130 loss: 0.0005576751893386245
batch 135 loss: 0.0005577711155638099
batch 140 loss: 0.0005577779491432011
batch 145 loss: 0.0005576352821663022
batch 150 loss: 0.0005576928961090744
batch 155 loss: 0.0005576843512244522
batch 160 loss: 0.0005576620460487902
batch 165 loss: 0.000557662546634674
batch 170 loss: 0.0005577887175604701
batch 175 loss: 0.0005576467490755021
batch 180 loss: 0.000557718682102859
batch 185 loss: 0.0005577423726208508
batch 190 loss: 0.0005576186464168132
batch 195 loss: 0.0005575819523073733
batch 200 loss: 0.0005577342119067908
batch 205 loss: 0.0005578550742939114
batch 210 loss: 0.0005577630479820072
batch 215 loss: 0.000557570846285671
batch 220 loss: 0.000557617424055934
batch 225 loss: 0.0005577122792601585
batch 230 loss: 0.000557680951897055
batch 235 loss: 0.0005577545729465782
batch 240 loss: 0.0005577222793363035
Training Loss: 0.0005576834060775582
Validation Loss: 0.0005576934316195547
Epoch 80:
batch 5 loss: 0.0005576296360231936
batch 10 loss: 0.0005576864117756486
batch 15 loss: 0.000557729764841497
batch 20 loss: 0.0005576466675847769
batch 25 loss: 0.0005576694966293871
batch 30 loss: 0.0005577551899477839
batch 35 loss: 0.0005575106246396899
batch 40 loss: 0.0005576400435529649
batch 45 loss: 0.0005577714648097754
batch 50 loss: 0.0005576599272899329
batch 55 loss: 0.0005577142699621617
batch 60 loss: 0.0005576961790211499
batch 65 loss: 0.0005578316631726921
batch 70 loss: 0.0005576846189796925
batch 75 loss: 0.000557658460456878
batch 80 loss: 0.0005576209630817174
batch 85 loss: 0.0005576586117967963
batch 90 loss: 0.0005576474010013044
batch 95 loss: 0.0005577409639954567
batch 100 loss: 0.0005576041992753744
batch 105 loss: 0.0005577334086410701
batch 110 loss: 0.0005576093215495348
batch 115 loss: 0.0005577834905125201
batch 120 loss: 0.0005576597410254181
batch 125 loss: 0.0005576456082053482
batch 130 loss: 0.0005577080999501049
batch 135 loss: 0.0005576957250013947
batch 140 loss: 0.0005576641764491796
batch 145 loss: 0.0005575237213633954
batch 150 loss: 0.0005578193115070463
batch 155 loss: 0.0005576772964559495
batch 160 loss: 0.0005576179246418178
batch 165 loss: 0.000557665538508445
batch 170 loss: 0.0005577661097049714
batch 175 loss: 0.0005576887866482139
batch 180 loss: 0.0005577159114181996
batch 185 loss: 0.0005576688679866492
batch 190 loss: 0.0005577961448580027
batch 195 loss: 0.0005576364463195204
batch 200 loss: 0.0005577101488597691
batch 205 loss: 0.00055766929872334
batch 210 loss: 0.0005576592753641308
batch 215 loss: 0.0005576234310865402
batch 220 loss: 0.0005577264353632927
batch 225 loss: 0.0005577338393777609
batch 230 loss: 0.0005576542462222278
batch 235 loss: 0.0005577239906415343
batch 240 loss: 0.0005576703231781721
Training Loss: 0.0005576833995291963
Validation Loss: 0.0005576934403507039
Epoch 81:
batch 5 loss: 0.000557771057356149
batch 10 loss: 0.0005576848518103362
batch 15 loss: 0.00055764673743397
batch 20 loss: 0.0005577721865847706
batch 25 loss: 0.0005577705800533294
batch 30 loss: 0.0005575483664870262
batch 35 loss: 0.0005575995310209692
batch 40 loss: 0.0005577603122219443
batch 45 loss: 0.0005577101954258978
batch 50 loss: 0.0005576576455496252
batch 55 loss: 0.000557685806415975
batch 60 loss: 0.0005577332223765552
batch 65 loss: 0.0005577083094976843
batch 70 loss: 0.0005577357369475067
batch 75 loss: 0.0005576096242293716
batch 80 loss: 0.0005577295669354498
batch 85 loss: 0.0005576581577770412
batch 90 loss: 0.0005576699622906744
batch 95 loss: 0.0005576321505941451
batch 100 loss: 0.0005576779018156231
batch 105 loss: 0.0005577020812779665
batch 110 loss: 0.0005577139556407929
batch 115 loss: 0.0005576479947194457
batch 120 loss: 0.0005577393574640154
batch 125 loss: 0.0005578667274676264
batch 130 loss: 0.0005576158408075571
batch 135 loss: 0.0005576415918767452
batch 140 loss: 0.0005576749914325774
batch 145 loss: 0.000557682930957526
batch 150 loss: 0.000557564664632082
batch 155 loss: 0.0005577724892646075
batch 160 loss: 0.0005576308933086694
batch 165 loss: 0.0005576055147685111
batch 170 loss: 0.0005577161093242467
batch 175 loss: 0.0005576573661528528
batch 180 loss: 0.000557685038074851
batch 185 loss: 0.0005576993338763714
batch 190 loss: 0.0005577474250458181
batch 195 loss: 0.0005577389849349857
batch 200 loss: 0.0005577334086410701
batch 205 loss: 0.0005576937110163271
batch 210 loss: 0.0005577094154432416
batch 215 loss: 0.000557595188729465
batch 220 loss: 0.0005576893105171621
batch 225 loss: 0.0005576584371738136
batch 230 loss: 0.0005576926865614951
batch 235 loss: 0.0005575390183366835
batch 240 loss: 0.0005576266557909548
Training Loss: 0.0005576833963762813
Validation Loss: 0.0005576934403507039
Epoch 82:
batch 5 loss: 0.0005576752126216888
batch 10 loss: 0.0005577526753768325
batch 15 loss: 0.000557738111820072
batch 20 loss: 0.0005576391937211156
batch 25 loss: 0.0005576790892519057
batch 30 loss: 0.0005576890078373253
batch 35 loss: 0.0005576869938522577
batch 40 loss: 0.0005576047697104514
batch 45 loss: 0.0005576897761784494
batch 50 loss: 0.0005577440490014852
batch 55 loss: 0.0005577300675213337
batch 60 loss: 0.0005577254691161215
batch 65 loss: 0.0005577091011218727
batch 70 loss: 0.000557742896489799
batch 75 loss: 0.0005576929543167353
batch 80 loss: 0.0005577047704719007
batch 85 loss: 0.0005576333031058311
batch 90 loss: 0.0005575832910835743
batch 95 loss: 0.0005575821618549526
batch 100 loss: 0.0005576657364144922
batch 105 loss: 0.0005577356787398458
batch 110 loss: 0.0005576628376729786
batch 115 loss: 0.000557637878227979
batch 120 loss: 0.0005578306736424565
batch 125 loss: 0.0005576608353294432
batch 130 loss: 0.0005577211966738104
batch 135 loss: 0.0005576280876994133
batch 140 loss: 0.0005576215567998588
batch 145 loss: 0.000557636609300971
batch 150 loss: 0.0005577184492722154
batch 155 loss: 0.0005576503346674145
batch 160 loss: 0.0005577377043664456
batch 165 loss: 0.0005576457479037344
batch 170 loss: 0.000557824329007417
batch 175 loss: 0.0005576743045821786
batch 180 loss: 0.0005576131748966873
batch 185 loss: 0.0005576416268013417
batch 190 loss: 0.0005576889147050679
batch 195 loss: 0.0005575979594141245
batch 200 loss: 0.0005576784373261034
batch 205 loss: 0.000557677389588207
batch 210 loss: 0.0005577367614023387
batch 215 loss: 0.0005576400668360293
batch 220 loss: 0.0005576529772952199
batch 225 loss: 0.0005578001728281379
batch 230 loss: 0.0005576759460382164
batch 235 loss: 0.0005576582858338952
batch 240 loss: 0.0005576867028139531
Training Loss: 0.0005576834014694517
Validation Loss: 0.0005576933976650859
Epoch 83:
batch 5 loss: 0.0005578093114309013
batch 10 loss: 0.0005576952244155109
batch 15 loss: 0.0005576036521233619
batch 20 loss: 0.0005577635951340198
batch 25 loss: 0.0005576339783146978
batch 30 loss: 0.0005575343151576817
batch 35 loss: 0.0005576942232437432
batch 40 loss: 0.000557670823764056
batch 45 loss: 0.000557707529515028
batch 50 loss: 0.0005577903124503791
batch 55 loss: 0.0005577489268034696
batch 60 loss: 0.0005576966679655016
batch 65 loss: 0.0005576670519076288
batch 70 loss: 0.0005576789961196482
batch 75 loss: 0.0005576799856498837
batch 80 loss: 0.000557617039885372
batch 85 loss: 0.0005577799049206078
batch 90 loss: 0.0005576848867349327
batch 95 loss: 0.0005577534902840853
batch 100 loss: 0.0005576579133048654
batch 105 loss: 0.0005576392286457121
batch 110 loss: 0.0005576428724452853
batch 115 loss: 0.0005575796938501298
batch 120 loss: 0.0005577581934630871
batch 125 loss: 0.0005577094038017094
batch 130 loss: 0.0005577055271714926
batch 135 loss: 0.0005577894975431263
batch 140 loss: 0.0005576034775003791
batch 145 loss: 0.0005577011732384562
batch 150 loss: 0.0005576972267590463
batch 155 loss: 0.0005576378433033824
batch 160 loss: 0.0005576541647315025
batch 165 loss: 0.0005576718482188881
batch 170 loss: 0.0005576075403951108
batch 175 loss: 0.0005575751769356429
batch 180 loss: 0.0005576922209002078
batch 185 loss: 0.0005576204275712371
batch 190 loss: 0.0005576716852374375
batch 195 loss: 0.0005577113595791161
batch 200 loss: 0.000557703129015863
batch 205 loss: 0.0005577365052886307
batch 210 loss: 0.0005577215692028403
batch 215 loss: 0.0005576911149546504
batch 220 loss: 0.0005575964343734086
batch 225 loss: 0.0005577859352342785
batch 230 loss: 0.0005576676223427058
batch 235 loss: 0.0005576954572461545
batch 240 loss: 0.000557668600231409
Training Loss: 0.0005576833907980471
Validation Loss: 0.0005576934102767458
Epoch 84:
batch 5 loss: 0.0005576692405156791
batch 10 loss: 0.000557729380670935
batch 15 loss: 0.0005576045601628721
batch 20 loss: 0.0005576309747993946
batch 25 loss: 0.0005577226402238011
batch 30 loss: 0.0005576729425229132
batch 35 loss: 0.0005576768075115979
batch 40 loss: 0.0005577453412115574
batch 45 loss: 0.0005576991359703242
batch 50 loss: 0.0005576662137173116
batch 55 loss: 0.0005577445961534977
batch 60 loss: 0.0005576559691689908
batch 65 loss: 0.0005577502772212029
batch 70 loss: 0.0005578320939093828
batch 75 loss: 0.0005577190429903566
batch 80 loss: 0.0005577669129706919
batch 85 loss: 0.0005575740477070212
batch 90 loss: 0.0005577584612183273
batch 95 loss: 0.0005576364113949239
batch 100 loss: 0.0005576031398959458
batch 105 loss: 0.000557668216060847
batch 110 loss: 0.0005577736767008901
batch 115 loss: 0.0005575935123488307
batch 120 loss: 0.0005576753406785429
batch 125 loss: 0.0005577539675869047
batch 130 loss: 0.00055762545671314
batch 135 loss: 0.0005576648283749819
batch 140 loss: 0.0005576159106567502
batch 145 loss: 0.0005576436291448772
batch 150 loss: 0.00055769287282601
batch 155 loss: 0.0005577349569648504
batch 160 loss: 0.0005575516377575695
batch 165 loss: 0.0005576100083999336
batch 170 loss: 0.0005575925926677883
batch 175 loss: 0.0005577113712206483
batch 180 loss: 0.0005577161791734397
batch 185 loss: 0.0005575909628532827
batch 190 loss: 0.0005576966563239693
batch 195 loss: 0.0005577032454311848
batch 200 loss: 0.0005577404517680407
batch 205 loss: 0.0005576884956099093
batch 210 loss: 0.0005577645963057876
batch 215 loss: 0.0005576859693974257
batch 220 loss: 0.0005576470983214676
batch 225 loss: 0.0005577572504989803
batch 230 loss: 0.0005577484727837146
batch 235 loss: 0.0005577032221481204
batch 240 loss: 0.0005575940245762468
Training Loss: 0.000557683391525643
Validation Loss: 0.0005576934025157243
Epoch 85:
batch 5 loss: 0.000557755772024393
batch 10 loss: 0.0005577042233198881
batch 15 loss: 0.0005575896939262748
batch 20 loss: 0.0005577060976065696
batch 25 loss: 0.0005577729665674269
batch 30 loss: 0.0005576349329203367
batch 35 loss: 0.0005576688214205206
batch 40 loss: 0.0005578588112257421
batch 45 loss: 0.0005576346884481609
batch 50 loss: 0.00055769809987396
batch 55 loss: 0.0005576870404183865
batch 60 loss: 0.000557637691963464
batch 65 loss: 0.0005576385534368456
batch 70 loss: 0.0005575508810579776
batch 75 loss: 0.0005575697054155171
batch 80 loss: 0.000557778577785939
batch 85 loss: 0.0005576930823735892
batch 90 loss: 0.000557608250528574
batch 95 loss: 0.0005576040712185204
batch 100 loss: 0.0005577330710366368
batch 105 loss: 0.0005576524185016751
batch 110 loss: 0.0005577616393566132
batch 115 loss: 0.0005577460397034883
batch 120 loss: 0.0005576932220719754
batch 125 loss: 0.0005578266456723214
batch 130 loss: 0.0005576650728471577
batch 135 loss: 0.0005576393916271627
batch 140 loss: 0.0005576737923547626
batch 145 loss: 0.0005576784722507
batch 150 loss: 0.00055774588836357
batch 155 loss: 0.0005576943163760007
batch 160 loss: 0.0005576830008067191
batch 165 loss: 0.0005577246076427401
batch 170 loss: 0.0005577525356784463
batch 175 loss: 0.0005576411611400544
batch 180 loss: 0.0005576220923103392
batch 185 loss: 0.0005577320349402726
batch 190 loss: 0.0005577740259468556
batch 195 loss: 0.0005575940129347145
batch 200 loss: 0.0005577646545134485
batch 205 loss: 0.0005576709867455065
batch 210 loss: 0.0005575096933171153
batch 215 loss: 0.000557596341241151
batch 220 loss: 0.0005576923140324652
batch 225 loss: 0.0005576576339080929
batch 230 loss: 0.0005576768307946622
batch 235 loss: 0.000557694782037288
batch 240 loss: 0.0005577140022069216
Training Loss: 0.000557683388372728
Validation Loss: 0.0005576934160975119
Epoch 86:
batch 5 loss: 0.0005576858762651682
batch 10 loss: 0.0005576388561166823
batch 15 loss: 0.0005576624185778201
batch 20 loss: 0.0005575327551923693
batch 25 loss: 0.0005576079129241407
batch 30 loss: 0.0005576792056672276
batch 35 loss: 0.0005576589726842939
batch 40 loss: 0.0005576871801167727
batch 45 loss: 0.0005576561321504414
batch 50 loss: 0.0005576152587309479
batch 55 loss: 0.0005577585543505847
batch 60 loss: 0.0005577587755396963
batch 65 loss: 0.0005576221621595324
batch 70 loss: 0.0005576394032686949
batch 75 loss: 0.0005577008705586195
batch 80 loss: 0.0005576873430982232
batch 85 loss: 0.0005576127441599965
batch 90 loss: 0.0005577611387707293
batch 95 loss: 0.0005577358300797641
batch 100 loss: 0.0005576024996116757
batch 105 loss: 0.0005576119991019368
batch 110 loss: 0.0005577147006988525
batch 115 loss: 0.0005576681811362505
batch 120 loss: 0.0005576851195655763
batch 125 loss: 0.0005577554460614919
batch 130 loss: 0.0005577709758654237
batch 135 loss: 0.0005577021860517561
batch 140 loss: 0.0005577222909778357
batch 145 loss: 0.0005576872383244336
batch 150 loss: 0.0005576097057200969
batch 155 loss: 0.0005577132455073297
batch 160 loss: 0.0005576985189691186
batch 165 loss: 0.0005576949333772063
batch 170 loss: 0.0005577086354605854
batch 175 loss: 0.0005576894734986126
batch 180 loss: 0.0005576811032369733
batch 185 loss: 0.0005577088100835681
batch 190 loss: 0.000557777495123446
batch 195 loss: 0.000557651522103697
batch 200 loss: 0.0005576880415901541
batch 205 loss: 0.0005576023715548218
batch 210 loss: 0.0005577091593295336
batch 215 loss: 0.0005576941766776145
batch 220 loss: 0.0005577060743235052
batch 225 loss: 0.0005577286356128752
batch 230 loss: 0.0005576465628109872
batch 235 loss: 0.0005577771342359483
batch 240 loss: 0.0005576952476985753
Training Loss: 0.0005576833932233664
Validation Loss: 0.0005576933976650859
Epoch 87:
batch 5 loss: 0.0005577561794780194
batch 10 loss: 0.0005577602423727512
batch 15 loss: 0.0005577459000051021
batch 20 loss: 0.000557725119870156
batch 25 loss: 0.0005576172494329512
batch 30 loss: 0.0005575516261160374
batch 35 loss: 0.0005575395771302283
batch 40 loss: 0.0005576321855187416
batch 45 loss: 0.0005575742339715362
batch 50 loss: 0.0005577952484600246
batch 55 loss: 0.0005577337113209068
batch 60 loss: 0.0005577327334322036
batch 65 loss: 0.0005576781462877989
batch 70 loss: 0.0005576689261943101
batch 75 loss: 0.0005577867501415313
batch 80 loss: 0.0005576901021413505
batch 85 loss: 0.0005576563882641495
batch 90 loss: 0.0005576088326051832
batch 95 loss: 0.0005576144787482918
batch 100 loss: 0.0005575895891524851
batch 105 loss: 0.0005576988798566163
batch 110 loss: 0.0005577410920523107
batch 115 loss: 0.0005577117786742747
batch 120 loss: 0.0005576419760473072
batch 125 loss: 0.0005576896714046597
batch 130 loss: 0.0005577522329986096
batch 135 loss: 0.00055770498001948
batch 140 loss: 0.0005576732219196856
batch 145 loss: 0.000557589519303292
batch 150 loss: 0.0005577020812779665
batch 155 loss: 0.0005577407311648131
batch 160 loss: 0.0005576253752224147
batch 165 loss: 0.0005577001720666885
batch 170 loss: 0.0005577154224738479
batch 175 loss: 0.0005577481235377491
batch 180 loss: 0.0005576184252277016
batch 185 loss: 0.0005576880532316864
batch 190 loss: 0.0005576611845754087
batch 195 loss: 0.000557798775844276
batch 200 loss: 0.0005577310919761657
batch 205 loss: 0.0005577479838393629
batch 210 loss: 0.0005576469469815492
batch 215 loss: 0.0005576996598392725
batch 220 loss: 0.0005576388095505536
batch 225 loss: 0.0005577612551860512
batch 230 loss: 0.0005576772498898208
batch 235 loss: 0.0005576135125011205
batch 240 loss: 0.0005576271913014352
Training Loss: 0.0005576833878876642
Validation Loss: 0.0005576934607233852
Epoch 88:
batch 5 loss: 0.0005576872965320945
batch 10 loss: 0.0005577328032813966
batch 15 loss: 0.000557660311460495
batch 20 loss: 0.0005576901137828826
batch 25 loss: 0.0005576935247518123
batch 30 loss: 0.0005576741066761314
batch 35 loss: 0.0005576563999056816
batch 40 loss: 0.0005576442345045507
batch 45 loss: 0.0005576880765147507
batch 50 loss: 0.0005576794850639999
batch 55 loss: 0.0005576360621489584
batch 60 loss: 0.0005575865041464567
batch 65 loss: 0.0005577133153565228
batch 70 loss: 0.0005577747011557222
batch 75 loss: 0.0005576863419264555
batch 80 loss: 0.0005576661205850542
batch 85 loss: 0.0005576692055910826
batch 90 loss: 0.0005576022667810321
batch 95 loss: 0.0005576967843808234
batch 100 loss: 0.0005576935829594732
batch 105 loss: 0.0005576945492066443
batch 110 loss: 0.0005576730705797672
batch 115 loss: 0.0005578138632699847
batch 120 loss: 0.0005576539202593267
batch 125 loss: 0.0005576268071308732
batch 130 loss: 0.0005575875286012888
batch 135 loss: 0.0005577539093792438
batch 140 loss: 0.000557691475842148
batch 145 loss: 0.0005576142924837768
batch 150 loss: 0.0005577146075665951
batch 155 loss: 0.0005577117088250815
batch 160 loss: 0.0005576879833824932
batch 165 loss: 0.0005575504386797547
batch 170 loss: 0.0005577271222136915
batch 175 loss: 0.0005576820927672088
batch 180 loss: 0.0005576032446697355
batch 185 loss: 0.0005577452480793
batch 190 loss: 0.0005576380528509617
batch 195 loss: 0.0005578204873017967
batch 200 loss: 0.00055770332692191
batch 205 loss: 0.0005576932453550398
batch 210 loss: 0.0005576722323894501
batch 215 loss: 0.0005577105446718633
batch 220 loss: 0.0005577879725024104
batch 225 loss: 0.0005576104274950921
batch 230 loss: 0.0005578508484177291
batch 235 loss: 0.0005576947238296271
batch 240 loss: 0.000557557528372854
Training Loss: 0.000557683385219813
Validation Loss: 0.0005576934063962351
Epoch 89:
batch 5 loss: 0.0005577326519414783
batch 10 loss: 0.0005577118718065321
batch 15 loss: 0.0005576434661634267
batch 20 loss: 0.0005576243274845183
batch 25 loss: 0.0005576539551839232
batch 30 loss: 0.0005575930466875434
batch 35 loss: 0.0005577231058850884
batch 40 loss: 0.0005577506148256361
batch 45 loss: 0.000557764177210629
batch 50 loss: 0.0005577887874096632
batch 55 loss: 0.0005576198920607567
batch 60 loss: 0.000557643745560199
batch 65 loss: 0.0005576276918873191
batch 70 loss: 0.0005577001255005598
batch 75 loss: 0.0005577092291787267
batch 80 loss: 0.0005576260504312813
batch 85 loss: 0.0005577041651122272
batch 90 loss: 0.0005577201023697853
batch 95 loss: 0.0005577241303399205
batch 100 loss: 0.0005576591356657445
batch 105 loss: 0.000557611440308392
batch 110 loss: 0.0005577727220952511
batch 115 loss: 0.000557638902682811
batch 120 loss: 0.0005576124880462885
batch 125 loss: 0.0005576504743658006
batch 130 loss: 0.0005576762254349887
batch 135 loss: 0.0005577635951340198
batch 140 loss: 0.0005576635361649096
batch 145 loss: 0.0005575900315307081
batch 150 loss: 0.0005577437113970518
batch 155 loss: 0.0005576832802034915
batch 160 loss: 0.0005576510564424097
batch 165 loss: 0.000557689880952239
batch 170 loss: 0.0005576835596002638
batch 175 loss: 0.0005576316732913255
batch 180 loss: 0.0005576743860729039
batch 185 loss: 0.0005577023606747389
batch 190 loss: 0.0005576733616180718
batch 195 loss: 0.0005577278789132833
batch 200 loss: 0.0005576195893809199
batch 205 loss: 0.0005578125943429768
batch 210 loss: 0.0005577330361120403
batch 215 loss: 0.0005576814175583423
batch 220 loss: 0.0005577354808337986
batch 225 loss: 0.0005576110910624265
batch 230 loss: 0.0005577245727181434
batch 235 loss: 0.0005576549447141588
batch 240 loss: 0.0005576689029112459
Training Loss: 0.0005576833847347492
Validation Loss: 0.0005576934112468734
Epoch 90:
batch 5 loss: 0.0005576476338319481
batch 10 loss: 0.0005576680763624608
batch 15 loss: 0.0005577218136750162
batch 20 loss: 0.00055766177829355
batch 25 loss: 0.0005575930932536721
batch 30 loss: 0.0005577142350375652
batch 35 loss: 0.0005576429539360106
batch 40 loss: 0.0005576494382694364
batch 45 loss: 0.0005577563308179379
batch 50 loss: 0.0005576300900429487
batch 55 loss: 0.0005576867726631463
batch 60 loss: 0.0005576576339080929
batch 65 loss: 0.0005576734431087971
batch 70 loss: 0.0005576473195105791
batch 75 loss: 0.0005577545030973852
batch 80 loss: 0.0005576212657615543
batch 85 loss: 0.0005577798467129469
batch 90 loss: 0.0005576042807660997
batch 95 loss: 0.0005576186696998775
batch 100 loss: 0.0005576553987339139
batch 105 loss: 0.0005577254574745894
batch 110 loss: 0.0005576916271820664
batch 115 loss: 0.0005576362716965377
batch 120 loss: 0.0005576517200097441
batch 125 loss: 0.0005577454227022827
batch 130 loss: 0.00055770498001948
batch 135 loss: 0.0005577816627919674
batch 140 loss: 0.0005578198470175266
batch 145 loss: 0.0005577031057327986
batch 150 loss: 0.0005577015108428895
batch 155 loss: 0.0005577564239501953
batch 160 loss: 0.0005576563416980207
batch 165 loss: 0.0005576458061113954
batch 170 loss: 0.0005576521274633705
batch 175 loss: 0.000557698158081621
batch 180 loss: 0.0005577207426540554
batch 185 loss: 0.0005576554802246391
batch 190 loss: 0.0005575792165473104
batch 195 loss: 0.0005576748168095947
batch 200 loss: 0.0005577042000368237
batch 205 loss: 0.0005576710100285709
batch 210 loss: 0.0005575671792030334
batch 215 loss: 0.0005577020579949022
batch 220 loss: 0.0005577067611739039
batch 225 loss: 0.0005578281125053763
batch 230 loss: 0.0005577220465056599
batch 235 loss: 0.0005575964343734086
batch 240 loss: 0.0005577191128395498
Training Loss: 0.0005576833793990469
Validation Loss: 0.0005576934131871288
Epoch 91:
batch 5 loss: 0.0005576884606853128
batch 10 loss: 0.0005576015217229723
batch 15 loss: 0.0005577091127634048
batch 20 loss: 0.0005576659925282002
batch 25 loss: 0.0005577056785114109
batch 30 loss: 0.0005576638854108751
batch 35 loss: 0.0005576770869083703
batch 40 loss: 0.0005576545372605324
batch 45 loss: 0.0005576100782491266
batch 50 loss: 0.0005575989256612956
batch 55 loss: 0.0005577468313276768
batch 60 loss: 0.0005575902410782874
batch 65 loss: 0.0005576901137828826
batch 70 loss: 0.0005576212424784899
batch 75 loss: 0.0005576597875915467
batch 80 loss: 0.0005576573661528528
batch 85 loss: 0.000557698484044522
batch 90 loss: 0.0005576657364144922
batch 95 loss: 0.0005576560040935874
batch 100 loss: 0.0005575980525463819
batch 105 loss: 0.0005577064002864063
batch 110 loss: 0.0005577731411904096
batch 115 loss: 0.0005576171795837581
batch 120 loss: 0.0005577085306867957
batch 125 loss: 0.0005577524425461888
batch 130 loss: 0.000557771825697273
batch 135 loss: 0.000557638902682811
batch 140 loss: 0.0005577151663601398
batch 145 loss: 0.0005576230119913817
batch 150 loss: 0.0005576782859861851
batch 155 loss: 0.0005575963761657477
batch 160 loss: 0.00055769745958969
batch 165 loss: 0.0005576925934292376
batch 170 loss: 0.0005577318836003542
batch 175 loss: 0.0005576482741162181
batch 180 loss: 0.0005577078321948647
batch 185 loss: 0.0005576416500844061
batch 190 loss: 0.0005576944909989834
batch 195 loss: 0.0005577273899689317
batch 200 loss: 0.0005577789735980332
batch 205 loss: 0.0005575950257480144
batch 210 loss: 0.000557807192672044
batch 215 loss: 0.0005577536998316645
batch 220 loss: 0.0005577198578976094
batch 225 loss: 0.0005577021511271596
batch 230 loss: 0.000557712628506124
batch 235 loss: 0.0005577275762334466
batch 240 loss: 0.0005577232921496033
Training Loss: 0.0005576833827944938
Validation Loss: 0.0005576933957248306
Epoch 92:
batch 5 loss: 0.0005576874478720129
batch 10 loss: 0.0005576819647103548
batch 15 loss: 0.0005577441886998713
batch 20 loss: 0.0005576662020757795
batch 25 loss: 0.0005577385076321661
batch 30 loss: 0.0005576523370109498
batch 35 loss: 0.0005575675633735955
batch 40 loss: 0.0005576933384872973
batch 45 loss: 0.0005576156894676387
batch 50 loss: 0.0005576593568548561
batch 55 loss: 0.0005576404160819948
batch 60 loss: 0.0005576825467869639
batch 65 loss: 0.0005576400784775615
batch 70 loss: 0.0005576206953264773
batch 75 loss: 0.000557587225921452
batch 80 loss: 0.0005575877381488681
batch 85 loss: 0.0005577622679993511
batch 90 loss: 0.0005576166557148099
batch 95 loss: 0.0005576533381827176
batch 100 loss: 0.0005577237927354873
batch 105 loss: 0.0005576849333010613
batch 110 loss: 0.0005576654453761876
batch 115 loss: 0.0005576754454523325
batch 120 loss: 0.0005578865646384656
batch 125 loss: 0.0005576843162998557
batch 130 loss: 0.0005576875875703991
batch 135 loss: 0.0005577151780016721
batch 140 loss: 0.0005576417781412602
batch 145 loss: 0.0005576862138696014
batch 150 loss: 0.0005578036652877927
batch 155 loss: 0.000557696120813489
batch 160 loss: 0.0005576762021519243
batch 165 loss: 0.0005576100666075945
batch 170 loss: 0.00055771543411538
batch 175 loss: 0.0005576200550422072
batch 180 loss: 0.0005578391952440143
batch 185 loss: 0.0005577296717092395
batch 190 loss: 0.0005577479605562985
batch 195 loss: 0.0005576337571255863
batch 200 loss: 0.0005576921510510146
batch 205 loss: 0.0005577034549787641
batch 210 loss: 0.0005576366675086319
batch 215 loss: 0.0005576803931035101
batch 220 loss: 0.0005576346418820321
batch 225 loss: 0.000557649729307741
batch 230 loss: 0.0005577065167017281
batch 235 loss: 0.0005577538162469864
batch 240 loss: 0.0005577238625846803
Training Loss: 0.0005576833786714512
Validation Loss: 0.0005576933986352135
Epoch 93:
batch 5 loss: 0.0005576269933953882
batch 10 loss: 0.0005577251315116883
batch 15 loss: 0.0005576636176556349
batch 20 loss: 0.0005577094387263059
batch 25 loss: 0.0005576179595664143
batch 30 loss: 0.0005577004048973322
batch 35 loss: 0.0005576617899350822
batch 40 loss: 0.0005576312425546349
batch 45 loss: 0.0005577443283982575
batch 50 loss: 0.0005577818141318857
batch 55 loss: 0.0005578141892328859
batch 60 loss: 0.0005577118252404034
batch 65 loss: 0.0005578190204687417
batch 70 loss: 0.0005575697985477745
batch 75 loss: 0.0005576399504207075
batch 80 loss: 0.0005578225594945252
batch 85 loss: 0.0005577561561949552
batch 90 loss: 0.000557713897433132
batch 95 loss: 0.0005577116506174206
batch 100 loss: 0.0005576839670538902
batch 105 loss: 0.0005576791940256953
batch 110 loss: 0.0005576222902163863
batch 115 loss: 0.0005575137678533792
batch 120 loss: 0.0005576728028245271
batch 125 loss: 0.0005576130002737046
batch 130 loss: 0.0005576168419793248
batch 135 loss: 0.0005576366442255676
batch 140 loss: 0.0005577310454100371
batch 145 loss: 0.0005577642354182899
batch 150 loss: 0.0005576413474045694
batch 155 loss: 0.0005576580646447837
batch 160 loss: 0.000557617808226496
batch 165 loss: 0.000557710020802915
batch 170 loss: 0.0005575643852353096
batch 175 loss: 0.0005576168652623892
batch 180 loss: 0.000557710905559361
batch 185 loss: 0.0005577513133175671
batch 190 loss: 0.0005577129079028964
batch 195 loss: 0.0005576598225161434
batch 200 loss: 0.0005577729665674269
batch 205 loss: 0.0005576367955654859
batch 210 loss: 0.0005576998926699161
batch 215 loss: 0.0005577304284088314
batch 220 loss: 0.0005577191244810819
batch 225 loss: 0.000557663629297167
batch 230 loss: 0.0005577281932346523
batch 235 loss: 0.000557617680169642
batch 240 loss: 0.0005576345953159034
Training Loss: 0.0005576833813393023
Validation Loss: 0.0005576933996053413
Epoch 94:
batch 5 loss: 0.0005575253395363689
batch 10 loss: 0.0005575512652285397
batch 15 loss: 0.000557607610244304
batch 20 loss: 0.0005577053409069777
batch 25 loss: 0.0005576918367296458
batch 30 loss: 0.0005576708470471203
batch 35 loss: 0.0005576316150836646
batch 40 loss: 0.0005577068775892257
batch 45 loss: 0.0005576301133260131
batch 50 loss: 0.0005577198695391417
batch 55 loss: 0.0005575917079113424
batch 60 loss: 0.0005576239433139563
batch 65 loss: 0.0005577641306445003
batch 70 loss: 0.0005577110219746828
batch 75 loss: 0.000557650369592011
batch 80 loss: 0.000557704537641257
batch 85 loss: 0.0005576606839895248
batch 90 loss: 0.0005577290314249695
batch 95 loss: 0.0005576705443672836
batch 100 loss: 0.0005576343042775989
batch 105 loss: 0.0005577072501182556
batch 110 loss: 0.0005577148403972387
batch 115 loss: 0.0005576885188929737
batch 120 loss: 0.0005576396128162741
batch 125 loss: 0.0005576265743002295
batch 130 loss: 0.0005577443283982575
batch 135 loss: 0.0005575996357947588
batch 140 loss: 0.0005577578907832503
batch 145 loss: 0.0005577699048444629
batch 150 loss: 0.0005576709983870388
batch 155 loss: 0.000557656807359308
batch 160 loss: 0.0005578019423410297
batch 165 loss: 0.0005575831048190594
batch 170 loss: 0.0005577230243943631
batch 175 loss: 0.0005576336639933288
batch 180 loss: 0.0005576850846409798
batch 185 loss: 0.0005576274124905467
batch 190 loss: 0.0005577046773396433
batch 195 loss: 0.0005577789153903723
batch 200 loss: 0.0005576396826654673
batch 205 loss: 0.0005578166339546442
batch 210 loss: 0.0005577242700383067
batch 215 loss: 0.000557668786495924
batch 220 loss: 0.0005577451200224459
batch 225 loss: 0.0005577416159212589
batch 230 loss: 0.000557722372468561
batch 235 loss: 0.0005576789262704551
batch 240 loss: 0.0005577695672400295
Training Loss: 0.0005576833781863873
Validation Loss: 0.0005576934170676395
Epoch 95:
batch 5 loss: 0.0005577012314461171
batch 10 loss: 0.0005575583199970424
batch 15 loss: 0.00055766636505723
batch 20 loss: 0.0005576354917138815
batch 25 loss: 0.0005575955961830914
batch 30 loss: 0.0005576055264100432
batch 35 loss: 0.0005575821385718882
batch 40 loss: 0.0005576855503022671
batch 45 loss: 0.0005576800787821413
batch 50 loss: 0.0005577057716436684
batch 55 loss: 0.0005577761679887771
batch 60 loss: 0.0005578231881372631
batch 65 loss: 0.000557598820887506
batch 70 loss: 0.0005576892872340977
batch 75 loss: 0.0005577696021646262
batch 80 loss: 0.0005576984258368611
batch 85 loss: 0.0005577607080340385
batch 90 loss: 0.0005576879368163646
batch 95 loss: 0.0005577047355473042
batch 100 loss: 0.0005575761781074107
batch 105 loss: 0.0005576469469815492
batch 110 loss: 0.0005576415802352131
batch 115 loss: 0.0005579030374065041
batch 120 loss: 0.0005576432682573796
batch 125 loss: 0.0005577271338552236
batch 130 loss: 0.0005576941068284214
batch 135 loss: 0.0005576245603151619
batch 140 loss: 0.0005577291827648878
batch 145 loss: 0.0005577542935498059
batch 150 loss: 0.0005577468662522733
batch 155 loss: 0.0005576842813752592
batch 160 loss: 0.0005577706382609904
batch 165 loss: 0.0005577350384555757
batch 170 loss: 0.0005575739545747638
batch 175 loss: 0.00055770268663764
batch 180 loss: 0.0005577567499130964
batch 185 loss: 0.0005576792755164206
batch 190 loss: 0.0005576313706114888
batch 195 loss: 0.0005577273201197386
batch 200 loss: 0.0005577445845119655
batch 205 loss: 0.0005576643859967589
batch 210 loss: 0.0005576436291448772
batch 215 loss: 0.000557655084412545
batch 220 loss: 0.0005576972151175141
batch 225 loss: 0.0005576616269536317
batch 230 loss: 0.0005576396593824029
batch 235 loss: 0.0005576441646553576
batch 240 loss: 0.0005575782153755427
Training Loss: 0.0005576833745484085
Validation Loss: 0.0005576934025157243
Epoch 96:
batch 5 loss: 0.000557668600231409
batch 10 loss: 0.0005576308118179441
batch 15 loss: 0.000557783292606473
batch 20 loss: 0.0005577322794124484
batch 25 loss: 0.0005577911040745675
batch 30 loss: 0.0005577223957516253
batch 35 loss: 0.000557614304125309
batch 40 loss: 0.0005577552597969771
batch 45 loss: 0.0005577171221375465
batch 50 loss: 0.0005577262840233743
batch 55 loss: 0.0005576635710895061
batch 60 loss: 0.0005576318711973727
batch 65 loss: 0.0005576816038228571
batch 70 loss: 0.0005576821160502732
batch 75 loss: 0.0005576199269853533
batch 80 loss: 0.0005577599746175111
batch 85 loss: 0.0005576995434239506
batch 90 loss: 0.000557720463257283
batch 95 loss: 0.0005576301366090775
batch 100 loss: 0.0005576761206611991
batch 105 loss: 0.0005576628493145109
batch 110 loss: 0.0005577469477429986
batch 115 loss: 0.0005577570991590619
batch 120 loss: 0.0005575750605203211
batch 125 loss: 0.0005577063653618098
batch 130 loss: 0.0005576792755164206
batch 135 loss: 0.0005577318486757576
batch 140 loss: 0.0005575922783464194
batch 145 loss: 0.0005577158066444099
batch 150 loss: 0.0005576194380410016
batch 155 loss: 0.0005576252820901573
batch 160 loss: 0.0005576291587203741
batch 165 loss: 0.0005576964234933257
batch 170 loss: 0.000557699496857822
batch 175 loss: 0.0005576951778493821
batch 180 loss: 0.0005576542927883566
batch 185 loss: 0.0005576912080869079
batch 190 loss: 0.0005576996714808047
batch 195 loss: 0.0005576613009907305
batch 200 loss: 0.0005576775176450611
batch 205 loss: 0.0005576873663812876
batch 210 loss: 0.0005576409050263465
batch 215 loss: 0.0005576011491939426
batch 220 loss: 0.0005576382973231375
batch 225 loss: 0.0005576634895987809
batch 230 loss: 0.0005578445852734149
batch 235 loss: 0.0005576477153226734
batch 240 loss: 0.0005576552357524633
Training Loss: 0.0005576833755185362
Validation Loss: 0.0005576933986352135
Epoch 97:
batch 5 loss: 0.0005576158524490893
batch 10 loss: 0.0005576441646553576
batch 15 loss: 0.000557718554046005
batch 20 loss: 0.0005578047945164144
batch 25 loss: 0.0005576423718594015
batch 30 loss: 0.0005576674710027873
batch 35 loss: 0.0005576331866905093
batch 40 loss: 0.0005577191594056785
batch 45 loss: 0.0005577074480243027
batch 50 loss: 0.0005576150608249009
batch 55 loss: 0.0005576358991675079
batch 60 loss: 0.0005578073323704302
batch 65 loss: 0.000557648716494441
batch 70 loss: 0.000557750416919589
batch 75 loss: 0.0005577102769166231
batch 80 loss: 0.0005577598698437214
batch 85 loss: 0.00055773607455194
batch 90 loss: 0.0005577425821684301
batch 95 loss: 0.0005576706025749445
batch 100 loss: 0.0005577233270742
batch 105 loss: 0.0005575909628532827
batch 110 loss: 0.000557606655638665
batch 115 loss: 0.0005576477851718664
batch 120 loss: 0.0005577116389758885
batch 125 loss: 0.0005576031515374779
batch 130 loss: 0.0005577383446507156
batch 135 loss: 0.0005576999974437058
batch 140 loss: 0.0005577089847065509
batch 145 loss: 0.0005577443516813219
batch 150 loss: 0.000557741557713598
batch 155 loss: 0.000557606655638665
batch 160 loss: 0.0005575088318437338
batch 165 loss: 0.0005577134666964412
batch 170 loss: 0.0005577667965553701
batch 175 loss: 0.0005576926865614951
batch 180 loss: 0.0005577158066444099
batch 185 loss: 0.0005576533731073141
batch 190 loss: 0.0005576170049607753
batch 195 loss: 0.000557794061023742
batch 200 loss: 0.0005576471332460642
batch 205 loss: 0.0005575836519710719
batch 210 loss: 0.000557711033616215
batch 215 loss: 0.0005576451192609965
batch 220 loss: 0.0005575634655542672
batch 225 loss: 0.0005577182280831039
batch 230 loss: 0.0005577591364271939
batch 235 loss: 0.0005577268544584513
batch 240 loss: 0.0005576320807449519
Training Loss: 0.0005576833745484085
Validation Loss: 0.0005576933976650859
Epoch 98:
batch 5 loss: 0.0005576805444434286
batch 10 loss: 0.0005576538736931979
batch 15 loss: 0.0005577985430136323
batch 20 loss: 0.0005576656083576381
batch 25 loss: 0.000557783362455666
batch 30 loss: 0.0005576714524067938
batch 35 loss: 0.0005576762254349887
batch 40 loss: 0.0005577004281803966
batch 45 loss: 0.0005577119998633861
batch 50 loss: 0.0005577297299169004
batch 55 loss: 0.0005578275537118316
batch 60 loss: 0.0005576624418608845
batch 65 loss: 0.0005576783209107816
batch 70 loss: 0.0005576543044298887
batch 75 loss: 0.0005576772964559495
batch 80 loss: 0.000557649484835565
batch 85 loss: 0.0005576081108301878
batch 90 loss: 0.0005576843046583235
batch 95 loss: 0.0005576412077061832
batch 100 loss: 0.0005577446310780943
batch 105 loss: 0.0005576607654802501
batch 110 loss: 0.0005576433730311691
batch 115 loss: 0.0005576888099312783
batch 120 loss: 0.0005576418712735176
batch 125 loss: 0.0005576347233727574
batch 130 loss: 0.0005577582400292158
batch 135 loss: 0.0005576506373472512
batch 140 loss: 0.000557786738499999
batch 145 loss: 0.0005575183313339949
batch 150 loss: 0.0005576425814069807
batch 155 loss: 0.0005575866787694394
batch 160 loss: 0.0005576457362622023
batch 165 loss: 0.000557810440659523
batch 170 loss: 0.0005577072617597878
batch 175 loss: 0.0005576932104304433
batch 180 loss: 0.0005576277966611087
batch 185 loss: 0.0005576518480665982
batch 190 loss: 0.0005577109172008931
batch 195 loss: 0.0005576448515057564
batch 200 loss: 0.0005576257477514446
batch 205 loss: 0.000557651009876281
batch 210 loss: 0.000557612394914031
batch 215 loss: 0.0005577580770477653
batch 220 loss: 0.0005577189265750348
batch 225 loss: 0.0005576703581027687
batch 230 loss: 0.0005576605326496065
batch 235 loss: 0.0005577064119279384
batch 240 loss: 0.0005577943054959178
Training Loss: 0.0005576833750334724
Validation Loss: 0.0005576934015455966
Epoch 99:
batch 5 loss: 0.0005577267380431295
batch 10 loss: 0.0005576138268224895
batch 15 loss: 0.0005575731047429144
batch 20 loss: 0.000557562243193388
batch 25 loss: 0.0005578039330430329
batch 30 loss: 0.0005576931172981859
batch 35 loss: 0.0005575899966061115
batch 40 loss: 0.0005575354909524322
batch 45 loss: 0.0005577542120590806
batch 50 loss: 0.0005576331750489771
batch 55 loss: 0.0005576445488259196
batch 60 loss: 0.000557742826640606
batch 65 loss: 0.0005577289382927119
batch 70 loss: 0.0005577242700383067
batch 75 loss: 0.0005577269126661122
batch 80 loss: 0.0005576528841629624
batch 85 loss: 0.0005576494382694364
batch 90 loss: 0.0005576286814175547
batch 95 loss: 0.0005577698233537376
batch 100 loss: 0.0005576226743869483
batch 105 loss: 0.0005576233379542828
batch 110 loss: 0.00055771607439965
batch 115 loss: 0.0005576147115789354
batch 120 loss: 0.000557718810159713
batch 125 loss: 0.0005577489617280662
batch 130 loss: 0.0005575997754931449
batch 135 loss: 0.0005576708819717169
batch 140 loss: 0.0005576950265094638
batch 145 loss: 0.0005576807423494756
batch 150 loss: 0.0005576772964559495
batch 155 loss: 0.0005576936178840697
batch 160 loss: 0.0005577293690294027
batch 165 loss: 0.0005576384952291846
batch 170 loss: 0.0005577755626291036
batch 175 loss: 0.0005577851086854934
batch 180 loss: 0.0005576380644924939
batch 185 loss: 0.0005577415809966624
batch 190 loss: 0.0005577055620960891
batch 195 loss: 0.0005576504161581397
batch 200 loss: 0.0005576643394306302
batch 205 loss: 0.0005576937925070524
batch 210 loss: 0.0005577199626713991
batch 215 loss: 0.0005577528383582831
batch 220 loss: 0.0005576767376624048
batch 225 loss: 0.0005576239782385528
batch 230 loss: 0.000557724665850401
batch 235 loss: 0.000557640683837235
batch 240 loss: 0.0005578247248195112
Training Loss: 0.0005576833740633447
Validation Loss: 0.0005576934005754689
Epoch 100:
batch 5 loss: 0.0005576909868977963
batch 10 loss: 0.0005577034433372318
batch 15 loss: 0.0005577628733590245
batch 20 loss: 0.0005576750729233027
batch 25 loss: 0.0005577053874731064
batch 30 loss: 0.0005576923722401261
batch 35 loss: 0.0005576558411121369
batch 40 loss: 0.0005577183095738292
batch 45 loss: 0.0005576309165917337
batch 50 loss: 0.000557697075419128
batch 55 loss: 0.0005576186347752809
batch 60 loss: 0.0005577402422204614
batch 65 loss: 0.0005576906609348953
batch 70 loss: 0.0005576807423494756
batch 75 loss: 0.0005577217205427587
batch 80 loss: 0.0005576655035838485
batch 85 loss: 0.0005577722331508994
batch 90 loss: 0.0005577203584834934
batch 95 loss: 0.0005575684830546379
batch 100 loss: 0.0005576570983976126
batch 105 loss: 0.0005576772266067565
batch 110 loss: 0.0005575680872425437
batch 115 loss: 0.0005576521856710315
batch 120 loss: 0.0005575768533162773
batch 125 loss: 0.0005575601710006595
batch 130 loss: 0.0005576216382905841
batch 135 loss: 0.0005576453637331724
batch 140 loss: 0.0005577333155088127
batch 145 loss: 0.0005576667375862598
batch 150 loss: 0.0005575531045906246
batch 155 loss: 0.0005577589152380824
batch 160 loss: 0.0005576338386163116
batch 165 loss: 0.0005576141877099872
batch 170 loss: 0.0005577660165727138
batch 175 loss: 0.0005577960168011486
batch 180 loss: 0.0005577719188295305
batch 185 loss: 0.0005577351665124297
batch 190 loss: 0.0005576498457230628
batch 195 loss: 0.0005576758645474911
batch 200 loss: 0.0005577462143264711
batch 205 loss: 0.0005577731295488775
batch 210 loss: 0.0005576590774580836
batch 215 loss: 0.0005577196832746267
batch 220 loss: 0.0005576772964559495
batch 225 loss: 0.0005576945142820478
batch 230 loss: 0.000557742326054722
batch 235 loss: 0.0005576759693212807
batch 240 loss: 0.0005576892406679689
Training Loss: 0.0005576833721230893
Validation Loss: 0.0005576934015455966
