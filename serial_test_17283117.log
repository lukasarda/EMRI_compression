no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 1400
20240628_035500
Epoch 1:
batch 5 loss: 0.01667121658101678
batch 10 loss: 0.0040846336632966995
batch 15 loss: 0.002516527660191059
batch 20 loss: 0.0020828394452109935
batch 25 loss: 0.001665098243393004
batch 30 loss: 0.0014575627632439137
batch 35 loss: 0.0012571850791573524
batch 40 loss: 0.0011045077349990605
batch 45 loss: 0.0009705362725071609
batch 50 loss: 0.0008309901808388531
batch 55 loss: 0.000777839682996273
batch 60 loss: 0.0007981290691532195
batch 65 loss: 0.0007247524219565094
batch 70 loss: 0.0006697750650346279
batch 75 loss: 0.0006565341842360795
batch 80 loss: 0.000653130526188761
batch 85 loss: 0.0006486230297014118
batch 90 loss: 0.0006541474373079837
batch 95 loss: 0.0008437786134891212
batch 100 loss: 0.0006874691811390221
batch 105 loss: 0.000651865522377193
batch 110 loss: 0.0006496379035525024
batch 115 loss: 0.000649261218495667
batch 120 loss: 0.0006485678604803979
batch 125 loss: 0.0006471775704994798
batch 130 loss: 0.0006453955895267427
batch 135 loss: 0.0006432455265894532
batch 140 loss: 0.000640886987093836
batch 145 loss: 0.0006380638224072754
batch 150 loss: 0.0006334663718007505
batch 155 loss: 0.0006670864066109061
batch 160 loss: 0.0007270242553204298
batch 165 loss: 0.0006736873881891369
batch 170 loss: 0.0006421524100005627
batch 175 loss: 0.0006408237852156163
batch 180 loss: 0.0006402979604899883
batch 185 loss: 0.0006387611967511476
batch 190 loss: 0.0006365115754306316
batch 195 loss: 0.000633994210511446
batch 200 loss: 0.0006312685669399798
batch 205 loss: 0.0006290414719842374
batch 210 loss: 0.0006273715756833553
batch 215 loss: 0.0006255628657527267
batch 220 loss: 0.0006229587248526514
batch 225 loss: 0.000619790330529213
batch 230 loss: 0.0006171498564071954
batch 235 loss: 0.0006124115781858564
batch 240 loss: 0.0006080192164517939
Training Loss: 0.0012020158038164178
Validation Loss: 0.0005886844854103401
Epoch 2:
batch 5 loss: 0.0006051124539226294
batch 10 loss: 0.0006037110113538801
batch 15 loss: 0.0006026636809110641
batch 20 loss: 0.0006018261541612446
batch 25 loss: 0.0006006110459566116
batch 30 loss: 0.0005994466133415699
batch 35 loss: 0.0005982774426229298
batch 40 loss: 0.0005970683298073709
batch 45 loss: 0.000595684302970767
batch 50 loss: 0.0005936952191404998
batch 55 loss: 0.0005921101896092295
batch 60 loss: 0.0005907522398047149
batch 65 loss: 0.0005898482631891966
batch 70 loss: 0.0005917421542108059
batch 75 loss: 0.0005894716246984899
batch 80 loss: 0.0005875538918189704
batch 85 loss: 0.0005869136890396476
batch 90 loss: 0.0005861280602402985
batch 95 loss: 0.0005884393118321896
batch 100 loss: 0.0005848326603882015
batch 105 loss: 0.000583843095228076
batch 110 loss: 0.0005830587237142026
batch 115 loss: 0.0005821169703267515
batch 120 loss: 0.0015778710832819343
batch 125 loss: 0.001317593059502542
batch 130 loss: 0.0009246346540749073
batch 135 loss: 0.0008204965270124376
batch 140 loss: 0.0007667926955036819
batch 145 loss: 0.0007029119762592018
batch 150 loss: 0.0006595165701583028
batch 155 loss: 0.000646972341928631
batch 160 loss: 0.0006339277140796184
batch 165 loss: 0.000613092293497175
batch 170 loss: 0.0006056561600416899
batch 175 loss: 0.000601394206751138
batch 180 loss: 0.0005961144459433853
batch 185 loss: 0.0005909124971367419
batch 190 loss: 0.000585864030290395
batch 195 loss: 0.0005820152931846678
batch 200 loss: 0.0005795748322270811
batch 205 loss: 0.0005779839702881873
batch 210 loss: 0.0005768300965428352
batch 215 loss: 0.0005757159902714193
batch 220 loss: 0.0005749470554292202
batch 225 loss: 0.0005743285291828215
batch 230 loss: 0.0005736210616305471
batch 235 loss: 0.0005729037104174495
batch 240 loss: 0.0005724830785766244
Training Loss: 0.0006466471041979579
Validation Loss: 0.0005701508305113142
Epoch 3:
batch 5 loss: 0.0005719059845432639
batch 10 loss: 0.0005714880418963731
batch 15 loss: 0.0005711033241823316
batch 20 loss: 0.0005707202479243278
batch 25 loss: 0.0005703772534616291
batch 30 loss: 0.0005700608366169035
batch 35 loss: 0.0005699187866412103
batch 40 loss: 0.0005696664331480861
batch 45 loss: 0.0005695026251487434
batch 50 loss: 0.0005693520186468958
batch 55 loss: 0.0005693485727533698
batch 60 loss: 0.0005692797130905092
batch 65 loss: 0.0005691786645911634
batch 70 loss: 0.0005692607956007123
batch 75 loss: 0.0005691495374776423
batch 80 loss: 0.0005688630510121584
batch 85 loss: 0.000568628590553999
batch 90 loss: 0.0005682849558070302
batch 95 loss: 0.0005682274582795799
batch 100 loss: 0.0005680810078047216
batch 105 loss: 0.0005682484828867018
batch 110 loss: 0.0005682380753569305
batch 115 loss: 0.0005680270725861192
batch 120 loss: 0.0005679994821548462
batch 125 loss: 0.0005677595618180931
batch 130 loss: 0.000568092882167548
batch 135 loss: 0.0005679559078998863
batch 140 loss: 0.0005678547895513475
batch 145 loss: 0.0005674220970831812
batch 150 loss: 0.0005673606763593852
batch 155 loss: 0.0005672186496667563
batch 160 loss: 0.0005671426770277321
batch 165 loss: 0.0005672358209267258
batch 170 loss: 0.0005673265666700899
batch 175 loss: 0.0005672532715834677
batch 180 loss: 0.0005672561936080455
batch 185 loss: 0.0005673834355548024
batch 190 loss: 0.0005672383587807417
batch 195 loss: 0.0005673722946085036
batch 200 loss: 0.0005670404643751681
batch 205 loss: 0.0005668285302817822
batch 210 loss: 0.0005666886805556715
batch 215 loss: 0.000566635187715292
batch 220 loss: 0.0005663840798661113
batch 225 loss: 0.000566485000308603
batch 230 loss: 0.0005664190393872559
batch 235 loss: 0.0005665950127877295
batch 240 loss: 0.0005668607540428638
Training Loss: 0.0005682650196831673
Validation Loss: 0.0005666247724244992
Epoch 4:
batch 5 loss: 0.0005666784010827542
batch 10 loss: 0.0005662286770530045
batch 15 loss: 0.0005662319832481444
batch 20 loss: 0.0005660985712893307
batch 25 loss: 0.0005660885362885892
batch 30 loss: 0.0005658520269207657
batch 35 loss: 0.0005660410621203482
batch 40 loss: 0.0005659430520609021
batch 45 loss: 0.0005658169975504279
batch 50 loss: 0.0005658673355355859
batch 55 loss: 0.000565992621704936
batch 60 loss: 0.0005658039124682545
batch 65 loss: 0.0005658270558342338
batch 70 loss: 0.0005655777757056057
batch 75 loss: 0.0005658554611727596
batch 80 loss: 0.0005658796522766352
batch 85 loss: 0.0005657625733874738
batch 90 loss: 0.0005656556924805046
batch 95 loss: 0.0005655827117152512
batch 100 loss: 0.0005657424102537334
batch 105 loss: 0.0005656268680468201
batch 110 loss: 0.0005657620145939291
batch 115 loss: 0.0005657266126945614
batch 120 loss: 0.0005654663196764887
batch 125 loss: 0.000565470231231302
batch 130 loss: 0.0005653037456795573
batch 135 loss: 0.000565242231823504
batch 140 loss: 0.0005651694838888944
batch 145 loss: 0.0005652608815580606
batch 150 loss: 0.0005650060484185815
batch 155 loss: 0.0005651940824463964
batch 160 loss: 0.0005652766441926361
batch 165 loss: 0.0005653877859003841
batch 170 loss: 0.0005654085194692016
batch 175 loss: 0.0005656197434291243
batch 180 loss: 0.0005652915686368942
batch 185 loss: 0.0005652105435729026
batch 190 loss: 0.0005651294835843146
batch 195 loss: 0.0005650871084071696
batch 200 loss: 0.000571665121242404
batch 205 loss: 0.000603013508953154
batch 210 loss: 0.00059388626832515
batch 215 loss: 0.0005876047071069479
batch 220 loss: 0.0005826082779094577
batch 225 loss: 0.0005796658457256854
batch 230 loss: 0.000576997420284897
batch 235 loss: 0.0005736243911087513
batch 240 loss: 0.0005705542047508061
Training Loss: 0.000568745545266817
Validation Loss: 0.0005628511251416057
Epoch 5:
batch 5 loss: 0.0005682731862179935
batch 10 loss: 0.0005671174032613635
batch 15 loss: 0.0005661169067025185
batch 20 loss: 0.000565500115044415
batch 25 loss: 0.0005652440711855888
batch 30 loss: 0.0005650887847878039
batch 35 loss: 0.0005649764440022409
batch 40 loss: 0.0005648829624988139
batch 45 loss: 0.0005648070597089827
batch 50 loss: 0.0005647800047881901
batch 55 loss: 0.00056471984134987
batch 60 loss: 0.0005645711673423648
batch 65 loss: 0.0005645885481499136
batch 70 loss: 0.0005644599325023592
batch 75 loss: 0.0005646500503644347
batch 80 loss: 0.0005644967895932496
batch 85 loss: 0.0005644512479193508
batch 90 loss: 0.000564511539414525
batch 95 loss: 0.0005643225857056677
batch 100 loss: 0.0005642808391712606
batch 105 loss: 0.0005643093609251082
batch 110 loss: 0.0005644235177896916
batch 115 loss: 0.0005643490352667868
batch 120 loss: 0.0005642896867357194
batch 125 loss: 0.0005644106306135655
batch 130 loss: 0.0005644093616865575
batch 135 loss: 0.00056427315576002
batch 140 loss: 0.0005641327821649611
batch 145 loss: 0.0005641544121317565
batch 150 loss: 0.0005641114781610668
batch 155 loss: 0.0005642427830025553
batch 160 loss: 0.0005641941796056926
batch 165 loss: 0.0005642505479045212
batch 170 loss: 0.0005642789416015148
batch 175 loss: 0.0005639985669404268
batch 180 loss: 0.001930365082807839
batch 185 loss: 0.0008968811947852373
batch 190 loss: 0.0007983244489878416
batch 195 loss: 0.0007888993481174111
batch 200 loss: 0.0007757091312669218
batch 205 loss: 0.0007613209658302367
batch 210 loss: 0.0007349223364144563
batch 215 loss: 0.0007226541638374329
batch 220 loss: 0.0007110470673069358
batch 225 loss: 0.000699242576956749
batch 230 loss: 0.0006824931129813194
batch 235 loss: 0.0006571426754817366
batch 240 loss: 0.0006346456240862608
Training Loss: 0.0006366524093512756
Validation Loss: 0.0006180481529251362
Epoch 6:
batch 5 loss: 0.0006211940315552055
batch 10 loss: 0.0006210034131072462
batch 15 loss: 0.0006205711863003672
batch 20 loss: 0.0006186307407915592
batch 25 loss: 0.0006182763027027249
batch 30 loss: 0.0006179747986607253
batch 35 loss: 0.000617695995606482
batch 40 loss: 0.0006172097870148719
batch 45 loss: 0.0006170523003675044
batch 50 loss: 0.0006170279229991138
batch 55 loss: 0.0006166934384964406
batch 60 loss: 0.000616772926878184
batch 65 loss: 0.0006165398517623544
batch 70 loss: 0.000616471585817635
batch 75 loss: 0.0006161966361105442
batch 80 loss: 0.0006157166790217161
batch 85 loss: 0.0006156489485874772
batch 90 loss: 0.0006149777676910162
batch 95 loss: 0.0006146009196527302
batch 100 loss: 0.0006143034901469946
batch 105 loss: 0.0006137471413239836
batch 110 loss: 0.0006131875910796225
batch 115 loss: 0.0006125386338680982
batch 120 loss: 0.0006120785023085773
batch 125 loss: 0.0006115496158599854
batch 130 loss: 0.0006109041976742446
batch 135 loss: 0.0006103990250267089
batch 140 loss: 0.0006096404511481523
batch 145 loss: 0.0006090628681704402
batch 150 loss: 0.0006083636544644833
batch 155 loss: 0.0006075266865082085
batch 160 loss: 0.0006068403949029743
batch 165 loss: 0.0006058892467990518
batch 170 loss: 0.0006049118586815893
batch 175 loss: 0.0006037713843397796
batch 180 loss: 0.0006025874754413962
batch 185 loss: 0.000601556827314198
batch 190 loss: 0.000600559450685978
batch 195 loss: 0.0005995175219140947
batch 200 loss: 0.0005985086900182068
batch 205 loss: 0.0005976365064270794
batch 210 loss: 0.0005964686162769795
batch 215 loss: 0.0005955095984973013
batch 220 loss: 0.0005946937133558094
batch 225 loss: 0.0005937552778050303
batch 230 loss: 0.0005930139333941043
batch 235 loss: 0.0005922956508584321
batch 240 loss: 0.0005918706534430385
Training Loss: 0.0006092279977262176
Validation Loss: 0.0005903719322911153
Epoch 7:
batch 5 loss: 0.0005912721622735262
batch 10 loss: 0.0005907795974053442
batch 15 loss: 0.0005902733188122511
batch 20 loss: 0.0005900249001570046
batch 25 loss: 0.0005895055714063346
batch 30 loss: 0.000589266198221594
batch 35 loss: 0.0005888411891646683
batch 40 loss: 0.0005885309423319995
batch 45 loss: 0.0005881166784092784
batch 50 loss: 0.0005877750460058451
batch 55 loss: 0.0005874614231288433
batch 60 loss: 0.0005870870663784444
batch 65 loss: 0.0005865690531209111
batch 70 loss: 0.0005861798650585115
batch 75 loss: 0.0005855755996890366
batch 80 loss: 0.0005844912840984762
batch 85 loss: 0.0005837219301611185
batch 90 loss: 0.0005827854154631495
batch 95 loss: 0.0005822227452881634
batch 100 loss: 0.0005807501263916492
batch 105 loss: 0.0005796867189928889
batch 110 loss: 0.0005792534211650491
batch 115 loss: 0.0005789165385067463
batch 120 loss: 0.0005787335685454309
batch 125 loss: 0.0005791835021227599
batch 130 loss: 0.0005808158311992884
batch 135 loss: 0.0005811497219838202
batch 140 loss: 0.0005802521598525345
batch 145 loss: 0.0005799002479761839
batch 150 loss: 0.0005776993464678525
batch 155 loss: 0.0005773497745394707
batch 160 loss: 0.0005773107754066586
batch 165 loss: 0.0005769159644842148
batch 170 loss: 0.0005765389767475426
batch 175 loss: 0.0005766910267993808
batch 180 loss: 0.0005765308043919504
batch 185 loss: 0.0005761608364991844
batch 190 loss: 0.0005758217419497668
batch 195 loss: 0.0005758213461376727
batch 200 loss: 0.0005753064295276999
batch 205 loss: 0.0005753512377850711
batch 210 loss: 0.0005748985917307436
batch 215 loss: 0.0005745687754824758
batch 220 loss: 0.0005744369351305068
batch 225 loss: 0.0005740287597291172
batch 230 loss: 0.0005738219129852951
batch 235 loss: 0.0005734675680287183
batch 240 loss: 0.0005731227109208703
Training Loss: 0.0005811451112094801
Validation Loss: 0.0005739507178077474
Epoch 8:
batch 5 loss: 0.0005727221490815282
batch 10 loss: 0.0005733596510253847
batch 15 loss: 0.0005723825073800981
batch 20 loss: 0.000571931479498744
batch 25 loss: 0.0005734761129133403
batch 30 loss: 0.0005737316329032183
batch 35 loss: 0.0005741675267927349
batch 40 loss: 0.0005731342011131346
batch 45 loss: 0.0005724808317609132
batch 50 loss: 0.0005723714362829924
batch 55 loss: 0.0005716301267966629
batch 60 loss: 0.0005704040057025849
batch 65 loss: 0.0005709370248951018
batch 70 loss: 0.0005713813821785152
batch 75 loss: 0.0005710310419090092
batch 80 loss: 0.000571883306838572
batch 85 loss: 0.0005730059696361422
batch 90 loss: 0.0005762172862887382
batch 95 loss: 0.000576599407941103
batch 100 loss: 0.0005743189831264317
batch 105 loss: 0.0005722220288589597
batch 110 loss: 0.0005720194894820452
batch 115 loss: 0.0005722097121179104
batch 120 loss: 0.0005726048490032554
batch 125 loss: 0.0005704849609173834
batch 130 loss: 0.0005706551251932979
batch 135 loss: 0.0005705583491362632
batch 140 loss: 0.0005694818682968616
batch 145 loss: 0.0005680204718373716
batch 150 loss: 0.000570688466541469
batch 155 loss: 0.0005721981171518565
batch 160 loss: 0.000571638299152255
batch 165 loss: 0.0005727301933802665
batch 170 loss: 0.0005730947712436318
batch 175 loss: 0.0005699502537027002
batch 180 loss: 0.0005699668545275927
batch 185 loss: 0.0005702081136405468
batch 190 loss: 0.0005693967221304774
batch 195 loss: 0.000569527258630842
batch 200 loss: 0.0005699166096746922
batch 205 loss: 0.0005697576329112053
batch 210 loss: 0.0005695346277207136
batch 215 loss: 0.0005685114068910479
batch 220 loss: 0.0005676904693245888
batch 225 loss: 0.0005665988079272211
batch 230 loss: 0.0005656319088302552
batch 235 loss: 0.000565339345484972
batch 240 loss: 0.0005655530141666532
Training Loss: 0.0005711115789987768
Validation Loss: 0.0005653154308674857
Epoch 9:
batch 5 loss: 0.0005656091845594347
batch 10 loss: 0.0005656706285662949
batch 15 loss: 0.0005654417327605188
batch 20 loss: 0.000565629848279059
batch 25 loss: 0.0005655480897985398
batch 30 loss: 0.0005656613619066774
batch 35 loss: 0.0005645748577080667
batch 40 loss: 0.0005640857270918787
batch 45 loss: 0.0005655292770825326
batch 50 loss: 0.0005653803935274481
batch 55 loss: 0.0005650091567076743
batch 60 loss: 0.0005645722267217934
batch 65 loss: 0.0005643927259370684
batch 70 loss: 0.0005638131638988853
batch 75 loss: 0.0005647383979521692
batch 80 loss: 0.0005637414171360434
batch 85 loss: 0.0005639594397507608
batch 90 loss: 0.0005652042338624597
batch 95 loss: 0.0005653561092913151
batch 100 loss: 0.0005644483142532408
batch 105 loss: 0.0005637966096401215
batch 110 loss: 0.0005635120091028511
batch 115 loss: 0.0005634621833451092
batch 120 loss: 0.0005630295840092003
batch 125 loss: 0.0005626130034215749
batch 130 loss: 0.0005641959840431809
batch 135 loss: 0.0005640527815558016
batch 140 loss: 0.0005634984350763262
batch 145 loss: 0.0005632217624224722
batch 150 loss: 0.0005639528390020132
batch 155 loss: 0.0005634130444377661
batch 160 loss: 0.0005625940742902457
batch 165 loss: 0.0005624821875244379
batch 170 loss: 0.0005632263142615557
batch 175 loss: 0.0005630290601402521
batch 180 loss: 0.0005630242172628641
batch 185 loss: 0.0005642611416988075
batch 190 loss: 0.0005635944893583655
batch 195 loss: 0.0005630818544887006
batch 200 loss: 0.0005623472621664405
batch 205 loss: 0.0005623493227176369
batch 210 loss: 0.0005628322949633002
batch 215 loss: 0.000563511112704873
batch 220 loss: 0.0005635193781927228
batch 225 loss: 0.0005632353364489973
batch 230 loss: 0.0005625751335173845
batch 235 loss: 0.0005627894075587392
batch 240 loss: 0.0005625536316074431
Training Loss: 0.0005639191821198134
Validation Loss: 0.0005617098737275228
Epoch 10:
batch 5 loss: 0.0005626868340186774
batch 10 loss: 0.0005629099905490875
batch 15 loss: 0.0005624391953460873
batch 20 loss: 0.0005626962753012777
batch 25 loss: 0.0005629114108160138
batch 30 loss: 0.0005623336881399154
batch 35 loss: 0.0005624177167192102
batch 40 loss: 0.000562466005794704
batch 45 loss: 0.0005623019649647176
batch 50 loss: 0.0005625708494335413
batch 55 loss: 0.0005622361786663532
batch 60 loss: 0.0005624336889013648
batch 65 loss: 0.000562914414331317
batch 70 loss: 0.000562745938077569
batch 75 loss: 0.0005621479242108763
batch 80 loss: 0.0005619916482828557
batch 85 loss: 0.0005632307031191886
batch 90 loss: 0.0005625904072076082
batch 95 loss: 0.0005621196352876723
batch 100 loss: 0.0005625476711429656
batch 105 loss: 0.0005620263167656958
batch 110 loss: 0.0005618646857328713
batch 115 loss: 0.0005619796342216432
batch 120 loss: 0.0005624445271678269
batch 125 loss: 0.0005620000301860273
batch 130 loss: 0.0005619247211143374
batch 135 loss: 0.0005622393568046391
batch 140 loss: 0.0005623199511319398
batch 145 loss: 0.0005620724754408002
batch 150 loss: 0.0005618141731247306
batch 155 loss: 0.0005619585164822638
batch 160 loss: 0.0005616286303848028
batch 165 loss: 0.0005619631381705403
batch 170 loss: 0.0005617989110760391
batch 175 loss: 0.0005625417572446168
batch 180 loss: 0.000562695215921849
batch 185 loss: 0.00056233040522784
batch 190 loss: 0.0005635407171212137
batch 195 loss: 0.000562814122531563
batch 200 loss: 0.0005626587313599885
batch 205 loss: 0.0005620359093882144
batch 210 loss: 0.0005620150244794786
batch 215 loss: 0.0005617395625449717
batch 220 loss: 0.0005621624877676368
batch 225 loss: 0.0005617738235741854
batch 230 loss: 0.0005623440956696868
batch 235 loss: 0.0005619850009679795
batch 240 loss: 0.000561531342100352
Training Loss: 0.0005623103209169737
Validation Loss: 0.0005613878767083709
Epoch 11:
batch 5 loss: 0.0005620994605123996
batch 10 loss: 0.0005622119642794132
batch 15 loss: 0.0005617851391434669
batch 20 loss: 0.0005628137849271297
batch 25 loss: 0.0005628183134831488
batch 30 loss: 0.0005632762447930872
batch 35 loss: 0.0005635962472297252
batch 40 loss: 0.0005626253783702851
batch 45 loss: 0.0005628542392514646
batch 50 loss: 0.0005624875193461776
batch 55 loss: 0.0005622771685011684
batch 60 loss: 0.0005624045617878437
batch 65 loss: 0.0005623381701298058
batch 70 loss: 0.0005622494034469127
batch 75 loss: 0.0005620570620521903
batch 80 loss: 0.0005622737924568355
batch 85 loss: 0.0005629465798847377
batch 90 loss: 0.0005626677651889622
batch 95 loss: 0.0005622943979687988
batch 100 loss: 0.0005624481360428035
batch 105 loss: 0.0005622420809231699
batch 110 loss: 0.0005619711126200854
batch 115 loss: 0.0005620978656224907
batch 120 loss: 0.0005621128948405385
batch 125 loss: 0.000561943999491632
batch 130 loss: 0.0005619173753075302
batch 135 loss: 0.0005622036405839026
batch 140 loss: 0.0005620091222226619
batch 145 loss: 0.000562366470694542
batch 150 loss: 0.0005622964003123343
batch 155 loss: 0.0005621046177111567
batch 160 loss: 0.0005621462478302419
batch 165 loss: 0.0005624427343718707
batch 170 loss: 0.0005627049948088824
batch 175 loss: 0.0005625445628538728
batch 180 loss: 0.0005620936281047761
batch 185 loss: 0.0005623859935440124
batch 190 loss: 0.0005620726384222508
batch 195 loss: 0.0005633789114654064
batch 200 loss: 0.0005631364183500409
batch 205 loss: 0.0005627553444355726
batch 210 loss: 0.0005629822029732168
batch 215 loss: 0.000562833552248776
batch 220 loss: 0.0005628581973724067
batch 225 loss: 0.0005628134589642286
batch 230 loss: 0.0005630142521113157
batch 235 loss: 0.0005625491612590849
batch 240 loss: 0.0005629619234241545
Training Loss: 0.000562488856909719
Validation Loss: 0.0005616661054470266
Epoch 12:
batch 5 loss: 0.000562543876003474
batch 10 loss: 0.000562771176919341
batch 15 loss: 0.0005624746438115836
batch 20 loss: 0.0005624292767606676
batch 25 loss: 0.0005630322266370058
batch 30 loss: 0.0005630440195091069
batch 35 loss: 0.0005631211213767528
batch 40 loss: 0.0005628900951705873
batch 45 loss: 0.0005627908045426011
batch 50 loss: 0.000562821642961353
batch 55 loss: 0.0005620956188067793
batch 60 loss: 0.0005618027411401272
batch 65 loss: 0.0005618376308120787
batch 70 loss: 0.0005617630551569164
batch 75 loss: 0.0005680275615304708
batch 80 loss: 0.0005654331995174289
batch 85 loss: 0.0005652757943607866
batch 90 loss: 0.000564789108466357
batch 95 loss: 0.0005643690819852054
batch 100 loss: 0.0005640910123474896
batch 105 loss: 0.0005634302739053965
batch 110 loss: 0.0005628843791782856
batch 115 loss: 0.000562700442969799
batch 120 loss: 0.0005632072105072438
batch 125 loss: 0.0005626713857054711
batch 130 loss: 0.0005626526894047856
batch 135 loss: 0.0005621848627924919
batch 140 loss: 0.0005618590395897627
batch 145 loss: 0.0005620869807898999
batch 150 loss: 0.0005618541268631816
batch 155 loss: 0.0005617258022539318
batch 160 loss: 0.0005619164439849555
batch 165 loss: 0.0005615842994302512
batch 170 loss: 0.0005613007117062808
batch 175 loss: 0.0005617566290311516
batch 180 loss: 0.0005613632965832949
batch 185 loss: 0.0005617504590190947
batch 190 loss: 0.0005615107715129852
batch 195 loss: 0.0005610419786535204
batch 200 loss: 0.0005614958005025983
batch 205 loss: 0.0005616078153252602
batch 210 loss: 0.0005614324705675245
batch 215 loss: 0.0005616231355816126
batch 220 loss: 0.0005614904686808587
batch 225 loss: 0.0005612886627204717
batch 230 loss: 0.0005610543885268271
batch 235 loss: 0.0005607811384834349
batch 240 loss: 0.0005607040366157889
Training Loss: 0.0005624659039312974
Validation Loss: 0.0005604943444874759
Epoch 13:
batch 5 loss: 0.0005608702893368899
batch 10 loss: 0.0005609655170701444
batch 15 loss: 0.000560886412858963
batch 20 loss: 0.0005609953426755965
batch 25 loss: 0.0005607604514807462
batch 30 loss: 0.0005607233382761478
batch 35 loss: 0.0005607958999462425
batch 40 loss: 0.0005605585058219731
batch 45 loss: 0.000560586852952838
batch 50 loss: 0.0005604595295153558
batch 55 loss: 0.0005604136153124273
batch 60 loss: 0.0005604086094535887
batch 65 loss: 0.000560368038713932
batch 70 loss: 0.0005604987847618759
batch 75 loss: 0.0005604723817668855
batch 80 loss: 0.0005604230449534952
batch 85 loss: 0.0005604233476333321
batch 90 loss: 0.0005604967591352761
batch 95 loss: 0.0005605176906101405
batch 100 loss: 0.000560281437356025
batch 105 loss: 0.000560467888135463
batch 110 loss: 0.0005604142090305686
batch 115 loss: 0.0005604212172329425
batch 120 loss: 0.0005604100064374506
batch 125 loss: 0.0005604622419923544
batch 130 loss: 0.0005605341750197113
batch 135 loss: 0.0005604914505966008
batch 140 loss: 0.0005603563738986849
batch 145 loss: 0.0005604415433481336
batch 150 loss: 0.0005605685990303754
batch 155 loss: 0.0005605722311884165
batch 160 loss: 0.0005605833954177797
batch 165 loss: 0.0005606623366475105
batch 170 loss: 0.0005605993792414665
batch 175 loss: 0.0005604838137514889
batch 180 loss: 0.0005605516373179853
batch 185 loss: 0.0005605301819741725
batch 190 loss: 0.0005604573176242411
batch 195 loss: 0.0005604114267043769
batch 200 loss: 0.0005603954894468188
batch 205 loss: 0.0005605048965662718
batch 210 loss: 0.0005606028134934604
batch 215 loss: 0.0005604389240033925
batch 220 loss: 0.0005603270605206489
batch 225 loss: 0.0005602297256700695
batch 230 loss: 0.0005603070370852947
batch 235 loss: 0.0005604251753538847
batch 240 loss: 0.000560529693029821
Training Loss: 0.000560522626862318
Validation Loss: 0.0005604245758149773
Epoch 14:
batch 5 loss: 0.0005605059443041683
batch 10 loss: 0.0005605292040854693
batch 15 loss: 0.0005605456768535078
batch 20 loss: 0.0005602676770649851
batch 25 loss: 0.0005604192265309393
batch 30 loss: 0.0005604804144240915
batch 35 loss: 0.0005604518228210509
batch 40 loss: 0.0005604838253930212
batch 45 loss: 0.0005603261408396066
batch 50 loss: 0.0005601826822385191
batch 55 loss: 0.0005601704120635987
batch 60 loss: 0.0005602946504950524
batch 65 loss: 0.000560224975924939
batch 70 loss: 0.000560329994186759
batch 75 loss: 0.000560302718076855
batch 80 loss: 0.0005602653021924197
batch 85 loss: 0.000560271751601249
batch 90 loss: 0.0005604104022495449
batch 95 loss: 0.0005754343583248556
batch 100 loss: 0.0005783948232419789
batch 105 loss: 0.0005584216909483075
batch 110 loss: 0.0005577666917815805
batch 115 loss: 0.0005578398355282843
batch 120 loss: 0.0005577124888077378
batch 125 loss: 0.0005578696145676076
batch 130 loss: 0.0005576931871473789
batch 135 loss: 0.0005575798102654516
batch 140 loss: 0.0005576930125243962
batch 145 loss: 0.0005578692303970456
batch 150 loss: 0.0005575950839556753
batch 155 loss: 0.0005577109870500863
batch 160 loss: 0.0005576337804086507
batch 165 loss: 0.0005577190080657601
batch 170 loss: 0.0005576929892413318
batch 175 loss: 0.00055767095182091
batch 180 loss: 0.000557678414043039
batch 185 loss: 0.0005577530129812658
batch 190 loss: 0.000557851791381836
batch 195 loss: 0.0005576023715548218
batch 200 loss: 0.0005577283911406994
batch 205 loss: 0.0005576177965849638
batch 210 loss: 0.0005575452232733369
batch 215 loss: 0.0005576614756137132
batch 220 loss: 0.0005577038740739226
batch 225 loss: 0.0005578680546022951
batch 230 loss: 0.0005576792289502918
batch 235 loss: 0.0005576980416662991
batch 240 loss: 0.0005575481685809791
Training Loss: 0.0005595145043722975
Validation Loss: 0.0005576933966949582
Epoch 15:
batch 5 loss: 0.0005577002419158816
batch 10 loss: 0.0005576327559538186
batch 15 loss: 0.0005575807299464941
batch 20 loss: 0.0005576961091719567
batch 25 loss: 0.0005577501025982202
batch 30 loss: 0.0005576342227868736
batch 35 loss: 0.0005577302072197199
batch 40 loss: 0.0005577661329880356
batch 45 loss: 0.0005576831521466374
batch 50 loss: 0.0005576146533712745
batch 55 loss: 0.0005576533731073141
batch 60 loss: 0.0005577367148362101
batch 65 loss: 0.000557749718427658
batch 70 loss: 0.0005577213712967932
batch 75 loss: 0.0005577884847298264
batch 80 loss: 0.0005576985538937151
batch 85 loss: 0.0005577050964348018
batch 90 loss: 0.0005576661205850542
batch 95 loss: 0.0005575865274295211
batch 100 loss: 0.0005577008239924907
batch 105 loss: 0.000557707843836397
batch 110 loss: 0.0005577021511271596
batch 115 loss: 0.0005575846182182432
batch 120 loss: 0.0005576904048211872
batch 125 loss: 0.000557705934625119
batch 130 loss: 0.0005576311377808452
batch 135 loss: 0.00055765719152987
batch 140 loss: 0.0005576267139986157
batch 145 loss: 0.0005575626855716109
batch 150 loss: 0.0005577001138590276
batch 155 loss: 0.0005577576579526067
batch 160 loss: 0.0005577088915742934
batch 165 loss: 0.0005577390198595822
batch 170 loss: 0.0005577242816798389
batch 175 loss: 0.000557837460655719
batch 180 loss: 0.0005576597177423537
batch 185 loss: 0.0005576194147579372
batch 190 loss: 0.0005575859453529119
batch 195 loss: 0.0005576844792813063
batch 200 loss: 0.0005577478092163801
batch 205 loss: 0.0005576962721534073
batch 210 loss: 0.0005576751194894314
batch 215 loss: 0.0005577146774157881
batch 220 loss: 0.0005577428848482669
batch 225 loss: 0.0005576623603701592
batch 230 loss: 0.0005576152703724802
batch 235 loss: 0.0005575987393967808
batch 240 loss: 0.0005576680996455252
Training Loss: 0.0005576833747909405
Validation Loss: 0.0005576934063962351
Epoch 16:
batch 5 loss: 0.0005576892755925656
batch 10 loss: 0.0005576304160058498
batch 15 loss: 0.0005576805444434286
batch 20 loss: 0.0005576824652962387
batch 25 loss: 0.0005576430121436715
batch 30 loss: 0.000557744293473661
batch 35 loss: 0.0005577183212153614
batch 40 loss: 0.0005577884730882943
batch 45 loss: 0.0005576347117312253
batch 50 loss: 0.0005576458759605884
batch 55 loss: 0.0005577016272582114
batch 60 loss: 0.0005576729541644454
batch 65 loss: 0.0005576885538175702
batch 70 loss: 0.0005575463990680873
batch 75 loss: 0.0005577225121669472
batch 80 loss: 0.0005576979834586382
batch 85 loss: 0.0005577425705268979
batch 90 loss: 0.0005576917086727917
batch 95 loss: 0.000557573651894927
batch 100 loss: 0.0005577389383688569
batch 105 loss: 0.0005577040370553732
batch 110 loss: 0.0005577152827754617
batch 115 loss: 0.0005577225121669472
batch 120 loss: 0.00055766343139112
batch 125 loss: 0.0005576285999268294
batch 130 loss: 0.0005577196483500302
batch 135 loss: 0.0005577141768299043
batch 140 loss: 0.0005576520110480487
batch 145 loss: 0.0005577358417212964
batch 150 loss: 0.0005577248288318515
batch 155 loss: 0.000557503814343363
batch 160 loss: 0.0005577581934630871
batch 165 loss: 0.0005576682044193149
batch 170 loss: 0.0005577385891228914
batch 175 loss: 0.0005577326752245426
batch 180 loss: 0.0005576280178502202
batch 185 loss: 0.0005576305324211716
batch 190 loss: 0.0005577077157795429
batch 195 loss: 0.0005576776340603829
batch 200 loss: 0.0005576957715675235
batch 205 loss: 0.0005575712188147008
batch 210 loss: 0.0005577628151513636
batch 215 loss: 0.000557847775053233
batch 220 loss: 0.0005577014992013574
batch 225 loss: 0.0005576785770244896
batch 230 loss: 0.0005576209165155888
batch 235 loss: 0.000557558867149055
batch 240 loss: 0.000557704595848918
Training Loss: 0.0005576833764886639
Validation Loss: 0.0005576934054261073
Epoch 17:
batch 5 loss: 0.0005576809635385871
batch 10 loss: 0.000557677005417645
batch 15 loss: 0.0005577099043875932
batch 20 loss: 0.0005577456555329263
batch 25 loss: 0.0005576419760473072
batch 30 loss: 0.0005576748750172556
batch 35 loss: 0.0005577084608376026
batch 40 loss: 0.0005576400551944971
batch 45 loss: 0.0005577657837420702
batch 50 loss: 0.0005576847703196109
batch 55 loss: 0.000557667191606015
batch 60 loss: 0.0005576419294811785
batch 65 loss: 0.0005576850264333189
batch 70 loss: 0.0005576792638748885
batch 75 loss: 0.0005576387280598283
batch 80 loss: 0.0005577918142080307
batch 85 loss: 0.0005577673786319792
batch 90 loss: 0.0005578137817792595
batch 95 loss: 0.0005577358067966997
batch 100 loss: 0.0005577255506068468
batch 105 loss: 0.0005576312774792314
batch 110 loss: 0.0005577407311648131
batch 115 loss: 0.0005577063304372132
batch 120 loss: 0.000557709694840014
batch 125 loss: 0.0005576464347541333
batch 130 loss: 0.0005576322437264025
batch 135 loss: 0.0005577112082391977
batch 140 loss: 0.0005577966687269509
batch 145 loss: 0.0005577763426117599
batch 150 loss: 0.0005576938856393099
batch 155 loss: 0.0005576887167990207
batch 160 loss: 0.0005575840943492949
batch 165 loss: 0.0005576408118940889
batch 170 loss: 0.0005575841176323592
batch 175 loss: 0.0005576824187301099
batch 180 loss: 0.0005577325006015599
batch 185 loss: 0.0005575551418587566
batch 190 loss: 0.0005576302413828671
batch 195 loss: 0.0005575435003265739
batch 200 loss: 0.0005577477626502514
batch 205 loss: 0.0005576249677687883
batch 210 loss: 0.0005575721035711468
batch 215 loss: 0.000557663175277412
batch 220 loss: 0.0005576993688009679
batch 225 loss: 0.0005576816038228571
batch 230 loss: 0.0005576806259341538
batch 235 loss: 0.0005577831179834902
batch 240 loss: 0.0005575871677137911
Training Loss: 0.0005576833786714512
Validation Loss: 0.000557693403485852
Epoch 18:
batch 5 loss: 0.0005576891242526471
batch 10 loss: 0.0005576954805292189
batch 15 loss: 0.0005577073548920453
batch 20 loss: 0.0005575710674747825
batch 25 loss: 0.0005579132353886962
batch 30 loss: 0.0005577343050390482
batch 35 loss: 0.000557668344117701
batch 40 loss: 0.0005576443509198725
batch 45 loss: 0.0005575849441811443
batch 50 loss: 0.000557660823687911
batch 55 loss: 0.0005576671799644827
batch 60 loss: 0.0005577630829066039
batch 65 loss: 0.0005576844210736454
batch 70 loss: 0.0005576593568548561
batch 75 loss: 0.0005575876333750784
batch 80 loss: 0.000557705492246896
batch 85 loss: 0.0005576812895014882
batch 90 loss: 0.0005577345029450953
batch 95 loss: 0.0005577936302870512
batch 100 loss: 0.0005576743045821786
batch 105 loss: 0.0005577206495217979
batch 110 loss: 0.0005577077507041394
batch 115 loss: 0.000557697715703398
batch 120 loss: 0.0005575531627982855
batch 125 loss: 0.0005577216274105012
batch 130 loss: 0.00055766865843907
batch 135 loss: 0.0005576960975304246
batch 140 loss: 0.0005576900555752217
batch 145 loss: 0.0005577566684223711
batch 150 loss: 0.0005577050382271409
batch 155 loss: 0.0005577404284849763
batch 160 loss: 0.0005577633972279727
batch 165 loss: 0.0005576292052865029
batch 170 loss: 0.0005576621973887086
batch 175 loss: 0.0005575323826633394
batch 180 loss: 0.0005577105679549277
batch 185 loss: 0.0005578262615017592
batch 190 loss: 0.0005576358642429113
batch 195 loss: 0.0005575731629505754
batch 200 loss: 0.0005576381343416869
batch 205 loss: 0.0005576842580921948
batch 210 loss: 0.0005577581468969584
batch 215 loss: 0.0005575563642196357
batch 220 loss: 0.0005576595314778388
batch 225 loss: 0.0005576920812018216
batch 230 loss: 0.0005577213363721967
batch 235 loss: 0.0005576136987656355
batch 240 loss: 0.0005576676921918988
Training Loss: 0.0005576833762461319
Validation Loss: 0.0005576934063962351
Epoch 19:
batch 5 loss: 0.0005575931631028652
batch 10 loss: 0.0005577864125370979
batch 15 loss: 0.0005576991243287921
batch 20 loss: 0.0005577184376306832
batch 25 loss: 0.0005576716386713088
batch 30 loss: 0.0005576869822107256
batch 35 loss: 0.0005576792405918241
batch 40 loss: 0.0005576601368375122
batch 45 loss: 0.0005575942690484226
batch 50 loss: 0.0005576603463850916
batch 55 loss: 0.0005576900206506252
batch 60 loss: 0.0005577961332164705
batch 65 loss: 0.0005577009404078126
batch 70 loss: 0.0005576712312176824
batch 75 loss: 0.000557684653904289
batch 80 loss: 0.0005576203926466406
batch 85 loss: 0.0005576410563662648
batch 90 loss: 0.0005576778086833656
batch 95 loss: 0.0005576458876021207
batch 100 loss: 0.0005576401017606258
batch 105 loss: 0.0005576362367719412
batch 110 loss: 0.0005577089847065509
batch 115 loss: 0.0005577061790972948
batch 120 loss: 0.0005576735828071832
batch 125 loss: 0.0005577067960985005
batch 130 loss: 0.000557524582836777
batch 135 loss: 0.0005576463183388114
batch 140 loss: 0.0005577151314355433
batch 145 loss: 0.0005577497999183833
batch 150 loss: 0.0005576284718699753
batch 155 loss: 0.0005576838506385684
batch 160 loss: 0.0005576033843681216
batch 165 loss: 0.0005576538038440049
batch 170 loss: 0.0005577517789788544
batch 175 loss: 0.0005578002310357988
batch 180 loss: 0.0005576534778811038
batch 185 loss: 0.0005576986470259726
batch 190 loss: 0.0005576873198151588
batch 195 loss: 0.000557801011018455
batch 200 loss: 0.0005576759111136198
batch 205 loss: 0.0005577216506935656
batch 210 loss: 0.0005577299161814153
batch 215 loss: 0.0005576640949584543
batch 220 loss: 0.0005576197407208383
batch 225 loss: 0.0005576906260102987
batch 230 loss: 0.0005577199044637382
batch 235 loss: 0.0005577040952630341
batch 240 loss: 0.0005577285424806178
Training Loss: 0.0005576833760036
Validation Loss: 0.000557693427739044
Epoch 20:
batch 5 loss: 0.000557758251670748
batch 10 loss: 0.0005575741059146821
batch 15 loss: 0.0005576615338213741
batch 20 loss: 0.0005576254450716078
batch 25 loss: 0.0005576514289714396
batch 30 loss: 0.0005576824187301099
batch 35 loss: 0.0005575900082476437
batch 40 loss: 0.00055775735527277
batch 45 loss: 0.0005576031981036067
batch 50 loss: 0.0005577172501944006
batch 55 loss: 0.0005576575524173677
batch 60 loss: 0.0005577297415584326
batch 65 loss: 0.0005576557363383472
batch 70 loss: 0.0005576563649810851
batch 75 loss: 0.0005576009629294276
batch 80 loss: 0.000557734293397516
batch 85 loss: 0.0005577538860961795
batch 90 loss: 0.0005577611504122615
batch 95 loss: 0.0005576300667598843
batch 100 loss: 0.0005577489966526628
batch 105 loss: 0.0005577533622272313
batch 110 loss: 0.0005576278665103019
batch 115 loss: 0.0005577305564656854
batch 120 loss: 0.0005576597526669502
batch 125 loss: 0.0005577026167884469
batch 130 loss: 0.0005577526055276394
batch 135 loss: 0.000557697075419128
batch 140 loss: 0.0005577748059295117
batch 145 loss: 0.0005577208125032484
batch 150 loss: 0.0005577282048761844
batch 155 loss: 0.0005576497060246766
batch 160 loss: 0.0005577213130891323
batch 165 loss: 0.0005577485891990364
batch 170 loss: 0.0005577023955993354
batch 175 loss: 0.0005576253752224147
batch 180 loss: 0.0005576534778811038
batch 185 loss: 0.0005577009869739413
batch 190 loss: 0.0005577198811806738
batch 195 loss: 0.0005576852709054947
batch 200 loss: 0.0005578003358095885
batch 205 loss: 0.0005575997289270163
batch 210 loss: 0.0005576286115683615
batch 215 loss: 0.0005576045368798077
batch 220 loss: 0.0005575749324634671
batch 225 loss: 0.0005576044670306147
batch 230 loss: 0.0005579075543209911
batch 235 loss: 0.0005575530347414314
batch 240 loss: 0.0005576247698627412
Training Loss: 0.0005576833827944938
Validation Loss: 0.0005576934083364904
Epoch 21:
batch 5 loss: 0.0005577631294727325
batch 10 loss: 0.000557653431314975
batch 15 loss: 0.0005577865755185485
batch 20 loss: 0.0005577763775363564
batch 25 loss: 0.0005576928029768169
batch 30 loss: 0.0005576774012297392
batch 35 loss: 0.0005577765521593391
batch 40 loss: 0.000557638902682811
batch 45 loss: 0.0005577854579314589
batch 50 loss: 0.0005577106727287173
batch 55 loss: 0.0005577380885370076
batch 60 loss: 0.0005576465744525194
batch 65 loss: 0.0005576431518420577
batch 70 loss: 0.0005576829076744616
batch 75 loss: 0.0005576451309025288
batch 80 loss: 0.0005577966221608222
batch 85 loss: 0.0005577122909016907
batch 90 loss: 0.000557589577510953
batch 95 loss: 0.0005576430354267359
batch 100 loss: 0.000557619403116405
batch 105 loss: 0.0005577185656875372
batch 110 loss: 0.0005576757597737014
batch 115 loss: 0.0005577887292020023
batch 120 loss: 0.0005576729658059776
batch 125 loss: 0.0005576704279519618
batch 130 loss: 0.0005576643394306302
batch 135 loss: 0.0005575838033109904
batch 140 loss: 0.0005576919647864997
batch 145 loss: 0.0005576605792157352
batch 150 loss: 0.0005577388918027281
batch 155 loss: 0.0005576382158324122
batch 160 loss: 0.0005576551309786737
batch 165 loss: 0.0005576953524723649
batch 170 loss: 0.0005578156444244087
batch 175 loss: 0.0005576365161687136
batch 180 loss: 0.0005576756084337831
batch 185 loss: 0.0005576295079663396
batch 190 loss: 0.0005577108939178288
batch 195 loss: 0.0005577601958066225
batch 200 loss: 0.000557669682893902
batch 205 loss: 0.0005576792289502918
batch 210 loss: 0.0005576157476752996
batch 215 loss: 0.0005576445022597909
batch 220 loss: 0.0005576957832090556
batch 225 loss: 0.0005576604511588812
batch 230 loss: 0.0005576843046583235
batch 235 loss: 0.0005575368064455688
batch 240 loss: 0.0005575543618761003
Training Loss: 0.0005576833760036
Validation Loss: 0.0005576934141572565
Epoch 22:
batch 5 loss: 0.0005575951654464006
batch 10 loss: 0.0005575775052420795
batch 15 loss: 0.0005576819879934191
batch 20 loss: 0.0005576699739322066
batch 25 loss: 0.000557665666565299
batch 30 loss: 0.0005577693111263216
batch 35 loss: 0.0005577319418080152
batch 40 loss: 0.0005577040603384376
batch 45 loss: 0.0005577299743890762
batch 50 loss: 0.0005577047588303686
batch 55 loss: 0.0005576042924076319
batch 60 loss: 0.0005577491945587098
batch 65 loss: 0.000557529809884727
batch 70 loss: 0.0005577296251431108
batch 75 loss: 0.000557631510309875
batch 80 loss: 0.0005576478433795273
batch 85 loss: 0.0005577187635935843
batch 90 loss: 0.0005577104049734772
batch 95 loss: 0.000557709252461791
batch 100 loss: 0.0005577047821134328
batch 105 loss: 0.0005575649673119187
batch 110 loss: 0.0005576623603701592
batch 115 loss: 0.0005575859337113798
batch 120 loss: 0.0005577898817136883
batch 125 loss: 0.0005577100091613829
batch 130 loss: 0.0005578510463237762
batch 135 loss: 0.0005578361102379859
batch 140 loss: 0.0005575934075750411
batch 145 loss: 0.0005576168303377926
batch 150 loss: 0.0005577184841968119
batch 155 loss: 0.000557647948153317
batch 160 loss: 0.000557668344117701
batch 165 loss: 0.0005577739677391947
batch 170 loss: 0.0005577367031946778
batch 175 loss: 0.000557625328656286
batch 180 loss: 0.0005577014060690999
batch 185 loss: 0.0005576278199441731
batch 190 loss: 0.0005576195195317269
batch 195 loss: 0.0005575971561484038
batch 200 loss: 0.0005576801253482699
batch 205 loss: 0.0005576927913352847
batch 210 loss: 0.0005577378091402352
batch 215 loss: 0.0005578171112574637
batch 220 loss: 0.0005576712428592145
batch 225 loss: 0.0005576544092036784
batch 230 loss: 0.0005577013362199068
batch 235 loss: 0.0005576772964559495
batch 240 loss: 0.0005576771683990956
Training Loss: 0.0005576833820668981
Validation Loss: 0.0005576934083364904
Epoch 23:
batch 5 loss: 0.0005576317314989864
batch 10 loss: 0.0005575388087891043
batch 15 loss: 0.0005577701376751065
batch 20 loss: 0.0005578013835474849
batch 25 loss: 0.0005577487172558904
batch 30 loss: 0.0005576150259003043
batch 35 loss: 0.0005578142474405468
batch 40 loss: 0.0005576728144660592
batch 45 loss: 0.0005575941293500363
batch 50 loss: 0.0005577757954597473
batch 55 loss: 0.0005577519536018371
batch 60 loss: 0.0005576984258368611
batch 65 loss: 0.0005577696138061583
batch 70 loss: 0.000557665922679007
batch 75 loss: 0.0005576827796176076
batch 80 loss: 0.0005576888681389392
batch 85 loss: 0.0005576865747570992
batch 90 loss: 0.0005577613017521798
batch 95 loss: 0.0005576036288402975
batch 100 loss: 0.000557641452178359
batch 105 loss: 0.0005576958879828453
batch 110 loss: 0.000557774817571044
batch 115 loss: 0.0005577042931690813
batch 120 loss: 0.0005577588686719537
batch 125 loss: 0.0005576551193371415
batch 130 loss: 0.0005576884956099093
batch 135 loss: 0.0005576768657192588
batch 140 loss: 0.000557631254196167
batch 145 loss: 0.0005576902534812689
batch 150 loss: 0.0005576074472628534
batch 155 loss: 0.000557692814618349
batch 160 loss: 0.0005576960276812315
batch 165 loss: 0.000557717913761735
batch 170 loss: 0.000557692488655448
batch 175 loss: 0.0005575819755904377
batch 180 loss: 0.0005576593684963882
batch 185 loss: 0.0005576954921707511
batch 190 loss: 0.0005576648167334497
batch 195 loss: 0.0005576159572228789
batch 200 loss: 0.0005575798219069839
batch 205 loss: 0.000557878881227225
batch 210 loss: 0.0005576657247729599
batch 215 loss: 0.0005577382282353938
batch 220 loss: 0.0005575871909968555
batch 225 loss: 0.0005575466784648597
batch 230 loss: 0.0005576473311521112
batch 235 loss: 0.0005576507304795086
batch 240 loss: 0.0005576942581683398
Training Loss: 0.0005576833815818342
Validation Loss: 0.0005576933986352135
Epoch 24:
batch 5 loss: 0.0005576959811151028
batch 10 loss: 0.0005576838506385684
batch 15 loss: 0.0005575177958235145
batch 20 loss: 0.0005577430129051208
batch 25 loss: 0.0005577496369369328
batch 30 loss: 0.0005577811039984226
batch 35 loss: 0.0005577008007094264
batch 40 loss: 0.0005575464572757482
batch 45 loss: 0.0005577249452471733
batch 50 loss: 0.0005575866322033107
batch 55 loss: 0.0005577812436968089
batch 60 loss: 0.0005576162366196513
batch 65 loss: 0.0005576507770456374
batch 70 loss: 0.0005576335359364748
batch 75 loss: 0.0005576491937972605
batch 80 loss: 0.0005575314629822969
batch 85 loss: 0.0005577769596129655
batch 90 loss: 0.0005576825235038996
batch 95 loss: 0.0005576387862674892
batch 100 loss: 0.0005577170639298856
batch 105 loss: 0.0005577340023592114
batch 110 loss: 0.0005576140829361975
batch 115 loss: 0.000557694397866726
batch 120 loss: 0.000557679042685777
batch 125 loss: 0.0005576680880039931
batch 130 loss: 0.00055761921685189
batch 135 loss: 0.0005576704861596227
batch 140 loss: 0.0005577346775680781
batch 145 loss: 0.000557711417786777
batch 150 loss: 0.0005577157950028777
batch 155 loss: 0.0005576638970524073
batch 160 loss: 0.0005576816154643893
batch 165 loss: 0.0005576716503128409
batch 170 loss: 0.0005576130002737046
batch 175 loss: 0.000557798845693469
batch 180 loss: 0.000557692360598594
batch 185 loss: 0.0005576801020652055
batch 190 loss: 0.0005577477510087192
batch 195 loss: 0.0005577695555984974
batch 200 loss: 0.000557685806415975
batch 205 loss: 0.0005576986470259726
batch 210 loss: 0.0005577402887865901
batch 215 loss: 0.0005576289026066661
batch 220 loss: 0.0005576711148023606
batch 225 loss: 0.0005576605442911386
batch 230 loss: 0.0005577323609031737
batch 235 loss: 0.0005576589261181653
batch 240 loss: 0.0005577579024247826
Training Loss: 0.0005576833849772811
Validation Loss: 0.0005576934063962351
Epoch 25:
batch 5 loss: 0.0005576211377047002
batch 10 loss: 0.000557577854488045
batch 15 loss: 0.0005576671799644827
batch 20 loss: 0.0005577488569542766
batch 25 loss: 0.0005576766445301474
batch 30 loss: 0.0005575281102210283
batch 35 loss: 0.0005576466326601803
batch 40 loss: 0.0005577665986493229
batch 45 loss: 0.0005576585885137319
batch 50 loss: 0.0005575583782047033
batch 55 loss: 0.0005576145951636136
batch 60 loss: 0.0005576431984081864
batch 65 loss: 0.0005575833260081708
batch 70 loss: 0.0005577987059950829
batch 75 loss: 0.0005577437812462449
batch 80 loss: 0.0005576768191531301
batch 85 loss: 0.0005578689510002732
batch 90 loss: 0.0005575909977778793
batch 95 loss: 0.0005577375879511238
batch 100 loss: 0.0005576116847805679
batch 105 loss: 0.0005578111042268575
batch 110 loss: 0.000557585118804127
batch 115 loss: 0.0005577070172876119
batch 120 loss: 0.0005577881122007966
batch 125 loss: 0.000557628576643765
batch 130 loss: 0.0005576557479798794
batch 135 loss: 0.0005577466334216296
batch 140 loss: 0.0005577161791734397
batch 145 loss: 0.0005577350733801722
batch 150 loss: 0.0005577284609898925
batch 155 loss: 0.0005576891358941794
batch 160 loss: 0.0005576220224611461
batch 165 loss: 0.0005577112431637942
batch 170 loss: 0.0005577527917921543
batch 175 loss: 0.0005577132455073297
batch 180 loss: 0.0005576809635385871
batch 185 loss: 0.000557705166283995
batch 190 loss: 0.0005577310454100371
batch 195 loss: 0.0005576565745286644
batch 200 loss: 0.0005576169234700501
batch 205 loss: 0.0005576956318691373
batch 210 loss: 0.0005577717209234833
batch 215 loss: 0.000557681149803102
batch 220 loss: 0.0005576635943725705
batch 225 loss: 0.000557648076210171
batch 230 loss: 0.0005577086587436498
batch 235 loss: 0.0005577173549681901
batch 240 loss: 0.000557615386787802
Training Loss: 0.0005576833820668981
Validation Loss: 0.0005576934228884057
Epoch 26:
batch 5 loss: 0.0005577205098234117
batch 10 loss: 0.0005576582276262343
batch 15 loss: 0.0005576063180342316
batch 20 loss: 0.0005576630355790257
batch 25 loss: 0.0005577712785452604
batch 30 loss: 0.0005577130941674113
batch 35 loss: 0.0005576320574618876
batch 40 loss: 0.0005577382748015225
batch 45 loss: 0.000557846890296787
batch 50 loss: 0.0005576893338002265
batch 55 loss: 0.0005576121970079839
batch 60 loss: 0.0005576475406996906
batch 65 loss: 0.0005577097996138036
batch 70 loss: 0.0005576169933192432
batch 75 loss: 0.0005576017429120839
batch 80 loss: 0.0005576392519287765
batch 85 loss: 0.000557668344117701
batch 90 loss: 0.0005577777628786862
batch 95 loss: 0.0005577699048444629
batch 100 loss: 0.0005575340590439736
batch 105 loss: 0.0005576711613684892
batch 110 loss: 0.0005576191586442291
batch 115 loss: 0.0005576865980401636
batch 120 loss: 0.0005576037336140871
batch 125 loss: 0.0005577156669460237
batch 130 loss: 0.0005576344905421138
batch 135 loss: 0.0005576387397013604
batch 140 loss: 0.0005576725117862225
batch 145 loss: 0.0005577900563366711
batch 150 loss: 0.000557588948868215
batch 155 loss: 0.0005577275529503823
batch 160 loss: 0.0005576055962592363
batch 165 loss: 0.0005576236522756517
batch 170 loss: 0.0005576476221904158
batch 175 loss: 0.0005576483206823468
batch 180 loss: 0.0005577641888521612
batch 185 loss: 0.0005576095310971141
batch 190 loss: 0.0005576627561822533
batch 195 loss: 0.0005577176925726235
batch 200 loss: 0.0005578153999522328
batch 205 loss: 0.0005576503812335431
batch 210 loss: 0.000557717913761735
batch 215 loss: 0.0005577009287662804
batch 220 loss: 0.0005577897885814309
batch 225 loss: 0.0005577348987571895
batch 230 loss: 0.0005577564355917275
batch 235 loss: 0.0005576442228630186
batch 240 loss: 0.0005577477859333158
Training Loss: 0.00055768338230943
Validation Loss: 0.0005576933947547028
Epoch 27:
batch 5 loss: 0.000557703641243279
batch 10 loss: 0.000557839241810143
batch 15 loss: 0.0005576472263783217
batch 20 loss: 0.0005576018360443413
batch 25 loss: 0.0005576650612056256
batch 30 loss: 0.0005577128729782998
batch 35 loss: 0.000557678600307554
batch 40 loss: 0.0005577033385634423
batch 45 loss: 0.0005576539668254554
batch 50 loss: 0.0005576624302193522
batch 55 loss: 0.0005577812320552766
batch 60 loss: 0.0005575843853875995
batch 65 loss: 0.0005578131531365216
batch 70 loss: 0.0005577879957854748
batch 75 loss: 0.0005576268769800663
batch 80 loss: 0.0005576134077273309
batch 85 loss: 0.0005577105446718633
batch 90 loss: 0.0005577164702117443
batch 95 loss: 0.0005577731528319419
batch 100 loss: 0.0005577438161708415
batch 105 loss: 0.0005577369593083858
batch 110 loss: 0.0005576651194132865
batch 115 loss: 0.0005575616145506501
batch 120 loss: 0.0005577028961852193
batch 125 loss: 0.0005576607771217823
batch 130 loss: 0.0005576426163315773
batch 135 loss: 0.0005577643983997405
batch 140 loss: 0.0005576070514507591
batch 145 loss: 0.0005577356787398458
batch 150 loss: 0.0005575499148108065
batch 155 loss: 0.0005576569936238229
batch 160 loss: 0.0005576891242526471
batch 165 loss: 0.0005577084142714739
batch 170 loss: 0.0005576844094321131
batch 175 loss: 0.0005575715214945376
batch 180 loss: 0.0005576865631155669
batch 185 loss: 0.0005576219875365496
batch 190 loss: 0.0005576920346356928
batch 195 loss: 0.0005577175063081086
batch 200 loss: 0.0005576152703724802
batch 205 loss: 0.0005577280186116696
batch 210 loss: 0.0005577104981057346
batch 215 loss: 0.0005576636642217637
batch 220 loss: 0.0005577264470048249
batch 225 loss: 0.0005576909054070712
batch 230 loss: 0.000557600858155638
batch 235 loss: 0.0005576912546530366
batch 240 loss: 0.000557700905483216
Training Loss: 0.0005576833886152599
Validation Loss: 0.0005576934063962351
Epoch 28:
batch 5 loss: 0.0005577164702117443
batch 10 loss: 0.0005576754920184612
batch 15 loss: 0.0005577182280831039
batch 20 loss: 0.0005577183677814901
batch 25 loss: 0.0005577010451816022
batch 30 loss: 0.0005576359573751688
batch 35 loss: 0.0005576742463745177
batch 40 loss: 0.0005576565046794713
batch 45 loss: 0.0005576965282671154
batch 50 loss: 0.0005577180185355246
batch 55 loss: 0.0005577433970756829
batch 60 loss: 0.0005577077623456717
batch 65 loss: 0.0005575358169153333
batch 70 loss: 0.000557748565915972
batch 75 loss: 0.000557656167075038
batch 80 loss: 0.0005577689502388239
batch 85 loss: 0.0005576321156695486
batch 90 loss: 0.0005577437812462449
batch 95 loss: 0.0005576971219852567
batch 100 loss: 0.000557568110525608
batch 105 loss: 0.0005576510564424097
batch 110 loss: 0.0005576420342549681
batch 115 loss: 0.0005576177849434316
batch 120 loss: 0.0005576708004809916
batch 125 loss: 0.0005576984374783933
batch 130 loss: 0.0005576289375312626
batch 135 loss: 0.0005575791699811816
batch 140 loss: 0.000557577342260629
batch 145 loss: 0.0005577736534178257
batch 150 loss: 0.0005576438154093922
batch 155 loss: 0.0005576593568548561
batch 160 loss: 0.0005575210903771222
batch 165 loss: 0.00055772983469069
batch 170 loss: 0.0005576646653935313
batch 175 loss: 0.0005576937459409237
batch 180 loss: 0.0005576639901846647
batch 185 loss: 0.0005577162024565041
batch 190 loss: 0.0005576983559876681
batch 195 loss: 0.000557806936558336
batch 200 loss: 0.0005578011623583734
batch 205 loss: 0.0005577292991802097
batch 210 loss: 0.0005577144562266767
batch 215 loss: 0.0005577474134042859
batch 220 loss: 0.0005576534895226359
batch 225 loss: 0.0005576982162892819
batch 230 loss: 0.0005576931056566536
batch 235 loss: 0.0005577404284849763
batch 240 loss: 0.0005576752941124141
Training Loss: 0.0005576833900704514
Validation Loss: 0.0005576934160975119
Epoch 29:
batch 5 loss: 0.0005576116847805679
batch 10 loss: 0.0005576859577558935
batch 15 loss: 0.0005576569703407586
batch 20 loss: 0.0005576522788032889
batch 25 loss: 0.0005577647592872381
batch 30 loss: 0.0005576636060141027
batch 35 loss: 0.000557717471383512
batch 40 loss: 0.0005576616851612926
batch 45 loss: 0.0005577563308179379
batch 50 loss: 0.0005577480420470238
batch 55 loss: 0.0005576588795520365
batch 60 loss: 0.0005576496245339513
batch 65 loss: 0.0005577307078056037
batch 70 loss: 0.000557722628582269
batch 75 loss: 0.0005577616626396775
batch 80 loss: 0.0005576320341788233
batch 85 loss: 0.0005576428724452853
batch 90 loss: 0.0005576548050157726
batch 95 loss: 0.0005577386589720845
batch 100 loss: 0.0005576757015660405
batch 105 loss: 0.000557596399448812
batch 110 loss: 0.0005576025694608688
batch 115 loss: 0.0005576639901846647
batch 120 loss: 0.0005575527320615947
batch 125 loss: 0.0005576707189902663
batch 130 loss: 0.0005576164578087628
batch 135 loss: 0.000557767495047301
batch 140 loss: 0.0005576544092036784
batch 145 loss: 0.0005576871219091118
batch 150 loss: 0.0005577396601438523
batch 155 loss: 0.0005576729541644454
batch 160 loss: 0.0005578304058872164
batch 165 loss: 0.0005577250849455595
batch 170 loss: 0.0005577050498686731
batch 175 loss: 0.0005576562951318919
batch 180 loss: 0.0005578086595050991
batch 185 loss: 0.0005577316624112427
batch 190 loss: 0.0005576511844992638
batch 195 loss: 0.000557674583978951
batch 200 loss: 0.0005577171221375465
batch 205 loss: 0.0005575826275162399
batch 210 loss: 0.0005576060619205236
batch 215 loss: 0.0005576278315857052
batch 220 loss: 0.0005577344330959022
batch 225 loss: 0.0005577921401709318
batch 230 loss: 0.0005575978313572705
batch 235 loss: 0.0005577700911089778
batch 240 loss: 0.0005575808696448803
Training Loss: 0.0005576833917681749
Validation Loss: 0.0005576933966949582
Epoch 30:
batch 5 loss: 0.0005577916861511766
batch 10 loss: 0.0005576804978772998
batch 15 loss: 0.0005576243507675827
batch 20 loss: 0.0005578214768320322
batch 25 loss: 0.0005577979609370232
batch 30 loss: 0.0005577573785558343
batch 35 loss: 0.000557740859221667
batch 40 loss: 0.0005577142466790975
batch 45 loss: 0.0005577401374466717
batch 50 loss: 0.0005574544775299728
batch 55 loss: 0.0005576611147262156
batch 60 loss: 0.0005576583091169596
batch 65 loss: 0.0005576821160502732
batch 70 loss: 0.0005577484611421823
batch 75 loss: 0.0005577485542744398
batch 80 loss: 0.0005576026625931263
batch 85 loss: 0.0005577146308496595
batch 90 loss: 0.0005576235358603299
batch 95 loss: 0.0005576840485446155
batch 100 loss: 0.0005576773779466748
batch 105 loss: 0.0005576759343966841
batch 110 loss: 0.0005576882162131369
batch 115 loss: 0.0005577072617597878
batch 120 loss: 0.0005577233387157321
batch 125 loss: 0.0005577886593528091
batch 130 loss: 0.0005576209630817174
batch 135 loss: 0.0005576210096478462
batch 140 loss: 0.0005576437804847955
batch 145 loss: 0.0005577074130997062
batch 150 loss: 0.0005576952011324465
batch 155 loss: 0.000557654385920614
batch 160 loss: 0.0005576796596869826
batch 165 loss: 0.0005576368770562112
batch 170 loss: 0.0005576041643507778
batch 175 loss: 0.0005576846888288856
batch 180 loss: 0.0005576294846832752
batch 185 loss: 0.0005576361902058125
batch 190 loss: 0.0005576828727498651
batch 195 loss: 0.0005577450967393816
batch 200 loss: 0.0005576596362516284
batch 205 loss: 0.0005576878087595105
batch 210 loss: 0.0005577281815931201
batch 215 loss: 0.0005576194613240659
batch 220 loss: 0.000557741813827306
batch 225 loss: 0.0005576477735303343
batch 230 loss: 0.0005576288560405374
batch 235 loss: 0.0005576551076956094
batch 240 loss: 0.0005576851079240441
Training Loss: 0.0005576833922532387
Validation Loss: 0.0005576934131871288
Epoch 31:
batch 5 loss: 0.0005576090072281659
batch 10 loss: 0.0005576888797804713
batch 15 loss: 0.0005576509982347488
batch 20 loss: 0.000557649158872664
batch 25 loss: 0.0005575759918428957
batch 30 loss: 0.0005576375988312066
batch 35 loss: 0.0005577709991484881
batch 40 loss: 0.0005577938165515662
batch 45 loss: 0.0005576579482294619
batch 50 loss: 0.0005576467025093734
batch 55 loss: 0.0005577857955358922
batch 60 loss: 0.0005576478200964629
batch 65 loss: 0.0005576136056333781
batch 70 loss: 0.0005576757597737014
batch 75 loss: 0.0005576344789005816
batch 80 loss: 0.0005577033385634423
batch 85 loss: 0.0005577081930823625
batch 90 loss: 0.000557702302467078
batch 95 loss: 0.0005577128380537033
batch 100 loss: 0.000557529751677066
batch 105 loss: 0.0005576622788794339
batch 110 loss: 0.0005577513482421636
batch 115 loss: 0.000557781953830272
batch 120 loss: 0.00055763068376109
batch 125 loss: 0.0005577401723712682
batch 130 loss: 0.0005576541181653738
batch 135 loss: 0.0005576706607826054
batch 140 loss: 0.0005577854230068624
batch 145 loss: 0.0005577669711783528
batch 150 loss: 0.0005576227442361415
batch 155 loss: 0.0005577412317506969
batch 160 loss: 0.000557626225054264
batch 165 loss: 0.000557712628506124
batch 170 loss: 0.0005576868541538715
batch 175 loss: 0.0005576550029218197
batch 180 loss: 0.0005576454801484943
batch 185 loss: 0.0005576918832957744
batch 190 loss: 0.0005577045725658536
batch 195 loss: 0.0005576307186856866
batch 200 loss: 0.0005576721043325961
batch 205 loss: 0.0005577329080551863
batch 210 loss: 0.0005576294614002108
batch 215 loss: 0.0005577505682595075
batch 220 loss: 0.000557705364190042
batch 225 loss: 0.0005576795199885964
batch 230 loss: 0.0005576243391260504
batch 235 loss: 0.0005577844800427556
batch 240 loss: 0.000557668530382216
Training Loss: 0.0005576834002567921
Validation Loss: 0.0005576934093066181
Epoch 32:
batch 5 loss: 0.0005577721283771097
batch 10 loss: 0.0005577407428063452
batch 15 loss: 0.0005576355615630746
batch 20 loss: 0.000557685166131705
batch 25 loss: 0.0005575516261160374
batch 30 loss: 0.0005577577045187354
batch 35 loss: 0.0005576557363383472
batch 40 loss: 0.0005575315677560866
batch 45 loss: 0.0005577350850217045
batch 50 loss: 0.0005576289142481983
batch 55 loss: 0.000557667447719723
batch 60 loss: 0.0005577720934525132
batch 65 loss: 0.000557648844551295
batch 70 loss: 0.0005576142575591803
batch 75 loss: 0.0005577045842073858
batch 80 loss: 0.0005577331059612334
batch 85 loss: 0.0005575953982770443
batch 90 loss: 0.0005577142466790975
batch 95 loss: 0.0005577621050179005
batch 100 loss: 0.0005577113246545195
batch 105 loss: 0.0005577332805842162
batch 110 loss: 0.0005576676223427058
batch 115 loss: 0.0005576392635703087
batch 120 loss: 0.0005576111725531518
batch 125 loss: 0.000557716644834727
batch 130 loss: 0.0005576745723374188
batch 135 loss: 0.0005575576913543045
batch 140 loss: 0.0005576404044404625
batch 145 loss: 0.0005575727787800133
batch 150 loss: 0.0005578025593422353
batch 155 loss: 0.0005577451316639781
batch 160 loss: 0.0005577888456173241
batch 165 loss: 0.0005576907889917493
batch 170 loss: 0.0005576988216489554
batch 175 loss: 0.0005576147348619997
batch 180 loss: 0.0005576876108534634
batch 185 loss: 0.0005577253527007997
batch 190 loss: 0.0005577417672611773
batch 195 loss: 0.000557707843836397
batch 200 loss: 0.0005574929644353687
batch 205 loss: 0.0005577521398663521
batch 210 loss: 0.0005576421972364187
batch 215 loss: 0.0005578245618380606
batch 220 loss: 0.000557661394122988
batch 225 loss: 0.0005576856201514602
batch 230 loss: 0.0005577210104092956
batch 235 loss: 0.000557729578576982
batch 240 loss: 0.0005576601717621088
Training Loss: 0.0005576834201444096
Validation Loss: 0.0005576934005754689
Epoch 33:
batch 5 loss: 0.0005577254691161215
batch 10 loss: 0.0005575759802013636
batch 15 loss: 0.0005576884024776519
batch 20 loss: 0.0005576925235800445
batch 25 loss: 0.0005576217663474381
batch 30 loss: 0.0005576880066655576
batch 35 loss: 0.0005575873656198382
batch 40 loss: 0.000557749904692173
batch 45 loss: 0.0005577174830250442
batch 50 loss: 0.0005575087387114763
batch 55 loss: 0.0005576175753958523
batch 60 loss: 0.0005577194970101118
batch 65 loss: 0.0005576844792813063
batch 70 loss: 0.0005577822681516409
batch 75 loss: 0.0005576209397986532
batch 80 loss: 0.0005575597868300974
batch 85 loss: 0.0005577486008405685
batch 90 loss: 0.0005576622439548373
batch 95 loss: 0.0005576674942858517
batch 100 loss: 0.0005577124306000769
batch 105 loss: 0.0005576397990807891
batch 110 loss: 0.0005576813593506813
batch 115 loss: 0.0005577435018494725
batch 120 loss: 0.0005576477269642055
batch 125 loss: 0.0005577301955781877
batch 130 loss: 0.0005577557138167321
batch 135 loss: 0.0005576642812229693
batch 140 loss: 0.0005576810566708445
batch 145 loss: 0.0005576152121648193
batch 150 loss: 0.0005577081465162337
batch 155 loss: 0.0005576394847594202
batch 160 loss: 0.0005577187519520521
batch 165 loss: 0.0005578001495450735
batch 170 loss: 0.0005577151663601398
batch 175 loss: 0.0005576016497798264
batch 180 loss: 0.0005576804280281067
batch 185 loss: 0.0005576521158218384
batch 190 loss: 0.0005575831164605916
batch 195 loss: 0.0005578219657763839
batch 200 loss: 0.0005576628376729786
batch 205 loss: 0.0005576968309469521
batch 210 loss: 0.0005577618372626603
batch 215 loss: 0.0005577346310019493
batch 220 loss: 0.0005576066207140685
batch 225 loss: 0.0005577219184488058
batch 230 loss: 0.0005577658652327955
batch 235 loss: 0.0005577166681177914
batch 240 loss: 0.000557725306134671
Training Loss: 0.0005576834019545156
Validation Loss: 0.0005576934325896824
Epoch 34:
batch 5 loss: 0.0005576441180892289
batch 10 loss: 0.0005576073541305959
batch 15 loss: 0.00055770562030375
batch 20 loss: 0.000557771825697273
batch 25 loss: 0.0005576696014031768
batch 30 loss: 0.0005576799274422228
batch 35 loss: 0.0005576918949373067
batch 40 loss: 0.0005577285191975534
batch 45 loss: 0.000557711988221854
batch 50 loss: 0.0005576515803113579
batch 55 loss: 0.000557578750886023
batch 60 loss: 0.0005576681927777827
batch 65 loss: 0.0005576048861257732
batch 70 loss: 0.000557729578576982
batch 75 loss: 0.0005577771924436092
batch 80 loss: 0.0005575677379965783
batch 85 loss: 0.0005577484262175858
batch 90 loss: 0.0005576096940785646
batch 95 loss: 0.0005577354575507342
batch 100 loss: 0.0005576222436502575
batch 105 loss: 0.0005577230826020241
batch 110 loss: 0.000557760486844927
batch 115 loss: 0.0005577174364589155
batch 120 loss: 0.0005576894269324839
batch 125 loss: 0.0005577152129262686
batch 130 loss: 0.0005576981347985566
batch 135 loss: 0.0005576657480560243
batch 140 loss: 0.0005577311967499554
batch 145 loss: 0.0005575626040808856
batch 150 loss: 0.0005577910225838423
batch 155 loss: 0.0005576020572334528
batch 160 loss: 0.0005575734539888799
batch 165 loss: 0.0005576951778493821
batch 170 loss: 0.0005576621508225799
batch 175 loss: 0.0005576543393544853
batch 180 loss: 0.0005576749215833842
batch 185 loss: 0.0005575417191721499
batch 190 loss: 0.0005576451774686575
batch 195 loss: 0.0005578055512160063
batch 200 loss: 0.00055765719152987
batch 205 loss: 0.0005577509058639407
batch 210 loss: 0.0005576886003836989
batch 215 loss: 0.0005577044212259352
batch 220 loss: 0.000557677261531353
batch 225 loss: 0.0005576945841312408
batch 230 loss: 0.0005577387986704707
batch 235 loss: 0.0005577559117227793
batch 240 loss: 0.0005577221396379173
Training Loss: 0.0005576834021970475
Validation Loss: 0.0005576936547489216
Epoch 35:
batch 5 loss: 0.0005575779592618346
batch 10 loss: 0.0005577977164648474
batch 15 loss: 0.0005577748757787049
batch 20 loss: 0.0005576814990490675
batch 25 loss: 0.0005577025469392539
batch 30 loss: 0.000557654700241983
batch 35 loss: 0.0005576229654252529
batch 40 loss: 0.000557638518512249
batch 45 loss: 0.000557710521388799
batch 50 loss: 0.000557635398581624
batch 55 loss: 0.0005576616269536317
batch 60 loss: 0.0005576823488809169
batch 65 loss: 0.0005577076110057533
batch 70 loss: 0.0005575444898568094
batch 75 loss: 0.0005576077033765614
batch 80 loss: 0.0005575931281782687
batch 85 loss: 0.000557726074475795
batch 90 loss: 0.0005576815805397928
batch 95 loss: 0.0005577252944931388
batch 100 loss: 0.0005575743387453258
batch 105 loss: 0.0005577487754635513
batch 110 loss: 0.0005578860174864531
batch 115 loss: 0.0005576341529376805
batch 120 loss: 0.0005577357718721032
batch 125 loss: 0.0005577683565206826
batch 130 loss: 0.0005577062838710845
batch 135 loss: 0.000557736773043871
batch 140 loss: 0.0005576493800617755
batch 145 loss: 0.0005576776224188506
batch 150 loss: 0.0005577253759838641
batch 155 loss: 0.0005577980191446841
batch 160 loss: 0.000557793932966888
batch 165 loss: 0.0005577440024353564
batch 170 loss: 0.0005576447700150311
batch 175 loss: 0.0005575475632213056
batch 180 loss: 0.0005577487754635513
batch 185 loss: 0.000557647121604532
batch 190 loss: 0.0005575584596954286
batch 195 loss: 0.0005576665280386806
batch 200 loss: 0.0005577026517130434
batch 205 loss: 0.0005577202886343002
batch 210 loss: 0.0005577556672506035
batch 215 loss: 0.0005576269351877273
batch 220 loss: 0.0005577138392254711
batch 225 loss: 0.000557660753838718
batch 230 loss: 0.0005575124989263714
batch 235 loss: 0.0005576653406023979
batch 240 loss: 0.0005577276344411076
Training Loss: 0.0005576834206294734
Validation Loss: 0.0005576934160975119
Epoch 36:
batch 5 loss: 0.0005576933035627007
batch 10 loss: 0.0005577267613261939
batch 15 loss: 0.0005576768191531301
batch 20 loss: 0.0005577290430665016
batch 25 loss: 0.0005577392294071615
batch 30 loss: 0.0005576816620305181
batch 35 loss: 0.000557907298207283
batch 40 loss: 0.0005575963878072798
batch 45 loss: 0.0005576036288402975
batch 50 loss: 0.0005577652249485254
batch 55 loss: 0.0005575775983743369
batch 60 loss: 0.0005577579373493791
batch 65 loss: 0.000557685422245413
batch 70 loss: 0.0005576491821557283
batch 75 loss: 0.0005575938150286675
batch 80 loss: 0.0005576716852374375
batch 85 loss: 0.000557571358513087
batch 90 loss: 0.0005577071802690625
batch 95 loss: 0.000557709636632353
batch 100 loss: 0.0005577154224738479
batch 105 loss: 0.0005575945950113236
batch 110 loss: 0.0005578026873990893
batch 115 loss: 0.0005577688687480986
batch 120 loss: 0.0005576853989623487
batch 125 loss: 0.0005576708703301847
batch 130 loss: 0.0005577182397246361
batch 135 loss: 0.0005576262716203928
batch 140 loss: 0.0005576307768933475
batch 145 loss: 0.0005577513133175671
batch 150 loss: 0.000557734933681786
batch 155 loss: 0.0005577602074481547
batch 160 loss: 0.0005576642812229693
batch 165 loss: 0.0005576903698965907
batch 170 loss: 0.000557661836501211
batch 175 loss: 0.0005576348747126758
batch 180 loss: 0.000557710591237992
batch 185 loss: 0.0005576223717071116
batch 190 loss: 0.0005577044445089996
batch 195 loss: 0.0005575809977017343
batch 200 loss: 0.0005576610099524259
batch 205 loss: 0.000557751371525228
batch 210 loss: 0.0005577363190241158
batch 215 loss: 0.0005577060277573764
batch 220 loss: 0.0005575167830102146
batch 225 loss: 0.000557710335124284
batch 230 loss: 0.0005576508003287018
batch 235 loss: 0.0005575596820563078
batch 240 loss: 0.0005577387288212776
Training Loss: 0.0005576834080178136
Validation Loss: 0.0005576935975113884
Epoch 37:
batch 5 loss: 0.0005576578434556723
batch 10 loss: 0.0005576687981374562
batch 15 loss: 0.0005577071919105947
batch 20 loss: 0.0005576898693107069
batch 25 loss: 0.0005577859352342785
batch 30 loss: 0.0005576843861490488
batch 35 loss: 0.0005576602066867054
batch 40 loss: 0.0005576307186856866
batch 45 loss: 0.0005576771683990956
batch 50 loss: 0.0005577483330853284
batch 55 loss: 0.0005577687756158411
batch 60 loss: 0.000557640753686428
batch 65 loss: 0.0005576228839345276
batch 70 loss: 0.000557703129015863
batch 75 loss: 0.0005575689719989896
batch 80 loss: 0.0005577821517363191
batch 85 loss: 0.0005577441770583391
batch 90 loss: 0.0005577697767876089
batch 95 loss: 0.0005576393683440984
batch 100 loss: 0.0005577128729782998
batch 105 loss: 0.0005577098694629967
batch 110 loss: 0.0005576065159402788
batch 115 loss: 0.0005576104973442853
batch 120 loss: 0.0005577141302637756
batch 125 loss: 0.000557707657571882
batch 130 loss: 0.0005576377734541893
batch 135 loss: 0.0005577889271080494
batch 140 loss: 0.0005577245610766113
batch 145 loss: 0.0005576534196734429
batch 150 loss: 0.0005576328025199473
batch 155 loss: 0.0005576755735091865
batch 160 loss: 0.0005577355739660561
batch 165 loss: 0.0005577122559770942
batch 170 loss: 0.0005575158982537686
batch 175 loss: 0.0005577189847826957
batch 180 loss: 0.0005576453753747046
batch 185 loss: 0.0005575792631134391
batch 190 loss: 0.0005576426978223026
batch 195 loss: 0.0005577237578108906
batch 200 loss: 0.0005576143506914377
batch 205 loss: 0.0005577609641477466
batch 210 loss: 0.0005577093223109841
batch 215 loss: 0.0005577614298090339
batch 220 loss: 0.0005576765979640186
batch 225 loss: 0.0005576559226028622
batch 230 loss: 0.0005576858296990394
batch 235 loss: 0.0005576354567892849
batch 240 loss: 0.0005577059229835868
Training Loss: 0.0005576834300882183
Validation Loss: 0.0005576934306494271
Epoch 38:
batch 5 loss: 0.0005576206953264773
batch 10 loss: 0.0005577234551310539
batch 15 loss: 0.0005575325340032578
batch 20 loss: 0.0005576478899456561
batch 25 loss: 0.0005577355856075883
batch 30 loss: 0.0005576469702646136
batch 35 loss: 0.0005577428615652025
batch 40 loss: 0.0005577291012741625
batch 45 loss: 0.0005576527095399797
batch 50 loss: 0.0005576487514190376
batch 55 loss: 0.0005576129420660436
batch 60 loss: 0.0005577247589826584
batch 65 loss: 0.0005577199044637382
batch 70 loss: 0.0005576836876571179
batch 75 loss: 0.0005576865398325026
batch 80 loss: 0.000557797506917268
batch 85 loss: 0.0005576460971496999
batch 90 loss: 0.0005577486241236329
batch 95 loss: 0.0005577356903813779
batch 100 loss: 0.0005577167379669845
batch 105 loss: 0.0005575957242399454
batch 110 loss: 0.0005576545605435968
batch 115 loss: 0.0005576637806370855
batch 120 loss: 0.0005575970862992108
batch 125 loss: 0.0005577155738137662
batch 130 loss: 0.0005575740593485534
batch 135 loss: 0.000557669484987855
batch 140 loss: 0.0005577506264671684
batch 145 loss: 0.0005577040836215019
batch 150 loss: 0.0005576860858127475
batch 155 loss: 0.000557676050812006
batch 160 loss: 0.0005576846655458212
batch 165 loss: 0.0005578417796641589
batch 170 loss: 0.0005577620817348361
batch 175 loss: 0.0005576647352427244
batch 180 loss: 0.0005575253278948366
batch 185 loss: 0.0005576244089752436
batch 190 loss: 0.0005576737225055694
batch 195 loss: 0.0005576114170253276
batch 200 loss: 0.00055761500261724
batch 205 loss: 0.0005576531868427992
batch 210 loss: 0.0005577122909016907
batch 215 loss: 0.0005577090196311474
batch 220 loss: 0.0005577621050179005
batch 225 loss: 0.0005577329080551863
batch 230 loss: 0.0005576877039857209
batch 235 loss: 0.0005577151197940112
batch 240 loss: 0.0005577884498052299
Training Loss: 0.0005576834184466862
Validation Loss: 0.0005576934694545343
Epoch 39:
batch 5 loss: 0.0005576260969974101
batch 10 loss: 0.0005576643859967589
batch 15 loss: 0.0005575106013566255
batch 20 loss: 0.0005577740143053233
batch 25 loss: 0.0005576684954576195
batch 30 loss: 0.0005575477727688849
batch 35 loss: 0.0005577016854658723
batch 40 loss: 0.0005575945251621306
batch 45 loss: 0.000557765201665461
batch 50 loss: 0.0005577492294833065
batch 55 loss: 0.0005576917203143239
batch 60 loss: 0.0005577063304372132
batch 65 loss: 0.0005577271338552236
batch 70 loss: 0.0005577592761255801
batch 75 loss: 0.0005576913594268262
batch 80 loss: 0.0005577356088906527
batch 85 loss: 0.0005576884956099093
batch 90 loss: 0.0005577199161052703
batch 95 loss: 0.0005576908704824745
batch 100 loss: 0.000557661650236696
batch 105 loss: 0.0005577073083259165
batch 110 loss: 0.00055766636505723
batch 115 loss: 0.0005576351308263838
batch 120 loss: 0.000557749264407903
batch 125 loss: 0.0005576952826231718
batch 130 loss: 0.0005575980525463819
batch 135 loss: 0.0005575604038313032
batch 140 loss: 0.0005576212774030864
batch 145 loss: 0.000557671720162034
batch 150 loss: 0.0005576590308919549
batch 155 loss: 0.0005577031755819917
batch 160 loss: 0.0005577200674451888
batch 165 loss: 0.0005577242933213711
batch 170 loss: 0.0005575753515586257
batch 175 loss: 0.0005576962605118752
batch 180 loss: 0.0005577413365244865
batch 185 loss: 0.0005577052477747201
batch 190 loss: 0.0005576201598159969
batch 195 loss: 0.0005577677162364125
batch 200 loss: 0.0005578178563155234
batch 205 loss: 0.0005576619529165328
batch 210 loss: 0.0005577088100835681
batch 215 loss: 0.0005576650728471577
batch 220 loss: 0.0005576735711656511
batch 225 loss: 0.0005576846422627568
batch 230 loss: 0.0005576990195550024
batch 235 loss: 0.0005577379954047502
batch 240 loss: 0.0005576639086939394
Training Loss: 0.0005576834300882183
Validation Loss: 0.0005576934325896824
Epoch 40:
batch 5 loss: 0.0005576765863224864
batch 10 loss: 0.0005575964925810695
batch 15 loss: 0.0005576770170591771
batch 20 loss: 0.0005576555500738323
batch 25 loss: 0.0005577275529503823
batch 30 loss: 0.0005577606265433132
batch 35 loss: 0.0005577262840233743
batch 40 loss: 0.0005576690076850354
batch 45 loss: 0.0005576315335929394
batch 50 loss: 0.0005576819297857582
batch 55 loss: 0.0005576573661528528
batch 60 loss: 0.0005576756782829761
batch 65 loss: 0.0005577614298090339
batch 70 loss: 0.0005576039664447307
batch 75 loss: 0.0005577284842729569
batch 80 loss: 0.000557724165264517
batch 85 loss: 0.0005576947703957557
batch 90 loss: 0.0005576475872658193
batch 95 loss: 0.0005575740826316178
batch 100 loss: 0.0005576603231020272
batch 105 loss: 0.0005576874827966094
batch 110 loss: 0.000557626853697002
batch 115 loss: 0.0005577191710472107
batch 120 loss: 0.000557667319662869
batch 125 loss: 0.0005576865863986313
batch 130 loss: 0.0005576863302849233
batch 135 loss: 0.0005576923838816583
batch 140 loss: 0.0005577899632044136
batch 145 loss: 0.0005576746887527406
batch 150 loss: 0.0005576912779361009
batch 155 loss: 0.00055768305901438
batch 160 loss: 0.0005576914525590837
batch 165 loss: 0.0005575554794631898
batch 170 loss: 0.0005576674360781908
batch 175 loss: 0.0005576017661951483
batch 180 loss: 0.0005577333737164736
batch 185 loss: 0.000557792722247541
batch 190 loss: 0.0005577603704296052
batch 195 loss: 0.0005576975760050118
batch 200 loss: 0.000557672989089042
batch 205 loss: 0.0005576248047873377
batch 210 loss: 0.0005576778552494943
batch 215 loss: 0.0005576579947955907
batch 220 loss: 0.0005576655967161059
batch 225 loss: 0.0005576724885031581
batch 230 loss: 0.0005577941541559994
batch 235 loss: 0.0005576543975621462
batch 240 loss: 0.0005577467265538872
Training Loss: 0.0005576833903129833
Validation Loss: 0.000557692744769156
Epoch 41:
batch 5 loss: 0.0005576835828833282
batch 10 loss: 0.0005576066090725362
batch 15 loss: 0.0005576330586336553
batch 20 loss: 0.0005576546536758542
batch 25 loss: 0.0005577386124059558
batch 30 loss: 0.000557704467792064
batch 35 loss: 0.0005575945600867271
batch 40 loss: 0.0005576881696470082
batch 45 loss: 0.000557646807283163
batch 50 loss: 0.0005575347342528403
batch 55 loss: 0.0005577591131441295
batch 60 loss: 0.0005577610339969396
batch 65 loss: 0.0005576334893703461
batch 70 loss: 0.0005577803240157664
batch 75 loss: 0.0005578181706368923
batch 80 loss: 0.0005576482857577503
batch 85 loss: 0.0005576723022386431
batch 90 loss: 0.0005576523486524821
batch 95 loss: 0.0005577206029556692
batch 100 loss: 0.0005575978080742061
batch 105 loss: 0.0005576256080530584
batch 110 loss: 0.0005577565869316458
batch 115 loss: 0.000557606085203588
batch 120 loss: 0.0005576848634518683
batch 125 loss: 0.0005577303702011705
batch 130 loss: 0.0005578183569014073
batch 135 loss: 0.0005575383664108812
batch 140 loss: 0.0005577116622589529
batch 145 loss: 0.0005576176219619811
batch 150 loss: 0.0005577429546974599
batch 155 loss: 0.0005575889372266829
batch 160 loss: 0.000557724735699594
batch 165 loss: 0.0005577193573117256
batch 170 loss: 0.000557593849953264
batch 175 loss: 0.0005576978554017841
batch 180 loss: 0.0005578228970989585
batch 185 loss: 0.0005576012539677322
batch 190 loss: 0.0005576600902713835
batch 195 loss: 0.0005576718249358237
batch 200 loss: 0.0005577202420681715
batch 205 loss: 0.0005576773663051426
batch 210 loss: 0.0005576694267801941
batch 215 loss: 0.0005578001495450735
batch 220 loss: 0.0005576573312282562
batch 225 loss: 0.000557706318795681
batch 230 loss: 0.0005577513249590993
batch 235 loss: 0.0005577106261625886
batch 240 loss: 0.0005576707771979272
Training Loss: 0.000557683449490772
Validation Loss: 0.0005576934044559796
Epoch 42:
batch 5 loss: 0.0005576764466241002
batch 10 loss: 0.0005577551550231874
batch 15 loss: 0.000557707843836397
batch 20 loss: 0.0005574745242483914
batch 25 loss: 0.0005577544216066599
batch 30 loss: 0.0005575883667916059
batch 35 loss: 0.0005577613599598407
batch 40 loss: 0.0005575971095822752
batch 45 loss: 0.0005577309522777796
batch 50 loss: 0.0005577889387495816
batch 55 loss: 0.0005576993338763714
batch 60 loss: 0.0005576561205089092
batch 65 loss: 0.0005577133619226515
batch 70 loss: 0.000557689880952239
batch 75 loss: 0.0005576469353400171
batch 80 loss: 0.0005577879375778139
batch 85 loss: 0.0005577044561505317
batch 90 loss: 0.0005576669820584357
batch 95 loss: 0.0005576563882641495
batch 100 loss: 0.0005576747003942728
batch 105 loss: 0.0005576928379014134
batch 110 loss: 0.0005577489384450019
batch 115 loss: 0.0005576737457886338
batch 120 loss: 0.0005576680530793964
batch 125 loss: 0.0005576140596531331
batch 130 loss: 0.0005576353752985597
batch 135 loss: 0.0005575275048613548
batch 140 loss: 0.0005577698117122054
batch 145 loss: 0.0005577462841756641
batch 150 loss: 0.0005576215102337301
batch 155 loss: 0.0005576546420343221
batch 160 loss: 0.0005576760624535382
batch 165 loss: 0.0005577546078711748
batch 170 loss: 0.0005577150150202215
batch 175 loss: 0.0005576488911174238
batch 180 loss: 0.0005576798575930297
batch 185 loss: 0.0005575406365096569
batch 190 loss: 0.0005576034891419113
batch 195 loss: 0.0005578245152719318
batch 200 loss: 0.0005577220348641276
batch 205 loss: 0.0005577114876359701
batch 210 loss: 0.0005576773779466748
batch 215 loss: 0.0005577421979978681
batch 220 loss: 0.0005577038158662617
batch 225 loss: 0.0005576925817877054
batch 230 loss: 0.0005576303927227854
batch 235 loss: 0.000557827961165458
batch 240 loss: 0.0005575700080953539
Training Loss: 0.0005576834356664525
Validation Loss: 0.0005576934054261073
Epoch 43:
batch 5 loss: 0.0005576419527642429
batch 10 loss: 0.0005577119998633861
batch 15 loss: 0.0005577369476668537
batch 20 loss: 0.0005577557021752
batch 25 loss: 0.0005576754454523325
batch 30 loss: 0.0005576114170253276
batch 35 loss: 0.0005576118361204863
batch 40 loss: 0.0005577303119935096
batch 45 loss: 0.0005576937575824559
batch 50 loss: 0.0005575458751991391
batch 55 loss: 0.0005576101131737233
batch 60 loss: 0.0005576241062954068
batch 65 loss: 0.0005576736410148441
batch 70 loss: 0.0005576588329859078
batch 75 loss: 0.0005577118834480643
batch 80 loss: 0.0005576127208769321
batch 85 loss: 0.0005577766918577254
batch 90 loss: 0.0005577157833613455
batch 95 loss: 0.0005577695788815618
batch 100 loss: 0.0005576609284617007
batch 105 loss: 0.000557639729231596
batch 110 loss: 0.0005577290547080338
batch 115 loss: 0.0005577013129368424
batch 120 loss: 0.0005576939089223743
batch 125 loss: 0.0005577760050073266
batch 130 loss: 0.0005576899973675608
batch 135 loss: 0.0005576423369348049
batch 140 loss: 0.0005576077732257545
batch 145 loss: 0.0005576487281359732
batch 150 loss: 0.0005577006726525724
batch 155 loss: 0.0005577628267928958
batch 160 loss: 0.0005576896481215953
batch 165 loss: 0.0005576930474489927
batch 170 loss: 0.0005575744085945189
batch 175 loss: 0.0005577288684435189
batch 180 loss: 0.0005576324067078531
batch 185 loss: 0.000557684141676873
batch 190 loss: 0.0005577662377618253
batch 195 loss: 0.0005577242234721779
batch 200 loss: 0.0005576345720328391
batch 205 loss: 0.0005577703821472823
batch 210 loss: 0.0005576834664680064
batch 215 loss: 0.000557650055270642
batch 220 loss: 0.0005575789255090058
batch 225 loss: 0.0005577849806286395
batch 230 loss: 0.0005577780772000551
batch 235 loss: 0.0005576686700806022
batch 240 loss: 0.0005576422554440796
Training Loss: 0.0005576834633150914
Validation Loss: 0.000557693723627987
Epoch 44:
batch 5 loss: 0.0005576685653068125
batch 10 loss: 0.0005576892872340977
batch 15 loss: 0.0005576579831540584
batch 20 loss: 0.0005577097996138036
batch 25 loss: 0.0005578369717113673
batch 30 loss: 0.0005577209987677634
batch 35 loss: 0.000557721417862922
batch 40 loss: 0.0005576836410909891
batch 45 loss: 0.0005576879251748323
batch 50 loss: 0.0005576424300670624
batch 55 loss: 0.0005577150266617536
batch 60 loss: 0.0005577511386945843
batch 65 loss: 0.0005576383788138628
batch 70 loss: 0.0005577281815931201
batch 75 loss: 0.000557618192397058
batch 80 loss: 0.0005577239440754056
batch 85 loss: 0.0005576690658926964
batch 90 loss: 0.0005576839786954225
batch 95 loss: 0.0005577173782512545
batch 100 loss: 0.0005576391005888582
batch 105 loss: 0.0005576486000791192
batch 110 loss: 0.0005576562602072954
batch 115 loss: 0.0005576467257924378
batch 120 loss: 0.0005577319534495473
batch 125 loss: 0.0005575057584792376
batch 130 loss: 0.000557796040084213
batch 135 loss: 0.0005577775766141713
batch 140 loss: 0.0005577033851295709
batch 145 loss: 0.0005578301730565727
batch 150 loss: 0.0005576342577114701
batch 155 loss: 0.0005576093215495348
batch 160 loss: 0.0005577540840022265
batch 165 loss: 0.0005576921510510146
batch 170 loss: 0.0005576541647315025
batch 175 loss: 0.0005575964576564729
batch 180 loss: 0.0005577063304372132
batch 185 loss: 0.0005576760042458773
batch 190 loss: 0.0005576402181759477
batch 195 loss: 0.0005576586467213928
batch 200 loss: 0.0005575852002948523
batch 205 loss: 0.0005577046773396433
batch 210 loss: 0.0005576980300247669
batch 215 loss: 0.0005576881463639438
batch 220 loss: 0.0005576567840762436
batch 225 loss: 0.0005576789262704551
batch 230 loss: 0.0005576981813646853
batch 235 loss: 0.0005576181807555258
batch 240 loss: 0.0005576576804742217
Training Loss: 0.00055768348587056
Validation Loss: 0.0005576936062425375
Epoch 45:
batch 5 loss: 0.0005576102761551738
batch 10 loss: 0.0005576104042120278
batch 15 loss: 0.000557592255063355
batch 20 loss: 0.0005576890776865185
batch 25 loss: 0.0005576078779995442
batch 30 loss: 0.000557759020011872
batch 35 loss: 0.0005576071212999523
batch 40 loss: 0.0005576131283305585
batch 45 loss: 0.0005576074821874499
batch 50 loss: 0.0005577706382609904
batch 55 loss: 0.0005576509400270879
batch 60 loss: 0.0005576915456913412
batch 65 loss: 0.0005577964941039682
batch 70 loss: 0.0005576246068812907
batch 75 loss: 0.0005576481344178319
batch 80 loss: 0.0005577348987571895
batch 85 loss: 0.0005576821276918054
batch 90 loss: 0.0005577916163019836
batch 95 loss: 0.0005576795665547252
batch 100 loss: 0.0005577092990279198
batch 105 loss: 0.0005576878553256393
batch 110 loss: 0.0005577644682489335
batch 115 loss: 0.0005576363997533918
batch 120 loss: 0.000557550392113626
batch 125 loss: 0.0005576913943514227
batch 130 loss: 0.0005577408359386027
batch 135 loss: 0.0005578082636930048
batch 140 loss: 0.0005576214520260691
batch 145 loss: 0.0005576309864409268
batch 150 loss: 0.0005575965740717947
batch 155 loss: 0.0005577142001129687
batch 160 loss: 0.0005577537813223898
batch 165 loss: 0.0005576905328780413
batch 170 loss: 0.0005576674011535943
batch 175 loss: 0.0005576781462877989
batch 180 loss: 0.0005576459225267172
batch 185 loss: 0.0005576896946877241
batch 190 loss: 0.0005577026517130434
batch 195 loss: 0.0005577846663072705
batch 200 loss: 0.0005576337222009897
batch 205 loss: 0.0005576449097134173
batch 210 loss: 0.0005576333380304277
batch 215 loss: 0.0005577837233431638
batch 220 loss: 0.0005576557363383472
batch 225 loss: 0.0005578188574872911
batch 230 loss: 0.0005577049101702869
batch 235 loss: 0.0005576876574195921
batch 240 loss: 0.0005577127216383815
Training Loss: 0.0005576834938741134
Validation Loss: 0.0005576935218414292
Epoch 46:
batch 5 loss: 0.0005578547366894782
batch 10 loss: 0.0005576114868745208
batch 15 loss: 0.0005576290655881167
batch 20 loss: 0.0005576126161031425
batch 25 loss: 0.0005576562834903598
batch 30 loss: 0.0005576619761995972
batch 35 loss: 0.0005576800904236734
batch 40 loss: 0.0005577458068728447
batch 45 loss: 0.0005577628035098314
batch 50 loss: 0.000557597610168159
batch 55 loss: 0.0005576216499321163
batch 60 loss: 0.0005577565287239849
batch 65 loss: 0.0005577951902523637
batch 70 loss: 0.0005576661787927151
batch 75 loss: 0.0005575827439315617
batch 80 loss: 0.0005577603937126696
batch 85 loss: 0.0005576093564741314
batch 90 loss: 0.0005576053052209317
batch 95 loss: 0.0005578423850238323
batch 100 loss: 0.000557672418653965
batch 105 loss: 0.0005578221054747701
batch 110 loss: 0.0005576710565946997
batch 115 loss: 0.0005575949442572891
batch 120 loss: 0.0005576048977673053
batch 125 loss: 0.0005576319643296301
batch 130 loss: 0.000557608634699136
batch 135 loss: 0.0005576651310548186
batch 140 loss: 0.0005577174481004477
batch 145 loss: 0.0005575940245762468
batch 150 loss: 0.0005577339441515505
batch 155 loss: 0.000557617109734565
batch 160 loss: 0.0005577170290052891
batch 165 loss: 0.000557670183479786
batch 170 loss: 0.0005576997296884656
batch 175 loss: 0.0005576977040618658
batch 180 loss: 0.0005576974479481577
batch 185 loss: 0.0005577211966738104
batch 190 loss: 0.0005577554344199598
batch 195 loss: 0.0005576296825893223
batch 200 loss: 0.0005577423493377864
batch 205 loss: 0.0005576756433583796
batch 210 loss: 0.0005576618714258075
batch 215 loss: 0.0005577161908149719
batch 220 loss: 0.0005577326170168817
batch 225 loss: 0.0005577277741394937
batch 230 loss: 0.0005575924646109342
batch 235 loss: 0.0005576729658059776
batch 240 loss: 0.0005577101255767047
Training Loss: 0.000557683464527751
Validation Loss: 0.0005576936207944527
Epoch 47:
batch 5 loss: 0.0005577186006121337
batch 10 loss: 0.0005577346659265458
batch 15 loss: 0.0005576165975071489
batch 20 loss: 0.0005576676921918988
batch 25 loss: 0.0005577780772000551
batch 30 loss: 0.0005578399868682027
batch 35 loss: 0.0005577570991590619
batch 40 loss: 0.0005577541887760162
batch 45 loss: 0.0005576442345045507
batch 50 loss: 0.0005577732459641993
batch 55 loss: 0.0005576685536652804
batch 60 loss: 0.0005576588562689721
batch 65 loss: 0.0005577629199251533
batch 70 loss: 0.0005576051073148847
batch 75 loss: 0.0005577279604040086
batch 80 loss: 0.0005576383788138628
batch 85 loss: 0.0005576199619099498
batch 90 loss: 0.0005575875984504819
batch 95 loss: 0.0005576109397225082
batch 100 loss: 0.000557631510309875
batch 105 loss: 0.0005577733623795212
batch 110 loss: 0.0005577676231041551
batch 115 loss: 0.0005577013711445033
batch 120 loss: 0.000557743280660361
batch 125 loss: 0.0005576708703301847
batch 130 loss: 0.0005577935720793903
batch 135 loss: 0.0005576283787377178
batch 140 loss: 0.0005577338975854218
batch 145 loss: 0.0005576875293627382
batch 150 loss: 0.000557660823687911
batch 155 loss: 0.0005575629184022546
batch 160 loss: 0.00055768764577806
batch 165 loss: 0.0005576189258135855
batch 170 loss: 0.0005576933734118938
batch 175 loss: 0.0005576658411882817
batch 180 loss: 0.0005576463299803436
batch 185 loss: 0.0005577053409069777
batch 190 loss: 0.0005575538263656199
batch 195 loss: 0.0005577330477535725
batch 200 loss: 0.0005576962372288108
batch 205 loss: 0.0005576310795731843
batch 210 loss: 0.0005576686817221344
batch 215 loss: 0.00055764903081581
batch 220 loss: 0.000557655468583107
batch 225 loss: 0.0005577005562372505
batch 230 loss: 0.0005576910101808607
batch 235 loss: 0.000557604047935456
batch 240 loss: 0.0005576861789450049
Training Loss: 0.0005576834671956021
Validation Loss: 0.0005576935654971749
Epoch 48:
batch 5 loss: 0.0005576728843152523
batch 10 loss: 0.0005575882154516876
batch 15 loss: 0.0005576828843913972
batch 20 loss: 0.000557660183403641
batch 25 loss: 0.0005577539093792438
batch 30 loss: 0.0005577896721661091
batch 35 loss: 0.000557709252461791
batch 40 loss: 0.0005576001130975783
batch 45 loss: 0.0005576478433795273
batch 50 loss: 0.000557616469450295
batch 55 loss: 0.000557711988221854
batch 60 loss: 0.0005577616393566132
batch 65 loss: 0.0005577713018283248
batch 70 loss: 0.0005576505092903972
batch 75 loss: 0.0005577122792601585
batch 80 loss: 0.0005577098112553358
batch 85 loss: 0.0005577336996793747
batch 90 loss: 0.0005577189265750348
batch 95 loss: 0.0005577618721872568
batch 100 loss: 0.0005576570401899517
batch 105 loss: 0.0005577788688242435
batch 110 loss: 0.000557629531249404
batch 115 loss: 0.0005576811381615698
batch 120 loss: 0.0005576471332460642
batch 125 loss: 0.0005577206495217979
batch 130 loss: 0.0005576895200647414
batch 135 loss: 0.0005577349802479148
batch 140 loss: 0.0005577066796831787
batch 145 loss: 0.0005577141768299043
batch 150 loss: 0.0005576591822318733
batch 155 loss: 0.0005576628493145109
batch 160 loss: 0.0005575796472840011
batch 165 loss: 0.000557647761888802
batch 170 loss: 0.0005577566451393067
batch 175 loss: 0.0005576200899668037
batch 180 loss: 0.0005576217197813093
batch 185 loss: 0.0005576416268013417
batch 190 loss: 0.0005577760166488588
batch 195 loss: 0.0005576601484790445
batch 200 loss: 0.0005576765630394221
batch 205 loss: 0.0005576644558459521
batch 210 loss: 0.0005576964467763901
batch 215 loss: 0.0005576720577664673
batch 220 loss: 0.0005576605442911386
batch 225 loss: 0.0005575835704803467
batch 230 loss: 0.0005577236297540367
batch 235 loss: 0.0005576618015766144
batch 240 loss: 0.0005576299619860948
Training Loss: 0.0005576834977546241
Validation Loss: 0.0005576934228884057
Epoch 49:
batch 5 loss: 0.0005576486815698445
batch 10 loss: 0.000557643617503345
batch 15 loss: 0.0005576102063059807
batch 20 loss: 0.0005577849107794464
batch 25 loss: 0.0005576261202804745
batch 30 loss: 0.0005577070405706763
batch 35 loss: 0.0005575977964326739
batch 40 loss: 0.0005577265517786145
batch 45 loss: 0.0005575918010435998
batch 50 loss: 0.0005577145959250629
batch 55 loss: 0.0005576617200858891
batch 60 loss: 0.0005577669944614172
batch 65 loss: 0.0005576758063398302
batch 70 loss: 0.0005577542120590806
batch 75 loss: 0.000557665666565299
batch 80 loss: 0.0005577387986704707
batch 85 loss: 0.0005576494382694364
batch 90 loss: 0.0005577078554779291
batch 95 loss: 0.0005577259580604732
batch 100 loss: 0.0005577061674557626
batch 105 loss: 0.00055766636505723
batch 110 loss: 0.0005577171919867397
batch 115 loss: 0.0005575938383117318
batch 120 loss: 0.000557646038942039
batch 125 loss: 0.000557701347861439
batch 130 loss: 0.0005577724776230752
batch 135 loss: 0.0005576928495429456
batch 140 loss: 0.0005576513358391821
batch 145 loss: 0.0005578137468546629
batch 150 loss: 0.0005576167837716639
batch 155 loss: 0.0005577073665335774
batch 160 loss: 0.0005576087976805865
batch 165 loss: 0.0005576466326601803
batch 170 loss: 0.0005576807190664113
batch 175 loss: 0.0005577183794230223
batch 180 loss: 0.0005576763767749071
batch 185 loss: 0.000557708228006959
batch 190 loss: 0.0005576445255428553
batch 195 loss: 0.0005576539202593267
batch 200 loss: 0.0005576284136623144
batch 205 loss: 0.0005576541181653738
batch 210 loss: 0.000557772209867835
batch 215 loss: 0.0005576873430982232
batch 220 loss: 0.0005577026982791722
batch 225 loss: 0.0005576777155511081
batch 230 loss: 0.0005576565396040678
batch 235 loss: 0.0005576614872552455
batch 240 loss: 0.0005577457952313126
Training Loss: 0.000557683482960177
Validation Loss: 0.0005576934170676395
Epoch 50:
batch 5 loss: 0.0005576923838816583
batch 10 loss: 0.0005576925817877054
batch 15 loss: 0.0005576990661211312
batch 20 loss: 0.0005576319643296301
batch 25 loss: 0.0005576246650889516
batch 30 loss: 0.0005578032694756984
batch 35 loss: 0.0005575723946094513
batch 40 loss: 0.0005576643743552268
batch 45 loss: 0.0005575920105911792
batch 50 loss: 0.0005577724892646075
batch 55 loss: 0.0005576607654802501
batch 60 loss: 0.0005576674942858517
batch 65 loss: 0.0005577339907176793
batch 70 loss: 0.0005577503237873316
batch 75 loss: 0.0005576631869189441
batch 80 loss: 0.0005576063063926995
batch 85 loss: 0.0005576711148023606
batch 90 loss: 0.0005577245843596757
batch 95 loss: 0.0005576994153670967
batch 100 loss: 0.0005577203584834934
batch 105 loss: 0.0005577743402682244
batch 110 loss: 0.0005576774128712714
batch 115 loss: 0.0005577639676630497
batch 120 loss: 0.0005576490890234709
batch 125 loss: 0.0005577208357863128
batch 130 loss: 0.0005576117662712931
batch 135 loss: 0.0005576277384534478
batch 140 loss: 0.0005576382391154766
batch 145 loss: 0.0005576045834459365
batch 150 loss: 0.0005576866562478245
batch 155 loss: 0.0005577380303293467
batch 160 loss: 0.0005577004165388643
batch 165 loss: 0.0005576982977800072
batch 170 loss: 0.0005577128962613642
batch 175 loss: 0.0005576811148785054
batch 180 loss: 0.0005575959803536534
batch 185 loss: 0.0005576431984081864
batch 190 loss: 0.0005576435592956841
batch 195 loss: 0.000557706004474312
batch 200 loss: 0.0005576570983976126
batch 205 loss: 0.00055770268663764
batch 210 loss: 0.0005577075062319636
batch 215 loss: 0.0005576484836637974
batch 220 loss: 0.0005577591713517904
batch 225 loss: 0.0005576589028351009
batch 230 loss: 0.0005576711613684892
batch 235 loss: 0.0005577374249696732
batch 240 loss: 0.0005577498115599156
Training Loss: 0.0005576835232204757
Validation Loss: 0.0005576934597532575
Epoch 51:
batch 5 loss: 0.0005576890660449862
batch 10 loss: 0.00055768599268049
batch 15 loss: 0.0005577367381192743
batch 20 loss: 0.0005577067378908396
batch 25 loss: 0.00055754886707291
batch 30 loss: 0.0005577345262281596
batch 35 loss: 0.0005576380179263651
batch 40 loss: 0.0005576655617915094
batch 45 loss: 0.0005576011026278138
batch 50 loss: 0.0005576202995143831
batch 55 loss: 0.000557730719447136
batch 60 loss: 0.0005576144438236951
batch 65 loss: 0.0005576408468186856
batch 70 loss: 0.0005577109404839575
batch 75 loss: 0.0005577042349614203
batch 80 loss: 0.0005577450385317207
batch 85 loss: 0.0005577269825153053
batch 90 loss: 0.0005576417664997279
batch 95 loss: 0.0005576460855081678
batch 100 loss: 0.0005577178089879453
batch 105 loss: 0.0005576676223427058
batch 110 loss: 0.0005576591705903411
batch 115 loss: 0.0005576961091719567
batch 120 loss: 0.0005576811032369733
batch 125 loss: 0.0005576549563556909
batch 130 loss: 0.0005576523486524821
batch 135 loss: 0.0005576070281676949
batch 140 loss: 0.0005575999850407243
batch 145 loss: 0.0005577510688453913
batch 150 loss: 0.0005576416617259384
batch 155 loss: 0.0005576781579293311
batch 160 loss: 0.0005576599854975939
batch 165 loss: 0.0005576813127845526
batch 170 loss: 0.0005577661795541644
batch 175 loss: 0.0005577443982474506
batch 180 loss: 0.0005576162016950548
batch 185 loss: 0.0005577482166700065
batch 190 loss: 0.0005576941417530179
batch 195 loss: 0.0005578061449341476
batch 200 loss: 0.0005575526738539338
batch 205 loss: 0.0005576832918450236
batch 210 loss: 0.0005576912313699723
batch 215 loss: 0.000557737541384995
batch 220 loss: 0.0005577351083047688
batch 225 loss: 0.000557764305267483
batch 230 loss: 0.0005577956209890545
batch 235 loss: 0.0005576581810601055
batch 240 loss: 0.0005576772731728852
Training Loss: 0.0005576834749566236
Validation Loss: 0.000557693488857088
Epoch 52:
batch 5 loss: 0.0005576687748543918
batch 10 loss: 0.0005576531402766705
batch 15 loss: 0.0005577841191552579
batch 20 loss: 0.0005576957715675235
batch 25 loss: 0.0005576855503022671
batch 30 loss: 0.000557664199732244
batch 35 loss: 0.0005575859802775085
batch 40 loss: 0.0005576660740189254
batch 45 loss: 0.000557715620379895
batch 50 loss: 0.0005577386356890202
batch 55 loss: 0.0005576743162237108
batch 60 loss: 0.0005577103816904128
batch 65 loss: 0.0005576441879384219
batch 70 loss: 0.0005577607429586351
batch 75 loss: 0.00055760646937415
batch 80 loss: 0.0005576806026510895
batch 85 loss: 0.0005576330702751874
batch 90 loss: 0.0005576955620199442
batch 95 loss: 0.0005576761672273278
batch 100 loss: 0.0005576433963142335
batch 105 loss: 0.0005577275878749788
batch 110 loss: 0.0005576899740844965
batch 115 loss: 0.0005577092873863876
batch 120 loss: 0.0005577617092058062
batch 125 loss: 0.0005578881013207137
batch 130 loss: 0.0005575579358264804
batch 135 loss: 0.0005576774710789323
batch 140 loss: 0.0005576192634180188
batch 145 loss: 0.0005577425938099623
batch 150 loss: 0.0005576093681156635
batch 155 loss: 0.0005577205214649439
batch 160 loss: 0.0005577124888077378
batch 165 loss: 0.000557615770958364
batch 170 loss: 0.0005576472729444503
batch 175 loss: 0.0005576845607720316
batch 180 loss: 0.0005576933501288295
batch 185 loss: 0.0005576340132392943
batch 190 loss: 0.0005577083444222808
batch 195 loss: 0.0005577843519859016
batch 200 loss: 0.0005576995317824185
batch 205 loss: 0.000557714537717402
batch 210 loss: 0.0005576505325734615
batch 215 loss: 0.0005576200783252716
batch 220 loss: 0.0005576272844336927
batch 225 loss: 0.0005576317431405187
batch 230 loss: 0.0005576610215939582
batch 235 loss: 0.0005577412666752934
batch 240 loss: 0.0005576935363933444
Training Loss: 0.0005576834638001553
Validation Loss: 0.0005576934393805762
Epoch 53:
batch 5 loss: 0.0005577294272370636
batch 10 loss: 0.0005576606024987995
batch 15 loss: 0.0005576595664024353
batch 20 loss: 0.0005576044437475503
batch 25 loss: 0.0005576176918111742
batch 30 loss: 0.0005577928386628628
batch 35 loss: 0.0005577575648203492
batch 40 loss: 0.0005576592637225985
batch 45 loss: 0.0005577000207267702
batch 50 loss: 0.0005577238160185515
batch 55 loss: 0.0005576476803980767
batch 60 loss: 0.0005576617200858891
batch 65 loss: 0.0005576113355346024
batch 70 loss: 0.0005575917311944067
batch 75 loss: 0.0005577167961746454
batch 80 loss: 0.0005577584262937308
batch 85 loss: 0.000557698414195329
batch 90 loss: 0.0005578084266744554
batch 95 loss: 0.0005575997987762093
batch 100 loss: 0.0005576049210503697
batch 105 loss: 0.0005576832918450236
batch 110 loss: 0.000557732442393899
batch 115 loss: 0.0005577078089118003
batch 120 loss: 0.0005577142001129687
batch 125 loss: 0.0005576983676292002
batch 130 loss: 0.0005577591829933227
batch 135 loss: 0.0005576886935159564
batch 140 loss: 0.0005575482617132365
batch 145 loss: 0.0005576950497925282
batch 150 loss: 0.0005576874944381415
batch 155 loss: 0.0005576386349275709
batch 160 loss: 0.0005576472030952573
batch 165 loss: 0.0005577355856075883
batch 170 loss: 0.0005578355165198445
batch 175 loss: 0.0005576653173193335
batch 180 loss: 0.0005576563766226172
batch 185 loss: 0.000557656935416162
batch 190 loss: 0.0005577378906309605
batch 195 loss: 0.0005575865390710533
batch 200 loss: 0.000557692046277225
batch 205 loss: 0.0005576161318458616
batch 210 loss: 0.0005577172501944006
batch 215 loss: 0.0005576193099841476
batch 220 loss: 0.0005576331983320415
batch 225 loss: 0.0005576632684096694
batch 230 loss: 0.0005577241186983884
batch 235 loss: 0.0005577671690843999
batch 240 loss: 0.0005576931871473789
Training Loss: 0.0005576834366365802
Validation Loss: 0.0005576933986352135
Epoch 54:
batch 5 loss: 0.0005576290423050523
batch 10 loss: 0.0005577348289079964
batch 15 loss: 0.0005576438503339887
batch 20 loss: 0.0005576108349487185
batch 25 loss: 0.0005576848168857395
batch 30 loss: 0.0005577147589065134
batch 35 loss: 0.0005576626281253994
batch 40 loss: 0.0005577519070357085
batch 45 loss: 0.0005576556664891541
batch 50 loss: 0.0005576785420998931
batch 55 loss: 0.0005577104166150093
batch 60 loss: 0.000557617493905127
batch 65 loss: 0.0005576943745836616
batch 70 loss: 0.0005575838848017156
batch 75 loss: 0.0005577552830800415
batch 80 loss: 0.000557678414043039
batch 85 loss: 0.0005577096715569496
batch 90 loss: 0.000557863584253937
batch 95 loss: 0.0005576975061558187
batch 100 loss: 0.0005577073665335774
batch 105 loss: 0.0005576345720328391
batch 110 loss: 0.000557702628429979
batch 115 loss: 0.0005577524076215923
batch 120 loss: 0.0005577825824730099
batch 125 loss: 0.000557656039018184
batch 130 loss: 0.0005576425814069807
batch 135 loss: 0.000557596143335104
batch 140 loss: 0.0005576514755375684
batch 145 loss: 0.0005576167954131961
batch 150 loss: 0.0005576587282121181
batch 155 loss: 0.0005577053176239133
batch 160 loss: 0.0005576753639616073
batch 165 loss: 0.0005576258641667664
batch 170 loss: 0.0005576050141826272
batch 175 loss: 0.0005576868774369359
batch 180 loss: 0.0005575922434218228
batch 185 loss: 0.0005576850613579154
batch 190 loss: 0.0005576784838922322
batch 195 loss: 0.0005577519303187727
batch 200 loss: 0.0005576602648943663
batch 205 loss: 0.0005577377509325743
batch 210 loss: 0.0005577121162787079
batch 215 loss: 0.0005575880524702371
batch 220 loss: 0.000557703897356987
batch 225 loss: 0.0005577342235483229
batch 230 loss: 0.0005577297531999647
batch 235 loss: 0.0005577135365456342
batch 240 loss: 0.0005577404284849763
Training Loss: 0.0005576834376067078
Validation Loss: 0.0005576933976650859
Epoch 55:
batch 5 loss: 0.0005576851894147694
batch 10 loss: 0.0005577452480793
batch 15 loss: 0.000557650183327496
batch 20 loss: 0.0005576856550760567
batch 25 loss: 0.0005575876566581428
batch 30 loss: 0.0005577986012212932
batch 35 loss: 0.0005576600320637226
batch 40 loss: 0.0005575713468715549
batch 45 loss: 0.0005576695199124515
batch 50 loss: 0.0005576155497692525
batch 55 loss: 0.0005577682051807642
batch 60 loss: 0.0005575831281021237
batch 65 loss: 0.000557707401458174
batch 70 loss: 0.0005577313713729382
batch 75 loss: 0.0005576626164838672
batch 80 loss: 0.0005576059338636696
batch 85 loss: 0.0005576695431955159
batch 90 loss: 0.0005576901137828826
batch 95 loss: 0.000557676306925714
batch 100 loss: 0.000557666621170938
batch 105 loss: 0.0005576378898695111
batch 110 loss: 0.0005576622439548373
batch 115 loss: 0.0005576839321292937
batch 120 loss: 0.0005575967254117132
batch 125 loss: 0.000557739194482565
batch 130 loss: 0.000557681021746248
batch 135 loss: 0.0005576619296334684
batch 140 loss: 0.0005576431285589933
batch 145 loss: 0.0005576606141403318
batch 150 loss: 0.0005577340489253402
batch 155 loss: 0.0005577566684223711
batch 160 loss: 0.0005576673313044012
batch 165 loss: 0.0005576361319981516
batch 170 loss: 0.000557710335124284
batch 175 loss: 0.0005577038391493261
batch 180 loss: 0.000557686504907906
batch 185 loss: 0.0005578345968388021
batch 190 loss: 0.0005576628725975751
batch 195 loss: 0.0005576410214416683
batch 200 loss: 0.0005577335832640529
batch 205 loss: 0.0005578063544817268
batch 210 loss: 0.0005577290197834372
batch 215 loss: 0.0005576450494118035
batch 220 loss: 0.0005576925934292376
batch 225 loss: 0.000557619531173259
batch 230 loss: 0.0005577600793913006
batch 235 loss: 0.0005577022908255458
batch 240 loss: 0.0005576866329647601
Training Loss: 0.0005576834456102613
Validation Loss: 0.0005576934025157243
Epoch 56:
batch 5 loss: 0.0005574967595748604
batch 10 loss: 0.0005577056086622179
batch 15 loss: 0.0005575669463723898
batch 20 loss: 0.0005575546296313405
batch 25 loss: 0.0005577554577030242
batch 30 loss: 0.0005577482865191996
batch 35 loss: 0.0005577282048761844
batch 40 loss: 0.0005576967727392912
batch 45 loss: 0.0005576847121119499
batch 50 loss: 0.0005577756674028933
batch 55 loss: 0.0005577031755819917
batch 60 loss: 0.0005576357012614608
batch 65 loss: 0.0005577309755608439
batch 70 loss: 0.0005577030940912664
batch 75 loss: 0.0005577058997005224
batch 80 loss: 0.0005576795781962573
batch 85 loss: 0.000557676237076521
batch 90 loss: 0.0005576498573645949
batch 95 loss: 0.0005576768657192588
batch 100 loss: 0.0005576725583523512
batch 105 loss: 0.000557639414910227
batch 110 loss: 0.0005576842930167913
batch 115 loss: 0.0005577357369475067
batch 120 loss: 0.0005576477153226734
batch 125 loss: 0.0005577669595368207
batch 130 loss: 0.0005577873904258013
batch 135 loss: 0.0005576009047217667
batch 140 loss: 0.0005575810791924596
batch 145 loss: 0.0005576374474912882
batch 150 loss: 0.0005576284369453788
batch 155 loss: 0.0005576406023465097
batch 160 loss: 0.000557688029948622
batch 165 loss: 0.0005577077041380108
batch 170 loss: 0.0005577043979428708
batch 175 loss: 0.0005576745956204831
batch 180 loss: 0.0005576737923547626
batch 185 loss: 0.0005576323135755956
batch 190 loss: 0.0005577287054620683
batch 195 loss: 0.0005576388677582145
batch 200 loss: 0.0005576388095505536
batch 205 loss: 0.000557782978285104
batch 210 loss: 0.0005577560747042299
batch 215 loss: 0.0005577802192419767
batch 220 loss: 0.0005576533498242497
batch 225 loss: 0.0005577690782956779
batch 230 loss: 0.0005576382041908801
batch 235 loss: 0.0005577250616624951
batch 240 loss: 0.0005577185191214084
Training Loss: 0.0005576834931465177
Validation Loss: 0.0005576933928144475
Epoch 57:
batch 5 loss: 0.0005577132804319262
batch 10 loss: 0.0005577614880166948
batch 15 loss: 0.0005577043746598064
batch 20 loss: 0.000557642662897706
batch 25 loss: 0.0005575755727477372
batch 30 loss: 0.0005577110685408115
batch 35 loss: 0.0005576183903031051
batch 40 loss: 0.0005576995434239506
batch 45 loss: 0.0005576908471994102
batch 50 loss: 0.000557668274268508
batch 55 loss: 0.0005577368894591928
batch 60 loss: 0.000557712756562978
batch 65 loss: 0.0005577379139140249
batch 70 loss: 0.0005576889961957932
batch 75 loss: 0.000557664129883051
batch 80 loss: 0.0005576370051130653
batch 85 loss: 0.0005577494041062892
batch 90 loss: 0.000557666877284646
batch 95 loss: 0.000557649543043226
batch 100 loss: 0.000557690323330462
batch 105 loss: 0.0005576941650360823
batch 110 loss: 0.00055769911268726
batch 115 loss: 0.0005576876923441887
batch 120 loss: 0.0005577104398980737
batch 125 loss: 0.0005576746189035475
batch 130 loss: 0.0005576499970629811
batch 135 loss: 0.0005576496245339513
batch 140 loss: 0.0005575897172093392
batch 145 loss: 0.0005577379721216858
batch 150 loss: 0.0005576949100941419
batch 155 loss: 0.0005577431642450392
batch 160 loss: 0.0005576672498136759
batch 165 loss: 0.0005576460855081678
batch 170 loss: 0.0005576381110586226
batch 175 loss: 0.0005577185424044728
batch 180 loss: 0.0005577240372076631
batch 185 loss: 0.0005576303112320602
batch 190 loss: 0.0005577289382927119
batch 195 loss: 0.0005576383206062019
batch 200 loss: 0.0005575964576564729
batch 205 loss: 0.0005577925359830261
batch 210 loss: 0.0005577617208473385
batch 215 loss: 0.0005577403586357832
batch 220 loss: 0.0005576674360781908
batch 225 loss: 0.0005576331983320415
batch 230 loss: 0.0005576154449954629
batch 235 loss: 0.0005577123141847551
batch 240 loss: 0.0005576436291448772
Training Loss: 0.0005576834468229208
Validation Loss: 0.0005576934500519808
Epoch 58:
batch 5 loss: 0.000557688158005476
batch 10 loss: 0.0005577231990173459
batch 15 loss: 0.0005577253410592675
batch 20 loss: 0.0005577355157583952
batch 25 loss: 0.0005576384835876524
batch 30 loss: 0.0005575487506575882
batch 35 loss: 0.0005576894269324839
batch 40 loss: 0.0005576800554990769
batch 45 loss: 0.0005576170864515006
batch 50 loss: 0.000557708297856152
batch 55 loss: 0.0005575990653596818
batch 60 loss: 0.0005577235599048436
batch 65 loss: 0.0005577407428063452
batch 70 loss: 0.0005577157135121524
batch 75 loss: 0.0005576579016633332
batch 80 loss: 0.0005576532916165888
batch 85 loss: 0.0005576892057433724
batch 90 loss: 0.0005577099393121899
batch 95 loss: 0.000557666108943522
batch 100 loss: 0.0005577878211624921
batch 105 loss: 0.0005577935720793903
batch 110 loss: 0.0005577022326178849
batch 115 loss: 0.0005576241645030677
batch 120 loss: 0.0005575571674853563
batch 125 loss: 0.0005576638272032141
batch 130 loss: 0.0005576415569521487
batch 135 loss: 0.0005576933501288295
batch 140 loss: 0.0005577427800744772
batch 145 loss: 0.0005577345611527563
batch 150 loss: 0.000557796738576144
batch 155 loss: 0.0005577066564001143
batch 160 loss: 0.0005577151430770754
batch 165 loss: 0.0005577487521804869
batch 170 loss: 0.0005576187279075385
batch 175 loss: 0.0005576625000685454
batch 180 loss: 0.0005576813593506813
batch 185 loss: 0.0005575854098424316
batch 190 loss: 0.0005576279829256237
batch 195 loss: 0.0005576668540015816
batch 200 loss: 0.0005577699397690594
batch 205 loss: 0.0005577218369580805
batch 210 loss: 0.000557701603975147
batch 215 loss: 0.0005576631054282188
batch 220 loss: 0.0005576515453867614
batch 225 loss: 0.0005576343741267919
batch 230 loss: 0.00055766407167539
batch 235 loss: 0.0005576290073804558
batch 240 loss: 0.0005577101255767047
Training Loss: 0.0005576834710761129
Validation Loss: 0.0005576933957248306
Epoch 59:
batch 5 loss: 0.0005575594725087285
batch 10 loss: 0.0005576958181336522
batch 15 loss: 0.0005577145726419986
batch 20 loss: 0.0005577928968705237
batch 25 loss: 0.000557710521388799
batch 30 loss: 0.0005577617324888706
batch 35 loss: 0.0005576448980718851
batch 40 loss: 0.0005576472845859826
batch 45 loss: 0.0005576689727604389
batch 50 loss: 0.0005577848642133176
batch 55 loss: 0.0005575686926022172
batch 60 loss: 0.0005576500669121742
batch 65 loss: 0.000557663245126605
batch 70 loss: 0.0005576099618338048
batch 75 loss: 0.0005576792056672276
batch 80 loss: 0.0005576970055699348
batch 85 loss: 0.0005576374707743526
batch 90 loss: 0.000557664898224175
batch 95 loss: 0.0005576908821240068
batch 100 loss: 0.000557586771901697
batch 105 loss: 0.0005576900206506252
batch 110 loss: 0.0005577264702878893
batch 115 loss: 0.000557607167866081
batch 120 loss: 0.0005576525582000613
batch 125 loss: 0.0005576476221904158
batch 130 loss: 0.000557741685770452
batch 135 loss: 0.0005576504161581397
batch 140 loss: 0.0005576957599259913
batch 145 loss: 0.0005576689261943101
batch 150 loss: 0.00055774359498173
batch 155 loss: 0.0005576068069785833
batch 160 loss: 0.0005577218718826771
batch 165 loss: 0.0005577336065471173
batch 170 loss: 0.0005577488336712122
batch 175 loss: 0.0005577731295488775
batch 180 loss: 0.0005576751427724957
batch 185 loss: 0.000557596911676228
batch 190 loss: 0.0005577248753979802
batch 195 loss: 0.0005577738978900015
batch 200 loss: 0.0005576802766881883
batch 205 loss: 0.0005576646653935313
batch 210 loss: 0.0005577505333349108
batch 215 loss: 0.0005577194271609188
batch 220 loss: 0.0005577338044531644
batch 225 loss: 0.0005576976342126727
batch 230 loss: 0.0005576935713179409
batch 235 loss: 0.0005576415685936808
batch 240 loss: 0.00055761500261724
Training Loss: 0.0005576834378492397
Validation Loss: 0.0005576933996053413
Epoch 60:
batch 5 loss: 0.0005576838855631649
batch 10 loss: 0.0005576651426963508
batch 15 loss: 0.0005578072043135763
batch 20 loss: 0.0005577445030212402
batch 25 loss: 0.0005576700787059963
batch 30 loss: 0.000557738111820072
batch 35 loss: 0.000557569379452616
batch 40 loss: 0.0005576551426202059
batch 45 loss: 0.0005576492752879858
batch 50 loss: 0.0005577683215960861
batch 55 loss: 0.0005576996016316115
batch 60 loss: 0.0005576103576458991
batch 65 loss: 0.0005576723255217075
batch 70 loss: 0.0005576339201070369
batch 75 loss: 0.0005576117080636322
batch 80 loss: 0.0005577206262387336
batch 85 loss: 0.0005576636758632958
batch 90 loss: 0.0005577960517257452
batch 95 loss: 0.000557705492246896
batch 100 loss: 0.0005576681345701217
batch 105 loss: 0.0005576558411121369
batch 110 loss: 0.0005576495546847582
batch 115 loss: 0.0005577439675107599
batch 120 loss: 0.0005576910800300539
batch 125 loss: 0.0005575884715653956
batch 130 loss: 0.0005576729425229132
batch 135 loss: 0.000557700905483216
batch 140 loss: 0.000557685096282512
batch 145 loss: 0.0005576719297096134
batch 150 loss: 0.0005577770760282874
batch 155 loss: 0.0005578004289418459
batch 160 loss: 0.0005576523602940142
batch 165 loss: 0.0005576450959779323
batch 170 loss: 0.0005576660623773932
batch 175 loss: 0.0005576447234489024
batch 180 loss: 0.0005576469004154206
batch 185 loss: 0.000557782722171396
batch 190 loss: 0.0005576775292865932
batch 195 loss: 0.000557667831890285
batch 200 loss: 0.0005577677628025412
batch 205 loss: 0.0005576451658271253
batch 210 loss: 0.0005576641531661152
batch 215 loss: 0.0005576110095717012
batch 220 loss: 0.0005576933384872973
batch 225 loss: 0.0005576486233621836
batch 230 loss: 0.0005577782518230379
batch 235 loss: 0.0005576415685936808
batch 240 loss: 0.0005576020572334528
Training Loss: 0.0005576834456102613
Validation Loss: 0.0005576935451244936
Epoch 61:
batch 5 loss: 0.0005577743286266923
batch 10 loss: 0.0005576045252382756
batch 15 loss: 0.0005576664465479553
batch 20 loss: 0.000557691021822393
batch 25 loss: 0.0005576679133810103
batch 30 loss: 0.0005576275987550616
batch 35 loss: 0.0005576997878961265
batch 40 loss: 0.000557666621170938
batch 45 loss: 0.0005577268660999834
batch 50 loss: 0.0005576742812991142
batch 55 loss: 0.0005577397765591741
batch 60 loss: 0.0005577964708209038
batch 65 loss: 0.0005576958646997809
batch 70 loss: 0.0005575874354690313
batch 75 loss: 0.0005576052819378674
batch 80 loss: 0.0005577622330747544
batch 85 loss: 0.0005576467607170344
batch 90 loss: 0.0005577927571721375
batch 95 loss: 0.0005577461910434067
batch 100 loss: 0.000557718612253666
batch 105 loss: 0.0005576860043220222
batch 110 loss: 0.0005576923489570617
batch 115 loss: 0.00055779266403988
batch 120 loss: 0.0005575931863859296
batch 125 loss: 0.000557757040951401
batch 130 loss: 0.0005576565163210034
batch 135 loss: 0.0005577497300691903
batch 140 loss: 0.0005575594259425998
batch 145 loss: 0.0005576585070230066
batch 150 loss: 0.0005576908239163459
batch 155 loss: 0.0005576007068157196
batch 160 loss: 0.0005575953633524478
batch 165 loss: 0.0005577293457463383
batch 170 loss: 0.0005575987044721842
batch 175 loss: 0.0005577328149229289
batch 180 loss: 0.0005576365627348423
batch 185 loss: 0.0005576965981163084
batch 190 loss: 0.0005577812436968089
batch 195 loss: 0.0005577415926381945
batch 200 loss: 0.0005576795898377896
batch 205 loss: 0.0005576430703513324
batch 210 loss: 0.0005575952818617225
batch 215 loss: 0.0005577045492827892
batch 220 loss: 0.0005576989846304059
batch 225 loss: 0.000557608949020505
batch 230 loss: 0.0005576777970418334
batch 235 loss: 0.0005576805211603642
batch 240 loss: 0.0005576767958700657
Training Loss: 0.0005576834477930485
Validation Loss: 0.0005576935354232167
Epoch 62:
batch 5 loss: 0.0005577466683462263
batch 10 loss: 0.000557662162464112
batch 15 loss: 0.0005575826507993042
batch 20 loss: 0.0005577518371865153
batch 25 loss: 0.0005577043048106134
batch 30 loss: 0.0005576311028562486
batch 35 loss: 0.0005576495546847582
batch 40 loss: 0.0005576421855948866
batch 45 loss: 0.0005576694034971297
batch 50 loss: 0.0005577156436629594
batch 55 loss: 0.0005576408351771533
batch 60 loss: 0.0005577368894591928
batch 65 loss: 0.0005576964700594545
batch 70 loss: 0.0005576468422077597
batch 75 loss: 0.0005577569943852722
batch 80 loss: 0.000557680067140609
batch 85 loss: 0.00055771250044927
batch 90 loss: 0.0005576284951530397
batch 95 loss: 0.0005576974130235612
batch 100 loss: 0.000557636539451778
batch 105 loss: 0.0005577074945904315
batch 110 loss: 0.0005577511270530522
batch 115 loss: 0.0005577110568992794
batch 120 loss: 0.0005576765048317611
batch 125 loss: 0.0005576979601755738
batch 130 loss: 0.0005576350959017873
batch 135 loss: 0.0005576944560743869
batch 140 loss: 0.0005577080184593797
batch 145 loss: 0.0005577303818427026
batch 150 loss: 0.0005576999159529805
batch 155 loss: 0.0005576740717515349
batch 160 loss: 0.0005576303810812533
batch 165 loss: 0.0005577838164754212
batch 170 loss: 0.0005574985872954131
batch 175 loss: 0.0005576707073487342
batch 180 loss: 0.00055768535239622
batch 185 loss: 0.0005577068310230971
batch 190 loss: 0.0005576984607614577
batch 195 loss: 0.0005576525931246578
batch 200 loss: 0.000557697203475982
batch 205 loss: 0.0005577398580498994
batch 210 loss: 0.0005578463082201779
batch 215 loss: 0.0005577579373493791
batch 220 loss: 0.0005575101939029991
batch 225 loss: 0.0005576822091825307
batch 230 loss: 0.0005575994960963726
batch 235 loss: 0.0005576990195550024
batch 240 loss: 0.0005576719297096134
Training Loss: 0.0005576834485206442
Validation Loss: 0.0005576934529623637
Epoch 63:
batch 5 loss: 0.0005577106960117817
batch 10 loss: 0.0005576840718276799
batch 15 loss: 0.000557664327789098
batch 20 loss: 0.0005577235715463758
batch 25 loss: 0.0005575840827077627
batch 30 loss: 0.0005577333271503448
batch 35 loss: 0.0005576226860284806
batch 40 loss: 0.0005576733383350074
batch 45 loss: 0.0005576957599259913
batch 50 loss: 0.0005576295894570649
batch 55 loss: 0.0005576452473178506
batch 60 loss: 0.0005576953059062362
batch 65 loss: 0.0005576637922786176
batch 70 loss: 0.0005577051313593983
batch 75 loss: 0.0005577244679443538
batch 80 loss: 0.0005576582509092987
batch 85 loss: 0.000557659228798002
batch 90 loss: 0.0005577136878855526
batch 95 loss: 0.0005576756084337831
batch 100 loss: 0.0005576350260525942
batch 105 loss: 0.0005576364113949239
batch 110 loss: 0.0005576221970841289
batch 115 loss: 0.0005577148054726422
batch 120 loss: 0.0005577896488830447
batch 125 loss: 0.000557791325263679
batch 130 loss: 0.0005576463416218757
batch 135 loss: 0.0005575780523940921
batch 140 loss: 0.0005576547584496439
batch 145 loss: 0.0005576001014560461
batch 150 loss: 0.000557615514844656
batch 155 loss: 0.0005576732335612177
batch 160 loss: 0.0005577266565524042
batch 165 loss: 0.0005577319767326117
batch 170 loss: 0.0005576326395384968
batch 175 loss: 0.000557775842025876
batch 180 loss: 0.0005577465519309044
batch 185 loss: 0.0005576671450398862
batch 190 loss: 0.0005576633615419268
batch 195 loss: 0.0005576427094638348
batch 200 loss: 0.0005578108364716172
batch 205 loss: 0.0005577634321525693
batch 210 loss: 0.000557746016420424
batch 215 loss: 0.0005576423718594015
batch 220 loss: 0.0005577077739872038
batch 225 loss: 0.0005577369243837893
batch 230 loss: 0.0005576802766881883
batch 235 loss: 0.0005576553172431886
batch 240 loss: 0.0005576559808105231
Training Loss: 0.0005576834458527932
Validation Loss: 0.0005576934199780226
Epoch 64:
batch 5 loss: 0.0005577094270847738
batch 10 loss: 0.0005576857831329108
batch 15 loss: 0.0005577219999395311
batch 20 loss: 0.0005577104166150093
batch 25 loss: 0.0005577163537964225
batch 30 loss: 0.0005577118252404034
batch 35 loss: 0.0005576543277129531
batch 40 loss: 0.0005577854812145233
batch 45 loss: 0.0005577330477535725
batch 50 loss: 0.0005577402655035258
batch 55 loss: 0.0005577317555435002
batch 60 loss: 0.0005575980874709785
batch 65 loss: 0.0005576145718805492
batch 70 loss: 0.0005576685420237481
batch 75 loss: 0.0005576260853558778
batch 80 loss: 0.0005576831172220409
batch 85 loss: 0.0005577143398113549
batch 90 loss: 0.0005576153518632055
batch 95 loss: 0.000557702814694494
batch 100 loss: 0.0005576477735303343
batch 105 loss: 0.0005577040603384376
batch 110 loss: 0.0005577332340180874
batch 115 loss: 0.0005576187279075385
batch 120 loss: 0.0005576566327363253
batch 125 loss: 0.0005576640483923257
batch 130 loss: 0.0005576483206823468
batch 135 loss: 0.0005577375297434628
batch 140 loss: 0.0005576630006544292
batch 145 loss: 0.0005575826158747077
batch 150 loss: 0.0005576852359808982
batch 155 loss: 0.0005576441530138254
batch 160 loss: 0.0005577380885370076
batch 165 loss: 0.0005576630821451545
batch 170 loss: 0.000557585374917835
batch 175 loss: 0.0005575036047957838
batch 180 loss: 0.0005577625473961234
batch 185 loss: 0.0005576986004598439
batch 190 loss: 0.0005576429422944785
batch 195 loss: 0.0005576230818405747
batch 200 loss: 0.0005577132804319262
batch 205 loss: 0.0005577838863246142
batch 210 loss: 0.0005576536292210222
batch 215 loss: 0.0005576898111030459
batch 220 loss: 0.0005576997646130621
batch 225 loss: 0.000557720148935914
batch 230 loss: 0.0005577707896009087
batch 235 loss: 0.0005577349686063827
batch 240 loss: 0.0005577125470153987
Training Loss: 0.0005576834390618994
Validation Loss: 0.0005576934257987886
Epoch 65:
batch 5 loss: 0.0005576297757215798
batch 10 loss: 0.0005576937925070524
batch 15 loss: 0.0005576647468842566
batch 20 loss: 0.0005576624302193522
batch 25 loss: 0.0005576928262598813
batch 30 loss: 0.0005577671225182712
batch 35 loss: 0.0005575418123044074
batch 40 loss: 0.0005577930947765708
batch 45 loss: 0.0005578036187216639
batch 50 loss: 0.000557642278727144
batch 55 loss: 0.0005577621748670935
batch 60 loss: 0.000557732058223337
batch 65 loss: 0.0005576362600550056
batch 70 loss: 0.0005576007417403162
batch 75 loss: 0.0005576697993092238
batch 80 loss: 0.0005577077157795429
batch 85 loss: 0.0005576860858127475
batch 90 loss: 0.0005577448755502701
batch 95 loss: 0.0005577015108428895
batch 100 loss: 0.0005576008232310414
batch 105 loss: 0.0005576303345151245
batch 110 loss: 0.0005577292642556131
batch 115 loss: 0.0005576127325184644
batch 120 loss: 0.0005576470983214676
batch 125 loss: 0.0005575854564085602
batch 130 loss: 0.0005577370990067721
batch 135 loss: 0.0005577342584729194
batch 140 loss: 0.0005576539784669876
batch 145 loss: 0.0005577710806392133
batch 150 loss: 0.0005576821276918054
batch 155 loss: 0.0005577375763095915
batch 160 loss: 0.0005577726988121867
batch 165 loss: 0.0005576384835876524
batch 170 loss: 0.0005576097057200969
batch 175 loss: 0.0005577453528530895
batch 180 loss: 0.0005577531992457807
batch 185 loss: 0.0005576515803113579
batch 190 loss: 0.0005577069125138224
batch 195 loss: 0.0005576496128924191
batch 200 loss: 0.0005577722797170282
batch 205 loss: 0.0005576092284172773
batch 210 loss: 0.000557703513186425
batch 215 loss: 0.0005575914517976343
batch 220 loss: 0.0005577052826993168
batch 225 loss: 0.000557719764765352
batch 230 loss: 0.0005576640134677291
batch 235 loss: 0.0005577130825258792
batch 240 loss: 0.0005575435352511704
Training Loss: 0.000557683421842133
Validation Loss: 0.0005576934587831298
Epoch 66:
batch 5 loss: 0.0005577392061240971
batch 10 loss: 0.000557677389588207
batch 15 loss: 0.0005576942930929363
batch 20 loss: 0.0005576329422183335
batch 25 loss: 0.000557708484120667
batch 30 loss: 0.0005577896605245769
batch 35 loss: 0.0005577157717198133
batch 40 loss: 0.0005577730480581522
batch 45 loss: 0.0005577821983024478
batch 50 loss: 0.0005577319418080152
batch 55 loss: 0.0005577965755946934
batch 60 loss: 0.0005575706134550273
batch 65 loss: 0.000557650183327496
batch 70 loss: 0.0005576997413299978
batch 75 loss: 0.0005575905204750597
batch 80 loss: 0.0005576814990490675
batch 85 loss: 0.0005577155039645732
batch 90 loss: 0.0005576869240030647
batch 95 loss: 0.0005576503812335431
batch 100 loss: 0.000557708996348083
batch 105 loss: 0.0005576633731834591
batch 110 loss: 0.0005577258649282158
batch 115 loss: 0.0005575154209509492
batch 120 loss: 0.0005576682509854436
batch 125 loss: 0.0005577027564868331
batch 130 loss: 0.0005576597410254181
batch 135 loss: 0.0005577169009484351
batch 140 loss: 0.0005576751544140279
batch 145 loss: 0.0005576065159402788
batch 150 loss: 0.000557652662973851
batch 155 loss: 0.0005577312782406807
batch 160 loss: 0.0005575987277552485
batch 165 loss: 0.0005576432915404439
batch 170 loss: 0.0005576381343416869
batch 175 loss: 0.0005577482748776674
batch 180 loss: 0.0005576796480454505
batch 185 loss: 0.0005577284027822316
batch 190 loss: 0.0005576796946115791
batch 195 loss: 0.0005575798102654516
batch 200 loss: 0.0005576490075327456
batch 205 loss: 0.0005576821626164019
batch 210 loss: 0.0005576027557253838
batch 215 loss: 0.0005577217089012265
batch 220 loss: 0.000557630171533674
batch 225 loss: 0.0005577608244493604
batch 230 loss: 0.0005577348405495286
batch 235 loss: 0.0005576788331381977
batch 240 loss: 0.0005577347707003355
Training Loss: 0.0005576834344537928
Validation Loss: 0.0005576934500519808
Epoch 67:
batch 5 loss: 0.0005576684838160872
batch 10 loss: 0.0005576231982558965
batch 15 loss: 0.0005577936884947121
batch 20 loss: 0.0005576838157139719
batch 25 loss: 0.0005576987285166978
batch 30 loss: 0.0005576721392571926
batch 35 loss: 0.0005576324881985783
batch 40 loss: 0.0005578097072429955
batch 45 loss: 0.0005576290073804558
batch 50 loss: 0.0005576315452344716
batch 55 loss: 0.0005576039082370698
batch 60 loss: 0.0005577763309702277
batch 65 loss: 0.0005576890776865185
batch 70 loss: 0.0005576460738666355
batch 75 loss: 0.0005576068069785833
batch 80 loss: 0.0005575505318120122
batch 85 loss: 0.0005577517673373223
batch 90 loss: 0.0005577404866926372
batch 95 loss: 0.0005577445845119655
batch 100 loss: 0.0005576216033659875
batch 105 loss: 0.0005577603704296052
batch 110 loss: 0.0005577534437179565
batch 115 loss: 0.0005576782510615885
batch 120 loss: 0.0005576424882747233
batch 125 loss: 0.0005576193798333406
batch 130 loss: 0.0005576968309469521
batch 135 loss: 0.0005577248986810446
batch 140 loss: 0.000557700451463461
batch 145 loss: 0.0005576197057962417
batch 150 loss: 0.0005577095435000956
batch 155 loss: 0.0005575855262577534
batch 160 loss: 0.0005577482515946031
batch 165 loss: 0.000557667831890285
batch 170 loss: 0.0005577081115916372
batch 175 loss: 0.0005576996947638691
batch 180 loss: 0.0005576676689088344
batch 185 loss: 0.0005577550618909299
batch 190 loss: 0.0005575819988735021
batch 195 loss: 0.0005576958414167166
batch 200 loss: 0.0005577620468102396
batch 205 loss: 0.0005576821160502732
batch 210 loss: 0.0005576662020757795
batch 215 loss: 0.0005577648407779634
batch 220 loss: 0.0005576685187406838
batch 225 loss: 0.0005577286705374717
batch 230 loss: 0.0005576711380854249
batch 235 loss: 0.0005576320807449519
batch 240 loss: 0.000557640555780381
Training Loss: 0.0005576834477930485
Validation Loss: 0.0005576934820661942
Epoch 68:
batch 5 loss: 0.0005577117903158068
batch 10 loss: 0.0005576722556725145
batch 15 loss: 0.0005576743395067751
batch 20 loss: 0.0005576366325840354
batch 25 loss: 0.000557750987354666
batch 30 loss: 0.0005577198229730129
batch 35 loss: 0.0005576325813308359
batch 40 loss: 0.0005577430012635887
batch 45 loss: 0.0005576010793447494
batch 50 loss: 0.000557741487864405
batch 55 loss: 0.0005576642579399049
batch 60 loss: 0.0005577396834269166
batch 65 loss: 0.000557720148935914
batch 70 loss: 0.000557633547578007
batch 75 loss: 0.0005576807772740722
batch 80 loss: 0.0005576728959567845
batch 85 loss: 0.000557762524113059
batch 90 loss: 0.0005576836178079247
batch 95 loss: 0.0005577570642344654
batch 100 loss: 0.0005576232215389609
batch 105 loss: 0.0005576930358074606
batch 110 loss: 0.0005577347474172711
batch 115 loss: 0.0005576897645369172
batch 120 loss: 0.0005577232339419424
batch 125 loss: 0.0005575798917561769
batch 130 loss: 0.000557603978086263
batch 135 loss: 0.0005577187403105199
batch 140 loss: 0.0005578268901444972
batch 145 loss: 0.0005577277042903006
batch 150 loss: 0.000557711417786777
batch 155 loss: 0.000557757169008255
batch 160 loss: 0.0005576168419793248
batch 165 loss: 0.0005576346302405
batch 170 loss: 0.0005577341769821941
batch 175 loss: 0.0005575944320298732
batch 180 loss: 0.0005576938739977777
batch 185 loss: 0.0005577050615102053
batch 190 loss: 0.0005576972616836429
batch 195 loss: 0.0005576261202804745
batch 200 loss: 0.0005577122210524976
batch 205 loss: 0.0005575483664870262
batch 210 loss: 0.0005576154449954629
batch 215 loss: 0.0005577013944275677
batch 220 loss: 0.0005576738156378269
batch 225 loss: 0.0005576529656536877
batch 230 loss: 0.0005576269002631306
batch 235 loss: 0.00055766177829355
batch 240 loss: 0.0005577212199568748
Training Loss: 0.0005576834332411333
Validation Loss: 0.000557693491767471
Epoch 69:
batch 5 loss: 0.0005575935589149595
batch 10 loss: 0.0005576226743869483
batch 15 loss: 0.0005576718598604202
batch 20 loss: 0.0005575823714025318
batch 25 loss: 0.0005577117670327425
batch 30 loss: 0.0005577225820161402
batch 35 loss: 0.0005578303942456841
batch 40 loss: 0.0005576389143243432
batch 45 loss: 0.0005575661081820727
batch 50 loss: 0.0005576350959017873
batch 55 loss: 0.000557666237000376
batch 60 loss: 0.0005577273084782064
batch 65 loss: 0.00055766764562577
batch 70 loss: 0.0005576075986027717
batch 75 loss: 0.0005577819189056754
batch 80 loss: 0.0005576548050157726
batch 85 loss: 0.0005576801719143987
batch 90 loss: 0.0005577147589065134
batch 95 loss: 0.0005576391005888582
batch 100 loss: 0.0005577627569437027
batch 105 loss: 0.0005577274248935282
batch 110 loss: 0.0005577200790867209
batch 115 loss: 0.000557744677644223
batch 120 loss: 0.0005577368312515319
batch 125 loss: 0.0005576131166890264
batch 130 loss: 0.0005576601484790445
batch 135 loss: 0.0005577100673690438
batch 140 loss: 0.0005576238152571022
batch 145 loss: 0.0005577003816142678
batch 150 loss: 0.0005577061092481017
batch 155 loss: 0.0005576508119702339
batch 160 loss: 0.0005576408118940889
batch 165 loss: 0.0005576947471126914
batch 170 loss: 0.0005576313007622958
batch 175 loss: 0.0005578063428401947
batch 180 loss: 0.0005577614530920983
batch 185 loss: 0.0005576360621489584
batch 190 loss: 0.0005577594158239663
batch 195 loss: 0.0005576502764597536
batch 200 loss: 0.0005577642819844187
batch 205 loss: 0.0005576500087045133
batch 210 loss: 0.0005577058414928615
batch 215 loss: 0.0005576080176979304
batch 220 loss: 0.0005576857132837176
batch 225 loss: 0.0005576768890023232
batch 230 loss: 0.000557665468659252
batch 235 loss: 0.0005577360861934721
batch 240 loss: 0.0005576617550104856
Training Loss: 0.0005576834492482401
Validation Loss: 0.0005576936149736866
Epoch 70:
batch 5 loss: 0.0005576954921707511
batch 10 loss: 0.0005576557596214116
batch 15 loss: 0.0005577139090746641
batch 20 loss: 0.0005577471689321101
batch 25 loss: 0.0005576397641561925
batch 30 loss: 0.0005575610673986375
batch 35 loss: 0.0005577958305366338
batch 40 loss: 0.0005576778319664299
batch 45 loss: 0.000557628576643765
batch 50 loss: 0.000557629147078842
batch 55 loss: 0.0005577532574534416
batch 60 loss: 0.0005576849798671901
batch 65 loss: 0.0005576623138040304
batch 70 loss: 0.0005576063995249569
batch 75 loss: 0.000557652220595628
batch 80 loss: 0.0005578535725362599
batch 85 loss: 0.0005577793694101274
batch 90 loss: 0.0005577655741944909
batch 95 loss: 0.0005577192758210003
batch 100 loss: 0.0005577059579081834
batch 105 loss: 0.0005576339899562299
batch 110 loss: 0.0005576948868110776
batch 115 loss: 0.0005576489493250847
batch 120 loss: 0.000557667191606015
batch 125 loss: 0.0005577260395511985
batch 130 loss: 0.0005576960393227637
batch 135 loss: 0.0005576318013481796
batch 140 loss: 0.0005576612311415374
batch 145 loss: 0.0005576783325523138
batch 150 loss: 0.0005576955620199442
batch 155 loss: 0.0005576426861807704
batch 160 loss: 0.0005576684954576195
batch 165 loss: 0.0005577580188401044
batch 170 loss: 0.0005577370175160468
batch 175 loss: 0.0005577070987783372
batch 180 loss: 0.000557579449377954
batch 185 loss: 0.0005577107076533139
batch 190 loss: 0.0005576168303377926
batch 195 loss: 0.00055767975281924
batch 200 loss: 0.0005576538853347302
batch 205 loss: 0.000557621184270829
batch 210 loss: 0.0005577573203481734
batch 215 loss: 0.0005576442461460828
batch 220 loss: 0.000557825225405395
batch 225 loss: 0.0005575892981141805
batch 230 loss: 0.0005577403353527188
batch 235 loss: 0.0005576494615525008
batch 240 loss: 0.0005575620802119374
Training Loss: 0.0005576834288755587
Validation Loss: 0.0005576934607233852
Epoch 71:
batch 5 loss: 0.0005577657837420702
batch 10 loss: 0.0005576999392360449
batch 15 loss: 0.000557597994338721
batch 20 loss: 0.0005576828494668007
batch 25 loss: 0.0005576192284934223
batch 30 loss: 0.0005578130367211998
batch 35 loss: 0.0005577625823207199
batch 40 loss: 0.0005576567258685827
batch 45 loss: 0.0005576511728577315
batch 50 loss: 0.0005576492752879858
batch 55 loss: 0.0005576051888056099
batch 60 loss: 0.0005576503812335431
batch 65 loss: 0.0005578213022090495
batch 70 loss: 0.0005576588679105044
batch 75 loss: 0.0005576361669227481
batch 80 loss: 0.0005576594034209847
batch 85 loss: 0.0005577171570621431
batch 90 loss: 0.0005577482050284743
batch 95 loss: 0.0005576397874392569
batch 100 loss: 0.0005577110685408115
batch 105 loss: 0.000557651137933135
batch 110 loss: 0.0005576702649705112
batch 115 loss: 0.000557735119946301
batch 120 loss: 0.0005578064825385809
batch 125 loss: 0.0005577083793468773
batch 130 loss: 0.0005576615338213741
batch 135 loss: 0.0005577336065471173
batch 140 loss: 0.0005576105322688818
batch 145 loss: 0.0005576743395067751
batch 150 loss: 0.0005576505791395903
batch 155 loss: 0.0005577392876148224
batch 160 loss: 0.000557717471383512
batch 165 loss: 0.0005576196708716452
batch 170 loss: 0.0005576233961619437
batch 175 loss: 0.0005578118958510459
batch 180 loss: 0.0005576817435212434
batch 185 loss: 0.0005577389150857925
batch 190 loss: 0.0005577259929850697
batch 195 loss: 0.0005576220457442105
batch 200 loss: 0.0005575855495408178
batch 205 loss: 0.0005576794035732746
batch 210 loss: 0.0005575784365646541
batch 215 loss: 0.0005576541763730348
batch 220 loss: 0.0005577271454967559
batch 225 loss: 0.0005576038616709411
batch 230 loss: 0.0005576323834247887
batch 235 loss: 0.0005577678675763309
batch 240 loss: 0.0005576462368480861
Training Loss: 0.0005576834077752816
Validation Loss: 0.0005576934180377672
Epoch 72:
batch 5 loss: 0.000557700451463461
batch 10 loss: 0.0005577475181780756
batch 15 loss: 0.000557649543043226
batch 20 loss: 0.0005576329655013979
batch 25 loss: 0.0005577412317506969
batch 30 loss: 0.0005577485542744398
batch 35 loss: 0.0005576270632445812
batch 40 loss: 0.0005578571697697043
batch 45 loss: 0.0005574815673753619
batch 50 loss: 0.0005577111151069403
batch 55 loss: 0.000557623291388154
batch 60 loss: 0.0005576732335612177
batch 65 loss: 0.0005577837349846959
batch 70 loss: 0.000557624944485724
batch 75 loss: 0.0005577788688242435
batch 80 loss: 0.000557685736566782
batch 85 loss: 0.0005576481693424284
batch 90 loss: 0.000557627179659903
batch 95 loss: 0.0005577626405283808
batch 100 loss: 0.0005577356903813779
batch 105 loss: 0.0005576791358180344
batch 110 loss: 0.0005575488787144423
batch 115 loss: 0.0005577246542088687
batch 120 loss: 0.0005576205439865589
batch 125 loss: 0.0005577140487730503
batch 130 loss: 0.0005578096723183989
batch 135 loss: 0.0005576613009907305
batch 140 loss: 0.000557693059090525
batch 145 loss: 0.0005576234892942011
batch 150 loss: 0.0005576474941335618
batch 155 loss: 0.0005576644558459521
batch 160 loss: 0.0005577895091846586
batch 165 loss: 0.0005576544674113393
batch 170 loss: 0.0005576448747888207
batch 175 loss: 0.00055769745958969
batch 180 loss: 0.0005577163072302937
batch 185 loss: 0.0005577271454967559
batch 190 loss: 0.0005576647352427244
batch 195 loss: 0.000557640369515866
batch 200 loss: 0.0005576136172749102
batch 205 loss: 0.0005576275289058685
batch 210 loss: 0.0005576587980613112
batch 215 loss: 0.0005577284493483603
batch 220 loss: 0.0005576984025537967
batch 225 loss: 0.0005576020572334528
batch 230 loss: 0.0005577112082391977
batch 235 loss: 0.000557734549511224
batch 240 loss: 0.0005576667841523886
Training Loss: 0.000557683409715537
Validation Loss: 0.0005576933976650859
Epoch 73:
batch 5 loss: 0.0005575914052315057
batch 10 loss: 0.0005576520576141775
batch 15 loss: 0.0005577667034231126
batch 20 loss: 0.0005577433854341507
batch 25 loss: 0.0005576973082497716
batch 30 loss: 0.000557755772024393
batch 35 loss: 0.0005577110219746828
batch 40 loss: 0.0005576399969868362
batch 45 loss: 0.0005576640949584543
batch 50 loss: 0.0005576993571594357
batch 55 loss: 0.0005577361793257296
batch 60 loss: 0.0005575961899012327
batch 65 loss: 0.0005576933152042329
batch 70 loss: 0.0005577604286372661
batch 75 loss: 0.0005578233394771814
batch 80 loss: 0.0005576607887633145
batch 85 loss: 0.0005576681229285896
batch 90 loss: 0.0005576954106800258
batch 95 loss: 0.0005576670169830322
batch 100 loss: 0.0005577721283771097
batch 105 loss: 0.0005577073781751097
batch 110 loss: 0.0005577069125138224
batch 115 loss: 0.0005577173084020615
batch 120 loss: 0.0005576132098212838
batch 125 loss: 0.0005575564107857644
batch 130 loss: 0.0005575709510594606
batch 135 loss: 0.0005577040603384376
batch 140 loss: 0.0005576005089096725
batch 145 loss: 0.0005575768067501485
batch 150 loss: 0.0005576290423050523
batch 155 loss: 0.0005576933268457651
batch 160 loss: 0.0005577021860517561
batch 165 loss: 0.0005577158182859421
batch 170 loss: 0.0005577319767326117
batch 175 loss: 0.0005578018841333688
batch 180 loss: 0.0005577487056143582
batch 185 loss: 0.0005576367606408894
batch 190 loss: 0.0005575915682129561
batch 195 loss: 0.0005577427102252841
batch 200 loss: 0.0005577402771450579
batch 205 loss: 0.0005576855852268636
batch 210 loss: 0.0005576351075433194
batch 215 loss: 0.0005576637922786176
batch 220 loss: 0.0005576098919846117
batch 225 loss: 0.0005577669362537563
batch 230 loss: 0.0005576439667493105
batch 235 loss: 0.0005577382980845869
batch 240 loss: 0.0005575783201493323
Training Loss: 0.0005576834109281965
Validation Loss: 0.0005576934151273841
Epoch 74:
batch 5 loss: 0.000557609146926552
batch 10 loss: 0.0005576638388447464
batch 15 loss: 0.0005577287753112614
batch 20 loss: 0.0005577536532655359
batch 25 loss: 0.0005578242358751595
batch 30 loss: 0.0005577114294283092
batch 35 loss: 0.0005577123607508838
batch 40 loss: 0.0005577054573222995
batch 45 loss: 0.000557644828222692
batch 50 loss: 0.0005575821036472916
batch 55 loss: 0.0005578094627708196
batch 60 loss: 0.0005578100332058966
batch 65 loss: 0.0005576583207584918
batch 70 loss: 0.0005576999275945127
batch 75 loss: 0.0005577258532866835
batch 80 loss: 0.0005576908239163459
batch 85 loss: 0.0005577928968705237
batch 90 loss: 0.0005577432457357645
batch 95 loss: 0.0005576968775130809
batch 100 loss: 0.0005576118011958898
batch 105 loss: 0.0005576964584179222
batch 110 loss: 0.0005576710449531674
batch 115 loss: 0.0005577637115493417
batch 120 loss: 0.0005576056777499616
batch 125 loss: 0.0005577084608376026
batch 130 loss: 0.0005576369585469365
batch 135 loss: 0.0005576629657298327
batch 140 loss: 0.0005576252122409641
batch 145 loss: 0.0005575604969635606
batch 150 loss: 0.000557764817494899
batch 155 loss: 0.0005576170748099685
batch 160 loss: 0.0005576028372161091
batch 165 loss: 0.0005576444556936622
batch 170 loss: 0.0005576629657298327
batch 175 loss: 0.0005576237570494413
batch 180 loss: 0.0005576463881880045
batch 185 loss: 0.0005576969007961452
batch 190 loss: 0.0005576029070653021
batch 195 loss: 0.000557641068007797
batch 200 loss: 0.0005577190313488245
batch 205 loss: 0.0005575562827289104
batch 210 loss: 0.0005577141186222434
batch 215 loss: 0.0005575895891524851
batch 220 loss: 0.0005577159463427961
batch 225 loss: 0.0005577062722295523
batch 230 loss: 0.0005577154108323157
batch 235 loss: 0.0005578039330430329
batch 240 loss: 0.0005576741765253246
Training Loss: 0.0005576834165064308
Validation Loss: 0.0005576934102767458
Epoch 75:
batch 5 loss: 0.0005575804389081895
batch 10 loss: 0.0005576575873419643
batch 15 loss: 0.0005576570285484195
batch 20 loss: 0.0005576894385740161
batch 25 loss: 0.0005576282041147351
batch 30 loss: 0.0005576356081292034
batch 35 loss: 0.0005577055620960891
batch 40 loss: 0.0005576891941018403
batch 45 loss: 0.0005576992291025818
batch 50 loss: 0.0005576752475462854
batch 55 loss: 0.0005576627212576568
batch 60 loss: 0.0005577241536229849
batch 65 loss: 0.000557799031957984
batch 70 loss: 0.0005575761431828141
batch 75 loss: 0.0005577676347456872
batch 80 loss: 0.0005576433148235082
batch 85 loss: 0.0005576112074777484
batch 90 loss: 0.00055769745958969
batch 95 loss: 0.0005576583906076848
batch 100 loss: 0.0005576654220931232
batch 105 loss: 0.0005576516268774867
batch 110 loss: 0.0005576612777076662
batch 115 loss: 0.0005576836410909891
batch 120 loss: 0.0005576177965849638
batch 125 loss: 0.0005577066680416465
batch 130 loss: 0.000557729322463274
batch 135 loss: 0.0005578004056587815
batch 140 loss: 0.0005577014409936965
batch 145 loss: 0.0005577648873440922
batch 150 loss: 0.0005577117786742747
batch 155 loss: 0.0005577566218562425
batch 160 loss: 0.0005577967269346118
batch 165 loss: 0.00055764903081581
batch 170 loss: 0.00055769911268726
batch 175 loss: 0.0005577165167778731
batch 180 loss: 0.0005576651310548186
batch 185 loss: 0.0005576358176767826
batch 190 loss: 0.0005575821734964848
batch 195 loss: 0.0005576293799094856
batch 200 loss: 0.0005576750496402383
batch 205 loss: 0.0005577298114076256
batch 210 loss: 0.0005576023017056286
batch 215 loss: 0.0005577570293098689
batch 220 loss: 0.0005576413823291659
batch 225 loss: 0.000557704851962626
batch 230 loss: 0.0005576894385740161
batch 235 loss: 0.0005577739560976624
batch 240 loss: 0.0005576484021730721
Training Loss: 0.0005576834291180906
Validation Loss: 0.0005576934539324915
Epoch 76:
batch 5 loss: 0.0005576294497586787
batch 10 loss: 0.0005576507304795086
batch 15 loss: 0.0005576334428042173
batch 20 loss: 0.0005576943862251937
batch 25 loss: 0.0005577272735536098
batch 30 loss: 0.0005576577270403505
batch 35 loss: 0.0005577104282565414
batch 40 loss: 0.0005577201140113175
batch 45 loss: 0.0005576453753747046
batch 50 loss: 0.0005576530238613486
batch 55 loss: 0.0005577490665018559
batch 60 loss: 0.000557654513977468
batch 65 loss: 0.0005576901021413505
batch 70 loss: 0.0005577140487730503
batch 75 loss: 0.0005577300442382693
batch 80 loss: 0.0005577299511060118
batch 85 loss: 0.0005577356670983136
batch 90 loss: 0.0005576032679527998
batch 95 loss: 0.0005576333031058311
batch 100 loss: 0.0005575178540311754
batch 105 loss: 0.0005576373310759664
batch 110 loss: 0.0005576700204983354
batch 115 loss: 0.0005577833275310695
batch 120 loss: 0.0005577085423283279
batch 125 loss: 0.0005576404975727201
batch 130 loss: 0.0005577057483606041
batch 135 loss: 0.0005577009171247483
batch 140 loss: 0.0005577337928116321
batch 145 loss: 0.0005576741183176637
batch 150 loss: 0.0005577040603384376
batch 155 loss: 0.0005576886353082955
batch 160 loss: 0.0005578077514655888
batch 165 loss: 0.0005577138741500676
batch 170 loss: 0.0005577059928327799
batch 175 loss: 0.0005577183212153614
batch 180 loss: 0.0005576964351348579
batch 185 loss: 0.0005576792638748885
batch 190 loss: 0.0005576347117312253
batch 195 loss: 0.0005575636168941855
batch 200 loss: 0.0005577376578003168
batch 205 loss: 0.0005577140254899859
batch 210 loss: 0.0005576370866037905
batch 215 loss: 0.0005577265634201467
batch 220 loss: 0.0005576098337769508
batch 225 loss: 0.0005576649215072394
batch 230 loss: 0.0005576717667281628
batch 235 loss: 0.0005576638737693429
batch 240 loss: 0.0005577314528636635
Training Loss: 0.0005576834148087074
Validation Loss: 0.0005576933996053413
Epoch 77:
batch 5 loss: 0.0005578111973591149
batch 10 loss: 0.0005578096606768668
batch 15 loss: 0.0005577295203693211
batch 20 loss: 0.0005576257593929767
batch 25 loss: 0.0005576905678026378
batch 30 loss: 0.000557682930957526
batch 35 loss: 0.0005577247007749975
batch 40 loss: 0.0005577208357863128
batch 45 loss: 0.0005577693460509181
batch 50 loss: 0.0005576696596108377
batch 55 loss: 0.0005576261435635387
batch 60 loss: 0.0005577040836215019
batch 65 loss: 0.0005576569819822907
batch 70 loss: 0.0005576603696681559
batch 75 loss: 0.0005576731869950891
batch 80 loss: 0.0005575588205829263
batch 85 loss: 0.0005576979368925094
batch 90 loss: 0.0005577169940806925
batch 95 loss: 0.0005575651070103049
batch 100 loss: 0.0005576550611294806
batch 105 loss: 0.0005576841300353408
batch 110 loss: 0.0005577172385528683
batch 115 loss: 0.0005575706716626882
batch 120 loss: 0.0005577558884397149
batch 125 loss: 0.0005576425581239163
batch 130 loss: 0.0005577675183303654
batch 135 loss: 0.0005577438860200346
batch 140 loss: 0.0005577510106377304
batch 145 loss: 0.0005576126859523356
batch 150 loss: 0.0005576825584284961
batch 155 loss: 0.0005575992399826646
batch 160 loss: 0.0005576190655119717
batch 165 loss: 0.0005577475880272686
batch 170 loss: 0.0005577264353632927
batch 175 loss: 0.0005577278789132833
batch 180 loss: 0.0005575998919084668
batch 185 loss: 0.0005575603456236422
batch 190 loss: 0.0005576542695052922
batch 195 loss: 0.0005576258874498308
batch 200 loss: 0.0005576792289502918
batch 205 loss: 0.0005577102187089622
batch 210 loss: 0.000557727343402803
batch 215 loss: 0.0005577481351792812
batch 220 loss: 0.0005576703697443008
batch 225 loss: 0.0005576515453867614
batch 230 loss: 0.0005577887641265988
batch 235 loss: 0.0005576050956733525
batch 240 loss: 0.0005576849798671901
Training Loss: 0.0005576834019545156
Validation Loss: 0.0005576934151273841
Epoch 78:
batch 5 loss: 0.0005577210220508277
batch 10 loss: 0.0005576648632995784
batch 15 loss: 0.0005576904281042516
batch 20 loss: 0.0005577028845436871
batch 25 loss: 0.0005578138749115169
batch 30 loss: 0.0005576099851168692
batch 35 loss: 0.0005576895084232092
batch 40 loss: 0.0005576815223321318
batch 45 loss: 0.0005576202180236578
batch 50 loss: 0.0005576492985710502
batch 55 loss: 0.0005576891475357115
batch 60 loss: 0.0005576593102887273
batch 65 loss: 0.0005577688454650343
batch 70 loss: 0.0005577935953624547
batch 75 loss: 0.0005575630348175764
batch 80 loss: 0.0005575965391471982
batch 85 loss: 0.000557698158081621
batch 90 loss: 0.0005576201365329325
batch 95 loss: 0.0005577278090640903
batch 100 loss: 0.000557649543043226
batch 105 loss: 0.0005576164927333594
batch 110 loss: 0.0005576465046033263
batch 115 loss: 0.0005577284144237637
batch 120 loss: 0.0005577336996793747
batch 125 loss: 0.0005576800787821413
batch 130 loss: 0.0005575752002187073
batch 135 loss: 0.0005575989023782312
batch 140 loss: 0.0005577185074798763
batch 145 loss: 0.0005576631287112832
batch 150 loss: 0.0005577337346039712
batch 155 loss: 0.0005576036055572331
batch 160 loss: 0.0005577688803896308
batch 165 loss: 0.0005578099400736392
batch 170 loss: 0.0005577531876042486
batch 175 loss: 0.0005577340838499367
batch 180 loss: 0.0005576948402449489
batch 185 loss: 0.0005577297881245614
batch 190 loss: 0.0005577291129156947
batch 195 loss: 0.0005577294272370636
batch 200 loss: 0.0005577203468419611
batch 205 loss: 0.0005576486233621836
batch 210 loss: 0.0005577494041062892
batch 215 loss: 0.0005575753399170935
batch 220 loss: 0.0005577010335400701
batch 225 loss: 0.0005576466326601803
batch 230 loss: 0.000557597039733082
batch 235 loss: 0.0005576504860073328
batch 240 loss: 0.0005576567491516471
Training Loss: 0.0005576833939509621
Validation Loss: 0.0005576934044559796
Epoch 79:
batch 5 loss: 0.0005576506606303156
batch 10 loss: 0.0005577019182965159
batch 15 loss: 0.0005577242351137102
batch 20 loss: 0.0005577085190452636
batch 25 loss: 0.0005576802417635917
batch 30 loss: 0.0005576636875048279
batch 35 loss: 0.00055778055684641
batch 40 loss: 0.0005575403338298202
batch 45 loss: 0.0005576974828727544
batch 50 loss: 0.0005577444098889828
batch 55 loss: 0.0005576507071964442
batch 60 loss: 0.0005576666328124702
batch 65 loss: 0.000557633233256638
batch 70 loss: 0.0005577865173108876
batch 75 loss: 0.0005576888215728104
batch 80 loss: 0.0005577151547186077
batch 85 loss: 0.0005577424424700439
batch 90 loss: 0.0005576142575591803
batch 95 loss: 0.0005577124073170125
batch 100 loss: 0.0005577332340180874
batch 105 loss: 0.0005577736883424222
batch 110 loss: 0.0005576901487074793
batch 115 loss: 0.0005576894502155483
batch 120 loss: 0.0005576834548264742
batch 125 loss: 0.0005576271330937743
batch 130 loss: 0.0005575921037234366
batch 135 loss: 0.0005576984491199255
batch 140 loss: 0.000557674327865243
batch 145 loss: 0.0005577233154326677
batch 150 loss: 0.0005577629781328142
batch 155 loss: 0.0005577567731961608
batch 160 loss: 0.0005576966563239693
batch 165 loss: 0.0005575870978645981
batch 170 loss: 0.0005576310912147164
batch 175 loss: 0.0005576850729994476
batch 180 loss: 0.0005576813593506813
batch 185 loss: 0.0005575561779551208
batch 190 loss: 0.0005576444207690656
batch 195 loss: 0.0005576823023147881
batch 200 loss: 0.000557654513977468
batch 205 loss: 0.0005577035713940859
batch 210 loss: 0.0005576557363383472
batch 215 loss: 0.0005577161675319076
batch 220 loss: 0.0005577389150857925
batch 225 loss: 0.0005577027332037687
batch 230 loss: 0.0005576247116550803
batch 235 loss: 0.0005576049210503697
batch 240 loss: 0.0005577304749749601
Training Loss: 0.0005576834000142601
Validation Loss: 0.0005576934151273841
Epoch 80:
batch 5 loss: 0.0005577531876042486
batch 10 loss: 0.0005576296825893223
batch 15 loss: 0.0005577532458119095
batch 20 loss: 0.0005576913012191653
batch 25 loss: 0.000557776412460953
batch 30 loss: 0.0005576531169936061
batch 35 loss: 0.000557696376927197
batch 40 loss: 0.0005576282390393316
batch 45 loss: 0.0005577276227995753
batch 50 loss: 0.0005577106145210564
batch 55 loss: 0.0005576676223427058
batch 60 loss: 0.0005576654337346553
batch 65 loss: 0.0005576734198257327
batch 70 loss: 0.000557758123613894
batch 75 loss: 0.0005575777846388519
batch 80 loss: 0.000557718810159713
batch 85 loss: 0.0005575829069130123
batch 90 loss: 0.0005576775409281253
batch 95 loss: 0.0005576360621489584
batch 100 loss: 0.0005575979477725923
batch 105 loss: 0.0005577073781751097
batch 110 loss: 0.000557758251670748
batch 115 loss: 0.0005576808587647974
batch 120 loss: 0.0005576040130108595
batch 125 loss: 0.0005576909170486033
batch 130 loss: 0.000557651708368212
batch 135 loss: 0.0005577243864536285
batch 140 loss: 0.0005575447343289852
batch 145 loss: 0.0005576877039857209
batch 150 loss: 0.0005577819305472076
batch 155 loss: 0.0005575985182076692
batch 160 loss: 0.0005575324641540647
batch 165 loss: 0.0005576089839451015
batch 170 loss: 0.0005577488336712122
batch 175 loss: 0.0005577553063631057
batch 180 loss: 0.0005577707197517157
batch 185 loss: 0.0005577035713940859
batch 190 loss: 0.000557606783695519
batch 195 loss: 0.0005576909170486033
batch 200 loss: 0.0005576504743658006
batch 205 loss: 0.0005577431060373783
batch 210 loss: 0.0005576800205744803
batch 215 loss: 0.0005576477502472699
batch 220 loss: 0.0005578201496973633
batch 225 loss: 0.000557721417862922
batch 230 loss: 0.0005577316740527749
batch 235 loss: 0.0005576801137067378
batch 240 loss: 0.0005577046307735145
Training Loss: 0.0005576833910405792
Validation Loss: 0.0005576934325896824
Epoch 81:
batch 5 loss: 0.0005576608702540398
batch 10 loss: 0.0005577090894803404
batch 15 loss: 0.0005576942930929363
batch 20 loss: 0.0005576971801929176
batch 25 loss: 0.0005575994960963726
batch 30 loss: 0.0005577430361881852
batch 35 loss: 0.0005576291703619062
batch 40 loss: 0.0005576101364567875
batch 45 loss: 0.0005575455958023668
batch 50 loss: 0.0005576586234383285
batch 55 loss: 0.0005576999392360449
batch 60 loss: 0.0005576625932008028
batch 65 loss: 0.0005576342227868736
batch 70 loss: 0.0005577560514211655
batch 75 loss: 0.0005576752708293497
batch 80 loss: 0.000557747355196625
batch 85 loss: 0.0005577377160079777
batch 90 loss: 0.0005578189040534199
batch 95 loss: 0.0005576554103754461
batch 100 loss: 0.0005576601251959801
batch 105 loss: 0.0005576650379225612
batch 110 loss: 0.0005576585535891354
batch 115 loss: 0.0005576383089646697
batch 120 loss: 0.0005576576571911573
batch 125 loss: 0.0005576639901846647
batch 130 loss: 0.0005576639086939394
batch 135 loss: 0.0005576179479248822
batch 140 loss: 0.0005576845142059028
batch 145 loss: 0.0005575943388976157
batch 150 loss: 0.0005577209987677634
batch 155 loss: 0.0005576891941018403
batch 160 loss: 0.0005577233503572642
batch 165 loss: 0.0005576740484684705
batch 170 loss: 0.0005577888106927276
batch 175 loss: 0.0005576167372055352
batch 180 loss: 0.0005577747710049152
batch 185 loss: 0.0005577464471571147
batch 190 loss: 0.0005577527568675578
batch 195 loss: 0.0005577070754952729
batch 200 loss: 0.0005577361909672618
batch 205 loss: 0.0005577103584073484
batch 210 loss: 0.0005576148862019182
batch 215 loss: 0.0005576837458647788
batch 220 loss: 0.0005576142109930515
batch 225 loss: 0.0005577195319347083
batch 230 loss: 0.0005577459116466344
batch 235 loss: 0.0005576745374128222
batch 240 loss: 0.0005576700437813997
Training Loss: 0.000557683394678558
Validation Loss: 0.0005576934228884057
Epoch 82:
batch 5 loss: 0.0005576254450716078
batch 10 loss: 0.0005576490540988744
batch 15 loss: 0.0005576198804192245
batch 20 loss: 0.0005576018360443413
batch 25 loss: 0.0005577395553700626
batch 30 loss: 0.0005577032803557813
batch 35 loss: 0.0005577794858254492
batch 40 loss: 0.0005577271920628845
batch 45 loss: 0.0005577256553806365
batch 50 loss: 0.0005578135023824871
batch 55 loss: 0.0005576377967372537
batch 60 loss: 0.0005575816379860043
batch 65 loss: 0.0005576432682573796
batch 70 loss: 0.0005576622672379017
batch 75 loss: 0.0005577012430876494
batch 80 loss: 0.000557769148144871
batch 85 loss: 0.0005576632916927338
batch 90 loss: 0.0005576362949796021
batch 95 loss: 0.0005577975651249289
batch 100 loss: 0.0005577576695941389
batch 105 loss: 0.0005577382165938616
batch 110 loss: 0.0005576159572228789
batch 115 loss: 0.0005577213363721967
batch 120 loss: 0.0005576623952947557
batch 125 loss: 0.0005576754920184612
batch 130 loss: 0.0005577287054620683
batch 135 loss: 0.0005576586816459894
batch 140 loss: 0.0005576969124376774
batch 145 loss: 0.0005578072974458337
batch 150 loss: 0.0005576814524829388
batch 155 loss: 0.0005576587631367147
batch 160 loss: 0.0005576550378464162
batch 165 loss: 0.0005576698575168848
batch 170 loss: 0.000557750859297812
batch 175 loss: 0.0005576644674874842
batch 180 loss: 0.0005576663766987622
batch 185 loss: 0.0005576607771217823
batch 190 loss: 0.0005576732801273465
batch 195 loss: 0.0005575652816332876
batch 200 loss: 0.0005577085306867957
batch 205 loss: 0.0005575729999691248
batch 210 loss: 0.000557771825697273
batch 215 loss: 0.0005576637107878923
batch 220 loss: 0.0005575683317147195
batch 225 loss: 0.0005577689385972917
batch 230 loss: 0.0005577392526902259
batch 235 loss: 0.0005575549905188382
batch 240 loss: 0.0005576682626269758
Training Loss: 0.0005576833971038771
Validation Loss: 0.0005576935247518123
Epoch 83:
batch 5 loss: 0.0005576879368163646
batch 10 loss: 0.0005577943520620465
batch 15 loss: 0.0005576244671829045
batch 20 loss: 0.0005576660856604576
batch 25 loss: 0.0005577063886448741
batch 30 loss: 0.0005576327443122864
batch 35 loss: 0.0005577343283221126
batch 40 loss: 0.0005576219293288887
batch 45 loss: 0.0005577155738137662
batch 50 loss: 0.0005575845367275179
batch 55 loss: 0.0005576043855398894
batch 60 loss: 0.0005576937692239881
batch 65 loss: 0.0005576393683440984
batch 70 loss: 0.0005576291820034385
batch 75 loss: 0.0005576495546847582
batch 80 loss: 0.0005576465162448585
batch 85 loss: 0.0005577641422860324
batch 90 loss: 0.0005576352123171091
batch 95 loss: 0.0005576439551077783
batch 100 loss: 0.0005577660980634391
batch 105 loss: 0.0005578630487434566
batch 110 loss: 0.0005576713243499398
batch 115 loss: 0.0005577287753112614
batch 120 loss: 0.0005576346651650965
batch 125 loss: 0.0005578253418207169
batch 130 loss: 0.0005577008472755551
batch 135 loss: 0.000557764747645706
batch 140 loss: 0.0005576696246862411
batch 145 loss: 0.0005577373434789479
batch 150 loss: 0.0005576152820140123
batch 155 loss: 0.0005576489027589559
batch 160 loss: 0.0005576251307502389
batch 165 loss: 0.000557597354054451
batch 170 loss: 0.0005576381110586226
batch 175 loss: 0.000557653047144413
batch 180 loss: 0.0005577525706030428
batch 185 loss: 0.0005576484836637974
batch 190 loss: 0.0005577277624979615
batch 195 loss: 0.000557563395705074
batch 200 loss: 0.0005576457944698632
batch 205 loss: 0.0005577211151830852
batch 210 loss: 0.0005578100681304932
batch 215 loss: 0.00055761334951967
batch 220 loss: 0.0005577728850767016
batch 225 loss: 0.0005577420582994818
batch 230 loss: 0.0005576255964115262
batch 235 loss: 0.0005576606490649283
batch 240 loss: 0.0005577050964348018
Training Loss: 0.0005576833937084302
Validation Loss: 0.000557693427739044
Epoch 84:
batch 5 loss: 0.0005577040021307767
batch 10 loss: 0.0005576069350354373
batch 15 loss: 0.0005577000090852379
batch 20 loss: 0.0005576875410042703
batch 25 loss: 0.0005577054573222995
batch 30 loss: 0.0005576489027589559
batch 35 loss: 0.000557610101532191
batch 40 loss: 0.0005576595431193709
batch 45 loss: 0.0005577075411565602
batch 50 loss: 0.000557650183327496
batch 55 loss: 0.0005576903815381229
batch 60 loss: 0.000557726772967726
batch 65 loss: 0.0005576601950451731
batch 70 loss: 0.0005576381459832192
batch 75 loss: 0.0005576862604357302
batch 80 loss: 0.0005576843628659845
batch 85 loss: 0.0005576835013926029
batch 90 loss: 0.0005576256546191872
batch 95 loss: 0.0005577481351792812
batch 100 loss: 0.0005576473660767079
batch 105 loss: 0.000557711604051292
batch 110 loss: 0.0005576508934609592
batch 115 loss: 0.0005577875301241875
batch 120 loss: 0.0005577213247306645
batch 125 loss: 0.0005576752475462854
batch 130 loss: 0.000557673943694681
batch 135 loss: 0.0005576225579716265
batch 140 loss: 0.0005577403120696544
batch 145 loss: 0.0005576833267696202
batch 150 loss: 0.0005577903939411044
batch 155 loss: 0.0005576353287324309
batch 160 loss: 0.0005576613824814558
batch 165 loss: 0.0005577079253271222
batch 170 loss: 0.0005577706033363938
batch 175 loss: 0.0005576161085627974
batch 180 loss: 0.0005577091360464692
batch 185 loss: 0.0005578109179623425
batch 190 loss: 0.0005576835246756673
batch 195 loss: 0.0005576693802140653
batch 200 loss: 0.0005577642121352255
batch 205 loss: 0.0005577858071774244
batch 210 loss: 0.0005576859344728291
batch 215 loss: 0.0005576608935371041
batch 220 loss: 0.0005576097872108221
batch 225 loss: 0.0005575692863203585
batch 230 loss: 0.0005576256895437837
batch 235 loss: 0.0005576805328018963
batch 240 loss: 0.0005576281808316707
Training Loss: 0.0005576833907980471
Validation Loss: 0.0005576934170676395
Epoch 85:
batch 5 loss: 0.0005577651201747358
batch 10 loss: 0.0005577259929850697
batch 15 loss: 0.0005577199277468026
batch 20 loss: 0.0005577109288424254
batch 25 loss: 0.0005576340365223587
batch 30 loss: 0.000557771185413003
batch 35 loss: 0.0005576980649493635
batch 40 loss: 0.0005576616735197603
batch 45 loss: 0.000557654700241983
batch 50 loss: 0.0005576209397986532
batch 55 loss: 0.0005576686235144734
batch 60 loss: 0.0005576653871685267
batch 65 loss: 0.0005576466675847769
batch 70 loss: 0.0005576948635280133
batch 75 loss: 0.0005575434188358486
batch 80 loss: 0.0005577055271714926
batch 85 loss: 0.0005577057949267328
batch 90 loss: 0.000557709822896868
batch 95 loss: 0.0005577027448453009
batch 100 loss: 0.0005576522904448211
batch 105 loss: 0.000557680893689394
batch 110 loss: 0.0005577663308940828
batch 115 loss: 0.000557706004474312
batch 120 loss: 0.0005577395902946591
batch 125 loss: 0.000557777110952884
batch 130 loss: 0.0005577205331064761
batch 135 loss: 0.0005577115807682276
batch 140 loss: 0.0005576121970079839
batch 145 loss: 0.0005576361785642802
batch 150 loss: 0.0005576131981797517
batch 155 loss: 0.0005576501367613673
batch 160 loss: 0.0005575973191298544
batch 165 loss: 0.0005575991468504071
batch 170 loss: 0.0005575647461228073
batch 175 loss: 0.0005577106145210564
batch 180 loss: 0.0005576408118940889
batch 185 loss: 0.0005577044910751283
batch 190 loss: 0.0005577727919444442
batch 195 loss: 0.0005576726049184799
batch 200 loss: 0.0005578181357122957
batch 205 loss: 0.0005575380870141089
batch 210 loss: 0.0005576241412200034
batch 215 loss: 0.0005577382748015225
batch 220 loss: 0.0005577736650593579
batch 225 loss: 0.0005577686359174549
batch 230 loss: 0.0005577027099207043
batch 235 loss: 0.0005575795425102114
batch 240 loss: 0.0005577255273237824
Training Loss: 0.0005576833898279195
Validation Loss: 0.0005576934044559796
Epoch 86:
batch 5 loss: 0.0005577091011218727
batch 10 loss: 0.0005576688912697136
batch 15 loss: 0.0005576557945460081
batch 20 loss: 0.0005577393341809512
batch 25 loss: 0.0005576556432060897
batch 30 loss: 0.0005578247597441077
batch 35 loss: 0.0005576204624958336
batch 40 loss: 0.0005576223600655794
batch 45 loss: 0.000557734735775739
batch 50 loss: 0.0005576631636358797
batch 55 loss: 0.000557751627638936
batch 60 loss: 0.0005576780764386058
batch 65 loss: 0.0005576005554758012
batch 70 loss: 0.0005577616393566132
batch 75 loss: 0.000557688728440553
batch 80 loss: 0.0005576814175583423
batch 85 loss: 0.0005576806725002825
batch 90 loss: 0.0005577041418291628
batch 95 loss: 0.0005577116389758885
batch 100 loss: 0.0005577263655140996
batch 105 loss: 0.0005577444331720472
batch 110 loss: 0.0005576443625614047
batch 115 loss: 0.0005577194038778544
batch 120 loss: 0.0005577750154770911
batch 125 loss: 0.00055765719152987
batch 130 loss: 0.0005576307652518153
batch 135 loss: 0.0005576339666731655
batch 140 loss: 0.0005576906842179596
batch 145 loss: 0.0005577358650043606
batch 150 loss: 0.000557770614977926
batch 155 loss: 0.0005576495896093547
batch 160 loss: 0.0005576533148996532
batch 165 loss: 0.0005576933268457651
batch 170 loss: 0.0005577102419920266
batch 175 loss: 0.0005577342701144517
batch 180 loss: 0.0005576573894359171
batch 185 loss: 0.0005575673887506128
batch 190 loss: 0.0005576470051892102
batch 195 loss: 0.000557712942827493
batch 200 loss: 0.0005575776332989335
batch 205 loss: 0.0005577444913797081
batch 210 loss: 0.0005577265401370823
batch 215 loss: 0.0005577008007094264
batch 220 loss: 0.0005576523253694177
batch 225 loss: 0.0005575793446041643
batch 230 loss: 0.0005576650728471577
batch 235 loss: 0.0005575920920819044
batch 240 loss: 0.0005576576688326896
Training Loss: 0.0005576833927383025
Validation Loss: 0.0005576934015455966
Epoch 87:
batch 5 loss: 0.0005576955736614764
batch 10 loss: 0.000557741813827306
batch 15 loss: 0.0005577104981057346
batch 20 loss: 0.0005576575873419643
batch 25 loss: 0.0005577106028795242
batch 30 loss: 0.0005576684372499586
batch 35 loss: 0.0005577399744652212
batch 40 loss: 0.0005577474483288824
batch 45 loss: 0.0005576928961090744
batch 50 loss: 0.0005576186114922166
batch 55 loss: 0.000557722628582269
batch 60 loss: 0.0005577541771344841
batch 65 loss: 0.000557703129015863
batch 70 loss: 0.0005577272037044168
batch 75 loss: 0.0005576677853241563
batch 80 loss: 0.0005576091120019555
batch 85 loss: 0.000557545234914869
batch 90 loss: 0.0005577404168434441
batch 95 loss: 0.0005576430819928646
batch 100 loss: 0.0005576241645030677
batch 105 loss: 0.0005576329538598656
batch 110 loss: 0.0005576970521360636
batch 115 loss: 0.0005577034433372318
batch 120 loss: 0.0005575735820457339
batch 125 loss: 0.0005576449329964817
batch 130 loss: 0.0005577284609898925
batch 135 loss: 0.0005576830008067191
batch 140 loss: 0.0005577434552833438
batch 145 loss: 0.0005575988558121026
batch 150 loss: 0.0005577024188823998
batch 155 loss: 0.0005577417090535164
batch 160 loss: 0.0005576612427830696
batch 165 loss: 0.0005576955387368798
batch 170 loss: 0.0005576883908361196
batch 175 loss: 0.000557691149879247
batch 180 loss: 0.0005575743969529867
batch 185 loss: 0.0005576880648732185
batch 190 loss: 0.0005577663192525506
batch 195 loss: 0.0005576769821345806
batch 200 loss: 0.0005577206611633301
batch 205 loss: 0.0005577705334872008
batch 210 loss: 0.0005577082862146199
batch 215 loss: 0.0005577935371547937
batch 220 loss: 0.0005575260845944285
batch 225 loss: 0.0005576207069680095
batch 230 loss: 0.0005576852709054947
batch 235 loss: 0.0005577608710154891
batch 240 loss: 0.0005576042109169066
Training Loss: 0.000557683385219813
Validation Loss: 0.0005576934063962351
Epoch 88:
batch 5 loss: 0.0005577223375439644
batch 10 loss: 0.0005576723138801754
batch 15 loss: 0.0005577825824730099
batch 20 loss: 0.0005576719529926776
batch 25 loss: 0.0005576884024776519
batch 30 loss: 0.0005576630472205579
batch 35 loss: 0.0005576361552812159
batch 40 loss: 0.0005576958297751844
batch 45 loss: 0.0005576309515163303
batch 50 loss: 0.000557623931672424
batch 55 loss: 0.0005576257128268481
batch 60 loss: 0.0005576993920840323
batch 65 loss: 0.0005576629424467683
batch 70 loss: 0.0005577117204666138
batch 75 loss: 0.0005576202645897865
batch 80 loss: 0.000557602581102401
batch 85 loss: 0.0005577217554673553
batch 90 loss: 0.0005576290423050523
batch 95 loss: 0.000557718996424228
batch 100 loss: 0.0005577103700488805
batch 105 loss: 0.0005576291121542453
batch 110 loss: 0.0005577236996032297
batch 115 loss: 0.000557725818362087
batch 120 loss: 0.0005576511728577315
batch 125 loss: 0.0005577054456807673
batch 130 loss: 0.0005577524192631244
batch 135 loss: 0.000557673373259604
batch 140 loss: 0.0005575948860496282
batch 145 loss: 0.0005577559815719724
batch 150 loss: 0.0005576739320531487
batch 155 loss: 0.0005576890776865185
batch 160 loss: 0.0005577122326940298
batch 165 loss: 0.0005578176002018154
batch 170 loss: 0.0005576582974754274
batch 175 loss: 0.0005577127332799137
batch 180 loss: 0.0005577532574534416
batch 185 loss: 0.0005575803108513356
batch 190 loss: 0.0005576399737037719
batch 195 loss: 0.000557699310593307
batch 200 loss: 0.0005576155963353813
batch 205 loss: 0.0005576202529482543
batch 210 loss: 0.0005576604162342846
batch 215 loss: 0.0005576314521022141
batch 220 loss: 0.0005578370182774961
batch 225 loss: 0.0005577472038567066
batch 230 loss: 0.0005576362833380699
batch 235 loss: 0.0005577242816798389
batch 240 loss: 0.0005576911615207791
Training Loss: 0.0005576833871600683
Validation Loss: 0.0005576934054261073
Epoch 89:
batch 5 loss: 0.0005577144562266767
batch 10 loss: 0.0005577174830250442
batch 15 loss: 0.0005575870163738728
batch 20 loss: 0.0005576782044954598
batch 25 loss: 0.0005575882736593485
batch 30 loss: 0.0005577445146627725
batch 35 loss: 0.0005577120697125793
batch 40 loss: 0.0005577464122325182
batch 45 loss: 0.0005577720352448523
batch 50 loss: 0.0005576714174821973
batch 55 loss: 0.0005577268893830478
batch 60 loss: 0.0005577942822128535
batch 65 loss: 0.0005577604635618628
batch 70 loss: 0.000557674840092659
batch 75 loss: 0.0005577331292442977
batch 80 loss: 0.0005576175753958523
batch 85 loss: 0.0005576555035077035
batch 90 loss: 0.0005575832328759134
batch 95 loss: 0.000557749334257096
batch 100 loss: 0.0005576635361649096
batch 105 loss: 0.0005576337454840541
batch 110 loss: 0.0005576459225267172
batch 115 loss: 0.0005575972376391291
batch 120 loss: 0.0005577004165388643
batch 125 loss: 0.0005577165167778731
batch 130 loss: 0.0005577415111474692
batch 135 loss: 0.0005577520001679659
batch 140 loss: 0.000557748693972826
batch 145 loss: 0.0005575843504630029
batch 150 loss: 0.0005577010568231344
batch 155 loss: 0.0005575552233494819
batch 160 loss: 0.0005575964110903442
batch 165 loss: 0.0005576691590249538
batch 170 loss: 0.0005576855270192027
batch 175 loss: 0.0005577950505539775
batch 180 loss: 0.000557684968225658
batch 185 loss: 0.0005576424766331912
batch 190 loss: 0.000557783676777035
batch 195 loss: 0.0005576130235567689
batch 200 loss: 0.0005576562252826989
batch 205 loss: 0.000557729380670935
batch 210 loss: 0.0005576820462010801
batch 215 loss: 0.0005575640709139407
batch 220 loss: 0.0005577064701355993
batch 225 loss: 0.0005577217554673553
batch 230 loss: 0.0005577394738793373
batch 235 loss: 0.0005575792631134391
batch 240 loss: 0.0005576858296990394
Training Loss: 0.0005576833781863873
Validation Loss: 0.0005576933986352135
Epoch 90:
batch 5 loss: 0.0005576887633651495
batch 10 loss: 0.0005576525582000613
batch 15 loss: 0.0005576204392127692
batch 20 loss: 0.0005576794035732746
batch 25 loss: 0.0005576794967055321
batch 30 loss: 0.0005577558418735862
batch 35 loss: 0.000557705550454557
batch 40 loss: 0.000557692744769156
batch 45 loss: 0.0005577426520176232
batch 50 loss: 0.0005576848518103362
batch 55 loss: 0.000557716959156096
batch 60 loss: 0.0005577252246439457
batch 65 loss: 0.000557620741892606
batch 70 loss: 0.0005577287753112614
batch 75 loss: 0.0005576702998951078
batch 80 loss: 0.0005575842456892133
batch 85 loss: 0.0005577775184065104
batch 90 loss: 0.0005576707422733307
batch 95 loss: 0.0005576998228207231
batch 100 loss: 0.0005576149793341756
batch 105 loss: 0.0005577192059718072
batch 110 loss: 0.0005576254916377366
batch 115 loss: 0.0005576577503234148
batch 120 loss: 0.0005576687282882631
batch 125 loss: 0.0005578061565756798
batch 130 loss: 0.0005576422438025475
batch 135 loss: 0.0005577380303293467
batch 140 loss: 0.0005575909977778793
batch 145 loss: 0.0005576542695052922
batch 150 loss: 0.0005576426745392382
batch 155 loss: 0.0005576688563451171
batch 160 loss: 0.0005577461677603423
batch 165 loss: 0.0005576455732807517
batch 170 loss: 0.0005577215226367116
batch 175 loss: 0.000557610031682998
batch 180 loss: 0.0005577050964348018
batch 185 loss: 0.0005577330710366368
batch 190 loss: 0.0005576033843681216
batch 195 loss: 0.0005576616851612926
batch 200 loss: 0.000557684781961143
batch 205 loss: 0.0005577141884714365
batch 210 loss: 0.0005576926399953663
batch 215 loss: 0.000557644315995276
batch 220 loss: 0.0005577655392698944
batch 225 loss: 0.0005577741423621774
batch 230 loss: 0.0005577569594606757
batch 235 loss: 0.0005575714283622801
batch 240 loss: 0.0005576458992436528
Training Loss: 0.0005576833842496854
Validation Loss: 0.0005576934083364904
Epoch 91:
batch 5 loss: 0.0005576351424679161
batch 10 loss: 0.0005576236872002482
batch 15 loss: 0.0005576757946982979
batch 20 loss: 0.0005576472613029182
batch 25 loss: 0.0005576862837187946
batch 30 loss: 0.0005577192059718072
batch 35 loss: 0.0005576942698098719
batch 40 loss: 0.0005577124422416091
batch 45 loss: 0.0005577111500315368
batch 50 loss: 0.0005577753647230566
batch 55 loss: 0.0005576088791713118
batch 60 loss: 0.0005575920687988401
batch 65 loss: 0.0005577182048000396
batch 70 loss: 0.0005577069357968866
batch 75 loss: 0.0005576965282671154
batch 80 loss: 0.0005577000789344311
batch 85 loss: 0.000557648844551295
batch 90 loss: 0.0005577896488830447
batch 95 loss: 0.0005577931879088283
batch 100 loss: 0.0005575196351855993
batch 105 loss: 0.0005577423493377864
batch 110 loss: 0.0005575956194661558
batch 115 loss: 0.000557759020011872
batch 120 loss: 0.0005577205447480083
batch 125 loss: 0.0005576221854425966
batch 130 loss: 0.0005576539435423911
batch 135 loss: 0.0005577288684435189
batch 140 loss: 0.0005576617550104856
batch 145 loss: 0.000557711860165
batch 150 loss: 0.0005576456198468805
batch 155 loss: 0.0005576507421210409
batch 160 loss: 0.000557709252461791
batch 165 loss: 0.0005576707306317985
batch 170 loss: 0.0005576154449954629
batch 175 loss: 0.0005577961215749383
batch 180 loss: 0.0005577158182859421
batch 185 loss: 0.0005575491697527469
batch 190 loss: 0.0005578210344538093
batch 195 loss: 0.0005577003583312034
batch 200 loss: 0.0005576991243287921
batch 205 loss: 0.0005575736751779914
batch 210 loss: 0.0005576848285272718
batch 215 loss: 0.0005576930823735892
batch 220 loss: 0.0005577220348641276
batch 225 loss: 0.0005576214171014726
batch 230 loss: 0.0005576418130658567
batch 235 loss: 0.0005577903357334435
batch 240 loss: 0.0005576509982347488
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576934073663627
Epoch 92:
batch 5 loss: 0.0005576844792813063
batch 10 loss: 0.0005576083669438959
batch 15 loss: 0.0005576245370320976
batch 20 loss: 0.000557778647635132
batch 25 loss: 0.0005576646304689348
batch 30 loss: 0.0005576128605753183
batch 35 loss: 0.0005576427327468991
batch 40 loss: 0.000557735632173717
batch 45 loss: 0.0005576324183493853
batch 50 loss: 0.0005576560623012483
batch 55 loss: 0.0005577112315222621
batch 60 loss: 0.0005576938390731811
batch 65 loss: 0.0005576467840000987
batch 70 loss: 0.0005577648058533669
batch 75 loss: 0.0005576417548581958
batch 80 loss: 0.000557729380670935
batch 85 loss: 0.0005576664349064231
batch 90 loss: 0.0005577256786637008
batch 95 loss: 0.0005576183553785086
batch 100 loss: 0.0005576878553256393
batch 105 loss: 0.0005576580297201872
batch 110 loss: 0.0005576319177635014
batch 115 loss: 0.0005575642921030521
batch 120 loss: 0.0005575584713369608
batch 125 loss: 0.0005577025236561895
batch 130 loss: 0.0005576340365223587
batch 135 loss: 0.0005577251431532204
batch 140 loss: 0.0005577426520176232
batch 145 loss: 0.0005576811381615698
batch 150 loss: 0.0005577510339207948
batch 155 loss: 0.0005577426985837519
batch 160 loss: 0.0005577773205004633
batch 165 loss: 0.0005576777155511081
batch 170 loss: 0.0005576960043981672
batch 175 loss: 0.0005576801137067378
batch 180 loss: 0.0005575909279286861
batch 185 loss: 0.0005576184252277016
batch 190 loss: 0.0005577063304372132
batch 195 loss: 0.0005577690666541457
batch 200 loss: 0.0005576526862569153
batch 205 loss: 0.0005577518022619188
batch 210 loss: 0.0005577707663178444
batch 215 loss: 0.0005576823721639812
batch 220 loss: 0.0005578508833423256
batch 225 loss: 0.0005576716270297766
batch 230 loss: 0.0005576391704380512
batch 235 loss: 0.0005576251540333032
batch 240 loss: 0.0005577234434895218
Training Loss: 0.0005576833798841107
Validation Loss: 0.0005576934054261073
Epoch 93:
batch 5 loss: 0.000557798205409199
batch 10 loss: 0.000557761627715081
batch 15 loss: 0.0005577823030762374
batch 20 loss: 0.0005576273193582893
batch 25 loss: 0.0005576628376729786
batch 30 loss: 0.0005576239433139563
batch 35 loss: 0.0005577647592872381
batch 40 loss: 0.0005576502531766892
batch 45 loss: 0.0005577226169407367
batch 50 loss: 0.0005577081232331693
batch 55 loss: 0.0005576363881118595
batch 60 loss: 0.000557700521312654
batch 65 loss: 0.0005577447824180126
batch 70 loss: 0.0005574928596615792
batch 75 loss: 0.0005576189374551177
batch 80 loss: 0.000557672861032188
batch 85 loss: 0.0005576561787165701
batch 90 loss: 0.0005575732211582363
batch 95 loss: 0.0005577135016210377
batch 100 loss: 0.0005575785995461047
batch 105 loss: 0.0005576613359153271
batch 110 loss: 0.0005577486008405685
batch 115 loss: 0.0005576298339292407
batch 120 loss: 0.0005575866089202464
batch 125 loss: 0.0005577802076004446
batch 130 loss: 0.0005576596944592893
batch 135 loss: 0.0005576887400820851
batch 140 loss: 0.0005575423943810165
batch 145 loss: 0.0005576070980168879
batch 150 loss: 0.0005578269832767546
batch 155 loss: 0.000557729764841497
batch 160 loss: 0.0005577251431532204
batch 165 loss: 0.0005576551775448024
batch 170 loss: 0.0005576466908678412
batch 175 loss: 0.0005576914991252124
batch 180 loss: 0.0005578473792411387
batch 185 loss: 0.0005577554227784276
batch 190 loss: 0.0005576660740189254
batch 195 loss: 0.0005576232215389609
batch 200 loss: 0.0005576585652306675
batch 205 loss: 0.0005575857823714614
batch 210 loss: 0.0005577336647547782
batch 215 loss: 0.0005577521282248199
batch 220 loss: 0.0005578434327617287
batch 225 loss: 0.0005576662020757795
batch 230 loss: 0.0005576473195105791
batch 235 loss: 0.0005576274823397398
batch 240 loss: 0.0005577258532866835
Training Loss: 0.0005576833779438554
Validation Loss: 0.0005576934015455966
Epoch 94:
batch 5 loss: 0.0005576063063926995
batch 10 loss: 0.0005577284493483603
batch 15 loss: 0.0005576404044404625
batch 20 loss: 0.0005577021976932884
batch 25 loss: 0.000557767681311816
batch 30 loss: 0.0005576896248385311
batch 35 loss: 0.000557745574042201
batch 40 loss: 0.0005576467607170344
batch 45 loss: 0.0005576441995799542
batch 50 loss: 0.0005576283787377178
batch 55 loss: 0.0005577525589615107
batch 60 loss: 0.0005576006951741874
batch 65 loss: 0.0005578094976954162
batch 70 loss: 0.0005575534305535257
batch 75 loss: 0.0005576175986789167
batch 80 loss: 0.0005577493575401604
batch 85 loss: 0.0005576719413511455
batch 90 loss: 0.0005575450253672898
batch 95 loss: 0.0005576167488470674
batch 100 loss: 0.0005577366217039525
batch 105 loss: 0.0005576605559326709
batch 110 loss: 0.0005576622323133051
batch 115 loss: 0.0005577873322181404
batch 120 loss: 0.0005576260387897492
batch 125 loss: 0.0005576357012614608
batch 130 loss: 0.0005577105097472667
batch 135 loss: 0.0005577427684329451
batch 140 loss: 0.0005577467847615481
batch 145 loss: 0.0005576539668254554
batch 150 loss: 0.0005576190771535039
batch 155 loss: 0.0005576900788582861
batch 160 loss: 0.0005577498697675765
batch 165 loss: 0.0005576864467002451
batch 170 loss: 0.0005576057825237513
batch 175 loss: 0.0005576080293394625
batch 180 loss: 0.000557611370459199
batch 185 loss: 0.0005578028387390078
batch 190 loss: 0.0005577023955993354
batch 195 loss: 0.0005576884723268449
batch 200 loss: 0.0005577300558798015
batch 205 loss: 0.0005577084724791348
batch 210 loss: 0.0005577334901317954
batch 215 loss: 0.0005576902418397367
batch 220 loss: 0.0005577428615652025
batch 225 loss: 0.0005576501484028995
batch 230 loss: 0.0005577497533522547
batch 235 loss: 0.000557610928080976
batch 240 loss: 0.000557742826640606
Training Loss: 0.0005576833767311958
Validation Loss: 0.0005576933996053413
Epoch 95:
batch 5 loss: 0.0005576733849011362
batch 10 loss: 0.0005575483664870262
batch 15 loss: 0.0005577385425567627
batch 20 loss: 0.0005576260387897492
batch 25 loss: 0.0005577294738031924
batch 30 loss: 0.0005576433264650405
batch 35 loss: 0.0005577473668381572
batch 40 loss: 0.0005576849915087223
batch 45 loss: 0.000557676306925714
batch 50 loss: 0.0005575844901613891
batch 55 loss: 0.0005576310097239912
batch 60 loss: 0.0005576073308475316
batch 65 loss: 0.0005575907765887677
batch 70 loss: 0.0005576930358074606
batch 75 loss: 0.0005577252595685422
batch 80 loss: 0.0005576781113632023
batch 85 loss: 0.000557644315995276
batch 90 loss: 0.0005577217089012265
batch 95 loss: 0.0005577047821134328
batch 100 loss: 0.0005576179712079466
batch 105 loss: 0.0005575431510806084
batch 110 loss: 0.0005576726864092052
batch 115 loss: 0.0005577047937549651
batch 120 loss: 0.0005577978678047657
batch 125 loss: 0.0005576428840868175
batch 130 loss: 0.0005576163996011019
batch 135 loss: 0.0005577375763095915
batch 140 loss: 0.000557758065406233
batch 145 loss: 0.0005577327683568001
batch 150 loss: 0.0005575836519710719
batch 155 loss: 0.0005576716153882444
batch 160 loss: 0.0005577483214437961
batch 165 loss: 0.0005577021394856275
batch 170 loss: 0.0005576717550866306
batch 175 loss: 0.0005577093688771129
batch 180 loss: 0.0005576482741162181
batch 185 loss: 0.0005576452123932541
batch 190 loss: 0.0005576915922574699
batch 195 loss: 0.0005576761090196669
batch 200 loss: 0.0005578267853707075
batch 205 loss: 0.0005577762611210346
batch 210 loss: 0.0005577624426223338
batch 215 loss: 0.0005576932220719754
batch 220 loss: 0.0005577550386078656
batch 225 loss: 0.0005577096133492887
batch 230 loss: 0.0005576847703196109
batch 235 loss: 0.0005577005678787828
batch 240 loss: 0.0005576724652200937
Training Loss: 0.0005576833747909405
Validation Loss: 0.0005576934025157243
Epoch 96:
batch 5 loss: 0.0005576833384111524
batch 10 loss: 0.000557724735699594
batch 15 loss: 0.0005576291237957776
batch 20 loss: 0.0005577310104854405
batch 25 loss: 0.000557743851095438
batch 30 loss: 0.000557622779160738
batch 35 loss: 0.0005578315816819668
batch 40 loss: 0.0005575389368459582
batch 45 loss: 0.0005576761206611991
batch 50 loss: 0.0005576268071308732
batch 55 loss: 0.0005578062846325338
batch 60 loss: 0.0005576435825787485
batch 65 loss: 0.0005578350625000894
batch 70 loss: 0.0005575970164500177
batch 75 loss: 0.0005576284253038466
batch 80 loss: 0.0005576591123826802
batch 85 loss: 0.0005577184143476188
batch 90 loss: 0.0005576827796176076
batch 95 loss: 0.0005578188342042267
batch 100 loss: 0.0005575977847911417
batch 105 loss: 0.0005577406147494913
batch 110 loss: 0.0005577240255661309
batch 115 loss: 0.0005577703821472823
batch 120 loss: 0.0005577072268351912
batch 125 loss: 0.0005576040362939239
batch 130 loss: 0.0005576615338213741
batch 135 loss: 0.0005576918716542423
batch 140 loss: 0.0005576944211497902
batch 145 loss: 0.0005576745606958866
batch 150 loss: 0.0005577249685302377
batch 155 loss: 0.0005576131283305585
batch 160 loss: 0.0005577196134254336
batch 165 loss: 0.0005576579365879297
batch 170 loss: 0.0005577380070462823
batch 175 loss: 0.0005577638163231313
batch 180 loss: 0.0005577450385317207
batch 185 loss: 0.0005576190073043108
batch 190 loss: 0.0005576488678343594
batch 195 loss: 0.0005577545147389173
batch 200 loss: 0.0005576756899245084
batch 205 loss: 0.0005576406023465097
batch 210 loss: 0.0005575993913225829
batch 215 loss: 0.0005576265743002295
batch 220 loss: 0.0005576354218646884
batch 225 loss: 0.0005575341754592955
batch 230 loss: 0.0005576774245128035
batch 235 loss: 0.0005576796014793217
batch 240 loss: 0.0005576841882430017
Training Loss: 0.0005576833796415788
Validation Loss: 0.0005576933976650859
Epoch 97:
batch 5 loss: 0.0005577383679337799
batch 10 loss: 0.0005577311618253589
batch 15 loss: 0.000557676877360791
batch 20 loss: 0.0005576496478170156
batch 25 loss: 0.0005577393923886121
batch 30 loss: 0.0005576931522227824
batch 35 loss: 0.0005575901130214333
batch 40 loss: 0.0005576741416007281
batch 45 loss: 0.0005577876698225736
batch 50 loss: 0.0005576855270192027
batch 55 loss: 0.0005577204283326864
batch 60 loss: 0.0005577457719482482
batch 65 loss: 0.0005575225106440485
batch 70 loss: 0.0005576462601311505
batch 75 loss: 0.0005576507421210409
batch 80 loss: 0.0005576205672696233
batch 85 loss: 0.0005576825351454318
batch 90 loss: 0.0005577101372182369
batch 95 loss: 0.0005576989380642772
batch 100 loss: 0.0005577229312621057
batch 105 loss: 0.0005576659459620714
batch 110 loss: 0.0005575940711423754
batch 115 loss: 0.000557714095339179
batch 120 loss: 0.0005576085764914751
batch 125 loss: 0.0005576930823735892
batch 130 loss: 0.0005576393217779696
batch 135 loss: 0.0005577597534283995
batch 140 loss: 0.0005577170173637569
batch 145 loss: 0.0005576138850301504
batch 150 loss: 0.0005576043273322284
batch 155 loss: 0.0005576008697971702
batch 160 loss: 0.0005576811847276986
batch 165 loss: 0.0005577368196099996
batch 170 loss: 0.0005576719646342099
batch 175 loss: 0.0005576438037678599
batch 180 loss: 0.0005577823612838984
batch 185 loss: 0.0005577446543611586
batch 190 loss: 0.0005576903466135263
batch 195 loss: 0.0005576394032686949
batch 200 loss: 0.0005576913012191653
batch 205 loss: 0.0005575949442572891
batch 210 loss: 0.0005576808121986687
batch 215 loss: 0.0005578329088166356
batch 220 loss: 0.0005576549097895622
batch 225 loss: 0.0005577255971729756
batch 230 loss: 0.0005577012081630528
batch 235 loss: 0.0005577548057772219
batch 240 loss: 0.0005576771916821599
Training Loss: 0.0005576833757610681
Validation Loss: 0.0005576933986352135
Epoch 98:
batch 5 loss: 0.0005576963303610682
batch 10 loss: 0.0005577241303399205
batch 15 loss: 0.0005576037452556193
batch 20 loss: 0.0005576865281909704
batch 25 loss: 0.0005576929659582674
batch 30 loss: 0.000557670183479786
batch 35 loss: 0.0005576245137490332
batch 40 loss: 0.0005577422794885934
batch 45 loss: 0.0005576085415668786
batch 50 loss: 0.0005576448165811598
batch 55 loss: 0.0005576514638960361
batch 60 loss: 0.0005577809526585043
batch 65 loss: 0.0005576475523412228
batch 70 loss: 0.0005576457479037344
batch 75 loss: 0.0005577520933002234
batch 80 loss: 0.0005576486932113766
batch 85 loss: 0.0005577510688453913
batch 90 loss: 0.0005575962248258292
batch 95 loss: 0.0005576046765781939
batch 100 loss: 0.0005577134899795056
batch 105 loss: 0.0005577223957516253
batch 110 loss: 0.0005576904164627195
batch 115 loss: 0.000557747227139771
batch 120 loss: 0.0005576218129135669
batch 125 loss: 0.0005577366799116134
batch 130 loss: 0.0005577567149884999
batch 135 loss: 0.0005577693926170468
batch 140 loss: 0.0005577261443249881
batch 145 loss: 0.0005576962023042143
batch 150 loss: 0.0005575817194767296
batch 155 loss: 0.0005577017087489367
batch 160 loss: 0.0005576273892074823
batch 165 loss: 0.0005577739793807268
batch 170 loss: 0.0005577253177762032
batch 175 loss: 0.000557722756639123
batch 180 loss: 0.0005576296593062579
batch 185 loss: 0.0005577213480137289
batch 190 loss: 0.0005577085074037313
batch 195 loss: 0.0005577780189923942
batch 200 loss: 0.0005576895084232092
batch 205 loss: 0.0005576384603045881
batch 210 loss: 0.0005576988216489554
batch 215 loss: 0.0005576828494668007
batch 220 loss: 0.0005576206836849451
batch 225 loss: 0.000557606783695519
batch 230 loss: 0.0005576278432272375
batch 235 loss: 0.0005576450726948679
batch 240 loss: 0.0005576685420237481
Training Loss: 0.0005576833740633447
Validation Loss: 0.0005576934044559796
Epoch 99:
batch 5 loss: 0.0005577941425144673
batch 10 loss: 0.0005576693918555975
batch 15 loss: 0.0005576860392466187
batch 20 loss: 0.0005576757248491049
batch 25 loss: 0.0005576329189352691
batch 30 loss: 0.0005576150608249009
batch 35 loss: 0.000557645340450108
batch 40 loss: 0.0005577050964348018
batch 45 loss: 0.000557726516854018
batch 50 loss: 0.0005575635703280568
batch 55 loss: 0.0005577407544478774
batch 60 loss: 0.000557681592181325
batch 65 loss: 0.0005576811148785054
batch 70 loss: 0.0005576648632995784
batch 75 loss: 0.0005576532566919923
batch 80 loss: 0.000557626539375633
batch 85 loss: 0.0005577685893513262
batch 90 loss: 0.0005576190655119717
batch 95 loss: 0.0005576006602495909
batch 100 loss: 0.0005577610339969396
batch 105 loss: 0.0005577470175921917
batch 110 loss: 0.0005577897187322378
batch 115 loss: 0.000557719508651644
batch 120 loss: 0.0005576584837399424
batch 125 loss: 0.0005576778086833656
batch 130 loss: 0.0005577087285928428
batch 135 loss: 0.000557624630164355
batch 140 loss: 0.0005577260162681341
batch 145 loss: 0.0005576866678893566
batch 150 loss: 0.0005576421041041613
batch 155 loss: 0.0005576342809945345
batch 160 loss: 0.0005575866904109717
batch 165 loss: 0.0005576264462433756
batch 170 loss: 0.000557761371601373
batch 175 loss: 0.0005577348521910608
batch 180 loss: 0.0005576771451160312
batch 185 loss: 0.0005576448980718851
batch 190 loss: 0.0005576298921369016
batch 195 loss: 0.0005576493800617755
batch 200 loss: 0.0005577020463533699
batch 205 loss: 0.0005577882053330541
batch 210 loss: 0.0005576258176006377
batch 215 loss: 0.0005577262956649065
batch 220 loss: 0.0005578311975114048
batch 225 loss: 0.0005577923264354467
batch 230 loss: 0.0005576276453211904
batch 235 loss: 0.0005576521041803062
batch 240 loss: 0.0005576193798333406
Training Loss: 0.0005576833735782808
Validation Loss: 0.0005576934025157243
Epoch 100:
batch 5 loss: 0.0005576885072514415
batch 10 loss: 0.0005576375406235456
batch 15 loss: 0.0005576319876126945
batch 20 loss: 0.0005575581686571241
batch 25 loss: 0.0005576199968345463
batch 30 loss: 0.0005576664465479553
batch 35 loss: 0.0005576498340815305
batch 40 loss: 0.0005578051670454443
batch 45 loss: 0.0005576415685936808
batch 50 loss: 0.0005576203344389796
batch 55 loss: 0.0005575775983743369
batch 60 loss: 0.0005577058764174581
batch 65 loss: 0.0005577580537647009
batch 70 loss: 0.0005577258649282158
batch 75 loss: 0.0005576219991780818
batch 80 loss: 0.0005577828385867178
batch 85 loss: 0.0005576851777732373
batch 90 loss: 0.0005577087635174393
batch 95 loss: 0.0005577468546107411
batch 100 loss: 0.0005578088108450175
batch 105 loss: 0.0005576705210842192
batch 110 loss: 0.0005576226976700127
batch 115 loss: 0.0005576424649916589
batch 120 loss: 0.0005576883093453943
batch 125 loss: 0.0005576796247623861
batch 130 loss: 0.0005577049334533513
batch 135 loss: 0.0005576993688009679
batch 140 loss: 0.0005576775525696576
batch 145 loss: 0.000557654199656099
batch 150 loss: 0.0005577394855208695
batch 155 loss: 0.0005576215451583266
batch 160 loss: 0.0005577780539169908
batch 165 loss: 0.0005577261094003916
batch 170 loss: 0.0005575732910074293
batch 175 loss: 0.0005576632800512015
batch 180 loss: 0.0005578007898293435
batch 185 loss: 0.0005575944553129375
batch 190 loss: 0.0005576397641561925
batch 195 loss: 0.0005575557588599622
batch 200 loss: 0.0005577473202720284
batch 205 loss: 0.0005576771101914347
batch 210 loss: 0.0005576712428592145
batch 215 loss: 0.0005578201264142991
batch 220 loss: 0.000557763664983213
batch 225 loss: 0.000557641638442874
batch 230 loss: 0.0005576631869189441
batch 235 loss: 0.0005576962255872786
batch 240 loss: 0.0005577479721978306
Training Loss: 0.0005576833767311958
Validation Loss: 0.0005576934054261073
