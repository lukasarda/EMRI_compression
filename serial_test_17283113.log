no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 1400
20240628_020712
Epoch 1:
batch 5 loss: 0.016749792639166115
batch 10 loss: 0.0035052004270255564
batch 15 loss: 0.002262332150712609
batch 20 loss: 0.0017634053016081452
batch 25 loss: 0.0015053229173645378
batch 30 loss: 0.001288091787137091
batch 35 loss: 0.0011212128214538098
batch 40 loss: 0.001017461996525526
batch 45 loss: 0.0008921812172047794
batch 50 loss: 0.0007714093546383083
batch 55 loss: 0.000749249232467264
batch 60 loss: 0.0007061350275762379
batch 65 loss: 0.0006874109501950442
batch 70 loss: 0.000677522411569953
batch 75 loss: 0.0006717426585964859
batch 80 loss: 0.0006696848897263408
batch 85 loss: 0.000795682636089623
batch 90 loss: 0.000764039356727153
batch 95 loss: 0.0006885547889396548
batch 100 loss: 0.0006756890681572259
batch 105 loss: 0.0006717119249515235
batch 110 loss: 0.0006682117935270071
batch 115 loss: 0.0006619672407396137
batch 120 loss: 0.000662064622156322
batch 125 loss: 0.0010681116487830876
batch 130 loss: 0.0007064895471557975
batch 135 loss: 0.0006698830053210259
batch 140 loss: 0.0006684880121611058
batch 145 loss: 0.0006667890236712992
batch 150 loss: 0.0006650871364399791
batch 155 loss: 0.0006629376905038953
batch 160 loss: 0.000659962696954608
batch 165 loss: 0.000655042554717511
batch 170 loss: 0.0006435979623347521
batch 175 loss: 0.0006858282838948071
batch 180 loss: 0.0006515016197226941
batch 185 loss: 0.0006469361833296716
batch 190 loss: 0.0006418698467314243
batch 195 loss: 0.0006366917747072876
batch 200 loss: 0.0006324404268525541
batch 205 loss: 0.0006278324057348072
batch 210 loss: 0.0006184714613482357
batch 215 loss: 0.0006066366215236485
batch 220 loss: 0.000604589085560292
batch 225 loss: 0.000603091053199023
batch 230 loss: 0.000602023396641016
batch 235 loss: 0.0006005765171721578
batch 240 loss: 0.0005989313242025674
Training Loss: 0.0011760393019358162
Validation Loss: 0.0005824819129581253
Epoch 2:
batch 5 loss: 0.0006028901669196784
batch 10 loss: 0.0006011475808918477
batch 15 loss: 0.0005992350867018104
batch 20 loss: 0.0005975560168735683
batch 25 loss: 0.000596415891777724
batch 30 loss: 0.0005953081301413477
batch 35 loss: 0.0005939470021985472
batch 40 loss: 0.0005928264814428985
batch 45 loss: 0.0005912039545364678
batch 50 loss: 0.0005901555879972876
batch 55 loss: 0.0005892293178476393
batch 60 loss: 0.0005882843281142414
batch 65 loss: 0.0005870850523933768
batch 70 loss: 0.0005939794122241437
batch 75 loss: 0.0005919105489738286
batch 80 loss: 0.0005899193347431719
batch 85 loss: 0.0005883998936042189
batch 90 loss: 0.0005872180219739676
batch 95 loss: 0.000586029828991741
batch 100 loss: 0.0005850740009918809
batch 105 loss: 0.000583909684792161
batch 110 loss: 0.0005825630621984601
batch 115 loss: 0.0005816507851704955
batch 120 loss: 0.0005811294075101614
batch 125 loss: 0.0005802936269901693
batch 130 loss: 0.000579710875172168
batch 135 loss: 0.0005790487397462129
batch 140 loss: 0.0005783020984381438
batch 145 loss: 0.0005781264859251678
batch 150 loss: 0.000582635251339525
batch 155 loss: 0.0005821351776830852
batch 160 loss: 0.0005813552532345056
batch 165 loss: 0.0005805483553558588
batch 170 loss: 0.0005794784985482693
batch 175 loss: 0.0005787363974377513
batch 180 loss: 0.0005780047853477299
batch 185 loss: 0.0005774468998424708
batch 190 loss: 0.0005767739261500537
batch 195 loss: 0.0005762787885032594
batch 200 loss: 0.0005758682498708367
batch 205 loss: 0.0005753761855885386
batch 210 loss: 0.0005750239244662226
batch 215 loss: 0.0005745304282754659
batch 220 loss: 0.0005742711131460964
batch 225 loss: 0.0005740679102018475
batch 230 loss: 0.0005738177220337093
batch 235 loss: 0.0005733483587391675
batch 240 loss: 0.0005783207248896361
Training Loss: 0.000584178507415345
Validation Loss: 0.002896675265704592
Epoch 3:
batch 5 loss: 0.0007357117254287004
batch 10 loss: 0.0005996624473482371
batch 15 loss: 0.0006070944597013295
batch 20 loss: 0.0006020693806931376
batch 25 loss: 0.0005955015425570309
batch 30 loss: 0.000590639840811491
batch 35 loss: 0.0005868764710612595
batch 40 loss: 0.0005843435414135456
batch 45 loss: 0.0005819821497425437
batch 50 loss: 0.0005800475948490203
batch 55 loss: 0.0005785496439784765
batch 60 loss: 0.00057732640998438
batch 65 loss: 0.0005763176130130887
batch 70 loss: 0.0005755185382440686
batch 75 loss: 0.0005749923409894108
batch 80 loss: 0.0005743737099692225
batch 85 loss: 0.0005739549407735467
batch 90 loss: 0.0005736120045185089
batch 95 loss: 0.0005731395212933421
batch 100 loss: 0.0005727979936636984
batch 105 loss: 0.0005725006689317524
batch 110 loss: 0.0005719930282793939
batch 115 loss: 0.0005719160195440054
batch 120 loss: 0.0005715421168133617
batch 125 loss: 0.0005712517886422575
batch 130 loss: 0.000571070471778512
batch 135 loss: 0.0005707853473722935
batch 140 loss: 0.0005706109455786646
batch 145 loss: 0.0005703466245904565
batch 150 loss: 0.0005700908368453383
batch 155 loss: 0.0005699921282939612
batch 160 loss: 0.000569900602567941
batch 165 loss: 0.0005696362699382007
batch 170 loss: 0.0005693943006917834
batch 175 loss: 0.0005693011567927897
batch 180 loss: 0.0005691652884706854
batch 185 loss: 0.0005692043341696262
batch 190 loss: 0.0005691806203685701
batch 195 loss: 0.0005689964978955686
batch 200 loss: 0.0005688256467692554
batch 205 loss: 0.0005686997785232962
batch 210 loss: 0.0005686262622475625
batch 215 loss: 0.0005685550393536686
batch 220 loss: 0.0005684614065103233
batch 225 loss: 0.0005682360846549272
batch 230 loss: 0.0005681153503246606
batch 235 loss: 0.000568023871164769
batch 240 loss: 0.0005679469439201057
Training Loss: 0.0005784766937722452
Validation Loss: 0.0005670571971374254
Epoch 4:
batch 5 loss: 0.0005678276414982974
batch 10 loss: 0.0005677054403349757
batch 15 loss: 0.0005676163826137781
batch 20 loss: 0.0005674740998074413
batch 25 loss: 0.0005674752523191273
batch 30 loss: 0.0005674549844115973
batch 35 loss: 0.0005672626080922783
batch 40 loss: 0.000567098252940923
batch 45 loss: 0.000567212828900665
batch 50 loss: 0.0005671121529303491
batch 55 loss: 0.0005669591715559364
batch 60 loss: 0.0005669288337230682
batch 65 loss: 0.0005669712438248098
batch 70 loss: 0.0006113517796620726
batch 75 loss: 0.0006259893765673041
batch 80 loss: 0.0006089383154176175
batch 85 loss: 0.0006094485754147172
batch 90 loss: 0.0006248792051337659
batch 95 loss: 0.0007788771996274591
batch 100 loss: 0.0006120025762356817
batch 105 loss: 0.0006112087517976761
batch 110 loss: 0.0006102353218011558
batch 115 loss: 0.0006082084379158914
batch 120 loss: 0.0006059504812583327
batch 125 loss: 0.0006035848637111485
batch 130 loss: 0.0006011041579768062
batch 135 loss: 0.0005987329059280455
batch 140 loss: 0.0005964867305010557
batch 145 loss: 0.0005942683783359826
batch 150 loss: 0.000592105567920953
batch 155 loss: 0.0005901645286940038
batch 160 loss: 0.0005884982063435018
batch 165 loss: 0.0005868725362233818
batch 170 loss: 0.0005853624898009002
batch 175 loss: 0.000583988381549716
batch 180 loss: 0.0005827087094075978
batch 185 loss: 0.0005815434153191746
batch 190 loss: 0.0005803186562843621
batch 195 loss: 0.0005794902332127094
batch 200 loss: 0.0005785002722404897
batch 205 loss: 0.0005778795224614441
batch 210 loss: 0.0005769771290943026
batch 215 loss: 0.0005764145753346384
batch 220 loss: 0.0005757143837399781
batch 225 loss: 0.0005751956137828529
batch 230 loss: 0.0005745983333326876
batch 235 loss: 0.000573872122913599
batch 240 loss: 0.0005734918988309801
Training Loss: 0.0005897929693067757
Validation Loss: 0.000570141941231365
Epoch 5:
batch 5 loss: 0.0005727230571210384
batch 10 loss: 0.000572163809556514
batch 15 loss: 0.0005717231077142059
batch 20 loss: 0.0005710917292162776
batch 25 loss: 0.0005703878938220442
batch 30 loss: 0.0005698109394870699
batch 35 loss: 0.0005693705985322595
batch 40 loss: 0.0005689762532711029
batch 45 loss: 0.0005683862953446805
batch 50 loss: 0.0005681097856722773
batch 55 loss: 0.0005677512963302433
batch 60 loss: 0.0005673970095813275
batch 65 loss: 0.0005673091043718159
batch 70 loss: 0.0005670191603712738
batch 75 loss: 0.0005668742582201957
batch 80 loss: 0.0005667048040777445
batch 85 loss: 0.000566501182038337
batch 90 loss: 0.0005663812393322587
batch 95 loss: 0.0005663905641995371
batch 100 loss: 0.0005664750235155225
batch 105 loss: 0.0005668371682986617
batch 110 loss: 0.0005674011539667845
batch 115 loss: 0.0005670892307534814
batch 120 loss: 0.0005667610093951225
batch 125 loss: 0.0005665443139150738
batch 130 loss: 0.0005661175702698529
batch 135 loss: 0.000566121400333941
batch 140 loss: 0.0005658414214849472
batch 145 loss: 0.0005657213623635471
batch 150 loss: 0.0005656509194523097
batch 155 loss: 0.0005656376830302179
batch 160 loss: 0.0005657640867866576
batch 165 loss: 0.0005654695560224354
batch 170 loss: 0.0005656399764120579
batch 175 loss: 0.0005659163231030107
batch 180 loss: 0.0005656519904732704
batch 185 loss: 0.0005653859698213637
batch 190 loss: 0.0005653876345604658
batch 195 loss: 0.0005653569358401
batch 200 loss: 0.0005652071326039731
batch 205 loss: 0.0005649348720908165
batch 210 loss: 0.0005649462109431624
batch 215 loss: 0.0005649860599078238
batch 220 loss: 0.0005648343940265477
batch 225 loss: 0.0005648700753226876
batch 230 loss: 0.0005648098886013031
batch 235 loss: 0.0005649689119309187
batch 240 loss: 0.0005651970393955707
Training Loss: 0.0005668874458933715
Validation Loss: 0.0005640757697013517
Epoch 6:
batch 5 loss: 0.0005649864790029824
batch 10 loss: 0.0005646948819048702
batch 15 loss: 0.0005648110061883926
batch 20 loss: 0.0005646383855491876
batch 25 loss: 0.0005646431818604469
batch 30 loss: 0.0005644672783091664
batch 35 loss: 0.0005644493387080729
batch 40 loss: 0.0005645845900289715
batch 45 loss: 0.0005646587582305074
batch 50 loss: 0.0005647288053296506
batch 55 loss: 0.0005645856377668679
batch 60 loss: 0.0005644475924782455
batch 65 loss: 0.0005639668204821646
batch 70 loss: 0.0005641216062940657
batch 75 loss: 0.0005640671472065151
batch 80 loss: 0.0005641411989927292
batch 85 loss: 0.0005640461225993931
batch 90 loss: 0.0005640215938910842
batch 95 loss: 0.0005636613233946264
batch 100 loss: 0.000563876295927912
batch 105 loss: 0.0005639729904942214
batch 110 loss: 0.0005637878901325166
batch 115 loss: 0.0005636834190227092
batch 120 loss: 0.0005637717084027826
batch 125 loss: 0.0005635052570141852
batch 130 loss: 0.000563817797228694
batch 135 loss: 0.0005635866196826101
batch 140 loss: 0.0005637050373479724
batch 145 loss: 0.0005636092508211731
batch 150 loss: 0.0005635386449284851
batch 155 loss: 0.00056341189192608
batch 160 loss: 0.0005635833251290024
batch 165 loss: 0.0005635515204630792
batch 170 loss: 0.0005636339774355292
batch 175 loss: 0.0005636118352413177
batch 180 loss: 0.0005636596470139921
batch 185 loss: 0.000563661113847047
batch 190 loss: 0.0005635602050460875
batch 195 loss: 0.000563376909121871
batch 200 loss: 0.0005632934393361211
batch 205 loss: 0.000563434953801334
batch 210 loss: 0.0005632088985294103
batch 215 loss: 0.0005634612869471311
batch 220 loss: 0.0005634380737319589
batch 225 loss: 0.0005634490749798715
batch 230 loss: 0.0005634351167827844
batch 235 loss: 0.0005634780274704099
batch 240 loss: 0.0005632868385873735
Training Loss: 0.0005638981832210751
Validation Loss: 0.0005628325743600726
Epoch 7:
batch 5 loss: 0.0005634028580971062
batch 10 loss: 0.0005631834734231234
batch 15 loss: 0.0005632359185256064
batch 20 loss: 0.0005632587359286845
batch 25 loss: 0.0005633755587041378
batch 30 loss: 0.0005633936962112784
batch 35 loss: 0.0005633316352032125
batch 40 loss: 0.0005633386550471187
batch 45 loss: 0.0005636428249999881
batch 50 loss: 0.0005636022775433958
batch 55 loss: 0.0005640164948999882
batch 60 loss: 0.0005646239966154099
batch 65 loss: 0.0005640788585878909
batch 70 loss: 0.0005636421381495893
batch 75 loss: 0.0005635039298795164
batch 80 loss: 0.0005633937194943428
batch 85 loss: 0.0005630978033877909
batch 90 loss: 0.0005631498876027763
batch 95 loss: 0.000563068047631532
batch 100 loss: 0.0005631284555420279
batch 105 loss: 0.0005630851606838405
batch 110 loss: 0.0005631691426970064
batch 115 loss: 0.0005630873143672943
batch 120 loss: 0.0005630154511891306
batch 125 loss: 0.0005629826337099075
batch 130 loss: 0.0012806569691747427
batch 135 loss: 0.0010103177395649255
batch 140 loss: 0.0008383105741813779
batch 145 loss: 0.0007824045256711543
batch 150 loss: 0.0008340704487636685
batch 155 loss: 0.0007624057005159556
batch 160 loss: 0.0007050371845252812
batch 165 loss: 0.0006823027157224715
batch 170 loss: 0.0006711243884637951
batch 175 loss: 0.000641632592305541
batch 180 loss: 0.0006303741480223835
batch 185 loss: 0.0006289318320341408
batch 190 loss: 0.0006286256830208004
batch 195 loss: 0.0006286557298153638
batch 200 loss: 0.0006257333676330745
batch 205 loss: 0.000624449597671628
batch 210 loss: 0.0006235896493308246
batch 215 loss: 0.0006222258205525577
batch 220 loss: 0.0006192166008986532
batch 225 loss: 0.0006172566791065037
batch 230 loss: 0.0006141905323602259
batch 235 loss: 0.0006143056787550449
batch 240 loss: 0.0006116856704466045
Training Loss: 0.0006329648436803837
Validation Loss: 0.0006046368633784975
Epoch 8:
batch 5 loss: 0.0006107013439759612
batch 10 loss: 0.0006092446623370051
batch 15 loss: 0.0006088038557209074
batch 20 loss: 0.0006080882623791695
batch 25 loss: 0.0006075463141314685
batch 30 loss: 0.0006071187905035913
batch 35 loss: 0.0006067831884138287
batch 40 loss: 0.0006064620800316333
batch 45 loss: 0.0006068051792681217
batch 50 loss: 0.0006068811751902103
batch 55 loss: 0.0006072037969715894
batch 60 loss: 0.0006072040763683617
batch 65 loss: 0.0006075690849684179
batch 70 loss: 0.0006072471500374377
batch 75 loss: 0.0006063165958039463
batch 80 loss: 0.0006061456399038434
batch 85 loss: 0.0006062287371605635
batch 90 loss: 0.0006053144112229347
batch 95 loss: 0.0006055595935322344
batch 100 loss: 0.0006052901619113982
batch 105 loss: 0.0006041876506060362
batch 110 loss: 0.0006048425217159093
batch 115 loss: 0.0006037055398337543
batch 120 loss: 0.0006037453655153513
batch 125 loss: 0.0006037427461706102
batch 130 loss: 0.0006035069120116532
batch 135 loss: 0.0006032143253833055
batch 140 loss: 0.0006031505879946053
batch 145 loss: 0.0006028585834428668
batch 150 loss: 0.0006026342045515776
batch 155 loss: 0.0006021147011779249
batch 160 loss: 0.0006016115192323923
batch 165 loss: 0.000601264915894717
batch 170 loss: 0.000600957463029772
batch 175 loss: 0.0006005173432640732
batch 180 loss: 0.0006002152687869966
batch 185 loss: 0.0005996290361508727
batch 190 loss: 0.000599037844222039
batch 195 loss: 0.0005986959207803011
batch 200 loss: 0.0005979784531518817
batch 205 loss: 0.0005969948484562338
batch 210 loss: 0.0005963716423138977
batch 215 loss: 0.0005953598651103675
batch 220 loss: 0.0005944773321971298
batch 225 loss: 0.0005936573375947774
batch 230 loss: 0.0005928099970333278
batch 235 loss: 0.0005914870300330222
batch 240 loss: 0.000590563053265214
Training Loss: 0.000602746793932359
Validation Loss: 0.0005868048542955269
Epoch 9:
batch 5 loss: 0.0005894334753975272
batch 10 loss: 0.0005879797739908099
batch 15 loss: 0.0005868222797289491
batch 20 loss: 0.0005858700256794691
batch 25 loss: 0.0005856644362211228
batch 30 loss: 0.0005862520541995764
batch 35 loss: 0.0005873939488083124
batch 40 loss: 0.0005868691485375166
batch 45 loss: 0.0005863731494173408
batch 50 loss: 0.0005864294245839119
batch 55 loss: 0.0005857103504240512
batch 60 loss: 0.0005863781319931149
batch 65 loss: 0.0005853099515661597
batch 70 loss: 0.0005836116033606231
batch 75 loss: 0.0005837777978740632
batch 80 loss: 0.0005836382857523859
batch 85 loss: 0.0005835670512169599
batch 90 loss: 0.0005826503853313625
batch 95 loss: 0.0005825712694786489
batch 100 loss: 0.0005818171892315149
batch 105 loss: 0.0005798524129204452
batch 110 loss: 0.0005791799048893154
batch 115 loss: 0.0005783810396678746
batch 120 loss: 0.000578759633935988
batch 125 loss: 0.0005800325307063758
batch 130 loss: 0.0005795008619315922
batch 135 loss: 0.0005788843729533256
batch 140 loss: 0.0005776278791017831
batch 145 loss: 0.000575405522249639
batch 150 loss: 0.000573757034726441
batch 155 loss: 0.0005726477364078164
batch 160 loss: 0.0005729926633648574
batch 165 loss: 0.00057239307789132
batch 170 loss: 0.0005713333957828581
batch 175 loss: 0.0005712541402317584
batch 180 loss: 0.0005697301705367864
batch 185 loss: 0.0005688698613084853
batch 190 loss: 0.0005679605412296951
batch 195 loss: 0.0005679608671925962
batch 200 loss: 0.0005681156180799007
batch 205 loss: 0.0005676325527019799
batch 210 loss: 0.0005686319316737353
batch 215 loss: 0.0005674079526215792
batch 220 loss: 0.0005674008396454155
batch 225 loss: 0.0005671068909578025
batch 230 loss: 0.000567458220757544
batch 235 loss: 0.0005679658381268383
batch 240 loss: 0.0005676339613273739
Training Loss: 0.0005777916080357197
Validation Loss: 0.0005664072406943888
Epoch 10:
batch 5 loss: 0.0005671326187439262
batch 10 loss: 0.0005669430363923311
batch 15 loss: 0.0005669051199220121
batch 20 loss: 0.0005665091099217534
batch 25 loss: 0.0005670773214660584
batch 30 loss: 0.000566999870352447
batch 35 loss: 0.000567151780705899
batch 40 loss: 0.0005664975033141673
batch 45 loss: 0.0005669976235367358
batch 50 loss: 0.0005670058773830533
batch 55 loss: 0.0005658931564539671
batch 60 loss: 0.0005656425491906703
batch 65 loss: 0.0005655460641719401
batch 70 loss: 0.0005652748746797443
batch 75 loss: 0.0005652403109706938
batch 80 loss: 0.0005655385786667467
batch 85 loss: 0.0005661206087097526
batch 90 loss: 0.000566208513919264
batch 95 loss: 0.0005654572043567896
batch 100 loss: 0.0005651404149830342
batch 105 loss: 0.0005650951759889721
batch 110 loss: 0.0005656735156662762
batch 115 loss: 0.0005650274222716689
batch 120 loss: 0.0005649900413118303
batch 125 loss: 0.0005651746643707157
batch 130 loss: 0.0005650099250487983
batch 135 loss: 0.0005645582918077707
batch 140 loss: 0.0005640467163175345
batch 145 loss: 0.0005645754048600793
batch 150 loss: 0.0005643323878757655
batch 155 loss: 0.0005643525859341025
batch 160 loss: 0.0005635830806568265
batch 165 loss: 0.000564156926702708
batch 170 loss: 0.0005643019801937043
batch 175 loss: 0.0005647889222018421
batch 180 loss: 0.0005645011900924146
batch 185 loss: 0.0005641658091917634
batch 190 loss: 0.0005638122907839716
batch 195 loss: 0.0005635581677779555
batch 200 loss: 0.0005632745567709207
batch 205 loss: 0.0005634832312352955
batch 210 loss: 0.0005641835159622133
batch 215 loss: 0.0005639115581288934
batch 220 loss: 0.0005634408560581505
batch 225 loss: 0.0005631614476442337
batch 230 loss: 0.0005642951931804418
batch 235 loss: 0.0005638727103359997
batch 240 loss: 0.000563203718047589
Training Loss: 0.000565079446338738
Validation Loss: 0.0005623210483463482
Epoch 11:
batch 5 loss: 0.0005631467676721514
batch 10 loss: 0.0005632700864225626
batch 15 loss: 0.0005635954439640045
batch 20 loss: 0.0005640793824568391
batch 25 loss: 0.0005634916597045958
batch 30 loss: 0.0005633593304082751
batch 35 loss: 0.0005632486892864109
batch 40 loss: 0.000563224358484149
batch 45 loss: 0.000563877890817821
batch 50 loss: 0.0005649279570207
batch 55 loss: 0.0005645443336106837
batch 60 loss: 0.000563769310247153
batch 65 loss: 0.000563563487958163
batch 70 loss: 0.0005630252650007606
batch 75 loss: 0.0005625410121865571
batch 80 loss: 0.0005623811972327531
batch 85 loss: 0.0005623181466944515
batch 90 loss: 0.0005622842465527356
batch 95 loss: 0.0005624894867651165
batch 100 loss: 0.0005628702347166836
batch 105 loss: 0.0005623576580546796
batch 110 loss: 0.0005621309042908252
batch 115 loss: 0.0005619812058284879
batch 120 loss: 0.0005620668875053525
batch 125 loss: 0.0005621583433821798
batch 130 loss: 0.0005621335003525019
batch 135 loss: 0.0005624534911476076
batch 140 loss: 0.0005627000005915761
batch 145 loss: 0.0005633616936393082
batch 150 loss: 0.0005627733189612627
batch 155 loss: 0.0005621635587885976
batch 160 loss: 0.0005621863761916757
batch 165 loss: 0.0005623655742965639
batch 170 loss: 0.0005624725017696619
batch 175 loss: 0.0005627147736959159
batch 180 loss: 0.0005638299509882927
batch 185 loss: 0.0005632157204672694
batch 190 loss: 0.0005630252533592283
batch 195 loss: 0.0005625345394946635
batch 200 loss: 0.0005622499855235219
batch 205 loss: 0.0005622733267955482
batch 210 loss: 0.00056262391153723
batch 215 loss: 0.0005624640034511685
batch 220 loss: 0.0005627179867587984
batch 225 loss: 0.0005626218509860337
batch 230 loss: 0.0005625834106467664
batch 235 loss: 0.0005624449579045177
batch 240 loss: 0.000562438543420285
Training Loss: 0.0005628552399381685
Validation Loss: 0.0005620370153337717
Epoch 12:
batch 5 loss: 0.0005622467957437039
batch 10 loss: 0.0005620055017061532
batch 15 loss: 0.0005623214412480593
batch 20 loss: 0.0005628351238556207
batch 25 loss: 0.0005626103025861085
batch 30 loss: 0.0005625156336463988
batch 35 loss: 0.0005625947727821767
batch 40 loss: 0.0005622038850560784
batch 45 loss: 0.0005621807649731636
batch 50 loss: 0.0005622952710837125
batch 55 loss: 0.0005621321615763009
batch 60 loss: 0.0005621600546874105
batch 65 loss: 0.0005620819167234004
batch 70 loss: 0.000562025757972151
batch 75 loss: 0.0005616619950160384
batch 80 loss: 0.000561491318512708
batch 85 loss: 0.0005616832175292074
batch 90 loss: 0.0005617410875856877
batch 95 loss: 0.0005615921691060066
batch 100 loss: 0.0005614215275272727
batch 105 loss: 0.0005621552933007479
batch 110 loss: 0.0005617522518150508
batch 115 loss: 0.0005616477923467755
batch 120 loss: 0.0005618360009975731
batch 125 loss: 0.0005626342142932117
batch 130 loss: 0.0005629628081806004
batch 135 loss: 0.0005629410152323544
batch 140 loss: 0.0005628381622955203
batch 145 loss: 0.0005628446349874138
batch 150 loss: 0.0005624467274174094
batch 155 loss: 0.0005625264602713287
batch 160 loss: 0.0005623640143312513
batch 165 loss: 0.0005622062599286437
batch 170 loss: 0.0005623077857308089
batch 175 loss: 0.0005621262243948877
batch 180 loss: 0.0005620080977678299
batch 185 loss: 0.000562001473736018
batch 190 loss: 0.000561849179212004
batch 195 loss: 0.0005617734394036233
batch 200 loss: 0.0005620188312605023
batch 205 loss: 0.0005620663519948721
batch 210 loss: 0.0005617904709652067
batch 215 loss: 0.0005615580128505826
batch 220 loss: 0.0005618998431600631
batch 225 loss: 0.0005618546856567264
batch 230 loss: 0.0005615678499452769
batch 235 loss: 0.0005614264053292572
batch 240 loss: 0.0005612424109131097
Training Loss: 0.0005620926540965836
Validation Loss: 0.0005642667073213185
Epoch 13:
batch 5 loss: 0.0005631762323901057
batch 10 loss: 0.0005655591725371778
batch 15 loss: 0.0005640459596179426
batch 20 loss: 0.0005630896193906665
batch 25 loss: 0.0005627191974781453
batch 30 loss: 0.0005620832089334726
batch 35 loss: 0.0005618195980787277
batch 40 loss: 0.0005622236290946603
batch 45 loss: 0.0005621418240480125
batch 50 loss: 0.0005620007868856192
batch 55 loss: 0.0005619574920274317
batch 60 loss: 0.0005619087140075862
batch 65 loss: 0.0005617306451313198
batch 70 loss: 0.0005617842311039567
batch 75 loss: 0.0005616410286165774
batch 80 loss: 0.000561726524028927
batch 85 loss: 0.0005616741254925728
batch 90 loss: 0.0005614833440631628
batch 95 loss: 0.000561529747210443
batch 100 loss: 0.0005614631110802293
batch 105 loss: 0.0005619982141070068
batch 110 loss: 0.0005618292489089072
batch 115 loss: 0.0005617153365164995
batch 120 loss: 0.0005616741720587016
batch 125 loss: 0.000561985350213945
batch 130 loss: 0.0005618850118480623
batch 135 loss: 0.0005615217378363013
batch 140 loss: 0.0005614746944047511
batch 145 loss: 0.0005616161390207708
batch 150 loss: 0.0005615492933429778
batch 155 loss: 0.0005616104113869369
batch 160 loss: 0.000561467558145523
batch 165 loss: 0.0005615661968477071
batch 170 loss: 0.0005621423362754286
batch 175 loss: 0.0005620265728794038
batch 180 loss: 0.0005621066666208208
batch 185 loss: 0.0005616948707029224
batch 190 loss: 0.0005616434616968035
batch 195 loss: 0.0005615989095531404
batch 200 loss: 0.0005619929172098637
batch 205 loss: 0.000561812671367079
batch 210 loss: 0.0005615779664367437
batch 215 loss: 0.0005617393180727959
batch 220 loss: 0.0005615428788587451
batch 225 loss: 0.0005616774433292449
batch 230 loss: 0.000561642658431083
batch 235 loss: 0.0005618603434413671
batch 240 loss: 0.0005617893300950527
Training Loss: 0.0005619687479338608
Validation Loss: 0.0005576935004986201
Epoch 14:
batch 5 loss: 0.0005577417323365808
batch 10 loss: 0.00055770396720618
batch 15 loss: 0.0005577533040195704
batch 20 loss: 0.0005577758885920048
batch 25 loss: 0.0005576318013481796
batch 30 loss: 0.0005576919298619032
batch 35 loss: 0.0005576415802352131
batch 40 loss: 0.0005576456664130091
batch 45 loss: 0.0005576384137384593
batch 50 loss: 0.0005576585885137319
batch 55 loss: 0.0005575946881435812
batch 60 loss: 0.0005577616975642741
batch 65 loss: 0.0005577320349402726
batch 70 loss: 0.0005577622679993511
batch 75 loss: 0.0005575908231548965
batch 80 loss: 0.0005577012663707138
batch 85 loss: 0.000557637179736048
batch 90 loss: 0.0005577778560109436
batch 95 loss: 0.0005576475523412228
batch 100 loss: 0.0005578000098466873
batch 105 loss: 0.0005576781113632023
batch 110 loss: 0.0005577467032708228
batch 115 loss: 0.0005578804528340697
batch 120 loss: 0.0005577246891334652
batch 125 loss: 0.0005576871684752404
batch 130 loss: 0.0005575655726715922
batch 135 loss: 0.0005577492062002421
batch 140 loss: 0.0005576553754508496
batch 145 loss: 0.000557660055346787
batch 150 loss: 0.0005576216848567128
batch 155 loss: 0.0005576820811256766
batch 160 loss: 0.0005576884723268449
batch 165 loss: 0.0005577175877988338
batch 170 loss: 0.0005576630239374936
batch 175 loss: 0.0005577902426011861
batch 180 loss: 0.0005577518139034509
batch 185 loss: 0.0005576717434450984
batch 190 loss: 0.0005576818250119687
batch 195 loss: 0.0005576977273449302
batch 200 loss: 0.0005577037227340043
batch 205 loss: 0.0005576742347329855
batch 210 loss: 0.0005576703697443008
batch 215 loss: 0.0005576708004809916
batch 220 loss: 0.000557581742759794
batch 225 loss: 0.0005577148520387709
batch 230 loss: 0.0005576260271482169
batch 235 loss: 0.0005575662711635232
batch 240 loss: 0.0005575494025833905
Training Loss: 0.0005576866501845264
Validation Loss: 0.0005576934093066181
Epoch 15:
batch 5 loss: 0.0005576995550654829
batch 10 loss: 0.0005576220573857427
batch 15 loss: 0.0005576998693868518
batch 20 loss: 0.0005576763302087784
batch 25 loss: 0.0005577077041380108
batch 30 loss: 0.0005576403462328016
batch 35 loss: 0.0005576397641561925
batch 40 loss: 0.0005576422321610153
batch 45 loss: 0.0005576056661084295
batch 50 loss: 0.0005577339557930827
batch 55 loss: 0.0005577314179390669
batch 60 loss: 0.0005577536998316645
batch 65 loss: 0.0005576894734986126
batch 70 loss: 0.0005577363423071802
batch 75 loss: 0.0005576851544901729
batch 80 loss: 0.0005576121155172586
batch 85 loss: 0.0005576803581789136
batch 90 loss: 0.0005577002419158816
batch 95 loss: 0.0005576370982453227
batch 100 loss: 0.0005576732917688787
batch 105 loss: 0.0005575732211582363
batch 110 loss: 0.0005576806725002825
batch 115 loss: 0.0005576931987889111
batch 120 loss: 0.0005575980292633175
batch 125 loss: 0.0005577406380325555
batch 130 loss: 0.0005576943629421293
batch 135 loss: 0.0005576709401793778
batch 140 loss: 0.0005577907897531987
batch 145 loss: 0.0005577832227572799
batch 150 loss: 0.000557612522970885
batch 155 loss: 0.000557785970158875
batch 160 loss: 0.0005577717209234833
batch 165 loss: 0.0005576191004365682
batch 170 loss: 0.000557681790087372
batch 175 loss: 0.0005576870869845151
batch 180 loss: 0.0005576698807999491
batch 185 loss: 0.0005577064119279384
batch 190 loss: 0.0005576855503022671
batch 195 loss: 0.0005576559808105231
batch 200 loss: 0.0005577395437285304
batch 205 loss: 0.000557605572976172
batch 210 loss: 0.0005575713352300226
batch 215 loss: 0.000557803048286587
batch 220 loss: 0.000557671021670103
batch 225 loss: 0.0005576277966611087
batch 230 loss: 0.0005577170290052891
batch 235 loss: 0.0005576697178184987
batch 240 loss: 0.0005577334435656667
Training Loss: 0.0005576834640426872
Validation Loss: 0.0005576933996053413
Epoch 16:
batch 5 loss: 0.0005577485426329076
batch 10 loss: 0.0005575392628088593
batch 15 loss: 0.0005576870520599186
batch 20 loss: 0.0005576480645686388
batch 25 loss: 0.0005577112082391977
batch 30 loss: 0.0005577322444878519
batch 35 loss: 0.0005576142109930515
batch 40 loss: 0.0005577169125899672
batch 45 loss: 0.0005576519761234522
batch 50 loss: 0.0005577683448791503
batch 55 loss: 0.0005576873663812876
batch 60 loss: 0.0005576152238063514
batch 65 loss: 0.0005576997296884656
batch 70 loss: 0.0005577061674557626
batch 75 loss: 0.0005577963194809854
batch 80 loss: 0.0005578644806519151
batch 85 loss: 0.000557752710301429
batch 90 loss: 0.0005575355142354965
batch 95 loss: 0.0005577351432293654
batch 100 loss: 0.0005576926749199628
batch 105 loss: 0.0005576572613790632
batch 110 loss: 0.0005576191819272935
batch 115 loss: 0.00055764215067029
batch 120 loss: 0.0005577020696364343
batch 125 loss: 0.0005577335832640529
batch 130 loss: 0.0005577461211942137
batch 135 loss: 0.0005576781462877989
batch 140 loss: 0.0005576250143349171
batch 145 loss: 0.000557698158081621
batch 150 loss: 0.0005575764807872474
batch 155 loss: 0.000557680893689394
batch 160 loss: 0.0005576918134465814
batch 165 loss: 0.0005577860749326647
batch 170 loss: 0.0005576492403633893
batch 175 loss: 0.0005576518597081304
batch 180 loss: 0.0005576486699283123
batch 185 loss: 0.0005578479263931513
batch 190 loss: 0.0005577020230703056
batch 195 loss: 0.0005576653522439301
batch 200 loss: 0.0005576315452344716
batch 205 loss: 0.0005576515803113579
batch 210 loss: 0.0005576522089540958
batch 215 loss: 0.0005576929077506065
batch 220 loss: 0.000557741872034967
batch 225 loss: 0.000557655212469399
batch 230 loss: 0.0005576840601861476
batch 235 loss: 0.0005576001130975783
batch 240 loss: 0.0005575917311944067
Training Loss: 0.0005576834667105383
Validation Loss: 0.0005576934170676395
Epoch 17:
batch 5 loss: 0.0005575977498665452
batch 10 loss: 0.0005577488336712122
batch 15 loss: 0.0005577268078923225
batch 20 loss: 0.0005576457246206701
batch 25 loss: 0.0005576932220719754
batch 30 loss: 0.0005575580638833344
batch 35 loss: 0.0005577059229835868
batch 40 loss: 0.0005576011841185391
batch 45 loss: 0.000557500752620399
batch 50 loss: 0.0005577016272582114
batch 55 loss: 0.0005576983094215393
batch 60 loss: 0.000557703769300133
batch 65 loss: 0.0005577209405601025
batch 70 loss: 0.0005577189032919705
batch 75 loss: 0.0005576647818088531
batch 80 loss: 0.0005577322328463197
batch 85 loss: 0.0005577697767876089
batch 90 loss: 0.0005577962612733245
batch 95 loss: 0.0005577410105615854
batch 100 loss: 0.000557685422245413
batch 105 loss: 0.0005577542469836771
batch 110 loss: 0.000557632592972368
batch 115 loss: 0.0005576684256084263
batch 120 loss: 0.0005577924312092364
batch 125 loss: 0.0005577458767220377
batch 130 loss: 0.0005576617317274213
batch 135 loss: 0.0005577389034442604
batch 140 loss: 0.0005576334078796208
batch 145 loss: 0.0005576525465585292
batch 150 loss: 0.0005576904048211872
batch 155 loss: 0.0005575913935899734
batch 160 loss: 0.0005577201955020428
batch 165 loss: 0.0005576469236984849
batch 170 loss: 0.0005575941409915686
batch 175 loss: 0.000557737797498703
batch 180 loss: 0.0005577651085332036
batch 185 loss: 0.0005577132105827332
batch 190 loss: 0.0005577269359491766
batch 195 loss: 0.000557660055346787
batch 200 loss: 0.0005575662944465876
batch 205 loss: 0.0005576041177846492
batch 210 loss: 0.0005577088450081646
batch 215 loss: 0.0005576551426202059
batch 220 loss: 0.0005576597293838858
batch 225 loss: 0.0005577676114626229
batch 230 loss: 0.0005577599513344467
batch 235 loss: 0.0005576291587203741
batch 240 loss: 0.0005576160736382007
Training Loss: 0.0005576834281479629
Validation Loss: 0.000557693464603896
Epoch 18:
batch 5 loss: 0.0005576947703957557
batch 10 loss: 0.0005576847586780786
batch 15 loss: 0.0005577251780778169
batch 20 loss: 0.000557797949295491
batch 25 loss: 0.0005576573545113206
batch 30 loss: 0.0005577081814408302
batch 35 loss: 0.0005576792173087597
batch 40 loss: 0.0005576800322160125
batch 45 loss: 0.0005578109761700034
batch 50 loss: 0.0005575961200520396
batch 55 loss: 0.0005577022209763526
batch 60 loss: 0.0005577419651672244
batch 65 loss: 0.0005576847121119499
batch 70 loss: 0.0005577654577791691
batch 75 loss: 0.0005577184841968119
batch 80 loss: 0.0005575922201387584
batch 85 loss: 0.0005576781230047345
batch 90 loss: 0.0005577141419053078
batch 95 loss: 0.0005576605559326709
batch 100 loss: 0.0005576331750489771
batch 105 loss: 0.0005576673080213368
batch 110 loss: 0.0005576254683546722
batch 115 loss: 0.0005577271687798202
batch 120 loss: 0.0005577216157689691
batch 125 loss: 0.0005575552117079496
batch 130 loss: 0.0005576209397986532
batch 135 loss: 0.0005576836527325213
batch 140 loss: 0.0005575806950218976
batch 145 loss: 0.0005575284361839294
batch 150 loss: 0.0005577844567596912
batch 155 loss: 0.0005576979951001704
batch 160 loss: 0.0005577124655246734
batch 165 loss: 0.0005576971336267889
batch 170 loss: 0.0005577418371103704
batch 175 loss: 0.0005577391129918396
batch 180 loss: 0.0005576504510827363
batch 185 loss: 0.000557590916287154
batch 190 loss: 0.0005576802417635917
batch 195 loss: 0.0005576242110691965
batch 200 loss: 0.0005577218718826771
batch 205 loss: 0.0005576436291448772
batch 210 loss: 0.0005576745024882257
batch 215 loss: 0.0005577428382821381
batch 220 loss: 0.0005577999167144298
batch 225 loss: 0.0005576306954026222
batch 230 loss: 0.0005576832918450236
batch 235 loss: 0.0005576029187068343
batch 240 loss: 0.000557749334257096
Training Loss: 0.0005576834148087074
Validation Loss: 0.0005576941883191466
Epoch 19:
batch 5 loss: 0.0005576434195972979
batch 10 loss: 0.0005577582982368767
batch 15 loss: 0.0005577159696258605
batch 20 loss: 0.0005576208466663956
batch 25 loss: 0.0005577794159762562
batch 30 loss: 0.0005576046532951295
batch 35 loss: 0.0005576617666520178
batch 40 loss: 0.0005577761796303094
batch 45 loss: 0.0005576584371738136
batch 50 loss: 0.000557688542176038
batch 55 loss: 0.000557843258138746
batch 60 loss: 0.0005577947595156729
batch 65 loss: 0.0005576803465373814
batch 70 loss: 0.0005576968775130809
batch 75 loss: 0.0005576205207034945
batch 80 loss: 0.0005576548399403691
batch 85 loss: 0.0005577544565312564
batch 90 loss: 0.000557589519303292
batch 95 loss: 0.0005577233037911356
batch 100 loss: 0.0005576003924943507
batch 105 loss: 0.0005577422678470611
batch 110 loss: 0.0005576489260420203
batch 115 loss: 0.0005576370749622583
batch 120 loss: 0.0005576940253376961
batch 125 loss: 0.0005576706957072019
batch 130 loss: 0.0005575611488893628
batch 135 loss: 0.0005577603238634765
batch 140 loss: 0.0005577047471888363
batch 145 loss: 0.0005576752591878176
batch 150 loss: 0.0005578187061473727
batch 155 loss: 0.0005578274489380419
batch 160 loss: 0.0005576006253249944
batch 165 loss: 0.0005576456780545414
batch 170 loss: 0.0005577107775025069
batch 175 loss: 0.0005576965631917119
batch 180 loss: 0.0005576161784119904
batch 185 loss: 0.00055775175569579
batch 190 loss: 0.0005576022318564356
batch 195 loss: 0.0005577143165282905
batch 200 loss: 0.0005576165742240847
batch 205 loss: 0.0005576551426202059
batch 210 loss: 0.0005576738738454878
batch 215 loss: 0.0005576682859100401
batch 220 loss: 0.0005576685420237481
batch 225 loss: 0.0005577004631049931
batch 230 loss: 0.0005576596944592893
batch 235 loss: 0.0005576516734436155
batch 240 loss: 0.0005575645016506314
Training Loss: 0.0005576834021970475
Validation Loss: 0.0005576935043791309
Epoch 20:
batch 5 loss: 0.0005575856193900108
batch 10 loss: 0.0005576628376729786
batch 15 loss: 0.000557578180450946
batch 20 loss: 0.0005577134783379733
batch 25 loss: 0.0005575390649028122
batch 30 loss: 0.0005576626863330603
batch 35 loss: 0.0005577001953497529
batch 40 loss: 0.0005576245486736298
batch 45 loss: 0.0005576127674430609
batch 50 loss: 0.0005576478200964629
batch 55 loss: 0.0005576797644607723
batch 60 loss: 0.0005577178439125418
batch 65 loss: 0.0005576512427069247
batch 70 loss: 0.000557739962823689
batch 75 loss: 0.000557678088080138
batch 80 loss: 0.0005577866919338703
batch 85 loss: 0.0005577628500759602
batch 90 loss: 0.0005575409159064293
batch 95 loss: 0.0005577471805736422
batch 100 loss: 0.000557636097073555
batch 105 loss: 0.000557736272457987
batch 110 loss: 0.0005577083211392164
batch 115 loss: 0.0005577598232775927
batch 120 loss: 0.0005575539777055383
batch 125 loss: 0.0005576950963586569
batch 130 loss: 0.0005576260969974101
batch 135 loss: 0.000557633291464299
batch 140 loss: 0.0005577879608608782
batch 145 loss: 0.0005576578667387366
batch 150 loss: 0.0005576562834903598
batch 155 loss: 0.0005577565170824528
batch 160 loss: 0.0005577408708631992
batch 165 loss: 0.0005577663891017437
batch 170 loss: 0.0005577035248279571
batch 175 loss: 0.0005576360039412975
batch 180 loss: 0.0005576364463195204
batch 185 loss: 0.0005577829200774431
batch 190 loss: 0.0005576843046583235
batch 195 loss: 0.0005575484479777515
batch 200 loss: 0.0005576088442467153
batch 205 loss: 0.0005576872266829014
batch 210 loss: 0.000557730149012059
batch 215 loss: 0.0005577258416451514
batch 220 loss: 0.0005578720360063016
batch 225 loss: 0.0005577390897087753
batch 230 loss: 0.0005577928735874593
batch 235 loss: 0.0005576146417297423
batch 240 loss: 0.0005576938739977777
Training Loss: 0.0005576833922532387
Validation Loss: 0.0005576934180377672
Epoch 21:
batch 5 loss: 0.0005577467498369515
batch 10 loss: 0.0005576873081736267
batch 15 loss: 0.0005577372736297548
batch 20 loss: 0.0005576048395596444
batch 25 loss: 0.0005576589377596974
batch 30 loss: 0.0005576654453761876
batch 35 loss: 0.0005576292634941638
batch 40 loss: 0.0005577181931585073
batch 45 loss: 0.0005577034782618285
batch 50 loss: 0.0005576867028139531
batch 55 loss: 0.0005576863302849233
batch 60 loss: 0.0005577654694207013
batch 65 loss: 0.0005577884381636977
batch 70 loss: 0.0005576466559432447
batch 75 loss: 0.0005578045966103673
batch 80 loss: 0.0005576747003942728
batch 85 loss: 0.0005576647818088531
batch 90 loss: 0.0005577886360697449
batch 95 loss: 0.0005576253519393503
batch 100 loss: 0.0005575777380727231
batch 105 loss: 0.0005577060393989087
batch 110 loss: 0.0005576822557486593
batch 115 loss: 0.000557625584769994
batch 120 loss: 0.0005576927214860916
batch 125 loss: 0.0005576214171014726
batch 130 loss: 0.0005577327217906713
batch 135 loss: 0.0005577072617597878
batch 140 loss: 0.0005576574825681746
batch 145 loss: 0.0005577017203904688
batch 150 loss: 0.0005576927680522204
batch 155 loss: 0.0005577107775025069
batch 160 loss: 0.000557772780302912
batch 165 loss: 0.000557799416128546
batch 170 loss: 0.0005576799274422228
batch 175 loss: 0.0005577316624112427
batch 180 loss: 0.0005577780772000551
batch 185 loss: 0.0005576360737904907
batch 190 loss: 0.0005575772491283714
batch 195 loss: 0.00055766113800928
batch 200 loss: 0.0005575924180448056
batch 205 loss: 0.0005576436291448772
batch 210 loss: 0.0005575939663685858
batch 215 loss: 0.0005576768075115979
batch 220 loss: 0.0005576583789661527
batch 225 loss: 0.0005576043855398894
batch 230 loss: 0.0005577301024459302
batch 235 loss: 0.0005577484960667789
batch 240 loss: 0.0005575265386141837
Training Loss: 0.0005576833893428557
Validation Loss: 0.0005576934539324915
Epoch 22:
batch 5 loss: 0.0005576354102231562
batch 10 loss: 0.0005577389732934534
batch 15 loss: 0.0005576966679655016
batch 20 loss: 0.0005576814175583423
batch 25 loss: 0.0005576763884164393
batch 30 loss: 0.0005576829076744616
batch 35 loss: 0.0005576243158429862
batch 40 loss: 0.0005577037576586008
batch 45 loss: 0.0005577255971729756
batch 50 loss: 0.0005577200325205922
batch 55 loss: 0.0005576391820795834
batch 60 loss: 0.0005576709751039744
batch 65 loss: 0.0005577221396379173
batch 70 loss: 0.0005576365045271813
batch 75 loss: 0.0005576792755164206
batch 80 loss: 0.0005576328607276082
batch 85 loss: 0.0005575757240876555
batch 90 loss: 0.0005576845491304993
batch 95 loss: 0.0005577321746386588
batch 100 loss: 0.000557656493037939
batch 105 loss: 0.0005576788564212621
batch 110 loss: 0.0005577593459747731
batch 115 loss: 0.0005577230593189597
batch 120 loss: 0.0005576114170253276
batch 125 loss: 0.0005577113362960518
batch 130 loss: 0.0005576011724770069
batch 135 loss: 0.0005577643169090152
batch 140 loss: 0.0005576110328547656
batch 145 loss: 0.0005577200441621244
batch 150 loss: 0.0005575956893153489
batch 155 loss: 0.0005577815929427743
batch 160 loss: 0.0005577267147600651
batch 165 loss: 0.0005577229778282345
batch 170 loss: 0.0005577136995270848
batch 175 loss: 0.0005577313248068094
batch 180 loss: 0.0005576886469498277
batch 185 loss: 0.0005576981347985566
batch 190 loss: 0.0005577206960879266
batch 195 loss: 0.0005576499388553202
batch 200 loss: 0.0005577011615969241
batch 205 loss: 0.0005576497525908053
batch 210 loss: 0.0005577760166488588
batch 215 loss: 0.0005576222669333219
batch 220 loss: 0.0005576475756242872
batch 225 loss: 0.0005576672963798046
batch 230 loss: 0.000557691475842148
batch 235 loss: 0.0005576489376835525
batch 240 loss: 0.0005576730240136385
Training Loss: 0.0005576833927383025
Validation Loss: 0.0005576934228884057
Epoch 23:
batch 5 loss: 0.0005576391820795834
batch 10 loss: 0.0005576597759500146
batch 15 loss: 0.0005576512077823282
batch 20 loss: 0.0005575785064138473
batch 25 loss: 0.0005577895324677228
batch 30 loss: 0.0005576997995376587
batch 35 loss: 0.0005577669362537563
batch 40 loss: 0.0005575906950980424
batch 45 loss: 0.0005577853298746049
batch 50 loss: 0.0005576368188485503
batch 55 loss: 0.0005576176568865776
batch 60 loss: 0.0005575621500611306
batch 65 loss: 0.0005576547235250473
batch 70 loss: 0.0005576675059273839
batch 75 loss: 0.0005577218951657414
batch 80 loss: 0.0005576450494118035
batch 85 loss: 0.0005576956551522017
batch 90 loss: 0.0005576586700044572
batch 95 loss: 0.0005577342002652585
batch 100 loss: 0.0005576530587859452
batch 105 loss: 0.0005576524999924004
batch 110 loss: 0.0005577118135988712
batch 115 loss: 0.0005576376803219319
batch 120 loss: 0.0005576142342761159
batch 125 loss: 0.0005575728137046099
batch 130 loss: 0.0005576588097028434
batch 135 loss: 0.0005577412550337613
batch 140 loss: 0.0005577118834480643
batch 145 loss: 0.0005576915922574699
batch 150 loss: 0.0005577764590270817
batch 155 loss: 0.0005576590774580836
batch 160 loss: 0.0005577543750405311
batch 165 loss: 0.0005577547708526254
batch 170 loss: 0.0005577235482633114
batch 175 loss: 0.0005576739553362131
batch 180 loss: 0.0005577017320320011
batch 185 loss: 0.0005575947696343064
batch 190 loss: 0.0005576819879934191
batch 195 loss: 0.0005577652482315898
batch 200 loss: 0.0005578113137744367
batch 205 loss: 0.0005577246076427401
batch 210 loss: 0.0005577760166488588
batch 215 loss: 0.0005576788797043263
batch 220 loss: 0.0005575273418799043
batch 225 loss: 0.000557681021746248
batch 230 loss: 0.0005577085888944566
batch 235 loss: 0.0005577077623456717
batch 240 loss: 0.0005577007308602333
Training Loss: 0.0005576833983165368
Validation Loss: 0.0005576934160975119
Epoch 24:
batch 5 loss: 0.0005576051538810134
batch 10 loss: 0.0005577182397246361
batch 15 loss: 0.0005576268304139376
batch 20 loss: 0.0005576955270953476
batch 25 loss: 0.0005577529780566692
batch 30 loss: 0.0005576367489993572
batch 35 loss: 0.000557669042609632
batch 40 loss: 0.0005576999974437058
batch 45 loss: 0.0005576887400820851
batch 50 loss: 0.0005576806026510895
batch 55 loss: 0.0005576876807026565
batch 60 loss: 0.0005578532698564231
batch 65 loss: 0.0005577507195994258
batch 70 loss: 0.000557643047068268
batch 75 loss: 0.0005577304633334279
batch 80 loss: 0.0005577675183303654
batch 85 loss: 0.0005577835254371166
batch 90 loss: 0.0005576644325628877
batch 95 loss: 0.0005576955969445408
batch 100 loss: 0.0005577103933319449
batch 105 loss: 0.0005576041527092457
batch 110 loss: 0.0005576101597398519
batch 115 loss: 0.0005576816853135824
batch 120 loss: 0.000557624630164355
batch 125 loss: 0.0005577577976509929
batch 130 loss: 0.0005576985189691186
batch 135 loss: 0.0005577674484811724
batch 140 loss: 0.0005576240713708102
batch 145 loss: 0.0005577202420681715
batch 150 loss: 0.0005576681694947183
batch 155 loss: 0.0005577335483394563
batch 160 loss: 0.0005576123599894345
batch 165 loss: 0.0005576828611083329
batch 170 loss: 0.0005576319294050336
batch 175 loss: 0.000557732058223337
batch 180 loss: 0.0005576526862569153
batch 185 loss: 0.000557786610443145
batch 190 loss: 0.0005576125462539494
batch 195 loss: 0.0005577060393989087
batch 200 loss: 0.0005576979951001704
batch 205 loss: 0.0005576705909334124
batch 210 loss: 0.0005575422430410982
batch 215 loss: 0.0005575449671596288
batch 220 loss: 0.0005576470284722745
batch 225 loss: 0.0005576881929300725
batch 230 loss: 0.0005577229196205736
batch 235 loss: 0.0005576732335612177
batch 240 loss: 0.0005576477269642055
Training Loss: 0.000557683394193494
Validation Loss: 0.0005576933957248306
Epoch 25:
batch 5 loss: 0.0005577103816904128
batch 10 loss: 0.0005576246534474194
batch 15 loss: 0.0005577164934948087
batch 20 loss: 0.0005578076350502669
batch 25 loss: 0.00055758518865332
batch 30 loss: 0.0005577158299274743
batch 35 loss: 0.0005576353170908988
batch 40 loss: 0.0005576653056778014
batch 45 loss: 0.0005576701136305928
batch 50 loss: 0.0005577309057116508
batch 55 loss: 0.0005577589268796146
batch 60 loss: 0.000557709822896868
batch 65 loss: 0.0005575976218096911
batch 70 loss: 0.0005576297175139189
batch 75 loss: 0.0005577218485996127
batch 80 loss: 0.0005576333729550243
batch 85 loss: 0.0005576802068389952
batch 90 loss: 0.0005576304625719786
batch 95 loss: 0.0005576280178502202
batch 100 loss: 0.0005577161558903754
batch 105 loss: 0.0005575909279286861
batch 110 loss: 0.0005577103467658163
batch 115 loss: 0.0005577714764513075
batch 120 loss: 0.000557678984478116
batch 125 loss: 0.0005576582974754274
batch 130 loss: 0.0005577715928666294
batch 135 loss: 0.0005577392876148224
batch 140 loss: 0.0005577759351581335
batch 145 loss: 0.0005577546311542392
batch 150 loss: 0.0005576668540015816
batch 155 loss: 0.0005576738971285522
batch 160 loss: 0.0005576183088123798
batch 165 loss: 0.000557786924764514
batch 170 loss: 0.0005577288917265833
batch 175 loss: 0.000557691534049809
batch 180 loss: 0.0005576148978434503
batch 185 loss: 0.0005576375755481422
batch 190 loss: 0.0005576266208663583
batch 195 loss: 0.0005576090305112302
batch 200 loss: 0.0005576018011197448
batch 205 loss: 0.000557651708368212
batch 210 loss: 0.0005577197880484164
batch 215 loss: 0.0005577401840128005
batch 220 loss: 0.0005576408235356212
batch 225 loss: 0.0005576739786192775
batch 230 loss: 0.0005577269592322409
batch 235 loss: 0.0005577030940912664
batch 240 loss: 0.0005576706142164766
Training Loss: 0.000557683394678558
Validation Loss: 0.0005576934199780226
Epoch 26:
batch 5 loss: 0.0005576916038990021
batch 10 loss: 0.0005576660274527967
batch 15 loss: 0.0005576847936026752
batch 20 loss: 0.0005576438154093922
batch 25 loss: 0.0005577950039878487
batch 30 loss: 0.0005577119998633861
batch 35 loss: 0.0005577139789238572
batch 40 loss: 0.0005575871909968555
batch 45 loss: 0.0005577705916948617
batch 50 loss: 0.0005576990894041956
batch 55 loss: 0.0005576966097578407
batch 60 loss: 0.0005576444789767265
batch 65 loss: 0.0005577067611739039
batch 70 loss: 0.0005577010568231344
batch 75 loss: 0.0005577340489253402
batch 80 loss: 0.0005576872616074979
batch 85 loss: 0.0005576176103204489
batch 90 loss: 0.0005577024188823998
batch 95 loss: 0.0005576895899139344
batch 100 loss: 0.0005577163305133581
batch 105 loss: 0.0005576622439548373
batch 110 loss: 0.0005576312192715705
batch 115 loss: 0.0005577033152803779
batch 120 loss: 0.0005576739553362131
batch 125 loss: 0.0005576872383244336
batch 130 loss: 0.0005577090079896152
batch 135 loss: 0.0005577329080551863
batch 140 loss: 0.0005576995317824185
batch 145 loss: 0.0005576861440204084
batch 150 loss: 0.0005576370051130653
batch 155 loss: 0.0005577067960985005
batch 160 loss: 0.0005577563191764056
batch 165 loss: 0.0005576984956860542
batch 170 loss: 0.0005577740375883877
batch 175 loss: 0.0005576862953603267
batch 180 loss: 0.0005576570751145482
batch 185 loss: 0.0005576722323894501
batch 190 loss: 0.0005575777147896587
batch 195 loss: 0.000557626411318779
batch 200 loss: 0.0005576552823185921
batch 205 loss: 0.0005577257717959583
batch 210 loss: 0.0005575675400905311
batch 215 loss: 0.0005577301257289946
batch 220 loss: 0.0005576161318458616
batch 225 loss: 0.0005576430354267359
batch 230 loss: 0.0005576894269324839
batch 235 loss: 0.0005576334544457496
batch 240 loss: 0.0005577041883952916
Training Loss: 0.0005576833992866644
Validation Loss: 0.0005576934316195547
Epoch 27:
batch 5 loss: 0.000557732896413654
batch 10 loss: 0.0005576546071097254
batch 15 loss: 0.0005576819065026938
batch 20 loss: 0.0005576768657192588
batch 25 loss: 0.0005576964002102613
batch 30 loss: 0.0005576992756687104
batch 35 loss: 0.000557781383395195
batch 40 loss: 0.0005577630596235395
batch 45 loss: 0.0005576554802246391
batch 50 loss: 0.0005576932337135077
batch 55 loss: 0.0005576153984293341
batch 60 loss: 0.0005577031872235239
batch 65 loss: 0.0005576787516474724
batch 70 loss: 0.0005577561096288264
batch 75 loss: 0.0005576108349487185
batch 80 loss: 0.0005576916388235986
batch 85 loss: 0.0005576703231781721
batch 90 loss: 0.0005575957824476063
batch 95 loss: 0.0005577086703851819
batch 100 loss: 0.00055771250044927
batch 105 loss: 0.0005577454692684114
batch 110 loss: 0.0005576449912041426
batch 115 loss: 0.0005576356081292034
batch 120 loss: 0.0005576763302087784
batch 125 loss: 0.0005577015806920826
batch 130 loss: 0.0005576477735303343
batch 135 loss: 0.0005576676339842379
batch 140 loss: 0.0005576634663157165
batch 145 loss: 0.000557700905483216
batch 150 loss: 0.0005575683084316551
batch 155 loss: 0.0005576674127951264
batch 160 loss: 0.000557796738576144
batch 165 loss: 0.000557695934548974
batch 170 loss: 0.0005577136413194239
batch 175 loss: 0.0005576728377491236
batch 180 loss: 0.0005577351781539619
batch 185 loss: 0.0005578510463237762
batch 190 loss: 0.0005576746654696763
batch 195 loss: 0.0005576966213993728
batch 200 loss: 0.0005575965624302626
batch 205 loss: 0.000557621568441391
batch 210 loss: 0.0005577380536124111
batch 215 loss: 0.0005576964467763901
batch 220 loss: 0.0005576631287112832
batch 225 loss: 0.0005576596362516284
batch 230 loss: 0.0005577073083259165
batch 235 loss: 0.0005575979710556566
batch 240 loss: 0.0005575879942625761
Training Loss: 0.0005576833983165368
Validation Loss: 0.0005576934267689164
Epoch 28:
batch 5 loss: 0.0005577335716225207
batch 10 loss: 0.0005576380994170904
batch 15 loss: 0.0005577178555540741
batch 20 loss: 0.0005576294031925499
batch 25 loss: 0.00055775863584131
batch 30 loss: 0.0005577170522883534
batch 35 loss: 0.0005576227675192058
batch 40 loss: 0.0005576952244155109
batch 45 loss: 0.0005576801137067378
batch 50 loss: 0.0005576130351983011
batch 55 loss: 0.0005577480536885559
batch 60 loss: 0.0005576530937105417
batch 65 loss: 0.0005577094852924347
batch 70 loss: 0.0005577237927354873
batch 75 loss: 0.0005577246192842722
batch 80 loss: 0.0005576233030296862
batch 85 loss: 0.0005577736999839544
batch 90 loss: 0.0005577514530159533
batch 95 loss: 0.0005576420458965004
batch 100 loss: 0.0005576620693318546
batch 105 loss: 0.0005576621857471764
batch 110 loss: 0.0005576737807132303
batch 115 loss: 0.0005575434304773807
batch 120 loss: 0.0005576278432272375
batch 125 loss: 0.0005577011383138597
batch 130 loss: 0.0005576642928645015
batch 135 loss: 0.0005576384253799915
batch 140 loss: 0.0005575611488893628
batch 145 loss: 0.0005576051888056099
batch 150 loss: 0.0005577420000918209
batch 155 loss: 0.0005577582633122802
batch 160 loss: 0.0005577977397479116
batch 165 loss: 0.0005577777163125574
batch 170 loss: 0.0005576985306106508
batch 175 loss: 0.0005578320939093828
batch 180 loss: 0.0005576935014687479
batch 185 loss: 0.0005576874013058841
batch 190 loss: 0.0005578249925747514
batch 195 loss: 0.0005576301249675452
batch 200 loss: 0.000557600671891123
batch 205 loss: 0.0005576112424023449
batch 210 loss: 0.0005575545248575509
batch 215 loss: 0.0005577203235588967
batch 220 loss: 0.000557608250528574
batch 225 loss: 0.0005576892173849047
batch 230 loss: 0.0005577064002864063
batch 235 loss: 0.0005576604744419456
batch 240 loss: 0.0005577159929089249
Training Loss: 0.0005576834223271969
Validation Loss: 0.0005576934044559796
Epoch 29:
batch 5 loss: 0.0005575863062404097
batch 10 loss: 0.0005577255506068468
batch 15 loss: 0.0005577534204348922
batch 20 loss: 0.000557762139942497
batch 25 loss: 0.000557641068007797
batch 30 loss: 0.0005576894385740161
batch 35 loss: 0.0005576745374128222
batch 40 loss: 0.0005577024654485285
batch 45 loss: 0.0005576772149652242
batch 50 loss: 0.000557708484120667
batch 55 loss: 0.0005576599622145295
batch 60 loss: 0.0005577322444878519
batch 65 loss: 0.0005577014875598252
batch 70 loss: 0.0005576399620622396
batch 75 loss: 0.0005576816271059215
batch 80 loss: 0.0005576217081397772
batch 85 loss: 0.0005576935829594732
batch 90 loss: 0.0005576451425440609
batch 95 loss: 0.0005576388561166823
batch 100 loss: 0.0005577241070568561
batch 105 loss: 0.0005576807423494756
batch 110 loss: 0.0005576920113526285
batch 115 loss: 0.0005576496361754835
batch 120 loss: 0.0005577586241997778
batch 125 loss: 0.0005577081697992981
batch 130 loss: 0.0005576798925176263
batch 135 loss: 0.0005575876217335463
batch 140 loss: 0.0005576759576797485
batch 145 loss: 0.0005576169700361788
batch 150 loss: 0.0005578183918260038
batch 155 loss: 0.0005576743744313717
batch 160 loss: 0.000557667890097946
batch 165 loss: 0.0005575750023126602
batch 170 loss: 0.0005575835821218789
batch 175 loss: 0.0005576560273766517
batch 180 loss: 0.0005576700554229319
batch 185 loss: 0.0005576920695602894
batch 190 loss: 0.0005577660747803747
batch 195 loss: 0.0005577732459641993
batch 200 loss: 0.0005578653304837644
batch 205 loss: 0.0005575921619310975
batch 210 loss: 0.000557683315128088
batch 215 loss: 0.0005575940478593111
batch 220 loss: 0.0005577060277573764
batch 225 loss: 0.0005576247116550803
batch 230 loss: 0.0005577281815931201
batch 235 loss: 0.0005577432457357645
batch 240 loss: 0.0005576816387474537
Training Loss: 0.0005576834230547926
Validation Loss: 0.0005576934519922361
Epoch 30:
batch 5 loss: 0.0005576389958150685
batch 10 loss: 0.0005577077623456717
batch 15 loss: 0.000557668344117701
batch 20 loss: 0.0005576437339186669
batch 25 loss: 0.0005576962954364717
batch 30 loss: 0.0005576625815592706
batch 35 loss: 0.0005575841758400201
batch 40 loss: 0.0005577131174504757
batch 45 loss: 0.0005576017661951483
batch 50 loss: 0.0005577777745202183
batch 55 loss: 0.0005576763651333749
batch 60 loss: 0.0005576255498453975
batch 65 loss: 0.0005576785071752966
batch 70 loss: 0.0005576674942858517
batch 75 loss: 0.0005576749215833842
batch 80 loss: 0.0005577459814958274
batch 85 loss: 0.0005576762487180531
batch 90 loss: 0.0005576600902713835
batch 95 loss: 0.0005576113355346024
batch 100 loss: 0.0005575165036134422
batch 105 loss: 0.000557763164397329
batch 110 loss: 0.0005575929535552859
batch 115 loss: 0.0005577586707659066
batch 120 loss: 0.0005576362367719412
batch 125 loss: 0.0005577055388130247
batch 130 loss: 0.0005577174481004477
batch 135 loss: 0.0005578216048888862
batch 140 loss: 0.0005577641655690968
batch 145 loss: 0.0005578523385338485
batch 150 loss: 0.0005576954688876867
batch 155 loss: 0.0005577101488597691
batch 160 loss: 0.0005576476687565446
batch 165 loss: 0.000557653175201267
batch 170 loss: 0.000557701475918293
batch 175 loss: 0.000557680451311171
batch 180 loss: 0.0005576573312282562
batch 185 loss: 0.000557696574833244
batch 190 loss: 0.0005577015108428895
batch 195 loss: 0.0005576158408075571
batch 200 loss: 0.0005576665862463415
batch 205 loss: 0.0005577703239396214
batch 210 loss: 0.0005577400792390108
batch 215 loss: 0.0005577136762440205
batch 220 loss: 0.0005575987859629094
batch 225 loss: 0.0005576823372393846
batch 230 loss: 0.0005575918243266642
batch 235 loss: 0.0005577843636274338
batch 240 loss: 0.000557656167075038
Training Loss: 0.0005576834053499624
Validation Loss: 0.0005576934122170011
Epoch 31:
batch 5 loss: 0.0005576444673351943
batch 10 loss: 0.0005577268660999834
batch 15 loss: 0.0005575873656198382
batch 20 loss: 0.0005577534786425531
batch 25 loss: 0.0005577364703640342
batch 30 loss: 0.0005577257368713617
batch 35 loss: 0.0005576694267801941
batch 40 loss: 0.0005577412550337613
batch 45 loss: 0.0005577707197517157
batch 50 loss: 0.0005577130825258792
batch 55 loss: 0.0005577738396823406
batch 60 loss: 0.0005577768664807081
batch 65 loss: 0.0005575994960963726
batch 70 loss: 0.0005577607895247638
batch 75 loss: 0.0005576934549026191
batch 80 loss: 0.0005576775525696576
batch 85 loss: 0.0005577008007094264
batch 90 loss: 0.0005577075760811567
batch 95 loss: 0.0005576574825681746
batch 100 loss: 0.0005576525116339325
batch 105 loss: 0.0005576673313044012
batch 110 loss: 0.0005577137228101492
batch 115 loss: 0.0005576886702328921
batch 120 loss: 0.0005576516734436155
batch 125 loss: 0.0005576265277341009
batch 130 loss: 0.0005577361211180687
batch 135 loss: 0.0005576221155934036
batch 140 loss: 0.000557669042609632
batch 145 loss: 0.0005576868657954037
batch 150 loss: 0.0005575466202571988
batch 155 loss: 0.000557622208725661
batch 160 loss: 0.0005576872965320945
batch 165 loss: 0.0005576202529482543
batch 170 loss: 0.000557606271468103
batch 175 loss: 0.0005575401242822409
batch 180 loss: 0.0005577232572250068
batch 185 loss: 0.0005576712777838111
batch 190 loss: 0.0005577088333666325
batch 195 loss: 0.0005576981347985566
batch 200 loss: 0.0005577741656452417
batch 205 loss: 0.0005576502881012857
batch 210 loss: 0.0005577206495217979
batch 215 loss: 0.0005576582392677665
batch 220 loss: 0.0005576829775236547
batch 225 loss: 0.0005576794035732746
batch 230 loss: 0.000557742826640606
batch 235 loss: 0.000557625899091363
batch 240 loss: 0.0005577142350375652
Training Loss: 0.0005576834223271969
Validation Loss: 0.0005576934093066181
Epoch 32:
batch 5 loss: 0.0005577171454206109
batch 10 loss: 0.0005577213480137289
batch 15 loss: 0.0005577289266511798
batch 20 loss: 0.0005577378091402352
batch 25 loss: 0.0005577603005804121
batch 30 loss: 0.0005576559808105231
batch 35 loss: 0.0005576776922680438
batch 40 loss: 0.0005577337578870356
batch 45 loss: 0.0005576569237746298
batch 50 loss: 0.0005575951188802719
batch 55 loss: 0.0005575352581217885
batch 60 loss: 0.0005578109645284713
batch 65 loss: 0.0005576736060902476
batch 70 loss: 0.0005578305921517312
batch 75 loss: 0.0005577400675974786
batch 80 loss: 0.0005576751660555601
batch 85 loss: 0.0005577036878094077
batch 90 loss: 0.0005575999966822565
batch 95 loss: 0.000557607423979789
batch 100 loss: 0.0005575891118496656
batch 105 loss: 0.0005576962255872786
batch 110 loss: 0.0005575930350460112
batch 115 loss: 0.0005577386007644236
batch 120 loss: 0.0005577314645051956
batch 125 loss: 0.0005578440264798701
batch 130 loss: 0.0005577301373705268
batch 135 loss: 0.000557710591237992
batch 140 loss: 0.000557660253252834
batch 145 loss: 0.0005575120216235518
batch 150 loss: 0.0005577714880928397
batch 155 loss: 0.0005576340365223587
batch 160 loss: 0.0005576575291343034
batch 165 loss: 0.0005576475989073515
batch 170 loss: 0.0005577063537202775
batch 175 loss: 0.0005576861556619405
batch 180 loss: 0.0005576354102231562
batch 185 loss: 0.0005576220457442105
batch 190 loss: 0.0005576993338763714
batch 195 loss: 0.0005577791249379516
batch 200 loss: 0.0005575870745815337
batch 205 loss: 0.0005576668540015816
batch 210 loss: 0.0005576922907494008
batch 215 loss: 0.000557690521236509
batch 220 loss: 0.000557703897356987
batch 225 loss: 0.0005576038034632802
batch 230 loss: 0.0005577272968366742
batch 235 loss: 0.0005576372728683055
batch 240 loss: 0.0005576892290264368
Training Loss: 0.0005576834281479629
Validation Loss: 0.0005576935004986201
Epoch 33:
batch 5 loss: 0.0005577160161919891
batch 10 loss: 0.0005576761323027313
batch 15 loss: 0.0005576851894147694
batch 20 loss: 0.0005576564581133425
batch 25 loss: 0.0005576462368480861
batch 30 loss: 0.0005576830473728478
batch 35 loss: 0.0005576860858127475
batch 40 loss: 0.0005577948992140591
batch 45 loss: 0.0005576898693107069
batch 50 loss: 0.0005577388918027281
batch 55 loss: 0.0005576521391049028
batch 60 loss: 0.0005576197989284992
batch 65 loss: 0.0005576933617703617
batch 70 loss: 0.0005576468422077597
batch 75 loss: 0.0005577372619882226
batch 80 loss: 0.0005576319992542267
batch 85 loss: 0.0005576667026616633
batch 90 loss: 0.0005577455158345401
batch 95 loss: 0.0005576182506047189
batch 100 loss: 0.0005576588679105044
batch 105 loss: 0.0005575187504291534
batch 110 loss: 0.0005575783550739288
batch 115 loss: 0.0005575951421633363
batch 120 loss: 0.0005577238858677447
batch 125 loss: 0.0005577068310230971
batch 130 loss: 0.0005577628267928958
batch 135 loss: 0.0005577627453021705
batch 140 loss: 0.000557672290597111
batch 145 loss: 0.0005578142357990145
batch 150 loss: 0.000557657447643578
batch 155 loss: 0.0005577735370025039
batch 160 loss: 0.0005576541181653738
batch 165 loss: 0.0005576802883297205
batch 170 loss: 0.0005576579365879297
batch 175 loss: 0.0005576667026616633
batch 180 loss: 0.0005576659343205393
batch 185 loss: 0.0005576637107878923
batch 190 loss: 0.0005577037460170686
batch 195 loss: 0.0005577359232120215
batch 200 loss: 0.0005576430587098003
batch 205 loss: 0.0005577079718932509
batch 210 loss: 0.0005576640251092613
batch 215 loss: 0.0005575997987762093
batch 220 loss: 0.0005577670526690781
batch 225 loss: 0.0005577728617936373
batch 230 loss: 0.0005576551193371415
batch 235 loss: 0.0005576897645369172
batch 240 loss: 0.0005576662020757795
Training Loss: 0.0005576834131109839
Validation Loss: 0.0005576934704246621
Epoch 34:
batch 5 loss: 0.0005576388211920857
batch 10 loss: 0.0005577188567258418
batch 15 loss: 0.0005576672381721437
batch 20 loss: 0.0005577248404733837
batch 25 loss: 0.0005576970055699348
batch 30 loss: 0.0005577270407229662
batch 35 loss: 0.0005576479714363813
batch 40 loss: 0.0005576395546086132
batch 45 loss: 0.0005577078554779291
batch 50 loss: 0.0005577555508352816
batch 55 loss: 0.0005577185424044728
batch 60 loss: 0.0005576233612373471
batch 65 loss: 0.0005577270989306271
batch 70 loss: 0.0005577809060923755
batch 75 loss: 0.0005576797295361758
batch 80 loss: 0.000557597994338721
batch 85 loss: 0.0005575422663241624
batch 90 loss: 0.0005575924762524665
batch 95 loss: 0.0005577116389758885
batch 100 loss: 0.0005577175063081086
batch 105 loss: 0.0005576478084549308
batch 110 loss: 0.0005576824420131743
batch 115 loss: 0.000557792093604803
batch 120 loss: 0.00055768076563254
batch 125 loss: 0.0005577142583206296
batch 130 loss: 0.0005578543758019805
batch 135 loss: 0.00055772002087906
batch 140 loss: 0.000557599903549999
batch 145 loss: 0.0005576531053520739
batch 150 loss: 0.0005576819996349514
batch 155 loss: 0.000557672418653965
batch 160 loss: 0.0005575994960963726
batch 165 loss: 0.0005577033502049744
batch 170 loss: 0.0005577526986598969
batch 175 loss: 0.0005577133037149906
batch 180 loss: 0.0005578091717325151
batch 185 loss: 0.0005576572963036597
batch 190 loss: 0.0005576742463745177
batch 195 loss: 0.0005577641306445003
batch 200 loss: 0.0005576870171353221
batch 205 loss: 0.000557694595772773
batch 210 loss: 0.0005575977265834808
batch 215 loss: 0.0005575686576776206
batch 220 loss: 0.0005576659692451358
batch 225 loss: 0.0005576356197707355
batch 230 loss: 0.0005575829651206732
batch 235 loss: 0.0005576104042120278
batch 240 loss: 0.0005577727220952511
Training Loss: 0.0005576834337261971
Validation Loss: 0.0005576935441543658
Epoch 35:
batch 5 loss: 0.0005577118834480643
batch 10 loss: 0.0005577019415795803
batch 15 loss: 0.0005576999858021737
batch 20 loss: 0.0005576101946644485
batch 25 loss: 0.0005577073898166418
batch 30 loss: 0.0005576749215833842
batch 35 loss: 0.0005577461328357458
batch 40 loss: 0.0005576121737249195
batch 45 loss: 0.0005577229778282345
batch 50 loss: 0.0005576308933086694
batch 55 loss: 0.0005576326744630933
batch 60 loss: 0.0005577393574640154
batch 65 loss: 0.0005576560855843127
batch 70 loss: 0.0005576864932663739
batch 75 loss: 0.0005576487281359732
batch 80 loss: 0.0005576584720984101
batch 85 loss: 0.0005577446892857552
batch 90 loss: 0.0005577288335189224
batch 95 loss: 0.0005576095543801784
batch 100 loss: 0.0005578189739026129
batch 105 loss: 0.0005576490540988744
batch 110 loss: 0.0005576580064371228
batch 115 loss: 0.0005577140022069216
batch 120 loss: 0.0005575665971264243
batch 125 loss: 0.0005576300784014166
batch 130 loss: 0.0005576276453211904
batch 135 loss: 0.000557681277859956
batch 140 loss: 0.0005575920687988401
batch 145 loss: 0.0005577733507379889
batch 150 loss: 0.0005576549679972232
batch 155 loss: 0.0005576310912147164
batch 160 loss: 0.00055768535239622
batch 165 loss: 0.0005576749565079809
batch 170 loss: 0.0005577165167778731
batch 175 loss: 0.000557626667432487
batch 180 loss: 0.0005576744209975004
batch 185 loss: 0.0005577076692134142
batch 190 loss: 0.0005578018957749009
batch 195 loss: 0.0005576512892730534
batch 200 loss: 0.0005578160751610994
batch 205 loss: 0.0005576902185566723
batch 210 loss: 0.0005576545023359359
batch 215 loss: 0.0005577235366217792
batch 220 loss: 0.0005577717907726765
batch 225 loss: 0.0005576261319220066
batch 230 loss: 0.0005576537922024726
batch 235 loss: 0.0005576773546636105
batch 240 loss: 0.0005577330477535725
Training Loss: 0.000557683452401155
Validation Loss: 0.0005576936295256018
Epoch 36:
batch 5 loss: 0.000557597924489528
batch 10 loss: 0.0005577580770477653
batch 15 loss: 0.0005577320931479335
batch 20 loss: 0.0005576591240242124
batch 25 loss: 0.0005578187410719693
batch 30 loss: 0.0005576554336585105
batch 35 loss: 0.0005575882503762841
batch 40 loss: 0.0005577157367952168
batch 45 loss: 0.0005576447350904345
batch 50 loss: 0.0005577077274210751
batch 55 loss: 0.0005577138159424067
batch 60 loss: 0.0005577578791417182
batch 65 loss: 0.0005577144678682089
batch 70 loss: 0.000557624245993793
batch 75 loss: 0.000557850452605635
batch 80 loss: 0.0005575885879807174
batch 85 loss: 0.000557752070017159
batch 90 loss: 0.0005576629424467683
batch 95 loss: 0.0005576470284722745
batch 100 loss: 0.0005575774819590151
batch 105 loss: 0.0005576832802034915
batch 110 loss: 0.0005576731404289603
batch 115 loss: 0.0005576040828600525
batch 120 loss: 0.0005576048279181123
batch 125 loss: 0.0005577029194682836
batch 130 loss: 0.0005577129544690251
batch 135 loss: 0.0005577436415478588
batch 140 loss: 0.0005577541771344841
batch 145 loss: 0.0005576076451689005
batch 150 loss: 0.0005576525349169969
batch 155 loss: 0.0005576270399615168
batch 160 loss: 0.000557665468659252
batch 165 loss: 0.0005576722207479179
batch 170 loss: 0.000557780172675848
batch 175 loss: 0.0005576956435106694
batch 180 loss: 0.0005577112431637942
batch 185 loss: 0.000557653361465782
batch 190 loss: 0.0005576120573095977
batch 195 loss: 0.0005576748284511268
batch 200 loss: 0.0005577564355917275
batch 205 loss: 0.0005577503005042672
batch 210 loss: 0.0005577098578214646
batch 215 loss: 0.000557652220595628
batch 220 loss: 0.0005577684263698756
batch 225 loss: 0.0005576702067628502
batch 230 loss: 0.0005577070522122085
batch 235 loss: 0.0005575698218308389
batch 240 loss: 0.0005576233961619437
Training Loss: 0.0005576834536138146
Validation Loss: 0.0005576936159438143
Epoch 37:
batch 5 loss: 0.0005576169118285179
batch 10 loss: 0.0005576942232437432
batch 15 loss: 0.000557697145268321
batch 20 loss: 0.0005577584845013917
batch 25 loss: 0.0005576492287218571
batch 30 loss: 0.000557734165340662
batch 35 loss: 0.0005577005445957184
batch 40 loss: 0.0005576461320742965
batch 45 loss: 0.0005576471099629998
batch 50 loss: 0.0005577283445745707
batch 55 loss: 0.000557768379803747
batch 60 loss: 0.0005575532210059464
batch 65 loss: 0.0005576591822318733
batch 70 loss: 0.0005577058880589902
batch 75 loss: 0.0005576482391916216
batch 80 loss: 0.0005576744559220969
batch 85 loss: 0.0005577399744652212
batch 90 loss: 0.000557583209592849
batch 95 loss: 0.0005577610223554075
batch 100 loss: 0.0005576941650360823
batch 105 loss: 0.0005577062256634235
batch 110 loss: 0.0005576014751568437
batch 115 loss: 0.0005577062140218914
batch 120 loss: 0.000557548541110009
batch 125 loss: 0.0005576299503445625
batch 130 loss: 0.0005577157600782812
batch 135 loss: 0.0005576478084549308
batch 140 loss: 0.0005576959694735706
batch 145 loss: 0.0005577978678047657
batch 150 loss: 0.0005577110103331506
batch 155 loss: 0.000557710719294846
batch 160 loss: 0.0005576433963142335
batch 165 loss: 0.0005576824420131743
batch 170 loss: 0.0005576798925176263
batch 175 loss: 0.0005577183677814901
batch 180 loss: 0.0005576602765358984
batch 185 loss: 0.0005577872623689472
batch 190 loss: 0.0005577363772317767
batch 195 loss: 0.0005577481468208134
batch 200 loss: 0.0005577696487307548
batch 205 loss: 0.0005577512900345027
batch 210 loss: 0.0005575821967795491
batch 215 loss: 0.0005575611139647663
batch 220 loss: 0.0005576617200858891
batch 225 loss: 0.0005576491937972605
batch 230 loss: 0.0005576959811151028
batch 235 loss: 0.0005577325820922852
batch 240 loss: 0.000557614502031356
Training Loss: 0.0005576834574943253
Validation Loss: 0.0005576935266920676
Epoch 38:
batch 5 loss: 0.000557781325187534
batch 10 loss: 0.000557587412185967
batch 15 loss: 0.000557737227063626
batch 20 loss: 0.0005576743395067751
batch 25 loss: 0.0005577408359386027
batch 30 loss: 0.0005575335933826863
batch 35 loss: 0.0005577549221925437
batch 40 loss: 0.0005577660165727138
batch 45 loss: 0.0005577387404628098
batch 50 loss: 0.00055766407167539
batch 55 loss: 0.0005577982519753277
batch 60 loss: 0.0005576587282121181
batch 65 loss: 0.0005576867726631463
batch 70 loss: 0.0005576016963459551
batch 75 loss: 0.0005576538620516658
batch 80 loss: 0.0005576103925704956
batch 85 loss: 0.0005578308133408427
batch 90 loss: 0.0005577383213676512
batch 95 loss: 0.0005577132804319262
batch 100 loss: 0.0005576601368375122
batch 105 loss: 0.0005577257834374905
batch 110 loss: 0.000557655154261738
batch 115 loss: 0.000557572313118726
batch 120 loss: 0.0005576671333983541
batch 125 loss: 0.0005576279712840915
batch 130 loss: 0.0005577640724368394
batch 135 loss: 0.0005576105439104139
batch 140 loss: 0.0005576933268457651
batch 145 loss: 0.0005577380419708788
batch 150 loss: 0.0005576646421104669
batch 155 loss: 0.0005576682509854436
batch 160 loss: 0.0005577147006988525
batch 165 loss: 0.000557624886278063
batch 170 loss: 0.0005577891250140965
batch 175 loss: 0.000557722698431462
batch 180 loss: 0.0005577780189923942
batch 185 loss: 0.0005577614647336304
batch 190 loss: 0.0005576394847594202
batch 195 loss: 0.0005575414863415063
batch 200 loss: 0.0005576620344072581
batch 205 loss: 0.0005576301948167384
batch 210 loss: 0.0005576725583523512
batch 215 loss: 0.0005575637449510395
batch 220 loss: 0.0005577423726208508
batch 225 loss: 0.0005575827788561582
batch 230 loss: 0.0005576777854003012
batch 235 loss: 0.0005577441886998713
batch 240 loss: 0.0005576412426307797
Training Loss: 0.000557683473743964
Validation Loss: 0.0005576934975882372
Epoch 39:
batch 5 loss: 0.0005577123141847551
batch 10 loss: 0.0005576735828071832
batch 15 loss: 0.0005577091942541301
batch 20 loss: 0.0005576055147685111
batch 25 loss: 0.000557669613044709
batch 30 loss: 0.0005576979951001704
batch 35 loss: 0.0005577901843935251
batch 40 loss: 0.0005575806368142367
batch 45 loss: 0.0005577289615757763
batch 50 loss: 0.0005577066680416465
batch 55 loss: 0.0005577571922913193
batch 60 loss: 0.0005576500785537064
batch 65 loss: 0.0005577425239607692
batch 70 loss: 0.0005577225587330758
batch 75 loss: 0.0005576471914537251
batch 80 loss: 0.0005576501251198351
batch 85 loss: 0.0005576982861384749
batch 90 loss: 0.0005576720461249352
batch 95 loss: 0.0005576649098657071
batch 100 loss: 0.0005576633149757982
batch 105 loss: 0.0005577078671194613
batch 110 loss: 0.0005575812654569745
batch 115 loss: 0.0005576880183070898
batch 120 loss: 0.0005577335483394563
batch 125 loss: 0.0005576609983108938
batch 130 loss: 0.0005577047704719007
batch 135 loss: 0.0005576233263127506
batch 140 loss: 0.0005576015217229723
batch 145 loss: 0.0005577291012741625
batch 150 loss: 0.0005576565745286644
batch 155 loss: 0.0005576550145633518
batch 160 loss: 0.0005576788564212621
batch 165 loss: 0.000557692814618349
batch 170 loss: 0.0005576829076744616
batch 175 loss: 0.0005576246650889516
batch 180 loss: 0.0005576726980507374
batch 185 loss: 0.0005576954456046224
batch 190 loss: 0.000557735818438232
batch 195 loss: 0.0005577604635618628
batch 200 loss: 0.0005576301831752062
batch 205 loss: 0.0005577868665568531
batch 210 loss: 0.0005578059470281004
batch 215 loss: 0.0005576292518526315
batch 220 loss: 0.0005577256204560399
batch 225 loss: 0.0005576155846938491
batch 230 loss: 0.000557654129806906
batch 235 loss: 0.0005576491472311318
batch 240 loss: 0.0005576817551627755
Training Loss: 0.0005576834802923259
Validation Loss: 0.0005576938148199891
Epoch 40:
batch 5 loss: 0.0005576344672590494
batch 10 loss: 0.000557676306925714
batch 15 loss: 0.0005576785770244896
batch 20 loss: 0.0005576986819505692
batch 25 loss: 0.0005575604503974318
batch 30 loss: 0.0005577673786319792
batch 35 loss: 0.0005576865747570992
batch 40 loss: 0.0005576423602178693
batch 45 loss: 0.0005576539668254554
batch 50 loss: 0.0005576469702646136
batch 55 loss: 0.0005576765397563577
batch 60 loss: 0.0005576599389314652
batch 65 loss: 0.0005577739560976624
batch 70 loss: 0.0005575543618761003
batch 75 loss: 0.0005577711388468742
batch 80 loss: 0.0005576241877861321
batch 85 loss: 0.0005576533963903785
batch 90 loss: 0.0005577096133492887
batch 95 loss: 0.0005575851537287235
batch 100 loss: 0.000557709182612598
batch 105 loss: 0.0005577124888077378
batch 110 loss: 0.0005577066913247108
batch 115 loss: 0.0005575874820351601
batch 120 loss: 0.0005576415685936808
batch 125 loss: 0.0005576428025960922
batch 130 loss: 0.0005575852119363844
batch 135 loss: 0.0005577265634201467
batch 140 loss: 0.0005577560397796333
batch 145 loss: 0.0005576759576797485
batch 150 loss: 0.0005577632924541831
batch 155 loss: 0.0005577895906753838
batch 160 loss: 0.0005576949333772063
batch 165 loss: 0.0005576313473284244
batch 170 loss: 0.000557654385920614
batch 175 loss: 0.0005576584255322814
batch 180 loss: 0.0005576773430220783
batch 185 loss: 0.0005577219999395311
batch 190 loss: 0.0005576282390393316
batch 195 loss: 0.0005577200907282531
batch 200 loss: 0.0005577484029345214
batch 205 loss: 0.0005576028837822377
batch 210 loss: 0.0005577353178523481
batch 215 loss: 0.0005578154232352972
batch 220 loss: 0.0005576379015110433
batch 225 loss: 0.0005577334435656667
batch 230 loss: 0.0005577429081313312
batch 235 loss: 0.0005577765172347426
batch 240 loss: 0.0005576776689849794
Training Loss: 0.0005576835026052625
Validation Loss: 0.0005576935014687479
Epoch 41:
batch 5 loss: 0.0005576203693635762
batch 10 loss: 0.0005577259697020053
batch 15 loss: 0.0005576034891419113
batch 20 loss: 0.0005576843861490488
batch 25 loss: 0.0005576619179919363
batch 30 loss: 0.0005577343166805804
batch 35 loss: 0.0005576747236773372
batch 40 loss: 0.0005577231524512172
batch 45 loss: 0.0005576467840000987
batch 50 loss: 0.0005576550029218197
batch 55 loss: 0.0005577121395617723
batch 60 loss: 0.0005577158532105386
batch 65 loss: 0.0005576370866037905
batch 70 loss: 0.0005576333729550243
batch 75 loss: 0.0005575785413384438
batch 80 loss: 0.0005577405681833625
batch 85 loss: 0.000557660253252834
batch 90 loss: 0.0005576454568654299
batch 95 loss: 0.0005577474134042859
batch 100 loss: 0.0005576070048846305
batch 105 loss: 0.0005577335134148598
batch 110 loss: 0.0005576346651650965
batch 115 loss: 0.0005576631985604763
batch 120 loss: 0.0005576812545768917
batch 125 loss: 0.0005576954921707511
batch 130 loss: 0.0005577167379669845
batch 135 loss: 0.0005576815688982606
batch 140 loss: 0.0005577509291470051
batch 145 loss: 0.0005577100557275117
batch 150 loss: 0.0005576221155934036
batch 155 loss: 0.000557814072817564
batch 160 loss: 0.0005576490075327456
batch 165 loss: 0.0005575896124355495
batch 170 loss: 0.0005576862720772624
batch 175 loss: 0.0005577111034654081
batch 180 loss: 0.0005577317904680967
batch 185 loss: 0.0005576438503339887
batch 190 loss: 0.0005577037576586008
batch 195 loss: 0.0005577111034654081
batch 200 loss: 0.0005576720344834029
batch 205 loss: 0.0005576635361649096
batch 210 loss: 0.0005577513831667602
batch 215 loss: 0.000557621056213975
batch 220 loss: 0.0005576560972258449
batch 225 loss: 0.0005577186006121337
batch 230 loss: 0.0005577259580604732
batch 235 loss: 0.0005576776806265116
batch 240 loss: 0.0005577821168117226
Training Loss: 0.0005576834659829426
Validation Loss: 0.0005576934335598101
Epoch 42:
batch 5 loss: 0.0005577032337896526
batch 10 loss: 0.0005577174597419798
batch 15 loss: 0.0005577052594162524
batch 20 loss: 0.0005577021162025631
batch 25 loss: 0.0005576705560088157
batch 30 loss: 0.0005576172727160156
batch 35 loss: 0.00055777597008273
batch 40 loss: 0.0005576643627136946
batch 45 loss: 0.0005576773313805461
batch 50 loss: 0.0005575960851274431
batch 55 loss: 0.0005575983552262187
batch 60 loss: 0.0005577015923336148
batch 65 loss: 0.000557765073608607
batch 70 loss: 0.000557601684704423
batch 75 loss: 0.0005576464580371975
batch 80 loss: 0.0005578080890700221
batch 85 loss: 0.0005577205447480083
batch 90 loss: 0.0005577249336056411
batch 95 loss: 0.0005576314288191497
batch 100 loss: 0.0005577211268246174
batch 105 loss: 0.0005575801944360137
batch 110 loss: 0.0005576840601861476
batch 115 loss: 0.0005577189614996314
batch 120 loss: 0.0005577450967393816
batch 125 loss: 0.0005577089497819543
batch 130 loss: 0.0005576586117967963
batch 135 loss: 0.0005576069233939052
batch 140 loss: 0.0005576641880907119
batch 145 loss: 0.0005577114876359701
batch 150 loss: 0.0005577591597102583
batch 155 loss: 0.0005576904164627195
batch 160 loss: 0.0005577813019044698
batch 165 loss: 0.000557662092614919
batch 170 loss: 0.0005577927106060088
batch 175 loss: 0.0005576934549026191
batch 180 loss: 0.0005576328374445438
batch 185 loss: 0.0005576489260420203
batch 190 loss: 0.0005576594965532422
batch 195 loss: 0.0005576433730311691
batch 200 loss: 0.0005576675990596414
batch 205 loss: 0.0005576503113843501
batch 210 loss: 0.0005576169234700501
batch 215 loss: 0.0005575684481300414
batch 220 loss: 0.0005577003350481391
batch 225 loss: 0.0005577271105721593
batch 230 loss: 0.0005577210686169565
batch 235 loss: 0.000557704153470695
batch 240 loss: 0.0005576594616286456
Training Loss: 0.000557683470591049
Validation Loss: 0.0005576934190078948
Epoch 43:
batch 5 loss: 0.0005576500436291099
batch 10 loss: 0.000557572057005018
batch 15 loss: 0.0005576596246100962
batch 20 loss: 0.0005577846779488028
batch 25 loss: 0.0005576626281253994
batch 30 loss: 0.0005577921285293997
batch 35 loss: 0.000557615770958364
batch 40 loss: 0.0005576005321927368
batch 45 loss: 0.0005576364928856492
batch 50 loss: 0.0005576341063715518
batch 55 loss: 0.0005576375522650778
batch 60 loss: 0.0005576489726081491
batch 65 loss: 0.0005576345953159034
batch 70 loss: 0.0005576658644713461
batch 75 loss: 0.0005577708943746984
batch 80 loss: 0.0005576915689744055
batch 85 loss: 0.0005576034775003791
batch 90 loss: 0.0005576676921918988
batch 95 loss: 0.0005577299976721406
batch 100 loss: 0.0005576204624958336
batch 105 loss: 0.0005576644674874842
batch 110 loss: 0.0005577952018938958
batch 115 loss: 0.0005577151780016721
batch 120 loss: 0.0005576859111897647
batch 125 loss: 0.0005577143165282905
batch 130 loss: 0.0005576565745286644
batch 135 loss: 0.0005576237221248448
batch 140 loss: 0.0005576679017394781
batch 145 loss: 0.0005577967967838049
batch 150 loss: 0.0005576046300120652
batch 155 loss: 0.0005577809526585043
batch 160 loss: 0.0005576727329753339
batch 165 loss: 0.000557715236209333
batch 170 loss: 0.0005577355623245239
batch 175 loss: 0.0005577755626291036
batch 180 loss: 0.0005576911265961826
batch 185 loss: 0.0005577053874731064
batch 190 loss: 0.0005576362484134734
batch 195 loss: 0.0005576550145633518
batch 200 loss: 0.0005576465395279228
batch 205 loss: 0.0005576619529165328
batch 210 loss: 0.0005577543284744024
batch 215 loss: 0.0005576566443778574
batch 220 loss: 0.000557654001750052
batch 225 loss: 0.0005577440257184208
batch 230 loss: 0.0005576305789873004
batch 235 loss: 0.0005577550735324621
batch 240 loss: 0.0005577334435656667
Training Loss: 0.0005576835052731136
Validation Loss: 0.0005576935538556427
Epoch 44:
batch 5 loss: 0.0005576408351771533
batch 10 loss: 0.0005576412659138441
batch 15 loss: 0.0005577006842941046
batch 20 loss: 0.0005577166797593236
batch 25 loss: 0.0005576734663918614
batch 30 loss: 0.0005575591116212308
batch 35 loss: 0.0005576578783802688
batch 40 loss: 0.0005577594391070306
batch 45 loss: 0.0005576274590566755
batch 50 loss: 0.00055765132419765
batch 55 loss: 0.0005577382515184581
batch 60 loss: 0.0005576862022280693
batch 65 loss: 0.0005575905903242529
batch 70 loss: 0.0005575782852247357
batch 75 loss: 0.0005576207535341382
batch 80 loss: 0.000557654828298837
batch 85 loss: 0.0005577241885475814
batch 90 loss: 0.0005578070529736578
batch 95 loss: 0.000557606341317296
batch 100 loss: 0.0005577178788371384
batch 105 loss: 0.0005577074130997062
batch 110 loss: 0.0005577073199674488
batch 115 loss: 0.0005577286938205361
batch 120 loss: 0.0005577305913902819
batch 125 loss: 0.0005576490075327456
batch 130 loss: 0.0005577699979767203
batch 135 loss: 0.0005576801719143987
batch 140 loss: 0.0005576903000473976
batch 145 loss: 0.0005576893803663552
batch 150 loss: 0.0005575429764576256
batch 155 loss: 0.0005576460505835712
batch 160 loss: 0.0005576643394306302
batch 165 loss: 0.0005576053750701249
batch 170 loss: 0.0005576610448770225
batch 175 loss: 0.0005576816038228571
batch 180 loss: 0.0005576588679105044
batch 185 loss: 0.0005576764815486968
batch 190 loss: 0.0005577684962190688
batch 195 loss: 0.0005577606847509742
batch 200 loss: 0.0005577530013397336
batch 205 loss: 0.0005577389034442604
batch 210 loss: 0.0005576658993959426
batch 215 loss: 0.0005576973431743682
batch 220 loss: 0.0005577428382821381
batch 225 loss: 0.0005577169009484351
batch 230 loss: 0.0005577021162025631
batch 235 loss: 0.0005577217903919518
batch 240 loss: 0.0005576997296884656
Training Loss: 0.0005576835382574548
Validation Loss: 0.0005576934519922361
Epoch 45:
batch 5 loss: 0.0005576667492277921
batch 10 loss: 0.000557631824631244
batch 15 loss: 0.0005576599040068686
batch 20 loss: 0.0005576792173087597
batch 25 loss: 0.0005577089497819543
batch 30 loss: 0.000557745574042201
batch 35 loss: 0.0005577400675974786
batch 40 loss: 0.0005577492411248386
batch 45 loss: 0.0005575969582423568
batch 50 loss: 0.0005577458068728447
batch 55 loss: 0.0005576901719905436
batch 60 loss: 0.0005576382158324122
batch 65 loss: 0.0005576537805609405
batch 70 loss: 0.0005577236996032297
batch 75 loss: 0.0005577333853580057
batch 80 loss: 0.000557692430447787
batch 85 loss: 0.0005577371106483042
batch 90 loss: 0.0005577719421125948
batch 95 loss: 0.0005577113362960518
batch 100 loss: 0.0005577559117227793
batch 105 loss: 0.0005577466916292905
batch 110 loss: 0.0005576874129474163
batch 115 loss: 0.0005575049319304526
batch 120 loss: 0.0005577476578764618
batch 125 loss: 0.0005576859577558935
batch 130 loss: 0.0005577143747359514
batch 135 loss: 0.0005576374707743526
batch 140 loss: 0.0005575799965299666
batch 145 loss: 0.0005575985996983945
batch 150 loss: 0.0005576566443778574
batch 155 loss: 0.0005576474592089653
batch 160 loss: 0.0005576490773819387
batch 165 loss: 0.0005576176336035132
batch 170 loss: 0.0005576808471232653
batch 175 loss: 0.0005577107309363783
batch 180 loss: 0.0005577460746280849
batch 185 loss: 0.0005577386124059558
batch 190 loss: 0.0005576707539148628
batch 195 loss: 0.0005576509749516845
batch 200 loss: 0.0005577137344516814
batch 205 loss: 0.0005576079711318016
batch 210 loss: 0.0005575681920163333
batch 215 loss: 0.0005576836178079247
batch 220 loss: 0.0005576512892730534
batch 225 loss: 0.0005578114185482263
batch 230 loss: 0.0005575735354796052
batch 235 loss: 0.0005577125935815275
batch 240 loss: 0.0005577802076004446
Training Loss: 0.000557683473743964
Validation Loss: 0.0005576934015455966
Epoch 46:
batch 5 loss: 0.0005577848525717855
batch 10 loss: 0.0005577133619226515
batch 15 loss: 0.0005576813593506813
batch 20 loss: 0.0005577394040301442
batch 25 loss: 0.0005576395196840167
batch 30 loss: 0.0005576489609666168
batch 35 loss: 0.0005577063770033419
batch 40 loss: 0.0005576685420237481
batch 45 loss: 0.0005575628718361258
batch 50 loss: 0.0005577099043875932
batch 55 loss: 0.0005576077266596258
batch 60 loss: 0.0005577306263148784
batch 65 loss: 0.000557718297932297
batch 70 loss: 0.0005576248629949987
batch 75 loss: 0.0005577416741289198
batch 80 loss: 0.0005576563300564886
batch 85 loss: 0.0005576882977038622
batch 90 loss: 0.0005577040486969054
batch 95 loss: 0.0005576890427619219
batch 100 loss: 0.0005576985073275864
batch 105 loss: 0.0005576328025199473
batch 110 loss: 0.0005577305215410888
batch 115 loss: 0.0005576683091931045
batch 120 loss: 0.0005576605093665421
batch 125 loss: 0.0005576370749622583
batch 130 loss: 0.0005577033502049744
batch 135 loss: 0.000557669042609632
batch 140 loss: 0.0005576447816565633
batch 145 loss: 0.0005578067502938211
batch 150 loss: 0.0005576761439442634
batch 155 loss: 0.0005576685070991516
batch 160 loss: 0.0005576511030085385
batch 165 loss: 0.0005576839088462293
batch 170 loss: 0.0005576849915087223
batch 175 loss: 0.0005575880291871727
batch 180 loss: 0.0005576870171353221
batch 185 loss: 0.0005577642470598221
batch 190 loss: 0.0005577449686825275
batch 195 loss: 0.0005576032446697355
batch 200 loss: 0.0005576203111559153
batch 205 loss: 0.0005577448406256736
batch 210 loss: 0.0005576020921580493
batch 215 loss: 0.0005577257135882974
batch 220 loss: 0.0005577527452260255
batch 225 loss: 0.0005576944444328546
batch 230 loss: 0.0005576711846515536
batch 235 loss: 0.0005576897296123206
batch 240 loss: 0.0005576868541538715
Training Loss: 0.0005576834955718368
Validation Loss: 0.0005576934529623637
Epoch 47:
batch 5 loss: 0.0005577110918238759
batch 10 loss: 0.0005576853058300912
batch 15 loss: 0.0005577127216383815
batch 20 loss: 0.0005575385759584606
batch 25 loss: 0.0005577272153459489
batch 30 loss: 0.0005578144802711904
batch 35 loss: 0.0005576955736614764
batch 40 loss: 0.0005576909752562642
batch 45 loss: 0.0005577302887104452
batch 50 loss: 0.0005577067960985005
batch 55 loss: 0.0005576222203671933
batch 60 loss: 0.0005577289266511798
batch 65 loss: 0.0005576963652856648
batch 70 loss: 0.0005577380070462823
batch 75 loss: 0.0005576663417741657
batch 80 loss: 0.000557624886278063
batch 85 loss: 0.0005577404052019119
batch 90 loss: 0.0005577189731411636
batch 95 loss: 0.0005578056327067316
batch 100 loss: 0.0005576758412644267
batch 105 loss: 0.0005576043506152928
batch 110 loss: 0.0005577375763095915
batch 115 loss: 0.0005577073316089809
batch 120 loss: 0.0005577853298746049
batch 125 loss: 0.000557710020802915
batch 130 loss: 0.0005577227100729942
batch 135 loss: 0.0005576453753747046
batch 140 loss: 0.0005577901261858642
batch 145 loss: 0.0005576567025855183
batch 150 loss: 0.0005577092757448554
batch 155 loss: 0.0005576461204327643
batch 160 loss: 0.0005577001487836242
batch 165 loss: 0.0005576419644057751
batch 170 loss: 0.0005576053634285926
batch 175 loss: 0.0005576859810389578
batch 180 loss: 0.0005576138151809573
batch 185 loss: 0.0005575353745371104
batch 190 loss: 0.0005575904040597379
batch 195 loss: 0.0005575788905844093
batch 200 loss: 0.0005576508585363627
batch 205 loss: 0.0005576380412094295
batch 210 loss: 0.00055773314088583
batch 215 loss: 0.0005576640600338578
batch 220 loss: 0.0005576656782068312
batch 225 loss: 0.000557735119946301
batch 230 loss: 0.0005577468080446124
batch 235 loss: 0.0005576329422183335
batch 240 loss: 0.0005576440249569714
Training Loss: 0.0005576835033328583
Validation Loss: 0.0005576934694545343
Epoch 48:
batch 5 loss: 0.0005578133161179722
batch 10 loss: 0.0005577119416557252
batch 15 loss: 0.0005577560747042299
batch 20 loss: 0.0005576997762545943
batch 25 loss: 0.0005576148745603859
batch 30 loss: 0.0005577674601227045
batch 35 loss: 0.0005576869705691933
batch 40 loss: 0.0005576478783041239
batch 45 loss: 0.000557596841827035
batch 50 loss: 0.0005576063646003604
batch 55 loss: 0.0005576465046033263
batch 60 loss: 0.0005577157950028777
batch 65 loss: 0.0005577137577347458
batch 70 loss: 0.0005577325588092208
batch 75 loss: 0.0005576754687353968
batch 80 loss: 0.0005576694267801941
batch 85 loss: 0.0005576496827416122
batch 90 loss: 0.0005577293108217418
batch 95 loss: 0.000557609647512436
batch 100 loss: 0.0005576207069680095
batch 105 loss: 0.0005576560040935874
batch 110 loss: 0.0005576792755164206
batch 115 loss: 0.0005577493924647569
batch 120 loss: 0.000557798647787422
batch 125 loss: 0.0005575387156568467
batch 130 loss: 0.0005576412775553762
batch 135 loss: 0.000557642022613436
batch 140 loss: 0.0005576770636253059
batch 145 loss: 0.0005577152012847364
batch 150 loss: 0.0005576869356445968
batch 155 loss: 0.0005576617084443569
batch 160 loss: 0.000557690835557878
batch 165 loss: 0.0005577304633334279
batch 170 loss: 0.0005577396834269166
batch 175 loss: 0.0005577313480898738
batch 180 loss: 0.0005576475523412228
batch 185 loss: 0.0005576910567469895
batch 190 loss: 0.0005578080308623612
batch 195 loss: 0.0005577072384767235
batch 200 loss: 0.0005576933152042329
batch 205 loss: 0.0005575579940341413
batch 210 loss: 0.0005576833966188133
batch 215 loss: 0.0005576974712312221
batch 220 loss: 0.0005576004274189472
batch 225 loss: 0.0005576309049502015
batch 230 loss: 0.0005577232921496033
batch 235 loss: 0.000557740917429328
batch 240 loss: 0.0005576237454079092
Training Loss: 0.0005576835057581775
Validation Loss: 0.0005576934044559796
Epoch 49:
batch 5 loss: 0.0005576481227762998
batch 10 loss: 0.0005577052943408489
batch 15 loss: 0.000557771185413003
batch 20 loss: 0.0005577008938416839
batch 25 loss: 0.0005577452597208321
batch 30 loss: 0.0005577171337790787
batch 35 loss: 0.0005575409973971546
batch 40 loss: 0.0005576716153882444
batch 45 loss: 0.0005577379837632179
batch 50 loss: 0.0005576965399086475
batch 55 loss: 0.0005577128147706389
batch 60 loss: 0.0005577613483183086
batch 65 loss: 0.00055769186001271
batch 70 loss: 0.0005575679359026253
batch 75 loss: 0.0005575758288614452
batch 80 loss: 0.0005577948875725269
batch 85 loss: 0.0005576413357630372
batch 90 loss: 0.0005577311851084233
batch 95 loss: 0.0005577043164521455
batch 100 loss: 0.0005576925002969801
batch 105 loss: 0.0005576493102125823
batch 110 loss: 0.0005576456896960735
batch 115 loss: 0.0005577024188823998
batch 120 loss: 0.0005577547708526254
batch 125 loss: 0.0005577049450948834
batch 130 loss: 0.0005575970630161464
batch 135 loss: 0.0005576607072725892
batch 140 loss: 0.0005577828153036535
batch 145 loss: 0.0005577121162787079
batch 150 loss: 0.0005576425814069807
batch 155 loss: 0.000557699881028384
batch 160 loss: 0.0005576580064371228
batch 165 loss: 0.0005576785420998931
batch 170 loss: 0.0005577437579631806
batch 175 loss: 0.0005576005671173335
batch 180 loss: 0.0005576802999712527
batch 185 loss: 0.0005577215692028403
batch 190 loss: 0.0005576605792157352
batch 195 loss: 0.0005576512194238603
batch 200 loss: 0.0005576415918767452
batch 205 loss: 0.0005577012081630528
batch 210 loss: 0.0005576405790634453
batch 215 loss: 0.0005576527677476406
batch 220 loss: 0.0005576865631155669
batch 225 loss: 0.0005578230251558125
batch 230 loss: 0.0005576298106461763
batch 235 loss: 0.0005576979951001704
batch 240 loss: 0.0005575770512223244
Training Loss: 0.0005576834681657298
Validation Loss: 0.0005576934219182779
Epoch 50:
batch 5 loss: 0.0005576363648287952
batch 10 loss: 0.0005576839321292937
batch 15 loss: 0.0005577095435000956
batch 20 loss: 0.000557690137065947
batch 25 loss: 0.0005575446179136634
batch 30 loss: 0.0005577513831667602
batch 35 loss: 0.0005576591705903411
batch 40 loss: 0.0005576200434006751
batch 45 loss: 0.0005576377385295928
batch 50 loss: 0.0005575895309448242
batch 55 loss: 0.0005576557479798794
batch 60 loss: 0.0005577099160291255
batch 65 loss: 0.0005576649564318359
batch 70 loss: 0.0005577444215305149
batch 75 loss: 0.0005577999283559621
batch 80 loss: 0.0005576158524490893
batch 85 loss: 0.0005576952593401074
batch 90 loss: 0.0005576798459514976
batch 95 loss: 0.0005576586583629251
batch 100 loss: 0.0005576472147367894
batch 105 loss: 0.0005575883667916059
batch 110 loss: 0.00055778743699193
batch 115 loss: 0.0005577175179496408
batch 120 loss: 0.0005576202995143831
batch 125 loss: 0.0005577238393016159
batch 130 loss: 0.0005577567382715643
batch 135 loss: 0.0005576784606091678
batch 140 loss: 0.0005577090429142118
batch 145 loss: 0.0005577723612077534
batch 150 loss: 0.0005577344447374344
batch 155 loss: 0.000557648076210171
batch 160 loss: 0.000557600031606853
batch 165 loss: 0.0005576327093876898
batch 170 loss: 0.0005576396943069994
batch 175 loss: 0.0005577269592322409
batch 180 loss: 0.0005575775168836117
batch 185 loss: 0.000557706505060196
batch 190 loss: 0.0005577253992669285
batch 195 loss: 0.0005576654337346553
batch 200 loss: 0.0005576784373261034
batch 205 loss: 0.0005577370291575789
batch 210 loss: 0.0005576582509092987
batch 215 loss: 0.0005577369127422571
batch 220 loss: 0.0005577482399530709
batch 225 loss: 0.0005578294978477061
batch 230 loss: 0.0005576501716859639
batch 235 loss: 0.0005577016156166792
batch 240 loss: 0.0005576615454629064
Training Loss: 0.0005576834749566236
Validation Loss: 0.0005576938322822874
Epoch 51:
batch 5 loss: 0.0005576055380515754
batch 10 loss: 0.0005576624069362878
batch 15 loss: 0.0005577034200541676
batch 20 loss: 0.0005577728152275085
batch 25 loss: 0.0005577388103120029
batch 30 loss: 0.0005576962372288108
batch 35 loss: 0.0005576559691689908
batch 40 loss: 0.0005575757473707199
batch 45 loss: 0.0005576699622906744
batch 50 loss: 0.0005577038857154549
batch 55 loss: 0.000557672546710819
batch 60 loss: 0.0005577753065153957
batch 65 loss: 0.000557678600307554
batch 70 loss: 0.0005577543401159346
batch 75 loss: 0.0005576515337452293
batch 80 loss: 0.0005577517207711935
batch 85 loss: 0.0005576945841312408
batch 90 loss: 0.0005575879011303186
batch 95 loss: 0.0005577228381298483
batch 100 loss: 0.0005576904746703804
batch 105 loss: 0.000557727471459657
batch 110 loss: 0.0005576397175900638
batch 115 loss: 0.0005576860043220222
batch 120 loss: 0.0005576416500844061
batch 125 loss: 0.0005576245370320976
batch 130 loss: 0.0005576138966716826
batch 135 loss: 0.0005577668081969023
batch 140 loss: 0.0005576777155511081
batch 145 loss: 0.0005575698218308389
batch 150 loss: 0.0005576580413617193
batch 155 loss: 0.0005577380768954753
batch 160 loss: 0.0005576588679105044
batch 165 loss: 0.0005577483214437961
batch 170 loss: 0.0005577140022069216
batch 175 loss: 0.000557591242250055
batch 180 loss: 0.00055763233685866
batch 185 loss: 0.0005576033028773964
batch 190 loss: 0.0005576792056672276
batch 195 loss: 0.0005576147232204676
batch 200 loss: 0.0005577325588092208
batch 205 loss: 0.0005577058997005224
batch 210 loss: 0.0005578659940510989
batch 215 loss: 0.0005576657713390887
batch 220 loss: 0.0005576679715886713
batch 225 loss: 0.0005576551309786737
batch 230 loss: 0.0005577517440542579
batch 235 loss: 0.0005576831637881697
batch 240 loss: 0.0005577291129156947
Training Loss: 0.0005576834943591772
Validation Loss: 0.0005576933986352135
Epoch 52:
batch 5 loss: 0.0005577073083259165
batch 10 loss: 0.0005576843512244522
batch 15 loss: 0.0005577534437179565
batch 20 loss: 0.0005577887059189379
batch 25 loss: 0.0005576357594691217
batch 30 loss: 0.0005576831172220409
batch 35 loss: 0.0005576590076088906
batch 40 loss: 0.000557710335124284
batch 45 loss: 0.0005576584953814745
batch 50 loss: 0.0005576001130975783
batch 55 loss: 0.0005576747003942728
batch 60 loss: 0.000557730917353183
batch 65 loss: 0.0005576408817432821
batch 70 loss: 0.0005577076459303498
batch 75 loss: 0.0005576350493356585
batch 80 loss: 0.0005577633040957153
batch 85 loss: 0.0005577142466790975
batch 90 loss: 0.0005576589377596974
batch 95 loss: 0.000557556957937777
batch 100 loss: 0.0005577563075348735
batch 105 loss: 0.0005577402655035258
batch 110 loss: 0.0005577406496740878
batch 115 loss: 0.0005576430936343968
batch 120 loss: 0.0005577393574640154
batch 125 loss: 0.0005577049683779478
batch 130 loss: 0.000557638006284833
batch 135 loss: 0.0005576213006861508
batch 140 loss: 0.0005575769464485347
batch 145 loss: 0.0005576420109719038
batch 150 loss: 0.0005577717209234833
batch 155 loss: 0.0005577000789344311
batch 160 loss: 0.000557632721029222
batch 165 loss: 0.0005577088915742934
batch 170 loss: 0.0005576923023909331
batch 175 loss: 0.0005576870869845151
batch 180 loss: 0.000557672360446304
batch 185 loss: 0.0005576213705353438
batch 190 loss: 0.0005576720926910639
batch 195 loss: 0.0005578069714829326
batch 200 loss: 0.0005577408708631992
batch 205 loss: 0.0005576822091825307
batch 210 loss: 0.0005575765622779727
batch 215 loss: 0.0005575963063165545
batch 220 loss: 0.0005576819530688226
batch 225 loss: 0.000557685422245413
batch 230 loss: 0.000557669554837048
batch 235 loss: 0.0005577464122325182
batch 240 loss: 0.0005576967378146946
Training Loss: 0.0005576834960569006
Validation Loss: 0.0005576935431842382
Epoch 53:
batch 5 loss: 0.000557677005417645
batch 10 loss: 0.0005576880997978151
batch 15 loss: 0.0005577600211836397
batch 20 loss: 0.0005576207186095417
batch 25 loss: 0.0005577471572905778
batch 30 loss: 0.0005576568306423724
batch 35 loss: 0.0005576574243605137
batch 40 loss: 0.000557611440308392
batch 45 loss: 0.0005576499737799167
batch 50 loss: 0.0005576459807343781
batch 55 loss: 0.0005576912662945688
batch 60 loss: 0.0005576575291343034
batch 65 loss: 0.0005576889961957932
batch 70 loss: 0.0005576668190769851
batch 75 loss: 0.0005577654344961047
batch 80 loss: 0.0005576840601861476
batch 85 loss: 0.0005576185300014913
batch 90 loss: 0.0005575890652835369
batch 95 loss: 0.0005578394862823189
batch 100 loss: 0.0005577123840339482
batch 105 loss: 0.0005576075753197074
batch 110 loss: 0.0005577874253503978
batch 115 loss: 0.0005576409981586039
batch 120 loss: 0.0005576969124376774
batch 125 loss: 0.0005576613708399236
batch 130 loss: 0.0005576114286668599
batch 135 loss: 0.00055764737771824
batch 140 loss: 0.0005576524534262716
batch 145 loss: 0.000557848799508065
batch 150 loss: 0.0005576055613346398
batch 155 loss: 0.0005577739211730659
batch 160 loss: 0.0005576811847276986
batch 165 loss: 0.0005575688672251999
batch 170 loss: 0.0005576540948823095
batch 175 loss: 0.0005576117662712931
batch 180 loss: 0.0005576578201726079
batch 185 loss: 0.0005577836651355028
batch 190 loss: 0.0005577467847615481
batch 195 loss: 0.0005577933392487466
batch 200 loss: 0.0005577040370553732
batch 205 loss: 0.0005576268071308732
batch 210 loss: 0.0005577532458119095
batch 215 loss: 0.0005576628493145109
batch 220 loss: 0.0005576908006332815
batch 225 loss: 0.0005577234551310539
batch 230 loss: 0.0005576632684096694
batch 235 loss: 0.0005576993804425001
batch 240 loss: 0.000557622779160738
Training Loss: 0.0005576834623449638
Validation Loss: 0.0005576938128797337
Epoch 54:
batch 5 loss: 0.0005576900555752217
batch 10 loss: 0.000557714409660548
batch 15 loss: 0.0005576313007622958
batch 20 loss: 0.0005577314295805991
batch 25 loss: 0.0005577570293098689
batch 30 loss: 0.000557758379727602
batch 35 loss: 0.0005575858056545257
batch 40 loss: 0.0005576550494879485
batch 45 loss: 0.0005576976225711405
batch 50 loss: 0.0005577265052124858
batch 55 loss: 0.0005576687632128597
batch 60 loss: 0.000557632592972368
batch 65 loss: 0.0005576813826337456
batch 70 loss: 0.0005577356438152492
batch 75 loss: 0.0005575986811891198
batch 80 loss: 0.0005577268195338548
batch 85 loss: 0.0005577321629971266
batch 90 loss: 0.0005575751769356429
batch 95 loss: 0.0005576428491622209
batch 100 loss: 0.0005576763534918428
batch 105 loss: 0.0005577429779805243
batch 110 loss: 0.0005576423485763371
batch 115 loss: 0.000557750288862735
batch 120 loss: 0.0005577313480898738
batch 125 loss: 0.0005576606956310571
batch 130 loss: 0.0005577606032602489
batch 135 loss: 0.0005576897063292563
batch 140 loss: 0.0005576426978223026
batch 145 loss: 0.00055769186001271
batch 150 loss: 0.0005575965740717947
batch 155 loss: 0.0005576259107328951
batch 160 loss: 0.00055777597008273
batch 165 loss: 0.000557641068007797
batch 170 loss: 0.000557737797498703
batch 175 loss: 0.0005578147480264306
batch 180 loss: 0.0005576085415668786
batch 185 loss: 0.000557645340450108
batch 190 loss: 0.0005577343865297735
batch 195 loss: 0.0005576669820584357
batch 200 loss: 0.0005576855270192027
batch 205 loss: 0.0005577108240686357
batch 210 loss: 0.00055766865843907
batch 215 loss: 0.0005576646537519992
batch 220 loss: 0.0005575919756665825
batch 225 loss: 0.0005576009745709598
batch 230 loss: 0.0005577387986704707
batch 235 loss: 0.0005577064584940672
batch 240 loss: 0.0005576618015766144
Training Loss: 0.0005576834902361346
Validation Loss: 0.0005576934985583648
Epoch 55:
batch 5 loss: 0.0005576788564212621
batch 10 loss: 0.0005577793926931918
batch 15 loss: 0.0005576828029006719
batch 20 loss: 0.0005577257252298295
batch 25 loss: 0.0005576427443884313
batch 30 loss: 0.0005577811622060836
batch 35 loss: 0.0005575884366407991
batch 40 loss: 0.0005577211501076818
batch 45 loss: 0.0005576617899350822
batch 50 loss: 0.000557686307001859
batch 55 loss: 0.000557677075266838
batch 60 loss: 0.0005576478899456561
batch 65 loss: 0.0005577466101385653
batch 70 loss: 0.000557560718152672
batch 75 loss: 0.0005576181574724615
batch 80 loss: 0.0005576789029873907
batch 85 loss: 0.0005576421739533543
batch 90 loss: 0.0005577755393460393
batch 95 loss: 0.0005578305572271347
batch 100 loss: 0.0005576642230153084
batch 105 loss: 0.0005576594383455812
batch 110 loss: 0.0005577082629315555
batch 115 loss: 0.0005576550262048841
batch 120 loss: 0.000557702174410224
batch 125 loss: 0.0005576112773269415
batch 130 loss: 0.0005578490789048373
batch 135 loss: 0.000557620101608336
batch 140 loss: 0.0005576619878411293
batch 145 loss: 0.000557696376927197
batch 150 loss: 0.0005576715338975191
batch 155 loss: 0.0005576113471761346
batch 160 loss: 0.0005577016738243401
batch 165 loss: 0.00055768140591681
batch 170 loss: 0.0005576613359153271
batch 175 loss: 0.0005577219650149346
batch 180 loss: 0.0005576899857260287
batch 185 loss: 0.0005577311618253589
batch 190 loss: 0.0005577990901656449
batch 195 loss: 0.0005576845374889672
batch 200 loss: 0.0005577041301876307
batch 205 loss: 0.0005576945259235799
batch 210 loss: 0.0005576972384005785
batch 215 loss: 0.0005576126161031425
batch 220 loss: 0.000557629147078842
batch 225 loss: 0.0005575196002610028
batch 230 loss: 0.00055770956678316
batch 235 loss: 0.0005576033261604607
batch 240 loss: 0.0005577291361987591
Training Loss: 0.0005576834846579004
Validation Loss: 0.0005576934752753004
Epoch 56:
batch 5 loss: 0.0005576508585363627
batch 10 loss: 0.0005576445837505162
batch 15 loss: 0.0005576251773163676
batch 20 loss: 0.000557761185336858
batch 25 loss: 0.0005576879950240255
batch 30 loss: 0.0005576747935265303
batch 35 loss: 0.0005576423252932727
batch 40 loss: 0.0005576731637120247
batch 45 loss: 0.0005577240022830665
batch 50 loss: 0.0005576744908466936
batch 55 loss: 0.0005577753647230566
batch 60 loss: 0.0005578220472671092
batch 65 loss: 0.0005576873198151588
batch 70 loss: 0.0005577654927037656
batch 75 loss: 0.0005577341304160655
batch 80 loss: 0.0005575875286012888
batch 85 loss: 0.0005576184019446373
batch 90 loss: 0.000557709950953722
batch 95 loss: 0.0005577748641371727
batch 100 loss: 0.0005576839088462293
batch 105 loss: 0.0005576623952947557
batch 110 loss: 0.0005576852825470268
batch 115 loss: 0.0005576608120463789
batch 120 loss: 0.0005576499272137881
batch 125 loss: 0.0005576606956310571
batch 130 loss: 0.0005576849100179971
batch 135 loss: 0.0005576372030191123
batch 140 loss: 0.0005577121279202402
batch 145 loss: 0.0005576049094088376
batch 150 loss: 0.0005577523959800601
batch 155 loss: 0.0005576940486207605
batch 160 loss: 0.0005576327675953507
batch 165 loss: 0.0005577073199674488
batch 170 loss: 0.0005577229778282345
batch 175 loss: 0.0005576462484896183
batch 180 loss: 0.0005575932329520584
batch 185 loss: 0.0005577402538619935
batch 190 loss: 0.0005577788106165826
batch 195 loss: 0.0005577277159318327
batch 200 loss: 0.0005576901487074793
batch 205 loss: 0.0005575986579060555
batch 210 loss: 0.0005575849674642086
batch 215 loss: 0.0005577407078817487
batch 220 loss: 0.000557710265275091
batch 225 loss: 0.0005576050258241594
batch 230 loss: 0.000557688414119184
batch 235 loss: 0.0005575905088335276
batch 240 loss: 0.0005577268777415157
Training Loss: 0.0005576834832027089
Validation Loss: 0.0005576934005754689
Epoch 57:
batch 5 loss: 0.0005576849333010613
batch 10 loss: 0.0005576549679972232
batch 15 loss: 0.000557656493037939
batch 20 loss: 0.0005576273892074823
batch 25 loss: 0.0005576092284172773
batch 30 loss: 0.0005577125935815275
batch 35 loss: 0.0005577218253165483
batch 40 loss: 0.0005575497634708881
batch 45 loss: 0.0005576258758082986
batch 50 loss: 0.0005577297531999647
batch 55 loss: 0.0005577727686613798
batch 60 loss: 0.0005577108822762966
batch 65 loss: 0.0005576568190008401
batch 70 loss: 0.0005576605792157352
batch 75 loss: 0.0005577608244493604
batch 80 loss: 0.0005576249794103205
batch 85 loss: 0.0005576801719143987
batch 90 loss: 0.0005577177624218165
batch 95 loss: 0.0005578613607212901
batch 100 loss: 0.0005577036296017468
batch 105 loss: 0.0005577058182097971
batch 110 loss: 0.0005577321979217231
batch 115 loss: 0.0005576831754297018
batch 120 loss: 0.0005576689611189068
batch 125 loss: 0.0005577072384767235
batch 130 loss: 0.0005577403120696544
batch 135 loss: 0.0005576862837187946
batch 140 loss: 0.0005577028845436871
batch 145 loss: 0.0005575712071731687
batch 150 loss: 0.0005577923613600433
batch 155 loss: 0.0005576946423389018
batch 160 loss: 0.0005576337571255863
batch 165 loss: 0.0005577606149017811
batch 170 loss: 0.000557674840092659
batch 175 loss: 0.0005577175761573017
batch 180 loss: 0.0005576378898695111
batch 185 loss: 0.000557684525847435
batch 190 loss: 0.000557660439517349
batch 195 loss: 0.0005577511386945843
batch 200 loss: 0.0005577094270847738
batch 205 loss: 0.0005576023366302251
batch 210 loss: 0.0005576422903686762
batch 215 loss: 0.0005577098578214646
batch 220 loss: 0.0005575571674853563
batch 225 loss: 0.0005576555035077035
batch 230 loss: 0.000557615200523287
batch 235 loss: 0.0005576751544140279
batch 240 loss: 0.0005577111151069403
Training Loss: 0.0005576834691358574
Validation Loss: 0.0005576934723649174
Epoch 58:
batch 5 loss: 0.0005576122202910483
batch 10 loss: 0.0005577734089456498
batch 15 loss: 0.0005576406023465097
batch 20 loss: 0.0005578243057243526
batch 25 loss: 0.0005577056319452823
batch 30 loss: 0.0005577240604907274
batch 35 loss: 0.0005576879251748323
batch 40 loss: 0.0005576890544034541
batch 45 loss: 0.0005576274241320788
batch 50 loss: 0.0005576670984737575
batch 55 loss: 0.00055766279110685
batch 60 loss: 0.0005576256546191872
batch 65 loss: 0.0005578259588219225
batch 70 loss: 0.000557702302467078
batch 75 loss: 0.0005576445255428553
batch 80 loss: 0.0005577040719799697
batch 85 loss: 0.0005575850256718696
batch 90 loss: 0.0005576890194788575
batch 95 loss: 0.0005576696945354343
batch 100 loss: 0.0005577701493166387
batch 105 loss: 0.0005576811265200377
batch 110 loss: 0.0005576668423600495
batch 115 loss: 0.0005576851195655763
batch 120 loss: 0.0005577112780883909
batch 125 loss: 0.000557654828298837
batch 130 loss: 0.0005577101605013013
batch 135 loss: 0.000557669554837048
batch 140 loss: 0.0005577511619776487
batch 145 loss: 0.0005575333605520427
batch 150 loss: 0.0005576801602728664
batch 155 loss: 0.0005576719297096134
batch 160 loss: 0.0005575896822847426
batch 165 loss: 0.0005576450843364
batch 170 loss: 0.0005576303927227854
batch 175 loss: 0.000557793362531811
batch 180 loss: 0.0005577222793363035
batch 185 loss: 0.0005576779833063484
batch 190 loss: 0.0005577531526796519
batch 195 loss: 0.0005575518938712775
batch 200 loss: 0.0005576852126978338
batch 205 loss: 0.0005577553063631057
batch 210 loss: 0.0005577736650593579
batch 215 loss: 0.0005576964118517935
batch 220 loss: 0.0005576333380304277
batch 225 loss: 0.0005577053758315742
batch 230 loss: 0.0005577275762334466
batch 235 loss: 0.0005575914634391666
batch 240 loss: 0.000557627622038126
Training Loss: 0.0005576834635576233
Validation Loss: 0.000557693595571133
Epoch 59:
batch 5 loss: 0.0005575834424234926
batch 10 loss: 0.0005577881471253932
batch 15 loss: 0.0005577325238846243
batch 20 loss: 0.0005576787400059402
batch 25 loss: 0.0005576774245128035
batch 30 loss: 0.0005576541414484381
batch 35 loss: 0.0005576897878199816
batch 40 loss: 0.0005576785188168287
batch 45 loss: 0.0005576366791501641
batch 50 loss: 0.0005577301140874624
batch 55 loss: 0.0005576797644607723
batch 60 loss: 0.0005576240830123425
batch 65 loss: 0.0005576836876571179
batch 70 loss: 0.0005578559939749539
batch 75 loss: 0.0005576738971285522
batch 80 loss: 0.000557689182460308
batch 85 loss: 0.0005576492636464536
batch 90 loss: 0.0005575935705564916
batch 95 loss: 0.000557730020955205
batch 100 loss: 0.0005576838972046972
batch 105 loss: 0.0005576254217885435
batch 110 loss: 0.0005576532101258635
batch 115 loss: 0.0005576578783802688
batch 120 loss: 0.0005577210919000209
batch 125 loss: 0.0005576913361437618
batch 130 loss: 0.0005577329429797828
batch 135 loss: 0.0005576253868639469
batch 140 loss: 0.0005577804171480238
batch 145 loss: 0.000557701091747731
batch 150 loss: 0.0005576049443334341
batch 155 loss: 0.0005576419644057751
batch 160 loss: 0.0005577123840339482
batch 165 loss: 0.0005576254450716078
batch 170 loss: 0.000557736458722502
batch 175 loss: 0.0005577122094109655
batch 180 loss: 0.0005575428600423038
batch 185 loss: 0.0005577046773396433
batch 190 loss: 0.0005576182622462511
batch 195 loss: 0.0005576625117100775
batch 200 loss: 0.0005577499861828983
batch 205 loss: 0.0005577731761150062
batch 210 loss: 0.0005576956551522017
batch 215 loss: 0.0005576774128712714
batch 220 loss: 0.0005577418487519026
batch 225 loss: 0.0005576599156484008
batch 230 loss: 0.0005577636184170842
batch 235 loss: 0.0005577450850978493
batch 240 loss: 0.0005575358285568654
Training Loss: 0.0005576834562816657
Validation Loss: 0.0005576934151273841
Epoch 60:
batch 5 loss: 0.0005576594965532422
batch 10 loss: 0.0005577004631049931
batch 15 loss: 0.0005576853174716235
batch 20 loss: 0.0005576532799750567
batch 25 loss: 0.0005575808230787515
batch 30 loss: 0.0005576893338002265
batch 35 loss: 0.0005576829891651869
batch 40 loss: 0.0005576680763624608
batch 45 loss: 0.0005577916745096445
batch 50 loss: 0.0005577208823524416
batch 55 loss: 0.0005577659816481173
batch 60 loss: 0.0005577420582994818
batch 65 loss: 0.000557611184194684
batch 70 loss: 0.0005577186588197946
batch 75 loss: 0.000557623989880085
batch 80 loss: 0.0005575969349592924
batch 85 loss: 0.0005576307186856866
batch 90 loss: 0.0005577335832640529
batch 95 loss: 0.0005576758994720876
batch 100 loss: 0.0005576774827204644
batch 105 loss: 0.0005575975519604981
batch 110 loss: 0.0005578327807597816
batch 115 loss: 0.0005576586583629251
batch 120 loss: 0.000557672418653965
batch 125 loss: 0.0005575859569944442
batch 130 loss: 0.0005577745847404003
batch 135 loss: 0.000557701475918293
batch 140 loss: 0.0005576157942414284
batch 145 loss: 0.0005577570060268045
batch 150 loss: 0.0005575975286774337
batch 155 loss: 0.0005578091368079185
batch 160 loss: 0.000557799485977739
batch 165 loss: 0.0005576404044404625
batch 170 loss: 0.0005576764582656324
batch 175 loss: 0.0005576838157139719
batch 180 loss: 0.0005577551200985909
batch 185 loss: 0.0005575868184678256
batch 190 loss: 0.0005578280077315867
batch 195 loss: 0.0005576950614340603
batch 200 loss: 0.0005576484254561365
batch 205 loss: 0.0005577452015131712
batch 210 loss: 0.0005576754454523325
batch 215 loss: 0.0005576853873208165
batch 220 loss: 0.0005576183204539121
batch 225 loss: 0.0005575926625169814
batch 230 loss: 0.0005576700554229319
batch 235 loss: 0.0005576286348514259
batch 240 loss: 0.000557665852829814
Training Loss: 0.0005576834766543471
Validation Loss: 0.0005576934694545343
Epoch 61:
batch 5 loss: 0.0005576925817877054
batch 10 loss: 0.0005577120929956437
batch 15 loss: 0.0005578160053119063
batch 20 loss: 0.0005576936877332628
batch 25 loss: 0.0005575295886956156
batch 30 loss: 0.0005577473435550928
batch 35 loss: 0.0005577380303293467
batch 40 loss: 0.000557603407651186
batch 45 loss: 0.0005576742580160499
batch 50 loss: 0.0005576947005465627
batch 55 loss: 0.0005577320931479335
batch 60 loss: 0.0005577486474066973
batch 65 loss: 0.0005576913943514227
batch 70 loss: 0.000557615514844656
batch 75 loss: 0.0005575975868850946
batch 80 loss: 0.0005576964817009866
batch 85 loss: 0.0005575861199758947
batch 90 loss: 0.0005577261559665203
batch 95 loss: 0.0005578698473982513
batch 100 loss: 0.0005576395429670811
batch 105 loss: 0.0005576726980507374
batch 110 loss: 0.0005576098337769508
batch 115 loss: 0.0005576723022386431
batch 120 loss: 0.0005577548174187541
batch 125 loss: 0.0005576710333116353
batch 130 loss: 0.0005576846073381603
batch 135 loss: 0.0005575923365540803
batch 140 loss: 0.0005576785886660218
batch 145 loss: 0.0005575932911597193
batch 150 loss: 0.0005577323958277702
batch 155 loss: 0.0005576937925070524
batch 160 loss: 0.0005577292526140809
batch 165 loss: 0.000557799160014838
batch 170 loss: 0.0005576492054387927
batch 175 loss: 0.0005576537572778761
batch 180 loss: 0.0005575855728238821
batch 185 loss: 0.0005576495663262903
batch 190 loss: 0.0005576245137490332
batch 195 loss: 0.0005576947238296271
batch 200 loss: 0.0005576926167123019
batch 205 loss: 0.0005577099742367864
batch 210 loss: 0.0005577276810072362
batch 215 loss: 0.0005576733965426683
batch 220 loss: 0.0005576895666308701
batch 225 loss: 0.0005575623246841133
batch 230 loss: 0.0005578441778197885
batch 235 loss: 0.0005577096599154174
batch 240 loss: 0.0005576500785537064
Training Loss: 0.000557683458464453
Validation Loss: 0.0005576934306494271
Epoch 62:
batch 5 loss: 0.0005576116614975035
batch 10 loss: 0.0005577236879616976
batch 15 loss: 0.000557630555704236
batch 20 loss: 0.0005576660274527967
batch 25 loss: 0.0005577047355473042
batch 30 loss: 0.0005576928495429456
batch 35 loss: 0.0005576690076850354
batch 40 loss: 0.0005576675874181091
batch 45 loss: 0.0005577856325544416
batch 50 loss: 0.0005575765040703118
batch 55 loss: 0.0005576818366535008
batch 60 loss: 0.0005576522671617568
batch 65 loss: 0.0005575375398620963
batch 70 loss: 0.0005577646545134485
batch 75 loss: 0.0005577892763540149
batch 80 loss: 0.000557628646492958
batch 85 loss: 0.0005577152478508651
batch 90 loss: 0.000557819870300591
batch 95 loss: 0.0005576377501711249
batch 100 loss: 0.0005576350726187229
batch 105 loss: 0.000557635398581624
batch 110 loss: 0.0005575759918428957
batch 115 loss: 0.0005576713825576008
batch 120 loss: 0.000557642593048513
batch 125 loss: 0.0005576917435973882
batch 130 loss: 0.0005576720694079995
batch 135 loss: 0.0005577477859333158
batch 140 loss: 0.0005577198928222061
batch 145 loss: 0.0005576245370320976
batch 150 loss: 0.0005576111725531518
batch 155 loss: 0.0005575717077590525
batch 160 loss: 0.000557762326207012
batch 165 loss: 0.0005576261319220066
batch 170 loss: 0.0005577282048761844
batch 175 loss: 0.0005576282273977995
batch 180 loss: 0.0005577184027060866
batch 185 loss: 0.0005576729075983166
batch 190 loss: 0.0005576829076744616
batch 195 loss: 0.0005577327101491392
batch 200 loss: 0.0005577883217483759
batch 205 loss: 0.0005576831521466374
batch 210 loss: 0.0005576901836320758
batch 215 loss: 0.000557785655837506
batch 220 loss: 0.0005577203119173646
batch 225 loss: 0.0005577352014370263
batch 230 loss: 0.000557669682893902
batch 235 loss: 0.0005577210802584886
batch 240 loss: 0.0005577046540565789
Training Loss: 0.0005576834322710056
Validation Loss: 0.0005576934578130022
Epoch 63:
batch 5 loss: 0.0005576725583523512
batch 10 loss: 0.0005576322204433381
batch 15 loss: 0.0005576164578087628
batch 20 loss: 0.0005577413830906153
batch 25 loss: 0.0005576595547609031
batch 30 loss: 0.0005576449213549495
batch 35 loss: 0.0005577392526902259
batch 40 loss: 0.0005577641655690968
batch 45 loss: 0.0005577609525062143
batch 50 loss: 0.0005576580879278481
batch 55 loss: 0.0005576024879701436
batch 60 loss: 0.0005575411138124764
batch 65 loss: 0.0005577364470809698
batch 70 loss: 0.0005577034549787641
batch 75 loss: 0.0005576909170486033
batch 80 loss: 0.0005576178315095604
batch 85 loss: 0.0005576836760155856
batch 90 loss: 0.0005575711256824434
batch 95 loss: 0.0005576579482294619
batch 100 loss: 0.0005576282972469925
batch 105 loss: 0.000557681336067617
batch 110 loss: 0.000557609018869698
batch 115 loss: 0.0005577131872996688
batch 120 loss: 0.0005576596595346928
batch 125 loss: 0.0005576891708187759
batch 130 loss: 0.0005576696596108377
batch 135 loss: 0.0005577376810833812
batch 140 loss: 0.000557663501240313
batch 145 loss: 0.0005576675175689161
batch 150 loss: 0.0005576830008067191
batch 155 loss: 0.0005575430695898831
batch 160 loss: 0.0005576052120886743
batch 165 loss: 0.0005576909170486033
batch 170 loss: 0.000557591242250055
batch 175 loss: 0.0005576901487074793
batch 180 loss: 0.0005577389150857925
batch 185 loss: 0.000557788775768131
batch 190 loss: 0.000557718111667782
batch 195 loss: 0.0005577899049967527
batch 200 loss: 0.0005577156669460237
batch 205 loss: 0.0005577032919973135
batch 210 loss: 0.0005576515337452293
batch 215 loss: 0.0005577094852924347
batch 220 loss: 0.0005577738513238728
batch 225 loss: 0.0005578198004513979
batch 230 loss: 0.0005578090553171932
batch 235 loss: 0.0005576833500526845
batch 240 loss: 0.0005576866329647601
Training Loss: 0.0005576834490057081
Validation Loss: 0.0005576934393805762
Epoch 64:
batch 5 loss: 0.000557660881895572
batch 10 loss: 0.0005577538278885186
batch 15 loss: 0.0005577280186116696
batch 20 loss: 0.0005576819996349514
batch 25 loss: 0.000557740859221667
batch 30 loss: 0.0005575691233389079
batch 35 loss: 0.0005575957708060742
batch 40 loss: 0.0005578250158578158
batch 45 loss: 0.0005576679133810103
batch 50 loss: 0.0005577344447374344
batch 55 loss: 0.0005575947114266455
batch 60 loss: 0.0005576134892180562
batch 65 loss: 0.0005576585186645389
batch 70 loss: 0.0005577177857048809
batch 75 loss: 0.0005576583556830883
batch 80 loss: 0.0005576936760917306
batch 85 loss: 0.0005576905095949769
batch 90 loss: 0.0005575584946200252
batch 95 loss: 0.0005577050615102053
batch 100 loss: 0.0005576917668804526
batch 105 loss: 0.0005576357245445252
batch 110 loss: 0.0005576877854764461
batch 115 loss: 0.0005575888440944255
batch 120 loss: 0.0005577024072408676
batch 125 loss: 0.0005577509407885372
batch 130 loss: 0.0005577576230280101
batch 135 loss: 0.0005576923373155296
batch 140 loss: 0.0005577121046371758
batch 145 loss: 0.0005577532458119095
batch 150 loss: 0.0005576586001552642
batch 155 loss: 0.0005577468662522733
batch 160 loss: 0.0005576066789217294
batch 165 loss: 0.0005576886585913598
batch 170 loss: 0.0005577761796303094
batch 175 loss: 0.0005577313364483416
batch 180 loss: 0.0005577230826020241
batch 185 loss: 0.0005576036986894905
batch 190 loss: 0.0005577025120146572
batch 195 loss: 0.0005577302537858487
batch 200 loss: 0.0005577781121246517
batch 205 loss: 0.0005576629308052361
batch 210 loss: 0.0005575877614319325
batch 215 loss: 0.0005576655734330416
batch 220 loss: 0.0005576623254455626
batch 225 loss: 0.0005576902418397367
batch 230 loss: 0.0005576404975727201
batch 235 loss: 0.0005576727213338018
batch 240 loss: 0.0005576560040935874
Training Loss: 0.000557683443184942
Validation Loss: 0.000557693449081853
Epoch 65:
batch 5 loss: 0.0005577831179834902
batch 10 loss: 0.0005576384603045881
batch 15 loss: 0.0005576956085860729
batch 20 loss: 0.000557699182536453
batch 25 loss: 0.0005576100898906589
batch 30 loss: 0.0005576637922786176
batch 35 loss: 0.0005577420699410141
batch 40 loss: 0.0005576238501816988
batch 45 loss: 0.0005576975410804153
batch 50 loss: 0.0005576482624746859
batch 55 loss: 0.0005577275995165109
batch 60 loss: 0.0005575368762947619
batch 65 loss: 0.0005577344330959022
batch 70 loss: 0.0005577763542532921
batch 75 loss: 0.0005576026742346585
batch 80 loss: 0.0005577298928983509
batch 85 loss: 0.0005576177849434316
batch 90 loss: 0.0005576818250119687
batch 95 loss: 0.0005576608469709754
batch 100 loss: 0.0005576231633313
batch 105 loss: 0.0005577439675107599
batch 110 loss: 0.0005576613591983914
batch 115 loss: 0.0005576451658271253
batch 120 loss: 0.0005576232098974288
batch 125 loss: 0.0005577943520620465
batch 130 loss: 0.0005578421987593174
batch 135 loss: 0.0005578461452387274
batch 140 loss: 0.0005575920571573078
batch 145 loss: 0.0005577521747909487
batch 150 loss: 0.0005577314645051956
batch 155 loss: 0.0005576849333010613
batch 160 loss: 0.0005576695781201124
batch 165 loss: 0.0005576454801484943
batch 170 loss: 0.0005577113479375839
batch 175 loss: 0.0005577114992775023
batch 180 loss: 0.000557678984478116
batch 185 loss: 0.0005576951312832535
batch 190 loss: 0.0005576233379542828
batch 195 loss: 0.0005576455732807517
batch 200 loss: 0.0005577626056037843
batch 205 loss: 0.0005576613359153271
batch 210 loss: 0.0005576798925176263
batch 215 loss: 0.0005575196933932603
batch 220 loss: 0.0005576836178079247
batch 225 loss: 0.0005575279821641744
batch 230 loss: 0.0005577253177762032
batch 235 loss: 0.0005577872740104795
batch 240 loss: 0.0005576652591116726
Training Loss: 0.0005576834242674522
Validation Loss: 0.0005576934296792994
Epoch 66:
batch 5 loss: 0.0005576728493906557
batch 10 loss: 0.000557697971817106
batch 15 loss: 0.0005577715230174363
batch 20 loss: 0.0005576891009695828
batch 25 loss: 0.0005576680414378643
batch 30 loss: 0.0005577685660682619
batch 35 loss: 0.0005576538038440049
batch 40 loss: 0.0005576536990702152
batch 45 loss: 0.000557661394122988
batch 50 loss: 0.0005576701485551893
batch 55 loss: 0.0005576347466558218
batch 60 loss: 0.0005577184841968119
batch 65 loss: 0.0005577242583967746
batch 70 loss: 0.0005576619878411293
batch 75 loss: 0.0005576213472522796
batch 80 loss: 0.0005577225936576724
batch 85 loss: 0.0005576533731073141
batch 90 loss: 0.0005576221970841289
batch 95 loss: 0.0005575561779551208
batch 100 loss: 0.0005577644100412726
batch 105 loss: 0.0005577089730650186
batch 110 loss: 0.0005576939904130996
batch 115 loss: 0.0005576397757977247
batch 120 loss: 0.0005576406139880419
batch 125 loss: 0.0005576922209002078
batch 130 loss: 0.0005576144438236951
batch 135 loss: 0.000557580147869885
batch 140 loss: 0.0005576663883402943
batch 145 loss: 0.0005577473784796894
batch 150 loss: 0.0005576720577664673
batch 155 loss: 0.0005576019524596631
batch 160 loss: 0.0005576701718382537
batch 165 loss: 0.0005577384843491017
batch 170 loss: 0.0005577656207606197
batch 175 loss: 0.0005576705792918801
batch 180 loss: 0.0005578346434049308
batch 185 loss: 0.0005577099393121899
batch 190 loss: 0.0005577610805630684
batch 195 loss: 0.00055765132419765
batch 200 loss: 0.0005576600437052548
batch 205 loss: 0.0005576302530243993
batch 210 loss: 0.0005578370881266892
batch 215 loss: 0.0005576471914537251
batch 220 loss: 0.0005576543393544853
batch 225 loss: 0.0005576657247729599
batch 230 loss: 0.0005577289965003728
batch 235 loss: 0.0005576143157668412
batch 240 loss: 0.0005577197181992233
Training Loss: 0.0005576834194168138
Validation Loss: 0.0005576935470647489
Epoch 67:
batch 5 loss: 0.0005577184609137475
batch 10 loss: 0.000557709310669452
batch 15 loss: 0.0005576589261181653
batch 20 loss: 0.0005576665746048093
batch 25 loss: 0.000557656493037939
batch 30 loss: 0.0005576987750828266
batch 35 loss: 0.0005576530587859452
batch 40 loss: 0.000557777809444815
batch 45 loss: 0.0005577233270742
batch 50 loss: 0.0005577666335739196
batch 55 loss: 0.0005577175877988338
batch 60 loss: 0.0005576063063926995
batch 65 loss: 0.0005576936295256018
batch 70 loss: 0.0005577356554567814
batch 75 loss: 0.0005576805910095572
batch 80 loss: 0.0005576372961513699
batch 85 loss: 0.0005577618489041924
batch 90 loss: 0.0005575029412284493
batch 95 loss: 0.0005576478200964629
batch 100 loss: 0.0005577869014814496
batch 105 loss: 0.0005575906252488494
batch 110 loss: 0.0005577216274105012
batch 115 loss: 0.0005576344323344528
batch 120 loss: 0.0005577104748226702
batch 125 loss: 0.000557588948868215
batch 130 loss: 0.0005576457246206701
batch 135 loss: 0.0005576579016633332
batch 140 loss: 0.0005576557479798794
batch 145 loss: 0.0005576318129897117
batch 150 loss: 0.0005576923256739974
batch 155 loss: 0.0005576073541305959
batch 160 loss: 0.0005575696704909205
batch 165 loss: 0.0005577847710810602
batch 170 loss: 0.0005576687399297953
batch 175 loss: 0.0005577530129812658
batch 180 loss: 0.0005576429190114141
batch 185 loss: 0.0005577371455729007
batch 190 loss: 0.000557645654771477
batch 195 loss: 0.000557579135056585
batch 200 loss: 0.0005578155047260225
batch 205 loss: 0.0005577781586907804
batch 210 loss: 0.0005576947005465627
batch 215 loss: 0.000557635398581624
batch 220 loss: 0.0005576649447903038
batch 225 loss: 0.0005577528150752187
batch 230 loss: 0.0005576884024776519
batch 235 loss: 0.000557727029081434
batch 240 loss: 0.0005577292176894844
Training Loss: 0.0005576834196593457
Validation Loss: 0.0005576934219182779
Epoch 68:
batch 5 loss: 0.0005576706142164766
batch 10 loss: 0.0005576811847276986
batch 15 loss: 0.0005576448049396277
batch 20 loss: 0.0005578187992796302
batch 25 loss: 0.0005576570751145482
batch 30 loss: 0.0005577158299274743
batch 35 loss: 0.0005576475290581584
batch 40 loss: 0.0005577335017733276
batch 45 loss: 0.0005576057243160904
batch 50 loss: 0.0005576811730861664
batch 55 loss: 0.0005575991352088749
batch 60 loss: 0.0005576822091825307
batch 65 loss: 0.0005577531410381198
batch 70 loss: 0.0005577795789577066
batch 75 loss: 0.0005577033152803779
batch 80 loss: 0.0005576782743446529
batch 85 loss: 0.0005576278315857052
batch 90 loss: 0.0005576140363700687
batch 95 loss: 0.000557760032825172
batch 100 loss: 0.0005576489609666168
batch 105 loss: 0.0005576580064371228
batch 110 loss: 0.0005577059579081834
batch 115 loss: 0.0005575905903242529
batch 120 loss: 0.0005577469593845308
batch 125 loss: 0.0005576341529376805
batch 130 loss: 0.0005576672847382724
batch 135 loss: 0.00055771708721295
batch 140 loss: 0.0005576576455496252
batch 145 loss: 0.0005578265525400638
batch 150 loss: 0.0005575974937528372
batch 155 loss: 0.0005577357369475067
batch 160 loss: 0.0005576602183282375
batch 165 loss: 0.0005576530122198165
batch 170 loss: 0.0005577540723606944
batch 175 loss: 0.0005577211850322783
batch 180 loss: 0.0005577166331931949
batch 185 loss: 0.0005575980292633175
batch 190 loss: 0.0005577331176027656
batch 195 loss: 0.0005577169125899672
batch 200 loss: 0.0005576796946115791
batch 205 loss: 0.0005576163879595697
batch 210 loss: 0.0005577239790000021
batch 215 loss: 0.0005576784838922322
batch 220 loss: 0.0005576927680522204
batch 225 loss: 0.0005575968651100993
batch 230 loss: 0.0005576872965320945
batch 235 loss: 0.0005576783558353781
batch 240 loss: 0.0005576574825681746
Training Loss: 0.0005576834315434098
Validation Loss: 0.000557693598481516
Epoch 69:
batch 5 loss: 0.0005577210686169565
batch 10 loss: 0.0005575920338742435
batch 15 loss: 0.0005576095310971141
batch 20 loss: 0.0005577051430009306
batch 25 loss: 0.0005576125462539494
batch 30 loss: 0.0005577974370680749
batch 35 loss: 0.000557687203399837
batch 40 loss: 0.0005577201838605106
batch 45 loss: 0.00055771938059479
batch 50 loss: 0.0005576016963459551
batch 55 loss: 0.0005576618830673396
batch 60 loss: 0.0005577124189585448
batch 65 loss: 0.0005577710107900202
batch 70 loss: 0.0005577474599704147
batch 75 loss: 0.0005576454102993011
batch 80 loss: 0.0005576807307079434
batch 85 loss: 0.0005575286806561053
batch 90 loss: 0.0005576607305556536
batch 95 loss: 0.0005576741648837924
batch 100 loss: 0.000557662860956043
batch 105 loss: 0.0005576943047344684
batch 110 loss: 0.0005576893105171621
batch 115 loss: 0.0005577007774263621
batch 120 loss: 0.0005577269359491766
batch 125 loss: 0.0005576214869506657
batch 130 loss: 0.0005576912313699723
batch 135 loss: 0.0005577137460932135
batch 140 loss: 0.0005576176568865776
batch 145 loss: 0.0005576137802563608
batch 150 loss: 0.000557720591314137
batch 155 loss: 0.0005577321164309979
batch 160 loss: 0.0005576174007728696
batch 165 loss: 0.0005576330237090588
batch 170 loss: 0.0005576697760261596
batch 175 loss: 0.0005577463191002608
batch 180 loss: 0.0005576992291025818
batch 185 loss: 0.0005578546901233494
batch 190 loss: 0.00055776780936867
batch 195 loss: 0.0005577042582444846
batch 200 loss: 0.0005577176809310914
batch 205 loss: 0.0005576884374022484
batch 210 loss: 0.0005576948868110776
batch 215 loss: 0.0005576625699177384
batch 220 loss: 0.000557615514844656
batch 225 loss: 0.0005576741532422602
batch 230 loss: 0.0005577720119617879
batch 235 loss: 0.0005575955263338983
batch 240 loss: 0.0005576587282121181
Training Loss: 0.0005576834485206442
Validation Loss: 0.0005576934141572565
Epoch 70:
batch 5 loss: 0.0005576735245995223
batch 10 loss: 0.0005576511146500706
batch 15 loss: 0.0005577186006121337
batch 20 loss: 0.0005577925359830261
batch 25 loss: 0.0005578001029789448
batch 30 loss: 0.0005576485418714583
batch 35 loss: 0.0005576451309025288
batch 40 loss: 0.0005577689618803561
batch 45 loss: 0.0005576900439336896
batch 50 loss: 0.0005575060262344777
batch 55 loss: 0.0005577480769716203
batch 60 loss: 0.0005577079835347831
batch 65 loss: 0.0005577788222581148
batch 70 loss: 0.0005577231058850884
batch 75 loss: 0.0005575919174589216
batch 80 loss: 0.0005577550968155265
batch 85 loss: 0.0005576052470132708
batch 90 loss: 0.0005576244788244366
batch 95 loss: 0.0005576013354584575
batch 100 loss: 0.0005576586816459894
batch 105 loss: 0.0005577582865953446
batch 110 loss: 0.0005576094845309854
batch 115 loss: 0.0005577612784691155
batch 120 loss: 0.0005577689385972917
batch 125 loss: 0.0005575495306402445
batch 130 loss: 0.0005576411727815867
batch 135 loss: 0.0005576462484896183
batch 140 loss: 0.0005577289033681154
batch 145 loss: 0.0005576190655119717
batch 150 loss: 0.000557649484835565
batch 155 loss: 0.000557776668574661
batch 160 loss: 0.0005576484720222652
batch 165 loss: 0.0005576260737143457
batch 170 loss: 0.0005575842224061489
batch 175 loss: 0.0005576886469498277
batch 180 loss: 0.0005576371331699193
batch 185 loss: 0.0005578320473432541
batch 190 loss: 0.0005576812080107629
batch 195 loss: 0.0005577107658609748
batch 200 loss: 0.0005576027557253838
batch 205 loss: 0.0005576735245995223
batch 210 loss: 0.0005577105446718633
batch 215 loss: 0.000557674269657582
batch 220 loss: 0.0005576684023253619
batch 225 loss: 0.0005577752599492669
batch 230 loss: 0.000557673501316458
batch 235 loss: 0.0005576708004809916
batch 240 loss: 0.0005577482981607318
Training Loss: 0.0005576834232973245
Validation Loss: 0.0005576935063193862
Epoch 71:
batch 5 loss: 0.0005576282972469925
batch 10 loss: 0.0005575927090831101
batch 15 loss: 0.0005576554336585105
batch 20 loss: 0.0005577511969022453
batch 25 loss: 0.0005576636060141027
batch 30 loss: 0.0005576168419793248
batch 35 loss: 0.0005576869589276612
batch 40 loss: 0.0005576384253799915
batch 45 loss: 0.0005576986470259726
batch 50 loss: 0.0005577038740739226
batch 55 loss: 0.0005577521282248199
batch 60 loss: 0.0005575676681473851
batch 65 loss: 0.000557691021822393
batch 70 loss: 0.0005577206378802657
batch 75 loss: 0.0005576089257374406
batch 80 loss: 0.0005577585659921169
batch 85 loss: 0.0005576857016421855
batch 90 loss: 0.000557714095339179
batch 95 loss: 0.0005576811963692307
batch 100 loss: 0.0005575925810262561
batch 105 loss: 0.0005577250733040274
batch 110 loss: 0.000557796808425337
batch 115 loss: 0.0005575737566687166
batch 120 loss: 0.0005575431976467371
batch 125 loss: 0.0005576569703407586
batch 130 loss: 0.0005577895091846586
batch 135 loss: 0.0005576971569098532
batch 140 loss: 0.0005576811730861664
batch 145 loss: 0.0005576687515713274
batch 150 loss: 0.0005576856900006532
batch 155 loss: 0.0005577238509431482
batch 160 loss: 0.000557789858430624
batch 165 loss: 0.0005576441064476967
batch 170 loss: 0.0005577015108428895
batch 175 loss: 0.0005576286115683615
batch 180 loss: 0.0005577992298640311
batch 185 loss: 0.0005576897645369172
batch 190 loss: 0.0005575508112087846
batch 195 loss: 0.0005577300093136728
batch 200 loss: 0.0005577435134910047
batch 205 loss: 0.0005576757830567658
batch 210 loss: 0.0005577219766564667
batch 215 loss: 0.0005576573195867241
batch 220 loss: 0.0005577406496740878
batch 225 loss: 0.000557681976351887
batch 230 loss: 0.0005576882045716048
batch 235 loss: 0.0005576978670433164
batch 240 loss: 0.0005577127099968493
Training Loss: 0.0005576834240249203
Validation Loss: 0.0005576934316195547
Epoch 72:
batch 5 loss: 0.0005576656782068312
batch 10 loss: 0.0005576893221586942
batch 15 loss: 0.0005575867951847612
batch 20 loss: 0.0005576534429565072
batch 25 loss: 0.0005575704271905124
batch 30 loss: 0.000557720975484699
batch 35 loss: 0.0005576096358709037
batch 40 loss: 0.0005577794276177883
batch 45 loss: 0.0005576616735197603
batch 50 loss: 0.0005577118950895965
batch 55 loss: 0.0005576398922130466
batch 60 loss: 0.0005576581810601055
batch 65 loss: 0.0005576456664130091
batch 70 loss: 0.0005576911033131182
batch 75 loss: 0.0005576878902502358
batch 80 loss: 0.0005577922449447215
batch 85 loss: 0.0005576253402978182
batch 90 loss: 0.0005575661663897336
batch 95 loss: 0.0005576451076194644
batch 100 loss: 0.0005577858071774244
batch 105 loss: 0.0005576812080107629
batch 110 loss: 0.0005576801137067378
batch 115 loss: 0.00055757244117558
batch 120 loss: 0.0005577416042797268
batch 125 loss: 0.0005577366799116134
batch 130 loss: 0.0005576582159847021
batch 135 loss: 0.0005576444091275334
batch 140 loss: 0.000557711545843631
batch 145 loss: 0.0005576325114816427
batch 150 loss: 0.0005576982861384749
batch 155 loss: 0.000557663943618536
batch 160 loss: 0.000557697075419128
batch 165 loss: 0.0005577200092375279
batch 170 loss: 0.0005575999733991921
batch 175 loss: 0.0005576124065555632
batch 180 loss: 0.00055766343139112
batch 185 loss: 0.0005578039446845651
batch 190 loss: 0.0005576554918661714
batch 195 loss: 0.0005578156444244087
batch 200 loss: 0.0005578043637797236
batch 205 loss: 0.0005577079718932509
batch 210 loss: 0.0005576606374233962
batch 215 loss: 0.0005576965399086475
batch 220 loss: 0.0005577563308179379
batch 225 loss: 0.0005577733041718602
batch 230 loss: 0.0005576946772634983
batch 235 loss: 0.0005576865514740348
batch 240 loss: 0.0005576475523412228
Training Loss: 0.0005576834070476858
Validation Loss: 0.000557693531542706
Epoch 73:
batch 5 loss: 0.0005577569478191436
batch 10 loss: 0.0005577187519520521
batch 15 loss: 0.0005575890187174082
batch 20 loss: 0.0005576434195972979
batch 25 loss: 0.0005576556082814932
batch 30 loss: 0.0005576855153776705
batch 35 loss: 0.0005576473544351756
batch 40 loss: 0.0005576016497798264
batch 45 loss: 0.0005576395080424846
batch 50 loss: 0.0005576813127845526
batch 55 loss: 0.0005576966446824372
batch 60 loss: 0.0005576763185672462
batch 65 loss: 0.0005577483680099249
batch 70 loss: 0.0005577265983447433
batch 75 loss: 0.0005576621973887086
batch 80 loss: 0.0005578085663728416
batch 85 loss: 0.00055758913513273
batch 90 loss: 0.0005576321622356772
batch 95 loss: 0.0005576750845648348
batch 100 loss: 0.0005576065625064075
batch 105 loss: 0.0005576554220169782
batch 110 loss: 0.0005577210686169565
batch 115 loss: 0.0005577318137511611
batch 120 loss: 0.0005576903698965907
batch 125 loss: 0.0005576928379014134
batch 130 loss: 0.0005577090312726795
batch 135 loss: 0.0005576923838816583
batch 140 loss: 0.0005577089148573577
batch 145 loss: 0.000557623733766377
batch 150 loss: 0.0005576278665103019
batch 155 loss: 0.0005577654344961047
batch 160 loss: 0.0005576469819061458
batch 165 loss: 0.0005576758529059588
batch 170 loss: 0.0005577553063631057
batch 175 loss: 0.0005576938157901168
batch 180 loss: 0.0005576480645686388
batch 185 loss: 0.0005576546071097254
batch 190 loss: 0.0005577655858360231
batch 195 loss: 0.0005577172501944006
batch 200 loss: 0.0005576777388341725
batch 205 loss: 0.0005576249328441918
batch 210 loss: 0.0005577259231358767
batch 215 loss: 0.0005577120115049184
batch 220 loss: 0.0005577655625529587
batch 225 loss: 0.00055764279095456
batch 230 loss: 0.0005576718482188881
batch 235 loss: 0.0005577654228545725
batch 240 loss: 0.0005576005904003977
Training Loss: 0.0005576834143236434
Validation Loss: 0.0005576935014687479
Epoch 74:
batch 5 loss: 0.0005576902069151402
batch 10 loss: 0.0005576472729444503
batch 15 loss: 0.0005576164927333594
batch 20 loss: 0.0005577855510637165
batch 25 loss: 0.0005576686700806022
batch 30 loss: 0.0005576475523412228
batch 35 loss: 0.0005576549796387553
batch 40 loss: 0.0005576697643846273
batch 45 loss: 0.0005576958297751844
batch 50 loss: 0.0005577003583312034
batch 55 loss: 0.0005576495779678226
batch 60 loss: 0.0005576676572673023
batch 65 loss: 0.0005576442345045507
batch 70 loss: 0.0005576246767304838
batch 75 loss: 0.0005577424191869796
batch 80 loss: 0.0005576496478170156
batch 85 loss: 0.0005576450610533356
batch 90 loss: 0.0005576390889473259
batch 95 loss: 0.0005577035015448927
batch 100 loss: 0.0005576764699071646
batch 105 loss: 0.0005576657014898956
batch 110 loss: 0.0005576986004598439
batch 115 loss: 0.0005576708703301847
batch 120 loss: 0.0005576573312282562
batch 125 loss: 0.000557711289729923
batch 130 loss: 0.0005576483556069434
batch 135 loss: 0.0005575925693847239
batch 140 loss: 0.0005576850613579154
batch 145 loss: 0.0005577491479925812
batch 150 loss: 0.0005576148862019182
batch 155 loss: 0.0005577881005592644
batch 160 loss: 0.0005577010218985379
batch 165 loss: 0.0005577309289947152
batch 170 loss: 0.0005576502415351569
batch 175 loss: 0.0005576327093876898
batch 180 loss: 0.0005576860043220222
batch 185 loss: 0.0005576975177973509
batch 190 loss: 0.0005577177973464132
batch 195 loss: 0.0005577231873758137
batch 200 loss: 0.0005576389259658754
batch 205 loss: 0.0005577211501076818
batch 210 loss: 0.000557731359731406
batch 215 loss: 0.0005576928029768169
batch 220 loss: 0.0005577996256761253
batch 225 loss: 0.0005576989729888737
batch 230 loss: 0.0005576347815804183
batch 235 loss: 0.0005577710340730846
batch 240 loss: 0.0005576756782829761
Training Loss: 0.0005576834305732821
Validation Loss: 0.0005576934063962351
Epoch 75:
batch 5 loss: 0.000557759846560657
batch 10 loss: 0.0005576289491727948
batch 15 loss: 0.0005577551783062518
batch 20 loss: 0.0005577514297328889
batch 25 loss: 0.0005577900097705424
batch 30 loss: 0.0005576472845859826
batch 35 loss: 0.0005577224656008184
batch 40 loss: 0.0005576013121753931
batch 45 loss: 0.0005577491014264524
batch 50 loss: 0.0005577514646574854
batch 55 loss: 0.0005577040486969054
batch 60 loss: 0.000557764561381191
batch 65 loss: 0.0005575998802669346
batch 70 loss: 0.0005576590076088906
batch 75 loss: 0.0005577224888838828
batch 80 loss: 0.000557686504907906
batch 85 loss: 0.0005577492993324995
batch 90 loss: 0.000557622394990176
batch 95 loss: 0.0005577088915742934
batch 100 loss: 0.0005577926058322191
batch 105 loss: 0.0005576791358180344
batch 110 loss: 0.000557677960023284
batch 115 loss: 0.0005576727213338018
batch 120 loss: 0.0005576383089646697
batch 125 loss: 0.0005576416733674705
batch 130 loss: 0.000557626609224826
batch 135 loss: 0.0005576721974648535
batch 140 loss: 0.0005576239665970207
batch 145 loss: 0.000557704281527549
batch 150 loss: 0.0005577306495979428
batch 155 loss: 0.0005576458061113954
batch 160 loss: 0.0005576858529821038
batch 165 loss: 0.0005575612303800881
batch 170 loss: 0.0005577206611633301
batch 175 loss: 0.0005576453870162368
batch 180 loss: 0.0005577047588303686
batch 185 loss: 0.0005576461437158287
batch 190 loss: 0.0005575524875894189
batch 195 loss: 0.0005577049450948834
batch 200 loss: 0.0005577285541221499
batch 205 loss: 0.0005576821276918054
batch 210 loss: 0.0005576237686909735
batch 215 loss: 0.0005575566203333438
batch 220 loss: 0.0005577564937993884
batch 225 loss: 0.0005576163879595697
batch 230 loss: 0.0005578090087510646
batch 235 loss: 0.000557630870025605
batch 240 loss: 0.0005576982046477496
Training Loss: 0.0005576834070476858
Validation Loss: 0.0005576934452013423
Epoch 76:
batch 5 loss: 0.0005577241419814527
batch 10 loss: 0.0005576511844992638
batch 15 loss: 0.000557644956279546
batch 20 loss: 0.0005577033618465066
batch 25 loss: 0.000557759334333241
batch 30 loss: 0.0005577079718932509
batch 35 loss: 0.0005577023955993354
batch 40 loss: 0.0005576537339948117
batch 45 loss: 0.0005576305906288326
batch 50 loss: 0.0005576672963798046
batch 55 loss: 0.0005575920105911792
batch 60 loss: 0.0005577162490226328
batch 65 loss: 0.0005576119641773402
batch 70 loss: 0.0005576956900767982
batch 75 loss: 0.0005577760864980518
batch 80 loss: 0.0005577344098128378
batch 85 loss: 0.0005576433264650405
batch 90 loss: 0.0005578359123319387
batch 95 loss: 0.0005576613708399236
batch 100 loss: 0.0005576984025537967
batch 105 loss: 0.0005577259347774088
batch 110 loss: 0.0005576380877755583
batch 115 loss: 0.0005577323026955128
batch 120 loss: 0.0005577378091402352
batch 125 loss: 0.0005576024064794183
batch 130 loss: 0.000557699950877577
batch 135 loss: 0.0005577665171585978
batch 140 loss: 0.0005576571333222091
batch 145 loss: 0.0005576114519499243
batch 150 loss: 0.0005575805087573826
batch 155 loss: 0.0005576191353611649
batch 160 loss: 0.0005577012663707138
batch 165 loss: 0.0005577208241447807
batch 170 loss: 0.0005576082388870418
batch 175 loss: 0.0005576732219196856
batch 180 loss: 0.0005576345953159034
batch 185 loss: 0.000557801453396678
batch 190 loss: 0.0005575835122726858
batch 195 loss: 0.0005576474475674331
batch 200 loss: 0.0005577652133069933
batch 205 loss: 0.0005576954456046224
batch 210 loss: 0.0005576303112320602
batch 215 loss: 0.0005576468305662274
batch 220 loss: 0.0005576901719905436
batch 225 loss: 0.0005576737108640373
batch 230 loss: 0.0005577427917160093
batch 235 loss: 0.0005577191244810819
batch 240 loss: 0.0005576876807026565
Training Loss: 0.0005576834055924944
Validation Loss: 0.0005576934510221084
Epoch 77:
batch 5 loss: 0.0005576272611506284
batch 10 loss: 0.0005576829425990582
batch 15 loss: 0.0005576445953920483
batch 20 loss: 0.0005577054223977029
batch 25 loss: 0.0005577613250352442
batch 30 loss: 0.000557681021746248
batch 35 loss: 0.000557696633040905
batch 40 loss: 0.0005577149684540927
batch 45 loss: 0.0005577326635830104
batch 50 loss: 0.0005577408126555383
batch 55 loss: 0.0005576391820795834
batch 60 loss: 0.0005576991941779851
batch 65 loss: 0.0005577319650910794
batch 70 loss: 0.000557638262398541
batch 75 loss: 0.0005577210569754243
batch 80 loss: 0.0005577211501076818
batch 85 loss: 0.0005576524185016751
batch 90 loss: 0.0005576355615630746
batch 95 loss: 0.0005576970404945313
batch 100 loss: 0.0005577260977588594
batch 105 loss: 0.0005576760158874094
batch 110 loss: 0.0005576810566708445
batch 115 loss: 0.0005576133611612022
batch 120 loss: 0.0005576947471126914
batch 125 loss: 0.00055766343139112
batch 130 loss: 0.0005576980533078313
batch 135 loss: 0.0005576343392021954
batch 140 loss: 0.0005576744093559683
batch 145 loss: 0.0005576824536547065
batch 150 loss: 0.0005576202645897865
batch 155 loss: 0.0005575610091909766
batch 160 loss: 0.0005576337804086507
batch 165 loss: 0.0005577332689426839
batch 170 loss: 0.0005576100433245301
batch 175 loss: 0.0005575132905505598
batch 180 loss: 0.0005577585892751813
batch 185 loss: 0.0005577324307523667
batch 190 loss: 0.0005576058407314122
batch 195 loss: 0.0005577336298301816
batch 200 loss: 0.0005576484487392009
batch 205 loss: 0.0005577128264121711
batch 210 loss: 0.000557772524189204
batch 215 loss: 0.0005577294738031924
batch 220 loss: 0.0005576529074460268
batch 225 loss: 0.000557706446852535
batch 230 loss: 0.0005577191011980176
batch 235 loss: 0.0005578140844590962
batch 240 loss: 0.0005576782859861851
Training Loss: 0.0005576834102006008
Validation Loss: 0.0005576934529623637
Epoch 78:
batch 5 loss: 0.0005575250019319355
batch 10 loss: 0.0005577389849349857
batch 15 loss: 0.0005576609750278294
batch 20 loss: 0.0005577048868872225
batch 25 loss: 0.0005576691590249538
batch 30 loss: 0.0005576259572990239
batch 35 loss: 0.0005576625582762063
batch 40 loss: 0.0005576417548581958
batch 45 loss: 0.0005577848525717855
batch 50 loss: 0.000557713198941201
batch 55 loss: 0.0005576299037784338
batch 60 loss: 0.0005576615338213741
batch 65 loss: 0.0005577615927904844
batch 70 loss: 0.0005576786119490862
batch 75 loss: 0.0005575571209192276
batch 80 loss: 0.0005576932802796364
batch 85 loss: 0.0005576652009040117
batch 90 loss: 0.000557771255262196
batch 95 loss: 0.0005575351417064667
batch 100 loss: 0.0005577177973464132
batch 105 loss: 0.0005577945150434971
batch 110 loss: 0.0005575960036367178
batch 115 loss: 0.0005576518597081304
batch 120 loss: 0.0005576132447458803
batch 125 loss: 0.0005577465402893722
batch 130 loss: 0.0005577424308285117
batch 135 loss: 0.0005576018011197448
batch 140 loss: 0.0005577501025982202
batch 145 loss: 0.0005577079486101866
batch 150 loss: 0.0005576568306423724
batch 155 loss: 0.0005576885771006346
batch 160 loss: 0.0005577681236900389
batch 165 loss: 0.0005575702409259975
batch 170 loss: 0.0005577445961534977
batch 175 loss: 0.0005577034666202962
batch 180 loss: 0.0005577301606535911
batch 185 loss: 0.0005577563308179379
batch 190 loss: 0.0005577063071541488
batch 195 loss: 0.0005577440955676138
batch 200 loss: 0.0005577275762334466
batch 205 loss: 0.0005577168660238386
batch 210 loss: 0.0005577082512900234
batch 215 loss: 0.0005576332681812346
batch 220 loss: 0.000557619845494628
batch 225 loss: 0.0005577245145104825
batch 230 loss: 0.0005575924762524665
batch 235 loss: 0.0005577015690505505
batch 240 loss: 0.0005577072268351912
Training Loss: 0.0005576834070476858
Validation Loss: 0.0005576934684844067
Epoch 79:
batch 5 loss: 0.0005576300551183522
batch 10 loss: 0.0005577014642767608
batch 15 loss: 0.00055762481642887
batch 20 loss: 0.0005577700678259135
batch 25 loss: 0.000557687843684107
batch 30 loss: 0.0005576576455496252
batch 35 loss: 0.0005577709875069558
batch 40 loss: 0.000557635200675577
batch 45 loss: 0.0005576709751039744
batch 50 loss: 0.0005576993571594357
batch 55 loss: 0.0005576939787715673
batch 60 loss: 0.0005577416974119842
batch 65 loss: 0.0005576508236117661
batch 70 loss: 0.0005576661555096507
batch 75 loss: 0.0005576680996455252
batch 80 loss: 0.0005578057374805212
batch 85 loss: 0.0005578501615673304
batch 90 loss: 0.0005577225121669472
batch 95 loss: 0.0005576996016316115
batch 100 loss: 0.0005575175862759351
batch 105 loss: 0.000557722186204046
batch 110 loss: 0.0005577043863013387
batch 115 loss: 0.0005577066564001143
batch 120 loss: 0.0005576980533078313
batch 125 loss: 0.0005576731287874282
batch 130 loss: 0.0005575814051553607
batch 135 loss: 0.0005576840601861476
batch 140 loss: 0.000557698612101376
batch 145 loss: 0.0005575897754170001
batch 150 loss: 0.0005576633149757982
batch 155 loss: 0.0005577292991802097
batch 160 loss: 0.0005577239440754056
batch 165 loss: 0.0005576989380642772
batch 170 loss: 0.0005575818824581802
batch 175 loss: 0.000557706959079951
batch 180 loss: 0.0005576156545430422
batch 185 loss: 0.0005575493210926652
batch 190 loss: 0.000557667831890285
batch 195 loss: 0.0005576596013270319
batch 200 loss: 0.0005577427684329451
batch 205 loss: 0.0005576970288529992
batch 210 loss: 0.0005576768540777266
batch 215 loss: 0.0005577259347774088
batch 220 loss: 0.0005576787167228759
batch 225 loss: 0.0005576558411121369
batch 230 loss: 0.0005576945608481765
batch 235 loss: 0.0005577081348747015
batch 240 loss: 0.0005577035364694894
Training Loss: 0.0005576833990441325
Validation Loss: 0.0005576934160975119
Epoch 80:
batch 5 loss: 0.0005576217314228416
batch 10 loss: 0.0005576692055910826
batch 15 loss: 0.0005576445371843874
batch 20 loss: 0.0005577368661761284
batch 25 loss: 0.0005577363655902446
batch 30 loss: 0.0005576901254244149
batch 35 loss: 0.0005576496827416122
batch 40 loss: 0.0005576678668148816
batch 45 loss: 0.0005577210919000209
batch 50 loss: 0.0005576223018579185
batch 55 loss: 0.0005576702184043825
batch 60 loss: 0.0005576377850957214
batch 65 loss: 0.0005577046307735145
batch 70 loss: 0.0005577979143708944
batch 75 loss: 0.0005577783565968275
batch 80 loss: 0.0005576132447458803
batch 85 loss: 0.0005577928153797984
batch 90 loss: 0.0005577728501521051
batch 95 loss: 0.000557731615845114
batch 100 loss: 0.0005576772498898208
batch 105 loss: 0.0005576716852374375
batch 110 loss: 0.0005577263305895031
batch 115 loss: 0.0005578027106821537
batch 120 loss: 0.0005576045252382756
batch 125 loss: 0.0005575847811996937
batch 130 loss: 0.0005576813593506813
batch 135 loss: 0.000557644630316645
batch 140 loss: 0.0005576915456913412
batch 145 loss: 0.0005575896007940173
batch 150 loss: 0.0005576725350692868
batch 155 loss: 0.0005576246883720159
batch 160 loss: 0.0005576964467763901
batch 165 loss: 0.0005577541189268232
batch 170 loss: 0.0005576938739977777
batch 175 loss: 0.0005577085190452636
batch 180 loss: 0.0005576717434450984
batch 185 loss: 0.0005577183328568935
batch 190 loss: 0.0005576833267696202
batch 195 loss: 0.0005576385301537812
batch 200 loss: 0.000557697273325175
batch 205 loss: 0.0005576538504101336
batch 210 loss: 0.0005576284835115075
batch 215 loss: 0.0005577440955676138
batch 220 loss: 0.0005576530005782842
batch 225 loss: 0.0005576301133260131
batch 230 loss: 0.0005576622439548373
batch 235 loss: 0.000557636609300971
batch 240 loss: 0.0005577018251642585
Training Loss: 0.0005576834007418559
Validation Loss: 0.0005576934219182779
Epoch 81:
batch 5 loss: 0.0005576442694291472
batch 10 loss: 0.0005577222094871104
batch 15 loss: 0.0005576121155172586
batch 20 loss: 0.0005575863760896027
batch 25 loss: 0.0005576589726842939
batch 30 loss: 0.0005576558411121369
batch 35 loss: 0.000557727797422558
batch 40 loss: 0.0005577784031629562
batch 45 loss: 0.0005576701718382537
batch 50 loss: 0.0005577311851084233
batch 55 loss: 0.000557596911676228
batch 60 loss: 0.0005577029660344124
batch 65 loss: 0.0005576172959990799
batch 70 loss: 0.0005577118834480643
batch 75 loss: 0.0005576678202487528
batch 80 loss: 0.0005576170748099685
batch 85 loss: 0.0005577077390626073
batch 90 loss: 0.0005577296949923038
batch 95 loss: 0.0005576478550210595
batch 100 loss: 0.0005576969822868704
batch 105 loss: 0.0005576591938734055
batch 110 loss: 0.0005576471448875964
batch 115 loss: 0.0005577195319347083
batch 120 loss: 0.0005578153301030398
batch 125 loss: 0.0005577776581048965
batch 130 loss: 0.0005576116731390357
batch 135 loss: 0.000557750347070396
batch 140 loss: 0.0005576311610639096
batch 145 loss: 0.0005577531526796519
batch 150 loss: 0.0005576750845648348
batch 155 loss: 0.0005576310330070555
batch 160 loss: 0.0005576696479693055
batch 165 loss: 0.0005576521623879671
batch 170 loss: 0.0005576642695814371
batch 175 loss: 0.0005576512776315212
batch 180 loss: 0.0005576342344284057
batch 185 loss: 0.0005577603937126696
batch 190 loss: 0.000557649100665003
batch 195 loss: 0.0005577195785008371
batch 200 loss: 0.000557732058223337
batch 205 loss: 0.0005576558411121369
batch 210 loss: 0.0005576767143793404
batch 215 loss: 0.0005576910683885216
batch 220 loss: 0.000557721743825823
batch 225 loss: 0.0005576605908572674
batch 230 loss: 0.0005577480886131525
batch 235 loss: 0.0005577490082941949
batch 240 loss: 0.0005576123367063701
Training Loss: 0.0005576833956486856
Validation Loss: 0.0005576934345299378
Epoch 82:
batch 5 loss: 0.0005576828261837363
batch 10 loss: 0.0005576756084337831
batch 15 loss: 0.0005576173658482731
batch 20 loss: 0.0005576589959673583
batch 25 loss: 0.0005576912430115044
batch 30 loss: 0.0005576146533712745
batch 35 loss: 0.0005576982512138784
batch 40 loss: 0.0005576369352638721
batch 45 loss: 0.0005577182746492326
batch 50 loss: 0.0005577527219429613
batch 55 loss: 0.0005576941766776145
batch 60 loss: 0.0005576551193371415
batch 65 loss: 0.0005576265044510365
batch 70 loss: 0.0005577919539064169
batch 75 loss: 0.000557648646645248
batch 80 loss: 0.0005577196599915624
batch 85 loss: 0.0005576302646659315
batch 90 loss: 0.0005576734081842005
batch 95 loss: 0.0005577163305133581
batch 100 loss: 0.0005577877978794277
batch 105 loss: 0.000557605386711657
batch 110 loss: 0.0005576489609666168
batch 115 loss: 0.0005577710457146168
batch 120 loss: 0.0005578145035542547
batch 125 loss: 0.0005575807415880262
batch 130 loss: 0.0005576509283855557
batch 135 loss: 0.0005577194737270474
batch 140 loss: 0.0005577062140218914
batch 145 loss: 0.0005576886236667633
batch 150 loss: 0.0005576130002737046
batch 155 loss: 0.0005577495787292719
batch 160 loss: 0.0005576960858888925
batch 165 loss: 0.0005576402647420764
batch 170 loss: 0.0005576587747782469
batch 175 loss: 0.0005577030591666698
batch 180 loss: 0.000557730719447136
batch 185 loss: 0.0005575773189775646
batch 190 loss: 0.000557680707424879
batch 195 loss: 0.0005577399162575603
batch 200 loss: 0.000557711289729923
batch 205 loss: 0.0005575894145295023
batch 210 loss: 0.0005575998104177416
batch 215 loss: 0.0005577215692028403
batch 220 loss: 0.0005577325937338173
batch 225 loss: 0.0005577787524089217
batch 230 loss: 0.0005576110212132335
batch 235 loss: 0.0005576692637987435
batch 240 loss: 0.0005577229429036378
Training Loss: 0.0005576833895853876
Validation Loss: 0.0005576934762454281
Epoch 83:
batch 5 loss: 0.0005576411844231188
batch 10 loss: 0.0005576724535785616
batch 15 loss: 0.0005576303112320602
batch 20 loss: 0.0005576814641244709
batch 25 loss: 0.0005576236755587161
batch 30 loss: 0.0005576209747232497
batch 35 loss: 0.0005577195552177727
batch 40 loss: 0.0005576578667387366
batch 45 loss: 0.0005576665047556162
batch 50 loss: 0.0005576433730311691
batch 55 loss: 0.0005576573894359171
batch 60 loss: 0.0005577491130679846
batch 65 loss: 0.0005577859352342785
batch 70 loss: 0.0005576640251092613
batch 75 loss: 0.0005577440722845494
batch 80 loss: 0.0005576814408414065
batch 85 loss: 0.0005575534887611866
batch 90 loss: 0.0005577787407673896
batch 95 loss: 0.0005577469361014664
batch 100 loss: 0.0005576394265517593
batch 105 loss: 0.0005576227093115449
batch 110 loss: 0.0005576109397225082
batch 115 loss: 0.0005576204974204301
batch 120 loss: 0.0005577588919550181
batch 125 loss: 0.0005575933144427836
batch 130 loss: 0.0005577050731517374
batch 135 loss: 0.0005575999733991921
batch 140 loss: 0.0005577498581260443
batch 145 loss: 0.0005576430354267359
batch 150 loss: 0.0005577116622589529
batch 155 loss: 0.0005576507188379765
batch 160 loss: 0.0005576783325523138
batch 165 loss: 0.0005577024654485285
batch 170 loss: 0.0005576987052336336
batch 175 loss: 0.0005576929193921387
batch 180 loss: 0.0005577716859988868
batch 185 loss: 0.0005576540366746485
batch 190 loss: 0.0005577145842835307
batch 195 loss: 0.0005577136413194239
batch 200 loss: 0.0005576210329309106
batch 205 loss: 0.0005577094038017094
batch 210 loss: 0.0005577143281698226
batch 215 loss: 0.0005577892996370792
batch 220 loss: 0.0005576719995588064
batch 225 loss: 0.000557676621247083
batch 230 loss: 0.000557767041027546
batch 235 loss: 0.0005577211384661495
batch 240 loss: 0.0005576808471232653
Training Loss: 0.0005576833893428557
Validation Loss: 0.0005576934393805762
Epoch 84:
batch 5 loss: 0.0005577879608608782
batch 10 loss: 0.0005577048636041581
batch 15 loss: 0.0005577532574534416
batch 20 loss: 0.0005576004972681403
batch 25 loss: 0.0005576230701990426
batch 30 loss: 0.0005576049792580306
batch 35 loss: 0.0005576724070124328
batch 40 loss: 0.0005575431860052049
batch 45 loss: 0.0005577289382927119
batch 50 loss: 0.0005576383788138628
batch 55 loss: 0.0005576260155066848
batch 60 loss: 0.0005576348048634827
batch 65 loss: 0.0005576042807660997
batch 70 loss: 0.0005577047937549651
batch 75 loss: 0.0005577378557063639
batch 80 loss: 0.0005577062955126166
batch 85 loss: 0.0005576027673669159
batch 90 loss: 0.0005576627212576568
batch 95 loss: 0.0005576609983108938
batch 100 loss: 0.0005576769588515162
batch 105 loss: 0.0005576436058618128
batch 110 loss: 0.0005575755145400763
batch 115 loss: 0.000557702174410224
batch 120 loss: 0.000557651522103697
batch 125 loss: 0.0005578067735768855
batch 130 loss: 0.0005577466799877584
batch 135 loss: 0.0005576913245022297
batch 140 loss: 0.0005577018833719194
batch 145 loss: 0.0005576415569521487
batch 150 loss: 0.0005576367140747607
batch 155 loss: 0.0005576692870818079
batch 160 loss: 0.0005577060743235052
batch 165 loss: 0.0005576576222665608
batch 170 loss: 0.0005578151438385248
batch 175 loss: 0.0005577931646257639
batch 180 loss: 0.0005576152703724802
batch 185 loss: 0.00055763857671991
batch 190 loss: 0.000557613163255155
batch 195 loss: 0.0005577456555329263
batch 200 loss: 0.0005577916046604515
batch 205 loss: 0.0005576496943831444
batch 210 loss: 0.0005576027208007873
batch 215 loss: 0.000557704281527549
batch 220 loss: 0.0005576668540015816
batch 225 loss: 0.000557770230807364
batch 230 loss: 0.0005577957956120372
batch 235 loss: 0.0005577644566074014
batch 240 loss: 0.0005577306030318141
Training Loss: 0.0005576833954061537
Validation Loss: 0.0005576934063962351
Epoch 85:
batch 5 loss: 0.0005577495903708041
batch 10 loss: 0.0005575975636020302
batch 15 loss: 0.000557604362256825
batch 20 loss: 0.000557750673033297
batch 25 loss: 0.0005576494615525008
batch 30 loss: 0.0005577330128289759
batch 35 loss: 0.0005577872972935439
batch 40 loss: 0.00055769745958969
batch 45 loss: 0.0005576953641138971
batch 50 loss: 0.0005577512318268418
batch 55 loss: 0.000557613791897893
batch 60 loss: 0.0005577422212809324
batch 65 loss: 0.0005576613126322627
batch 70 loss: 0.0005576123483479023
batch 75 loss: 0.0005577037925831973
batch 80 loss: 0.0005576984491199255
batch 85 loss: 0.0005576561321504414
batch 90 loss: 0.0005576289375312626
batch 95 loss: 0.0005576605442911386
batch 100 loss: 0.0005576820578426123
batch 105 loss: 0.000557616981677711
batch 110 loss: 0.0005577014060690999
batch 115 loss: 0.0005576636875048279
batch 120 loss: 0.0005576659110374749
batch 125 loss: 0.0005576457129791379
batch 130 loss: 0.0005575104034505785
batch 135 loss: 0.0005576805328018963
batch 140 loss: 0.0005576077033765614
batch 145 loss: 0.0005577215226367116
batch 150 loss: 0.000557716574985534
batch 155 loss: 0.0005577749223448336
batch 160 loss: 0.0005576781812123954
batch 165 loss: 0.0005576715455390513
batch 170 loss: 0.000557750859297812
batch 175 loss: 0.000557699881028384
batch 180 loss: 0.0005576244904659689
batch 185 loss: 0.0005576325347647071
batch 190 loss: 0.0005577261326834559
batch 195 loss: 0.0005577707896009087
batch 200 loss: 0.0005576902418397367
batch 205 loss: 0.0005576772731728852
batch 210 loss: 0.000557771825697273
batch 215 loss: 0.0005576716386713088
batch 220 loss: 0.0005576491937972605
batch 225 loss: 0.0005576654220931232
batch 230 loss: 0.0005577480653300882
batch 235 loss: 0.0005576855503022671
batch 240 loss: 0.000557708228006959
Training Loss: 0.0005576833920107068
Validation Loss: 0.0005576935208713014
Epoch 86:
batch 5 loss: 0.0005577605566941202
batch 10 loss: 0.0005576245952397585
batch 15 loss: 0.0005577705218456685
batch 20 loss: 0.000557725504040718
batch 25 loss: 0.0005576711730100215
batch 30 loss: 0.0005576908821240068
batch 35 loss: 0.0005576640251092613
batch 40 loss: 0.0005576578434556723
batch 45 loss: 0.0005576362484134734
batch 50 loss: 0.0005576597643084825
batch 55 loss: 0.0005575936986133457
batch 60 loss: 0.0005576818832196296
batch 65 loss: 0.0005575987743213773
batch 70 loss: 0.0005574933835305274
batch 75 loss: 0.0005577744450420141
batch 80 loss: 0.0005577366682700813
batch 85 loss: 0.0005577559466473758
batch 90 loss: 0.0005576469586230815
batch 95 loss: 0.0005577123141847551
batch 100 loss: 0.0005577015224844217
batch 105 loss: 0.0005576660390943289
batch 110 loss: 0.0005577092990279198
batch 115 loss: 0.0005575610208325088
batch 120 loss: 0.0005576657364144922
batch 125 loss: 0.00055768305901438
batch 130 loss: 0.0005577053176239133
batch 135 loss: 0.0005576345953159034
batch 140 loss: 0.000557732058223337
batch 145 loss: 0.0005576890078373253
batch 150 loss: 0.0005576796596869826
batch 155 loss: 0.000557679997291416
batch 160 loss: 0.0005576132680289447
batch 165 loss: 0.0005576124298386276
batch 170 loss: 0.0005577622214332223
batch 175 loss: 0.0005577418487519026
batch 180 loss: 0.0005576618248596787
batch 185 loss: 0.0005576424184255302
batch 190 loss: 0.0005577220115810632
batch 195 loss: 0.0005576822557486593
batch 200 loss: 0.0005576891475357115
batch 205 loss: 0.0005576915922574699
batch 210 loss: 0.0005576769472099841
batch 215 loss: 0.0005576222320087254
batch 220 loss: 0.0005577325820922852
batch 225 loss: 0.0005577404284849763
batch 230 loss: 0.0005577817326411604
batch 235 loss: 0.0005577221047133208
batch 240 loss: 0.0005577450850978493
Training Loss: 0.0005576833881301961
Validation Loss: 0.0005576934005754689
Epoch 87:
batch 5 loss: 0.0005576372845098376
batch 10 loss: 0.0005577160511165858
batch 15 loss: 0.0005576849100179971
batch 20 loss: 0.0005576480645686388
batch 25 loss: 0.0005576682509854436
batch 30 loss: 0.0005576695199124515
batch 35 loss: 0.0005576697643846273
batch 40 loss: 0.000557563710026443
batch 45 loss: 0.0005577540607191623
batch 50 loss: 0.0005577880539931357
batch 55 loss: 0.0005576309049502015
batch 60 loss: 0.0005578164593316615
batch 65 loss: 0.0005576134426519275
batch 70 loss: 0.0005575551535002887
batch 75 loss: 0.0005576535477302969
batch 80 loss: 0.0005576699390076101
batch 85 loss: 0.0005575466784648597
batch 90 loss: 0.0005577270057983696
batch 95 loss: 0.0005576151888817549
batch 100 loss: 0.0005576150259003043
batch 105 loss: 0.0005576610099524259
batch 110 loss: 0.0005577178671956062
batch 115 loss: 0.0005578096723183989
batch 120 loss: 0.0005576516501605511
batch 125 loss: 0.0005576695199124515
batch 130 loss: 0.0005577432457357645
batch 135 loss: 0.0005577812320552766
batch 140 loss: 0.000557781639508903
batch 145 loss: 0.0005576949915848672
batch 150 loss: 0.0005576808121986687
batch 155 loss: 0.0005577427800744772
batch 160 loss: 0.0005577324889600277
batch 165 loss: 0.0005577097879722714
batch 170 loss: 0.0005576740833930672
batch 175 loss: 0.0005577076459303498
batch 180 loss: 0.0005575938848778605
batch 185 loss: 0.0005576242110691965
batch 190 loss: 0.0005576001480221748
batch 195 loss: 0.000557699054479599
batch 200 loss: 0.0005576383671723306
batch 205 loss: 0.0005577459465712309
batch 210 loss: 0.0005577206495217979
batch 215 loss: 0.0005576872499659657
batch 220 loss: 0.000557709694840014
batch 225 loss: 0.0005577184609137475
batch 230 loss: 0.000557718996424228
batch 235 loss: 0.0005577070056460798
batch 240 loss: 0.0005576372146606446
Training Loss: 0.0005576833818243661
Validation Loss: 0.0005576934190078948
Epoch 88:
batch 5 loss: 0.0005576604744419456
batch 10 loss: 0.0005576826282776892
batch 15 loss: 0.0005575804389081895
batch 20 loss: 0.0005576545023359359
batch 25 loss: 0.0005575674818828702
batch 30 loss: 0.0005575535236857831
batch 35 loss: 0.0005576148978434503
batch 40 loss: 0.000557700649369508
batch 45 loss: 0.000557662604842335
batch 50 loss: 0.0005575261311605573
batch 55 loss: 0.000557788962032646
batch 60 loss: 0.000557705550454557
batch 65 loss: 0.0005577264353632927
batch 70 loss: 0.000557665154337883
batch 75 loss: 0.0005576424533501268
batch 80 loss: 0.0005576784373261034
batch 85 loss: 0.0005576793919317424
batch 90 loss: 0.0005577323725447058
batch 95 loss: 0.0005576498340815305
batch 100 loss: 0.0005577157135121524
batch 105 loss: 0.0005577100906521082
batch 110 loss: 0.0005577101488597691
batch 115 loss: 0.0005576790543273091
batch 120 loss: 0.0005576931987889111
batch 125 loss: 0.0005576344323344528
batch 130 loss: 0.0005576564348302781
batch 135 loss: 0.0005576408118940889
batch 140 loss: 0.0005577471572905778
batch 145 loss: 0.0005577032803557813
batch 150 loss: 0.0005577186238951981
batch 155 loss: 0.0005575722665525973
batch 160 loss: 0.0005576997413299978
batch 165 loss: 0.0005576948286034166
batch 170 loss: 0.0005578092648647725
batch 175 loss: 0.0005577604984864593
batch 180 loss: 0.0005575331859290599
batch 185 loss: 0.0005576896597631276
batch 190 loss: 0.0005576432798989117
batch 195 loss: 0.0005576750612817704
batch 200 loss: 0.0005577897536568344
batch 205 loss: 0.0005576862953603267
batch 210 loss: 0.0005577863659709692
batch 215 loss: 0.0005576811730861664
batch 220 loss: 0.00055769516620785
batch 225 loss: 0.0005578011623583734
batch 230 loss: 0.0005577260279096663
batch 235 loss: 0.0005578426993452012
batch 240 loss: 0.0005576349562034011
Training Loss: 0.0005576833803691746
Validation Loss: 0.0005576934083364904
Epoch 89:
batch 5 loss: 0.0005576258176006377
batch 10 loss: 0.0005577089032158256
batch 15 loss: 0.0005577000672928988
batch 20 loss: 0.0005576391704380512
batch 25 loss: 0.0005577359930612147
batch 30 loss: 0.0005576678435318172
batch 35 loss: 0.0005577591946348548
batch 40 loss: 0.0005577009520493448
batch 45 loss: 0.000557720591314137
batch 50 loss: 0.0005577356205321848
batch 55 loss: 0.0005577526753768325
batch 60 loss: 0.0005576019291765987
batch 65 loss: 0.0005576755735091865
batch 70 loss: 0.0005576040945015848
batch 75 loss: 0.0005576943745836616
batch 80 loss: 0.0005576856317929923
batch 85 loss: 0.0005577319301664829
batch 90 loss: 0.0005576751776970923
batch 95 loss: 0.0005576737457886338
batch 100 loss: 0.0005576751544140279
batch 105 loss: 0.0005577745498158038
batch 110 loss: 0.0005576510448008776
batch 115 loss: 0.0005576947587542236
batch 120 loss: 0.0005576507304795086
batch 125 loss: 0.0005576463183388114
batch 130 loss: 0.0005576791358180344
batch 135 loss: 0.0005577131290920079
batch 140 loss: 0.0005576083553023636
batch 145 loss: 0.000557679426856339
batch 150 loss: 0.0005577356787398458
batch 155 loss: 0.0005578283220529556
batch 160 loss: 0.0005576719529926776
batch 165 loss: 0.0005576796247623861
batch 170 loss: 0.000557680253405124
batch 175 loss: 0.00055759729584679
batch 180 loss: 0.000557578494772315
batch 185 loss: 0.0005575758987106383
batch 190 loss: 0.0005577190080657601
batch 195 loss: 0.0005576150375418365
batch 200 loss: 0.0005576742812991142
batch 205 loss: 0.0005577820353209973
batch 210 loss: 0.0005576062481850386
batch 215 loss: 0.0005577792413532734
batch 220 loss: 0.0005576515686698258
batch 225 loss: 0.0005576625117100775
batch 230 loss: 0.0005578290903940797
batch 235 loss: 0.0005577057017944753
batch 240 loss: 0.0005575681920163333
Training Loss: 0.0005576833818243661
Validation Loss: 0.0005576934073663627
Epoch 90:
batch 5 loss: 0.0005576868890784681
batch 10 loss: 0.0005577244330197573
batch 15 loss: 0.0005575561197474598
batch 20 loss: 0.0005576106137596071
batch 25 loss: 0.0005576836876571179
batch 30 loss: 0.0005577174481004477
batch 35 loss: 0.0005577817908488214
batch 40 loss: 0.0005576350726187229
batch 45 loss: 0.0005576499621383846
batch 50 loss: 0.0005577114527113736
batch 55 loss: 0.0005576126044616103
batch 60 loss: 0.0005576381343416869
batch 65 loss: 0.0005576745374128222
batch 70 loss: 0.0005577899981290102
batch 75 loss: 0.0005577573203481734
batch 80 loss: 0.0005576398689299822
batch 85 loss: 0.0005576696363277734
batch 90 loss: 0.0005577459116466344
batch 95 loss: 0.0005576786235906184
batch 100 loss: 0.0005576772731728852
batch 105 loss: 0.000557646353263408
batch 110 loss: 0.0005577533505856991
batch 115 loss: 0.0005577264702878893
batch 120 loss: 0.0005576752591878176
batch 125 loss: 0.0005576541181653738
batch 130 loss: 0.0005575867369771003
batch 135 loss: 0.0005576779018156231
batch 140 loss: 0.0005575792631134391
batch 145 loss: 0.000557733909226954
batch 150 loss: 0.0005576687632128597
batch 155 loss: 0.0005576050491072238
batch 160 loss: 0.0005577273899689317
batch 165 loss: 0.0005577577976509929
batch 170 loss: 0.000557694397866726
batch 175 loss: 0.0005575468996539712
batch 180 loss: 0.0005577024887315929
batch 185 loss: 0.0005575683782808483
batch 190 loss: 0.0005577291827648878
batch 195 loss: 0.0005577061325311661
batch 200 loss: 0.0005577421397902071
batch 205 loss: 0.0005577396950684488
batch 210 loss: 0.0005577140487730503
batch 215 loss: 0.0005577347241342068
batch 220 loss: 0.0005576521740294993
batch 225 loss: 0.0005577097879722714
batch 230 loss: 0.0005576715571805835
batch 235 loss: 0.0005578088341280818
batch 240 loss: 0.0005576478783041239
Training Loss: 0.0005576833762461319
Validation Loss: 0.0005576934151273841
Epoch 91:
batch 5 loss: 0.0005577120697125793
batch 10 loss: 0.0005576953990384936
batch 15 loss: 0.0005576275056228041
batch 20 loss: 0.0005576654803007841
batch 25 loss: 0.0005576194147579372
batch 30 loss: 0.0005577605566941202
batch 35 loss: 0.0005575453513301909
batch 40 loss: 0.0005576486932113766
batch 45 loss: 0.0005577519303187727
batch 50 loss: 0.0005576796946115791
batch 55 loss: 0.0005576840951107443
batch 60 loss: 0.0005576544092036784
batch 65 loss: 0.0005576784606091678
batch 70 loss: 0.0005576822208240628
batch 75 loss: 0.0005575005081482232
batch 80 loss: 0.0005577427800744772
batch 85 loss: 0.0005577539908699691
batch 90 loss: 0.0005576312425546349
batch 95 loss: 0.0005575805436819792
batch 100 loss: 0.0005577765172347426
batch 105 loss: 0.0005577279953286052
batch 110 loss: 0.0005577309057116508
batch 115 loss: 0.0005577220697887241
batch 120 loss: 0.0005576518131420016
batch 125 loss: 0.0005577066098339855
batch 130 loss: 0.0005577645031735301
batch 135 loss: 0.0005576472147367894
batch 140 loss: 0.0005577136762440205
batch 145 loss: 0.0005576639669016004
batch 150 loss: 0.000557621568441391
batch 155 loss: 0.000557651452254504
batch 160 loss: 0.0005578383337706327
batch 165 loss: 0.000557617424055934
batch 170 loss: 0.0005577071220614016
batch 175 loss: 0.000557727029081434
batch 180 loss: 0.0005576079594902695
batch 185 loss: 0.0005577882868237794
batch 190 loss: 0.0005577270523644984
batch 195 loss: 0.0005577425472438335
batch 200 loss: 0.0005576436524279416
batch 205 loss: 0.0005576008814387023
batch 210 loss: 0.0005577147123403848
batch 215 loss: 0.0005576941999606788
batch 220 loss: 0.0005575761198997498
batch 225 loss: 0.000557783292606473
batch 230 loss: 0.00055768764577806
batch 235 loss: 0.0005576520808972419
batch 240 loss: 0.0005577012430876494
Training Loss: 0.0005576833796415788
Validation Loss: 0.0005576933986352135
Epoch 92:
batch 5 loss: 0.0005575551767833531
batch 10 loss: 0.0005577458185143768
batch 15 loss: 0.0005576847237534821
batch 20 loss: 0.0005576506606303156
batch 25 loss: 0.0005577708128839731
batch 30 loss: 0.0005575634539127349
batch 35 loss: 0.0005577848292887211
batch 40 loss: 0.0005575488437898457
batch 45 loss: 0.0005576852825470268
batch 50 loss: 0.0005576311377808452
batch 55 loss: 0.000557690137065947
batch 60 loss: 0.000557685422245413
batch 65 loss: 0.0005577301257289946
batch 70 loss: 0.0005576274706982076
batch 75 loss: 0.0005576721741817892
batch 80 loss: 0.0005576195428147912
batch 85 loss: 0.0005576988100074231
batch 90 loss: 0.000557697075419128
batch 95 loss: 0.0005576960276812315
batch 100 loss: 0.0005577017436735332
batch 105 loss: 0.000557806110009551
batch 110 loss: 0.0005576600437052548
batch 115 loss: 0.0005576112424023449
batch 120 loss: 0.0005577440024353564
batch 125 loss: 0.0005576695781201124
batch 130 loss: 0.0005576726398430765
batch 135 loss: 0.0005576518829911947
batch 140 loss: 0.0005577625357545912
batch 145 loss: 0.0005576725699938834
batch 150 loss: 0.000557658018078655
batch 155 loss: 0.0005576589843258262
batch 160 loss: 0.0005576764815486968
batch 165 loss: 0.0005576983909122646
batch 170 loss: 0.000557704467792064
batch 175 loss: 0.0005576281109824777
batch 180 loss: 0.0005576710100285709
batch 185 loss: 0.000557726772967726
batch 190 loss: 0.0005577092058956623
batch 195 loss: 0.0005576882045716048
batch 200 loss: 0.0005577033502049744
batch 205 loss: 0.0005576813826337456
batch 210 loss: 0.0005575967370532453
batch 215 loss: 0.0005577026167884469
batch 220 loss: 0.0005577787873335182
batch 225 loss: 0.0005577741540037096
batch 230 loss: 0.0005577102187089622
batch 235 loss: 0.0005576981930062175
batch 240 loss: 0.0005576472496613861
Training Loss: 0.0005576833793990469
Validation Loss: 0.0005576934015455966
Epoch 93:
batch 5 loss: 0.0005577714880928397
batch 10 loss: 0.0005576692055910826
batch 15 loss: 0.000557728880085051
batch 20 loss: 0.0005575516144745052
batch 25 loss: 0.0005577359115704894
batch 30 loss: 0.0005576002993620932
batch 35 loss: 0.0005575993214733899
batch 40 loss: 0.0005578149924986064
batch 45 loss: 0.0005576519994065166
batch 50 loss: 0.0005576379015110433
batch 55 loss: 0.000557669170666486
batch 60 loss: 0.000557663117069751
batch 65 loss: 0.00055766865843907
batch 70 loss: 0.000557719066273421
batch 75 loss: 0.000557686504907906
batch 80 loss: 0.0005577263538725675
batch 85 loss: 0.0005577876465395093
batch 90 loss: 0.0005576367955654859
batch 95 loss: 0.0005576240248046816
batch 100 loss: 0.0005577730713412166
batch 105 loss: 0.0005577069823630154
batch 110 loss: 0.0005575695773586631
batch 115 loss: 0.000557657063473016
batch 120 loss: 0.0005577705334872008
batch 125 loss: 0.0005578562384471297
batch 130 loss: 0.0005575918010435998
batch 135 loss: 0.000557653047144413
batch 140 loss: 0.0005575963063165545
batch 145 loss: 0.0005576330586336553
batch 150 loss: 0.0005576590541750192
batch 155 loss: 0.0005576151655986905
batch 160 loss: 0.0005576816271059215
batch 165 loss: 0.0005576636875048279
batch 170 loss: 0.0005576615338213741
batch 175 loss: 0.0005576366791501641
batch 180 loss: 0.0005576691823080182
batch 185 loss: 0.0005576986470259726
batch 190 loss: 0.0005577672040089965
batch 195 loss: 0.0005577286356128752
batch 200 loss: 0.000557714735623449
batch 205 loss: 0.0005576310679316521
batch 210 loss: 0.0005576918367296458
batch 215 loss: 0.0005576723720878363
batch 220 loss: 0.0005577999050728977
batch 225 loss: 0.0005576417897827923
batch 230 loss: 0.0005576465046033263
batch 235 loss: 0.0005577444797381758
batch 240 loss: 0.0005577274947427213
Training Loss: 0.0005576833798841107
Validation Loss: 0.0005576933966949582
Epoch 94:
batch 5 loss: 0.0005576365510933101
batch 10 loss: 0.0005577050382271409
batch 15 loss: 0.0005578042473644018
batch 20 loss: 0.0005576422554440796
batch 25 loss: 0.0005577145726419986
batch 30 loss: 0.0005577741889283061
batch 35 loss: 0.0005576850497163832
batch 40 loss: 0.0005576242343522608
batch 45 loss: 0.0005576870637014509
batch 50 loss: 0.0005578036536462605
batch 55 loss: 0.0005576017778366804
batch 60 loss: 0.0005577406845986843
batch 65 loss: 0.0005577741423621774
batch 70 loss: 0.0005577052012085915
batch 75 loss: 0.0005577076226472855
batch 80 loss: 0.0005576589261181653
batch 85 loss: 0.0005577246192842722
batch 90 loss: 0.0005577358999289573
batch 95 loss: 0.0005576776922680438
batch 100 loss: 0.0005576671101152897
batch 105 loss: 0.0005576856201514602
batch 110 loss: 0.0005577294155955315
batch 115 loss: 0.0005576199968345463
batch 120 loss: 0.0005575886694714427
batch 125 loss: 0.0005576938856393099
batch 130 loss: 0.0005576385417953133
batch 135 loss: 0.0005577281815931201
batch 140 loss: 0.0005576813011430204
batch 145 loss: 0.0005575622781179845
batch 150 loss: 0.0005577329779043793
batch 155 loss: 0.0005576558411121369
batch 160 loss: 0.0005577429081313312
batch 165 loss: 0.0005576254683546722
batch 170 loss: 0.0005577394040301442
batch 175 loss: 0.0005577060859650373
batch 180 loss: 0.000557667831890285
batch 185 loss: 0.000557704851962626
batch 190 loss: 0.0005576164927333594
batch 195 loss: 0.0005576341063715518
batch 200 loss: 0.0005576773779466748
batch 205 loss: 0.0005576519528403878
batch 210 loss: 0.000557652406860143
batch 215 loss: 0.000557597354054451
batch 220 loss: 0.0005575572489760816
batch 225 loss: 0.0005577349103987217
batch 230 loss: 0.000557663629297167
batch 235 loss: 0.0005577584030106664
batch 240 loss: 0.0005576846422627568
Training Loss: 0.0005576833815818342
Validation Loss: 0.0005576934160975119
Epoch 95:
batch 5 loss: 0.0005576315219514072
batch 10 loss: 0.0005576638737693429
batch 15 loss: 0.0005576855503022671
batch 20 loss: 0.000557686691172421
batch 25 loss: 0.0005577188450843096
batch 30 loss: 0.0005576287512667477
batch 35 loss: 0.0005576042225584388
batch 40 loss: 0.0005576772498898208
batch 45 loss: 0.0005576436291448772
batch 50 loss: 0.0005577731411904096
batch 55 loss: 0.0005577543401159346
batch 60 loss: 0.0005576552241109312
batch 65 loss: 0.0005576047929935158
batch 70 loss: 0.00055765068391338
batch 75 loss: 0.0005576155381277204
batch 80 loss: 0.0005577413481660187
batch 85 loss: 0.0005576678086072206
batch 90 loss: 0.0005575890420004726
batch 95 loss: 0.0005576871917583048
batch 100 loss: 0.0005577195785008371
batch 105 loss: 0.0005577097530476749
batch 110 loss: 0.0005575980874709785
batch 115 loss: 0.0005577409057877958
batch 120 loss: 0.0005576618830673396
batch 125 loss: 0.0005576154100708664
batch 130 loss: 0.0005576029070653021
batch 135 loss: 0.0005577162955887616
batch 140 loss: 0.0005576401832513511
batch 145 loss: 0.000557747227139771
batch 150 loss: 0.0005577285424806178
batch 155 loss: 0.0005577348289079964
batch 160 loss: 0.0005577072384767235
batch 165 loss: 0.0005577685544267297
batch 170 loss: 0.0005578367505222559
batch 175 loss: 0.0005576543160714209
batch 180 loss: 0.0005576570169068873
batch 185 loss: 0.0005576980649493635
batch 190 loss: 0.0005576208583079279
batch 195 loss: 0.0005577621050179005
batch 200 loss: 0.0005576918716542423
batch 205 loss: 0.0005575978779233992
batch 210 loss: 0.0005576849216595292
batch 215 loss: 0.0005576314870268106
batch 220 loss: 0.0005577611736953259
batch 225 loss: 0.0005576843512244522
batch 230 loss: 0.0005577291827648878
batch 235 loss: 0.0005577303119935096
batch 240 loss: 0.0005576909868977963
Training Loss: 0.0005576833774587916
Validation Loss: 0.0005576933937845752
Epoch 96:
batch 5 loss: 0.0005576032446697355
batch 10 loss: 0.000557761755771935
batch 15 loss: 0.0005577189032919705
batch 20 loss: 0.0005576837109401822
batch 25 loss: 0.0005577633273787796
batch 30 loss: 0.0005577385541982949
batch 35 loss: 0.0005577954812906683
batch 40 loss: 0.000557762710377574
batch 45 loss: 0.0005576652125455439
batch 50 loss: 0.0005577705916948617
batch 55 loss: 0.0005577075178734958
batch 60 loss: 0.0005577073665335774
batch 65 loss: 0.0005576812312938273
batch 70 loss: 0.0005576644558459521
batch 75 loss: 0.0005577190197072924
batch 80 loss: 0.0005576741881668567
batch 85 loss: 0.0005577463773079217
batch 90 loss: 0.0005576062481850386
batch 95 loss: 0.0005576297058723867
batch 100 loss: 0.0005576391238719224
batch 105 loss: 0.0005576171795837581
batch 110 loss: 0.0005578511510975659
batch 115 loss: 0.0005577411269769072
batch 120 loss: 0.0005575894960202276
batch 125 loss: 0.0005576820694841444
batch 130 loss: 0.0005576753639616073
batch 135 loss: 0.0005576394964009523
batch 140 loss: 0.0005575669463723898
batch 145 loss: 0.0005576666793785989
batch 150 loss: 0.000557662989012897
batch 155 loss: 0.0005577072384767235
batch 160 loss: 0.0005576839903369546
batch 165 loss: 0.0005577387870289386
batch 170 loss: 0.0005577961797825992
batch 175 loss: 0.0005575982504524291
batch 180 loss: 0.000557666493114084
batch 185 loss: 0.0005576435243710876
batch 190 loss: 0.0005576355149969458
batch 195 loss: 0.0005576039897277951
batch 200 loss: 0.0005576588329859078
batch 205 loss: 0.0005576551426202059
batch 210 loss: 0.0005576705327257514
batch 215 loss: 0.0005576283321715892
batch 220 loss: 0.0005577054456807673
batch 225 loss: 0.000557623989880085
batch 230 loss: 0.0005575892399065196
batch 235 loss: 0.0005577574716880918
batch 240 loss: 0.0005577080301009119
Training Loss: 0.0005576833793990469
Validation Loss: 0.0005576934093066181
Epoch 97:
batch 5 loss: 0.0005576806957833469
batch 10 loss: 0.0005577236996032297
batch 15 loss: 0.0005575691931881011
batch 20 loss: 0.0005576550378464162
batch 25 loss: 0.0005576388444751501
batch 30 loss: 0.0005576015682891011
batch 35 loss: 0.0005577154573984444
batch 40 loss: 0.0005577721167355776
batch 45 loss: 0.0005577566334977746
batch 50 loss: 0.0005576455034315587
batch 55 loss: 0.0005576992640271783
batch 60 loss: 0.0005575705552473664
batch 65 loss: 0.0005576462717726827
batch 70 loss: 0.0005576967145316303
batch 75 loss: 0.0005577346310019493
batch 80 loss: 0.0005577467847615481
batch 85 loss: 0.000557699950877577
batch 90 loss: 0.0005577091011218727
batch 95 loss: 0.0005577441304922103
batch 100 loss: 0.0005576766445301474
batch 105 loss: 0.0005577299627475441
batch 110 loss: 0.0005577364121563732
batch 115 loss: 0.0005576426861807704
batch 120 loss: 0.0005576926283538342
batch 125 loss: 0.0005576737108640373
batch 130 loss: 0.000557762780226767
batch 135 loss: 0.0005577410454861819
batch 140 loss: 0.0005577389849349857
batch 145 loss: 0.0005575803224928677
batch 150 loss: 0.0005576089955866337
batch 155 loss: 0.0005577148403972387
batch 160 loss: 0.0005576465511694551
batch 165 loss: 0.0005576569237746298
batch 170 loss: 0.0005576957832090556
batch 175 loss: 0.0005576439783908427
batch 180 loss: 0.0005576404626481235
batch 185 loss: 0.0005578184151090682
batch 190 loss: 0.0005576420109719038
batch 195 loss: 0.0005576700787059963
batch 200 loss: 0.0005577358417212964
batch 205 loss: 0.0005577352596446872
batch 210 loss: 0.0005576997762545943
batch 215 loss: 0.0005576749215833842
batch 220 loss: 0.0005577462608925999
batch 225 loss: 0.000557418935932219
batch 230 loss: 0.0005576740019023419
batch 235 loss: 0.0005577097414061427
batch 240 loss: 0.0005576879368163646
Training Loss: 0.0005576833760036
Validation Loss: 0.0005576934005754689
Epoch 98:
batch 5 loss: 0.0005576982861384749
batch 10 loss: 0.0005576703697443008
batch 15 loss: 0.0005576716386713088
batch 20 loss: 0.0005576269351877273
batch 25 loss: 0.0005577070056460798
batch 30 loss: 0.0005576931871473789
batch 35 loss: 0.0005576753639616073
batch 40 loss: 0.0005576649447903038
batch 45 loss: 0.0005577122559770942
batch 50 loss: 0.0005577092873863876
batch 55 loss: 0.0005577784613706172
batch 60 loss: 0.0005576996947638691
batch 65 loss: 0.0005577581701800227
batch 70 loss: 0.0005577845848165452
batch 75 loss: 0.000557687331456691
batch 80 loss: 0.0005576697760261596
batch 85 loss: 0.0005576519179157913
batch 90 loss: 0.0005576442694291472
batch 95 loss: 0.0005577327683568001
batch 100 loss: 0.0005576462252065539
batch 105 loss: 0.000557578809093684
batch 110 loss: 0.000557743024546653
batch 115 loss: 0.0005576515104621649
batch 120 loss: 0.0005577239440754056
batch 125 loss: 0.0005576685187406838
batch 130 loss: 0.000557687203399837
batch 135 loss: 0.0005576333962380887
batch 140 loss: 0.00055763004347682
batch 145 loss: 0.0005576272844336927
batch 150 loss: 0.0005575790302827955
batch 155 loss: 0.0005575959919951856
batch 160 loss: 0.0005576042225584388
batch 165 loss: 0.0005575969931669533
batch 170 loss: 0.0005577665288001299
batch 175 loss: 0.0005578160868026316
batch 180 loss: 0.0005576631054282188
batch 185 loss: 0.0005576291005127132
batch 190 loss: 0.0005577461211942137
batch 195 loss: 0.0005576319177635014
batch 200 loss: 0.0005576798925176263
batch 205 loss: 0.00055763068376109
batch 210 loss: 0.0005577610107138753
batch 215 loss: 0.000557699054479599
batch 220 loss: 0.0005577193456701935
batch 225 loss: 0.0005577877047471703
batch 230 loss: 0.0005576381925493478
batch 235 loss: 0.0005576788564212621
batch 240 loss: 0.0005577520001679659
Training Loss: 0.0005576833760036
Validation Loss: 0.0005576933976650859
Epoch 99:
batch 5 loss: 0.0005577237461693585
batch 10 loss: 0.0005576469353400171
batch 15 loss: 0.000557593593839556
batch 20 loss: 0.0005576985538937151
batch 25 loss: 0.0005576334544457496
batch 30 loss: 0.0005576057825237513
batch 35 loss: 0.0005575934192165732
batch 40 loss: 0.000557707529515028
batch 45 loss: 0.0005576256080530584
batch 50 loss: 0.0005577355972491205
batch 55 loss: 0.0005576254101470113
batch 60 loss: 0.0005576497875154019
batch 65 loss: 0.0005576315335929394
batch 70 loss: 0.0005576595431193709
batch 75 loss: 0.0005577243631705642
batch 80 loss: 0.0005577023141086102
batch 85 loss: 0.0005577163421548903
batch 90 loss: 0.0005577093572355807
batch 95 loss: 0.0005575673072598875
batch 100 loss: 0.000557673699222505
batch 105 loss: 0.0005577503587119281
batch 110 loss: 0.0005577557953074574
batch 115 loss: 0.0005577140371315181
batch 120 loss: 0.000557715620379895
batch 125 loss: 0.000557787623256445
batch 130 loss: 0.0005576842580921948
batch 135 loss: 0.0005575674236752093
batch 140 loss: 0.0005577357369475067
batch 145 loss: 0.0005576582625508308
batch 150 loss: 0.0005577941192314029
batch 155 loss: 0.0005576723953709007
batch 160 loss: 0.0005577901727519929
batch 165 loss: 0.0005576048162765801
batch 170 loss: 0.0005576238501816988
batch 175 loss: 0.0005576728028245271
batch 180 loss: 0.0005577085190452636
batch 185 loss: 0.000557663943618536
batch 190 loss: 0.0005577408242970705
batch 195 loss: 0.0005578739917837083
batch 200 loss: 0.0005577539559453726
batch 205 loss: 0.0005576248746365309
batch 210 loss: 0.0005576286115683615
batch 215 loss: 0.0005577609990723431
batch 220 loss: 0.0005576699506491423
batch 225 loss: 0.0005576632334850729
batch 230 loss: 0.0005577528267167508
batch 235 loss: 0.000557548098731786
batch 240 loss: 0.0005576609284617007
Training Loss: 0.000557683373093217
Validation Loss: 0.0005576934044559796
Epoch 100:
batch 5 loss: 0.0005577101488597691
batch 10 loss: 0.0005577583680860699
batch 15 loss: 0.0005575926043093204
batch 20 loss: 0.000557664327789098
batch 25 loss: 0.0005577244563028216
batch 30 loss: 0.0005577165284194052
batch 35 loss: 0.0005578046897426247
batch 40 loss: 0.0005578245501965285
batch 45 loss: 0.0005577349569648504
batch 50 loss: 0.0005576444906182587
batch 55 loss: 0.0005577063071541488
batch 60 loss: 0.0005576267023570836
batch 65 loss: 0.0005576830008067191
batch 70 loss: 0.0005576415569521487
batch 75 loss: 0.0005575926275923848
batch 80 loss: 0.0005576790892519057
batch 85 loss: 0.000557719508651644
batch 90 loss: 0.0005577302421443164
batch 95 loss: 0.000557554978877306
batch 100 loss: 0.000557737797498703
batch 105 loss: 0.0005576835479587317
batch 110 loss: 0.0005576001247391104
batch 115 loss: 0.0005576839670538902
batch 120 loss: 0.000557628832757473
batch 125 loss: 0.0005575675284489989
batch 130 loss: 0.0005576939438469708
batch 135 loss: 0.0005577061674557626
batch 140 loss: 0.0005576537572778761
batch 145 loss: 0.000557615514844656
batch 150 loss: 0.0005576584138907492
batch 155 loss: 0.0005576767027378083
batch 160 loss: 0.0005576391704380512
batch 165 loss: 0.000557638902682811
batch 170 loss: 0.0005577455856837332
batch 175 loss: 0.0005576487979851663
batch 180 loss: 0.0005577087751589716
batch 185 loss: 0.0005576486582867801
batch 190 loss: 0.0005576872383244336
batch 195 loss: 0.0005576375988312066
batch 200 loss: 0.000557722628582269
batch 205 loss: 0.000557797122746706
batch 210 loss: 0.0005576676805503667
batch 215 loss: 0.0005576901254244149
batch 220 loss: 0.0005576191819272935
batch 225 loss: 0.0005576886935159564
batch 230 loss: 0.000557781511452049
batch 235 loss: 0.0005577843054197729
batch 240 loss: 0.0005576804862357676
Training Loss: 0.0005576833728506851
Validation Loss: 0.0005576934102767458
