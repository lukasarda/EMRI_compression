****************************************************************
*                      SLURM Batch System                      *
*           IN2P3 Computing Centre, Villeurbanne FR            *
****************************************************************
* Date:                 Sun Jun 30 13:33:55 CEST 2024
* Job name:             800
* Job id:               17512725
* User:                 lkarda
* Account:              lisaf
* Submit host:          ccahm001
* Partition:            gpu
* Quality of service:   gpu
* Nodelist:             ccwgslurm0115
****************************************************************
no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 800
20240630_133439
Epoch 1:
batch 5 loss: 0.01725541315972805
batch 10 loss: 0.003972687385976314
batch 15 loss: 0.0023738105315715074
batch 20 loss: 0.0017532134894281625
batch 25 loss: 0.0014285549987107515
batch 30 loss: 0.0011802273569628597
batch 35 loss: 0.0010154431685805321
batch 40 loss: 0.0009073255816474557
batch 45 loss: 0.0008684533415362239
batch 50 loss: 0.0008721832884475589
batch 55 loss: 0.0008245033561252058
batch 60 loss: 0.0008039486361667514
batch 65 loss: 0.0007855864707380534
batch 70 loss: 0.0007686494616791606
batch 75 loss: 0.0007434547645971179
batch 80 loss: 0.0008056439226493239
batch 85 loss: 0.0007630426669493317
batch 90 loss: 0.0007225860143080353
batch 95 loss: 0.000701768312137574
batch 100 loss: 0.0006852613878436387
batch 105 loss: 0.0006687622284516692
batch 110 loss: 0.000659418199211359
batch 115 loss: 0.0006448921514675022
batch 120 loss: 0.0006337798549793661
batch 125 loss: 0.0007089736522175371
batch 130 loss: 0.0008254292653873563
batch 135 loss: 0.0006672911462374031
batch 140 loss: 0.000649154675193131
batch 145 loss: 0.0006469097104854881
batch 150 loss: 0.0006436176598072052
batch 155 loss: 0.0006398211349733174
batch 160 loss: 0.0006365032051689923
batch 165 loss: 0.0006349137518554925
batch 170 loss: 0.0006334727397188545
batch 175 loss: 0.0006318879080936312
batch 180 loss: 0.0006303112721070647
batch 185 loss: 0.0006292009958997369
batch 190 loss: 0.0006280583445914089
batch 195 loss: 0.0006267228745855391
batch 200 loss: 0.0006278945598751307
batch 205 loss: 0.0007875589188188314
batch 210 loss: 0.0007966189412400126
batch 215 loss: 0.000645172770600766
batch 220 loss: 0.000641121668741107
batch 225 loss: 0.000639520084951073
batch 230 loss: 0.0006371746188960969
batch 235 loss: 0.000634692027233541
batch 240 loss: 0.0006309887627139688
Training Loss: 0.0012023254254017956
Validation Loss: 0.0005670994762719298
Epoch 2:
batch 5 loss: 0.0006267124670557677
batch 10 loss: 0.0006211242638528347
batch 15 loss: 0.0006142734899185597
batch 20 loss: 0.0006105275941081345
batch 25 loss: 0.0006064040935598314
batch 30 loss: 0.0006045753136277199
batch 35 loss: 0.0006029661046341062
batch 40 loss: 0.0006012163474224507
batch 45 loss: 0.0005996974417939782
batch 50 loss: 0.0005980507121421397
batch 55 loss: 0.0005960168666206301
batch 60 loss: 0.0005944828735664487
batch 65 loss: 0.0005929806269705295
batch 70 loss: 0.0005914536071941256
batch 75 loss: 0.0005897543509490788
batch 80 loss: 0.0005885188817046583
batch 85 loss: 0.0005870423628948629
batch 90 loss: 0.0005881840363144875
batch 95 loss: 0.0005854718619957566
batch 100 loss: 0.0005835910094901919
batch 105 loss: 0.0005824560415931046
batch 110 loss: 0.0005813483847305178
batch 115 loss: 0.0005805089138448238
batch 120 loss: 0.0005798044032417238
batch 125 loss: 0.0005791437695734202
batch 130 loss: 0.0005784968263469637
batch 135 loss: 0.0005779617466032505
batch 140 loss: 0.0005774758523330092
batch 145 loss: 0.0005784687818959355
batch 150 loss: 0.0006384775275364518
batch 155 loss: 0.0013946887105703355
batch 160 loss: 0.0006174843292683363
batch 165 loss: 0.0006118370103649795
batch 170 loss: 0.0006016536266542971
batch 175 loss: 0.0005944797652773559
batch 180 loss: 0.0005885301274247467
batch 185 loss: 0.0005839453195221722
batch 190 loss: 0.0005805770400911569
batch 195 loss: 0.0005778107442893088
batch 200 loss: 0.0005764010129496455
batch 205 loss: 0.0005753368604928255
batch 210 loss: 0.0005744242924265563
batch 215 loss: 0.0005737661151215435
batch 220 loss: 0.000573398161213845
batch 225 loss: 0.0005729942466132342
batch 230 loss: 0.0005726832430809736
batch 235 loss: 0.0005723111797124147
batch 240 loss: 0.0005721031222492457
Training Loss: 0.0006073252387674681
Validation Loss: 0.000568885241712754
Epoch 3:
batch 5 loss: 0.0005718462169170379
batch 10 loss: 0.0005715247592888772
batch 15 loss: 0.0005713949911296367
batch 20 loss: 0.0005711626261472702
batch 25 loss: 0.0005712602287530899
batch 30 loss: 0.0005707784788683057
batch 35 loss: 0.0005708823213353753
batch 40 loss: 0.0005708052543923259
batch 45 loss: 0.0005702893016859889
batch 50 loss: 0.0005704136099666357
batch 55 loss: 0.0005703385570086539
batch 60 loss: 0.0005700962617993354
batch 65 loss: 0.0005698038614355028
batch 70 loss: 0.0005696864915080369
batch 75 loss: 0.0005693702260032296
batch 80 loss: 0.0005690741236321628
batch 85 loss: 0.000569294742308557
batch 90 loss: 0.0005692123784683644
batch 95 loss: 0.0005690131918527185
batch 100 loss: 0.000568968802690506
batch 105 loss: 0.0005689548328518867
batch 110 loss: 0.000568619859404862
batch 115 loss: 0.0005684318253770471
batch 120 loss: 0.0005681920098140836
batch 125 loss: 0.0005681987502612173
batch 130 loss: 0.0005681448616087437
batch 135 loss: 0.000568017887417227
batch 140 loss: 0.0005680340109393001
batch 145 loss: 0.0005679555702954531
batch 150 loss: 0.0005679806228727103
batch 155 loss: 0.0005678714136593044
batch 160 loss: 0.0005678731366060674
batch 165 loss: 0.0005676655331626534
batch 170 loss: 0.0005675620399415493
batch 175 loss: 0.0005674415035173297
batch 180 loss: 0.0005671164020895958
batch 185 loss: 0.0005671675433404743
batch 190 loss: 0.0005672226427122951
batch 195 loss: 0.0005670199054293335
batch 200 loss: 0.0005672668805345893
batch 205 loss: 0.0005670163314789534
batch 210 loss: 0.0005670726648531854
batch 215 loss: 0.0005670854006893933
batch 220 loss: 0.0005669743637554348
batch 225 loss: 0.0005668233963660896
batch 230 loss: 0.0005667482619173825
batch 235 loss: 0.0005667172372341156
batch 240 loss: 0.0005666979355737567
Training Loss: 0.0005686893593519927
Validation Loss: 0.0005656181174951296
Epoch 4:
batch 5 loss: 0.0005666545592248439
batch 10 loss: 0.0005667062476277351
batch 15 loss: 0.0005668998812325299
batch 20 loss: 0.000566950743086636
batch 25 loss: 0.000566904537845403
batch 30 loss: 0.0005667096236720681
batch 35 loss: 0.0005667362594977022
batch 40 loss: 0.0005664500407874584
batch 45 loss: 0.0005662713549099863
batch 50 loss: 0.0005662302835844457
batch 55 loss: 0.0005661221221089363
batch 60 loss: 0.0005662022274918854
batch 65 loss: 0.0005660486523993314
batch 70 loss: 0.0005659103393554687
batch 75 loss: 0.0005660212598741054
batch 80 loss: 0.0005660706548951567
batch 85 loss: 0.0005660227383486926
batch 90 loss: 0.0005661279312334955
batch 95 loss: 0.000566563627216965
batch 100 loss: 0.0005671753198839724
batch 105 loss: 0.0005672250757925212
batch 110 loss: 0.0005670115351676941
batch 115 loss: 0.0005666116834618151
batch 120 loss: 0.0005662598414346576
batch 125 loss: 0.0005659584188833833
batch 130 loss: 0.0005659074289724231
batch 135 loss: 0.0005656998953782022
batch 140 loss: 0.0005657034926116467
batch 145 loss: 0.0005656358087435364
batch 150 loss: 0.0005655855406075716
batch 155 loss: 0.0005656605819240213
batch 160 loss: 0.0005655540619045496
batch 165 loss: 0.0005657984060235321
batch 170 loss: 0.0005657764151692391
batch 175 loss: 0.0005656443419866264
batch 180 loss: 0.0005654916982166469
batch 185 loss: 0.0005653916741721332
batch 190 loss: 0.0005654523149132729
batch 195 loss: 0.0005652529071085155
batch 200 loss: 0.000565316469874233
batch 205 loss: 0.0005652400781400502
batch 210 loss: 0.0005653209984302521
batch 215 loss: 0.0005651195184327662
batch 220 loss: 0.0005652473540976643
batch 225 loss: 0.0005650997161865234
batch 230 loss: 0.0005650659324601293
batch 235 loss: 0.0005653996020555496
batch 240 loss: 0.0005653506610542536
Training Loss: 0.0005659908303641715
Validation Loss: 0.0005642504334294547
Epoch 5:
batch 5 loss: 0.0005653641768731177
batch 10 loss: 0.000565324816852808
batch 15 loss: 0.0005650904378853739
batch 20 loss: 0.0005649416823871433
batch 25 loss: 0.0005650438833981752
batch 30 loss: 0.0005646935780532659
batch 35 loss: 0.0005648013204336167
batch 40 loss: 0.0005647618905641139
batch 45 loss: 0.000564786116592586
batch 50 loss: 0.0005648019374348223
batch 55 loss: 0.0005646286299452186
batch 60 loss: 0.0005647253827191889
batch 65 loss: 0.0028466696501709523
batch 70 loss: 0.003564471798017621
batch 75 loss: 0.0021015410777181385
batch 80 loss: 0.001623923541046679
batch 85 loss: 0.0013769353274255991
batch 90 loss: 0.0011943171732127667
batch 95 loss: 0.0010294483043253421
batch 100 loss: 0.0009322210913524032
batch 105 loss: 0.0008386110654100776
batch 110 loss: 0.0007748887641355395
batch 115 loss: 0.000727898592595011
batch 120 loss: 0.0007238370599225163
batch 125 loss: 0.0006839325185865163
batch 130 loss: 0.0006666925502941013
batch 135 loss: 0.0006511648767627776
batch 140 loss: 0.0006395014002919197
batch 145 loss: 0.0006330651696771384
batch 150 loss: 0.0006210457882843912
batch 155 loss: 0.0006662352127023042
batch 160 loss: 0.0006600855616852641
batch 165 loss: 0.0006322300410829485
batch 170 loss: 0.0006172045948915183
batch 175 loss: 0.0006062741274945437
batch 180 loss: 0.000599961809348315
batch 185 loss: 0.0005940492847003043
batch 190 loss: 0.0005865129176527261
batch 195 loss: 0.0005815630313009024
batch 200 loss: 0.0005789942457340658
batch 205 loss: 0.0005779425264336169
batch 210 loss: 0.0005769758019596339
batch 215 loss: 0.0005757622537203133
batch 220 loss: 0.0005746448296122253
batch 225 loss: 0.0005732786841690541
batch 230 loss: 0.0005707343108952046
batch 235 loss: 0.0005700503941625356
batch 240 loss: 0.0005690805031917989
Training Loss: 0.0008150147861063791
Validation Loss: 0.0005671398491055394
Epoch 6:
batch 5 loss: 0.0005681795417331159
batch 10 loss: 0.0005675241816788911
batch 15 loss: 0.0005661793751642108
batch 20 loss: 0.0005652959225699306
batch 25 loss: 0.000565089040901512
batch 30 loss: 0.0005649097263813018
batch 35 loss: 0.0005647583864629268
batch 40 loss: 0.0005646010860800744
batch 45 loss: 0.0005646362318657338
batch 50 loss: 0.00056439753388986
batch 55 loss: 0.0005644657881930471
batch 60 loss: 0.0005642764852382242
batch 65 loss: 0.0005641518160700798
batch 70 loss: 0.0005641110823489725
batch 75 loss: 0.000564194389153272
batch 80 loss: 0.0005640784045681357
batch 85 loss: 0.000563985260669142
batch 90 loss: 0.0005639747483655811
batch 95 loss: 0.000563994434196502
batch 100 loss: 0.0005641630734317005
batch 105 loss: 0.0005647384678013623
batch 110 loss: 0.0005641171941533685
batch 115 loss: 0.0005638844450004399
batch 120 loss: 0.0005639950511977076
batch 125 loss: 0.0005638846778310835
batch 130 loss: 0.00056373062543571
batch 135 loss: 0.0005636118934489786
batch 140 loss: 0.000563672196585685
batch 145 loss: 0.0005635499837808311
batch 150 loss: 0.0005634505418129265
batch 155 loss: 0.0005635270033963024
batch 160 loss: 0.0005633202963508666
batch 165 loss: 0.0005635098321363329
batch 170 loss: 0.0005634461878798902
batch 175 loss: 0.0005633394583128392
batch 180 loss: 0.0005838335026055574
batch 185 loss: 0.0009067652747035027
batch 190 loss: 0.0007664335775189101
batch 195 loss: 0.0007125965552404523
batch 200 loss: 0.0006746209692209959
batch 205 loss: 0.0006477330345660449
batch 210 loss: 0.0006365264067426324
batch 215 loss: 0.000622082140762359
batch 220 loss: 0.0006061340332962573
batch 225 loss: 0.0005896527669392526
batch 230 loss: 0.0005766093614511192
batch 235 loss: 0.0005716022569686174
batch 240 loss: 0.0005707662086933852
Training Loss: 0.0005878770927665756
Validation Loss: 0.0005671882642976319
Epoch 7:
batch 5 loss: 0.000570377241820097
batch 10 loss: 0.0005698270513676106
batch 15 loss: 0.0005692860577255487
batch 20 loss: 0.0005690961959771812
batch 25 loss: 0.0005688178003765643
batch 30 loss: 0.0005687506753019988
batch 35 loss: 0.0005685484851710498
batch 40 loss: 0.0005681852693669498
batch 45 loss: 0.0005681117065250874
batch 50 loss: 0.0005681114736944437
batch 55 loss: 0.0005677842069417238
batch 60 loss: 0.000567699340172112
batch 65 loss: 0.0005677709938026965
batch 70 loss: 0.0005676988163031638
batch 75 loss: 0.0005675946362316608
batch 80 loss: 0.0005676729371771217
batch 85 loss: 0.0005675646942108869
batch 90 loss: 0.0005675839260220527
batch 95 loss: 0.0005673536914400756
batch 100 loss: 0.000567391247022897
batch 105 loss: 0.000567270175088197
batch 110 loss: 0.0005672910250723362
batch 115 loss: 0.0005671826540492475
batch 120 loss: 0.0005672328756190836
batch 125 loss: 0.0005670909187756479
batch 130 loss: 0.0005670108366757631
batch 135 loss: 0.0005670157028362155
batch 140 loss: 0.000567005155608058
batch 145 loss: 0.0005668655852787196
batch 150 loss: 0.0005668926634825767
batch 155 loss: 0.0005668588331900537
batch 160 loss: 0.0005667475517839194
batch 165 loss: 0.0005667044781148434
batch 170 loss: 0.0005666489945724606
batch 175 loss: 0.0005666254553943872
batch 180 loss: 0.0005665586679242552
batch 185 loss: 0.0005665116477757692
batch 190 loss: 0.0005666085635311902
batch 195 loss: 0.0005663863616064191
batch 200 loss: 0.0005664749536663294
batch 205 loss: 0.0005663970019668341
batch 210 loss: 0.0005664015072397888
batch 215 loss: 0.0005662273499183357
batch 220 loss: 0.0005661708419211209
batch 225 loss: 0.0005662727868184448
batch 230 loss: 0.0005660666502080858
batch 235 loss: 0.000565950816962868
batch 240 loss: 0.000566019513644278
Training Loss: 0.0005673274169870032
Validation Loss: 0.0005640066228806973
Epoch 8:
batch 5 loss: 0.0005659302230924367
batch 10 loss: 0.000565728428773582
batch 15 loss: 0.0005656903027556837
batch 20 loss: 0.0005656477413140237
batch 25 loss: 0.0005655902554281056
batch 30 loss: 0.0005655228509567678
batch 35 loss: 0.0005652565625496209
batch 40 loss: 0.0005652516963891685
batch 45 loss: 0.0005650014150887728
batch 50 loss: 0.0005649416823871433
batch 55 loss: 0.0005645063472911715
batch 60 loss: 0.0005641560885123909
batch 65 loss: 0.0005635676207020879
batch 70 loss: 0.0005624834215268492
batch 75 loss: 0.0005623456090688705
batch 80 loss: 0.0005622876808047295
batch 85 loss: 0.0005623041768558323
batch 90 loss: 0.0005624271580018103
batch 95 loss: 0.0005623896140605211
batch 100 loss: 0.0005623426288366318
batch 105 loss: 0.0005624037818051875
batch 110 loss: 0.0005623816861771048
batch 115 loss: 0.0005622804630547762
batch 120 loss: 0.0005623132339678705
batch 125 loss: 0.0005622709752060473
batch 130 loss: 0.0005623756791464984
batch 135 loss: 0.0005622360273264349
batch 140 loss: 0.0005622304975986481
batch 145 loss: 0.0005622415221296251
batch 150 loss: 0.000562212208751589
batch 155 loss: 0.0005621860967949033
batch 160 loss: 0.0005622217548079788
batch 165 loss: 0.000562264013569802
batch 170 loss: 0.0005623229197226464
batch 175 loss: 0.0005623352015390992
batch 180 loss: 0.0005622323602437973
batch 185 loss: 0.0005624242825433612
batch 190 loss: 0.0005622932105325163
batch 195 loss: 0.000562232208903879
batch 200 loss: 0.0005620992160402238
batch 205 loss: 0.0005623014993034303
batch 210 loss: 0.0005623219651170075
batch 215 loss: 0.0005627617472782731
batch 220 loss: 0.0005625718971714377
batch 225 loss: 0.0005622894852422178
batch 230 loss: 0.0005623411620035767
batch 235 loss: 0.0005628874525427819
batch 240 loss: 0.0005626361817121506
Training Loss: 0.0005631050048881055
Validation Loss: 0.0005626692038883145
Epoch 9:
batch 5 loss: 0.0005629861727356911
batch 10 loss: 0.0005626694997772575
batch 15 loss: 0.0005626515252515673
batch 20 loss: 0.0005623453063890338
batch 25 loss: 0.0005623120698146522
batch 30 loss: 0.0005621465621516109
batch 35 loss: 0.0005622232798486948
batch 40 loss: 0.0005621929070912301
batch 45 loss: 0.0005626430036500096
batch 50 loss: 0.0005627027945593
batch 55 loss: 0.0005630462779663503
batch 60 loss: 0.0005630210624076426
batch 65 loss: 0.0005626788013614714
batch 70 loss: 0.0005626689875498414
batch 75 loss: 0.0005626678001135587
batch 80 loss: 0.0005629233783110976
batch 85 loss: 0.0005630065337754786
batch 90 loss: 0.0005629008286632597
batch 95 loss: 0.0005625184392556548
batch 100 loss: 0.0005619258154183626
batch 105 loss: 0.0005622104392386973
batch 110 loss: 0.0005623844801448286
batch 115 loss: 0.0005622145603410899
batch 120 loss: 0.000562593259382993
batch 125 loss: 0.0005626969854347408
batch 130 loss: 0.0005623384146019817
batch 135 loss: 0.0005626740748994052
batch 140 loss: 0.0005629951017908752
batch 145 loss: 0.0005627616657875479
batch 150 loss: 0.0005627458565868437
batch 155 loss: 0.0005626080906949938
batch 160 loss: 0.0005625532241538167
batch 165 loss: 0.0005626635742373764
batch 170 loss: 0.0005627294187434018
batch 175 loss: 0.000562629965133965
batch 180 loss: 0.0005622086697258055
batch 185 loss: 0.000562530371826142
batch 190 loss: 0.0005627513281069696
batch 195 loss: 0.0005627191625535488
batch 200 loss: 0.0005630371393635869
batch 205 loss: 0.0005625597783364356
batch 210 loss: 0.0005620926036499441
batch 215 loss: 0.0005625811871141195
batch 220 loss: 0.0005623779608868063
batch 225 loss: 0.0005623358301818371
batch 230 loss: 0.0005621718475595117
batch 235 loss: 0.0005620989482849837
batch 240 loss: 0.0005625364021398128
Training Loss: 0.0005625631538957047
Validation Loss: 0.0005618939428435018
Epoch 10:
batch 5 loss: 0.0005625703488476574
batch 10 loss: 0.0005626410245895385
batch 15 loss: 0.000562778813764453
batch 20 loss: 0.0005626068101264537
batch 25 loss: 0.0005623543402180076
batch 30 loss: 0.0005623676930554211
batch 35 loss: 0.0005620966083370149
batch 40 loss: 0.0005620920564979315
batch 45 loss: 0.0005619646282866598
batch 50 loss: 0.0005619941279292106
batch 55 loss: 0.0005619650590233504
batch 60 loss: 0.000562168937176466
batch 65 loss: 0.0005622208933345974
batch 70 loss: 0.000562470406293869
batch 75 loss: 0.0005630180588923394
batch 80 loss: 0.0005627050180919468
batch 85 loss: 0.0005624135723337531
batch 90 loss: 0.0005624644691124558
batch 95 loss: 0.0005623041768558323
batch 100 loss: 0.0005622335826046765
batch 105 loss: 0.0005625868798233569
batch 110 loss: 0.0005627887090668082
batch 115 loss: 0.0005623344564810395
batch 120 loss: 0.0005625490681268275
batch 125 loss: 0.0005625648889690637
batch 130 loss: 0.0005623961566016078
batch 135 loss: 0.0005621994729153812
batch 140 loss: 0.0005627100472338498
batch 145 loss: 0.0005627673701383174
batch 150 loss: 0.0005623961566016078
batch 155 loss: 0.000562106107827276
batch 160 loss: 0.0005621863761916757
batch 165 loss: 0.000561809865757823
batch 170 loss: 0.0005620336392894387
batch 175 loss: 0.0005621433025225997
batch 180 loss: 0.0005623485893011093
batch 185 loss: 0.0005622748634777963
batch 190 loss: 0.000562374317087233
batch 195 loss: 0.0005619537783786655
batch 200 loss: 0.0005617886665277183
batch 205 loss: 0.0005619060480967164
batch 210 loss: 0.0005618497147224843
batch 215 loss: 0.0005618901224806905
batch 220 loss: 0.0005619702977128326
batch 225 loss: 0.0005622250493615866
batch 230 loss: 0.000561983382795006
batch 235 loss: 0.0005616616457700729
batch 240 loss: 0.0005618304945528507
Training Loss: 0.0005622720852746473
Validation Loss: 0.0005618473582823451
Epoch 11:
batch 5 loss: 0.000562270893715322
batch 10 loss: 0.0005623267032206058
batch 15 loss: 0.0005619297851808369
batch 20 loss: 0.0005618110997602343
batch 25 loss: 0.000561697839293629
batch 30 loss: 0.0005619444069452584
batch 35 loss: 0.0005617715534754098
batch 40 loss: 0.0005616680835373699
batch 45 loss: 0.0005614856025204062
batch 50 loss: 0.0005620650947093964
batch 55 loss: 0.0005621196469292045
batch 60 loss: 0.0005618433351628482
batch 65 loss: 0.0005620129290036857
batch 70 loss: 0.0005618285038508474
batch 75 loss: 0.0005618278053589165
batch 80 loss: 0.0005618294700980187
batch 85 loss: 0.0005616785841993987
batch 90 loss: 0.0005616138805635273
batch 95 loss: 0.0005614480469375849
batch 100 loss: 0.0005615785485133529
batch 105 loss: 0.0005614469409920275
batch 110 loss: 0.0005619177129119635
batch 115 loss: 0.0005621972843073309
batch 120 loss: 0.0005617146729491651
batch 125 loss: 0.0005616585025563836
batch 130 loss: 0.0005617247661575675
batch 135 loss: 0.0005619330331683159
batch 140 loss: 0.0005624454934149981
batch 145 loss: 0.0005617091199383139
batch 150 loss: 0.0005616827984340489
batch 155 loss: 0.0005611799424514174
batch 160 loss: 0.0005615898407995701
batch 165 loss: 0.0005618572235107422
batch 170 loss: 0.0005618857918307185
batch 175 loss: 0.0005616076407022775
batch 180 loss: 0.0005617539747618139
batch 185 loss: 0.0005620238836854697
batch 190 loss: 0.000562056724447757
batch 195 loss: 0.0005619511823169887
batch 200 loss: 0.0005619837087579071
batch 205 loss: 0.0005617993301711977
batch 210 loss: 0.0005615799105726183
batch 215 loss: 0.000561379047576338
batch 220 loss: 0.0005615525878965855
batch 225 loss: 0.0005613796645775437
batch 230 loss: 0.0005613881396129728
batch 235 loss: 0.0005615151952952146
batch 240 loss: 0.0005615446134470403
Training Loss: 0.0005617752195879196
Validation Loss: 0.0005613128198698784
Epoch 12:
batch 5 loss: 0.0005613651825115085
batch 10 loss: 0.0005609762738458812
batch 15 loss: 0.0005611568223685027
batch 20 loss: 0.0005612888257019222
batch 25 loss: 0.0005616401322185993
batch 30 loss: 0.0005616105045191944
batch 35 loss: 0.0005614967667497694
batch 40 loss: 0.0005614725523628294
batch 45 loss: 0.0005615147296339273
batch 50 loss: 0.0005615164292976261
batch 55 loss: 0.0005613731918856502
batch 60 loss: 0.0005609782063402235
batch 65 loss: 0.000561203679535538
batch 70 loss: 0.0005611587315797806
batch 75 loss: 0.0005612612119875848
batch 80 loss: 0.0005610070074908435
batch 85 loss: 0.0005614223540760577
batch 90 loss: 0.0005617893999442458
batch 95 loss: 0.0005615363596007227
batch 100 loss: 0.0005613625515252351
batch 105 loss: 0.0005617295391857624
batch 110 loss: 0.0005613742396235466
batch 115 loss: 0.0005613836459815502
batch 120 loss: 0.0005615099682472647
batch 125 loss: 0.0005612335982732475
batch 130 loss: 0.0005613059853203595
batch 135 loss: 0.0005611700704321265
batch 140 loss: 0.0005614669644273818
batch 145 loss: 0.0005615113768726587
batch 150 loss: 0.0005617076647467911
batch 155 loss: 0.0005616434966214002
batch 160 loss: 0.0005614833207800984
batch 165 loss: 0.0005614824476651847
batch 170 loss: 0.0005614381632767617
batch 175 loss: 0.0005614797701127827
batch 180 loss: 0.0005615660920739174
batch 185 loss: 0.0005614554975181818
batch 190 loss: 0.0005611228290945292
batch 195 loss: 0.000560911966022104
batch 200 loss: 0.0005610105465166271
batch 205 loss: 0.0005608694511465729
batch 210 loss: 0.0005613270564936102
batch 215 loss: 0.0005609116051346064
batch 220 loss: 0.0005607115919701755
batch 225 loss: 0.0005608271458186209
batch 230 loss: 0.0005609893356449902
batch 235 loss: 0.0005609912564978004
batch 240 loss: 0.0005613981978967786
Training Loss: 0.0005613154945118974
Validation Loss: 0.0005610350500016163
Epoch 13:
batch 5 loss: 0.0005612838780507445
batch 10 loss: 0.0005610739579424262
batch 15 loss: 0.000560916867107153
batch 20 loss: 0.0005611161468550563
batch 25 loss: 0.0005609058076515794
batch 30 loss: 0.0005611496511846781
batch 35 loss: 0.0005613294430077076
batch 40 loss: 0.0005612362059764564
batch 45 loss: 0.0005610582884401083
batch 50 loss: 0.0005609965068288147
batch 55 loss: 0.0005611801985651255
batch 60 loss: 0.0005611233180388808
batch 65 loss: 0.0005609407788142561
batch 70 loss: 0.0005608142702840269
batch 75 loss: 0.0005609065410681069
batch 80 loss: 0.000560830207541585
batch 85 loss: 0.0005608876468613744
batch 90 loss: 0.0005609120824374258
batch 95 loss: 0.0005611603264696896
batch 100 loss: 0.0005611776956357062
batch 105 loss: 0.0005613534827716649
batch 110 loss: 0.000561343994922936
batch 115 loss: 0.0005610419902950525
batch 120 loss: 0.0005610458320006728
batch 125 loss: 0.0005608342355117202
batch 130 loss: 0.0005606961203739047
batch 135 loss: 0.0005606647580862045
batch 140 loss: 0.0005608464241959154
batch 145 loss: 0.0005609015352092683
batch 150 loss: 0.0005609873798675836
batch 155 loss: 0.0005609871470369399
batch 160 loss: 0.0005607912782579661
batch 165 loss: 0.0005611349595710635
batch 170 loss: 0.0005609793472103775
batch 175 loss: 0.0005609568441286683
batch 180 loss: 0.0005609580082818865
batch 185 loss: 0.0005610505817458033
batch 190 loss: 0.000560954783577472
batch 195 loss: 0.0005607726983726024
batch 200 loss: 0.0005606934893876315
batch 205 loss: 0.0005607904051430524
batch 210 loss: 0.0005610529915429652
batch 215 loss: 0.0005610321066342294
batch 220 loss: 0.0005609967862255872
batch 225 loss: 0.0005609578103758395
batch 230 loss: 0.0005610287771560252
batch 235 loss: 0.0005608184728771448
batch 240 loss: 0.0005610147840343416
Training Loss: 0.0005609934759074046
Validation Loss: 0.0005609326016080255
Epoch 14:
batch 5 loss: 0.000561026786454022
batch 10 loss: 0.0005609241896308958
batch 15 loss: 0.0005610125372186303
batch 20 loss: 0.0005610624328255653
batch 25 loss: 0.000560950767248869
batch 30 loss: 0.0005608267500065267
batch 35 loss: 0.0005606851889751851
batch 40 loss: 0.0005607669125311076
batch 45 loss: 0.0005610972992144525
batch 50 loss: 0.0005608145613223315
batch 55 loss: 0.0005609876126982272
batch 60 loss: 0.0005608450504951179
batch 65 loss: 0.0005610933410935103
batch 70 loss: 0.0005608367267996073
batch 75 loss: 0.0005609584972262382
batch 80 loss: 0.0005609148065559566
batch 85 loss: 0.0005607713828794658
batch 90 loss: 0.000560771522577852
batch 95 loss: 0.0005608287057839334
batch 100 loss: 0.0005605838261544705
batch 105 loss: 0.0005601170123554766
batch 110 loss: 0.0005577168078161776
batch 115 loss: 0.0005576677969656885
batch 120 loss: 0.0005576596944592893
batch 125 loss: 0.0005578211043030023
batch 130 loss: 0.0005577619420364499
batch 135 loss: 0.0005577220814302564
batch 140 loss: 0.0005577118485234678
batch 145 loss: 0.000557586271315813
batch 150 loss: 0.0005576098919846117
batch 155 loss: 0.0005577203002758325
batch 160 loss: 0.0005577488336712122
batch 165 loss: 0.00055760582908988
batch 170 loss: 0.0005576226045377553
batch 175 loss: 0.0005576435010880232
batch 180 loss: 0.0005577209405601025
batch 185 loss: 0.0005576932453550398
batch 190 loss: 0.0005577913601882756
batch 195 loss: 0.0005576725350692868
batch 200 loss: 0.0005576885305345058
batch 205 loss: 0.0005576691590249538
batch 210 loss: 0.0005576167837716639
batch 215 loss: 0.0005576399271376431
batch 220 loss: 0.0005576044670306147
batch 225 loss: 0.0005577441072091461
batch 230 loss: 0.0005577179021202028
batch 235 loss: 0.0005577352014370263
batch 240 loss: 0.0005577443167567253
Training Loss: 0.0005590731852862518
Validation Loss: 0.0005576934005754689
Epoch 15:
batch 5 loss: 0.0005576307186856866
batch 10 loss: 0.0005576841649599373
batch 15 loss: 0.0005576428724452853
batch 20 loss: 0.0005576929077506065
batch 25 loss: 0.0005577218718826771
batch 30 loss: 0.0005577700445428491
batch 35 loss: 0.0005576759576797485
batch 40 loss: 0.0005577652365900577
batch 45 loss: 0.0005577508010901511
batch 50 loss: 0.0005576946074143052
batch 55 loss: 0.0005575705668888986
batch 60 loss: 0.0005576830473728478
batch 65 loss: 0.0005577059462666511
batch 70 loss: 0.0005576707422733307
batch 75 loss: 0.0005576665396802127
batch 80 loss: 0.0005576546536758542
batch 85 loss: 0.0005575504386797547
batch 90 loss: 0.0005576888448558748
batch 95 loss: 0.0005576403229497374
batch 100 loss: 0.0005576619994826614
batch 105 loss: 0.0005577279371209443
batch 110 loss: 0.0005577061441726982
batch 115 loss: 0.000557722442317754
batch 120 loss: 0.0005576918134465814
batch 125 loss: 0.000557682290673256
batch 130 loss: 0.0005576730705797672
batch 135 loss: 0.0005576551659032703
batch 140 loss: 0.0005576623603701592
batch 145 loss: 0.0005576292052865029
batch 150 loss: 0.0005576993338763714
batch 155 loss: 0.0005577119183726609
batch 160 loss: 0.0005576982512138784
batch 165 loss: 0.0005576361087150872
batch 170 loss: 0.0005576251423917711
batch 175 loss: 0.0005577325355261564
batch 180 loss: 0.0005576436524279416
batch 185 loss: 0.0005577528732828796
batch 190 loss: 0.0005578592768870294
batch 195 loss: 0.000557606085203588
batch 200 loss: 0.0005576285184361041
batch 205 loss: 0.0005576987168751657
batch 210 loss: 0.0005578324547968805
batch 215 loss: 0.0005577400210313499
batch 220 loss: 0.0005576293566264212
batch 225 loss: 0.0005575945135205985
batch 230 loss: 0.0005576591589488089
batch 235 loss: 0.0005577503819949925
batch 240 loss: 0.0005576311028562486
Training Loss: 0.0005576833774587916
Validation Loss: 0.0005576934238585333
Epoch 16:
batch 5 loss: 0.0005576730705797672
batch 10 loss: 0.000557773932814598
batch 15 loss: 0.0005577181931585073
batch 20 loss: 0.0005576762137934566
batch 25 loss: 0.0005576994852162897
batch 30 loss: 0.0005576840019784867
batch 35 loss: 0.0005576786119490862
batch 40 loss: 0.0005577772157266736
batch 45 loss: 0.0005577280651777982
batch 50 loss: 0.0005577346659265458
batch 55 loss: 0.0005577207659371198
batch 60 loss: 0.0005576975527219474
batch 65 loss: 0.000557715876493603
batch 70 loss: 0.000557549053337425
batch 75 loss: 0.0005578594398684799
batch 80 loss: 0.0005576749448664486
batch 85 loss: 0.0005577528034336865
batch 90 loss: 0.000557610287796706
batch 95 loss: 0.0005577429430559278
batch 100 loss: 0.0005576556199230253
batch 105 loss: 0.000557664129883051
batch 110 loss: 0.0005575484712608159
batch 115 loss: 0.0005576922092586755
batch 120 loss: 0.0005576018244028092
batch 125 loss: 0.0005575766670517623
batch 130 loss: 0.0005576237570494413
batch 135 loss: 0.0005577095551416277
batch 140 loss: 0.0005576655035838485
batch 145 loss: 0.0005576564464718104
batch 150 loss: 0.0005576403811573983
batch 155 loss: 0.0005576538038440049
batch 160 loss: 0.0005576673429459333
batch 165 loss: 0.000557696376927197
batch 170 loss: 0.0005576616153120994
batch 175 loss: 0.0005576498457230628
batch 180 loss: 0.0005576670868322253
batch 185 loss: 0.0005576870986260474
batch 190 loss: 0.0005576943629421293
batch 195 loss: 0.000557651522103697
batch 200 loss: 0.0005576806957833469
batch 205 loss: 0.0005577387637458742
batch 210 loss: 0.0005576235591433943
batch 215 loss: 0.0005576791008934379
batch 220 loss: 0.0005576570518314838
batch 225 loss: 0.000557686307001859
batch 230 loss: 0.0005576711031608283
batch 235 loss: 0.0005578163545578718
batch 240 loss: 0.0005577183677814901
Training Loss: 0.0005576833760036
Validation Loss: 0.0005576934384104485
Epoch 17:
batch 5 loss: 0.0005576837109401822
batch 10 loss: 0.0005576318479143083
batch 15 loss: 0.0005576302413828671
batch 20 loss: 0.0005577143630944193
batch 25 loss: 0.0005576465395279228
batch 30 loss: 0.0005576281691901386
batch 35 loss: 0.0005576271330937743
batch 40 loss: 0.0005577554227784276
batch 45 loss: 0.0005576766328886151
batch 50 loss: 0.0005577831878326833
batch 55 loss: 0.0005576429306529462
batch 60 loss: 0.0005576714058406651
batch 65 loss: 0.000557753536850214
batch 70 loss: 0.0005576559458859264
batch 75 loss: 0.0005576533381827176
batch 80 loss: 0.0005577978910878301
batch 85 loss: 0.0005576547817327082
batch 90 loss: 0.0005577189614996314
batch 95 loss: 0.0005576420458965004
batch 100 loss: 0.0005576111609116197
batch 105 loss: 0.0005576430587098003
batch 110 loss: 0.000557638646569103
batch 115 loss: 0.0005576682277023792
batch 120 loss: 0.0005576117080636322
batch 125 loss: 0.0005577417323365808
batch 130 loss: 0.0005577071919105947
batch 135 loss: 0.0005577389150857925
batch 140 loss: 0.0005576696479693055
batch 145 loss: 0.0005577121395617723
batch 150 loss: 0.0005577117437496782
batch 155 loss: 0.0005577234551310539
batch 160 loss: 0.0005578068667091429
batch 165 loss: 0.0005576128954999149
batch 170 loss: 0.0005577782518230379
batch 175 loss: 0.0005576606374233962
batch 180 loss: 0.0005577505799010396
batch 185 loss: 0.0005577680305577815
batch 190 loss: 0.000557649286929518
batch 195 loss: 0.0005576947587542236
batch 200 loss: 0.0005575845367275179
batch 205 loss: 0.0005577088799327612
batch 210 loss: 0.0005577242583967746
batch 215 loss: 0.0005577318370342255
batch 220 loss: 0.0005577410222031177
batch 225 loss: 0.0005576482973992825
batch 230 loss: 0.0005576277733780443
batch 235 loss: 0.0005576247116550803
batch 240 loss: 0.000557543768081814
Training Loss: 0.0005576833772162596
Validation Loss: 0.0005576934190078948
Epoch 18:
batch 5 loss: 0.0005577005911618471
batch 10 loss: 0.0005576782277785242
batch 15 loss: 0.000557673699222505
batch 20 loss: 0.0005577290081419051
batch 25 loss: 0.000557739904616028
batch 30 loss: 0.0005575943971052766
batch 35 loss: 0.0005576250376179814
batch 40 loss: 0.0005576351541094482
batch 45 loss: 0.0005576957250013947
batch 50 loss: 0.0005577876116149127
batch 55 loss: 0.0005576230818405747
batch 60 loss: 0.0005576816387474537
batch 65 loss: 0.0005576828611083329
batch 70 loss: 0.0005575951305218041
batch 75 loss: 0.0005577007308602333
batch 80 loss: 0.0005577424191869796
batch 85 loss: 0.0005576015333645046
batch 90 loss: 0.000557732442393899
batch 95 loss: 0.0005576055846177042
batch 100 loss: 0.0005576770519837738
batch 105 loss: 0.0005576758878305555
batch 110 loss: 0.0005577485426329076
batch 115 loss: 0.0005576622788794339
batch 120 loss: 0.0005575993447564543
batch 125 loss: 0.0005577019182965159
batch 130 loss: 0.0005576646188274025
batch 135 loss: 0.0005576088093221188
batch 140 loss: 0.0005575852119363844
batch 145 loss: 0.0005577136296778917
batch 150 loss: 0.0005576829076744616
batch 155 loss: 0.0005576980183832347
batch 160 loss: 0.0005575663293711841
batch 165 loss: 0.0005576185882091522
batch 170 loss: 0.0005578237352892757
batch 175 loss: 0.0005577116389758885
batch 180 loss: 0.0005577399511821568
batch 185 loss: 0.0005576944793574512
batch 190 loss: 0.0005576754920184612
batch 195 loss: 0.0005577340722084046
batch 200 loss: 0.0005576775525696576
batch 205 loss: 0.0005576594616286456
batch 210 loss: 0.0005578072275966405
batch 215 loss: 0.0005577070987783372
batch 220 loss: 0.0005576733266934753
batch 225 loss: 0.0005577376228757203
batch 230 loss: 0.0005576834548264742
batch 235 loss: 0.0005577772622928024
batch 240 loss: 0.0005576716968789697
Training Loss: 0.0005576833747909405
Validation Loss: 0.0005576934015455966
Epoch 19:
batch 5 loss: 0.0005577453761361539
batch 10 loss: 0.0005576794617809355
batch 15 loss: 0.0005577059928327799
batch 20 loss: 0.0005576579482294619
batch 25 loss: 0.000557742326054722
batch 30 loss: 0.000557696504984051
batch 35 loss: 0.000557766854763031
batch 40 loss: 0.0005576769588515162
batch 45 loss: 0.0005577239207923412
batch 50 loss: 0.0005577016388997435
batch 55 loss: 0.0005577445845119655
batch 60 loss: 0.0005576604628004134
batch 65 loss: 0.0005577123141847551
batch 70 loss: 0.0005576638621278107
batch 75 loss: 0.0005577423842623829
batch 80 loss: 0.0005576198804192245
batch 85 loss: 0.0005576658761128784
batch 90 loss: 0.000557614176068455
batch 95 loss: 0.0005577069241553545
batch 100 loss: 0.0005576794967055321
batch 105 loss: 0.0005576385534368456
batch 110 loss: 0.0005577334552071989
batch 115 loss: 0.0005577605916187167
batch 120 loss: 0.0005575389019213617
batch 125 loss: 0.0005576582625508308
batch 130 loss: 0.000557660881895572
batch 135 loss: 0.0005576354451477528
batch 140 loss: 0.0005576054682023823
batch 145 loss: 0.0005576939671300352
batch 150 loss: 0.0005576822673901916
batch 155 loss: 0.0005577990203164517
batch 160 loss: 0.000557719066273421
batch 165 loss: 0.0005577307543717325
batch 170 loss: 0.0005577185424044728
batch 175 loss: 0.0005576712777838111
batch 180 loss: 0.000557596527505666
batch 185 loss: 0.0005576963187195361
batch 190 loss: 0.0005575933726504445
batch 195 loss: 0.0005576796014793217
batch 200 loss: 0.0005575237795710563
batch 205 loss: 0.0005576642579399049
batch 210 loss: 0.0005577040137723088
batch 215 loss: 0.0005576776689849794
batch 220 loss: 0.0005576761090196669
batch 225 loss: 0.0005576289491727948
batch 230 loss: 0.0005578495329245925
batch 235 loss: 0.0005576653638854623
batch 240 loss: 0.0005576932919211686
Training Loss: 0.0005576833789139831
Validation Loss: 0.0005576934015455966
Epoch 20:
batch 5 loss: 0.0005576296825893223
batch 10 loss: 0.0005576220690272748
batch 15 loss: 0.0005576932919211686
batch 20 loss: 0.0005577054223977029
batch 25 loss: 0.0005577195785008371
batch 30 loss: 0.0005577001022174955
batch 35 loss: 0.0005575781571678818
batch 40 loss: 0.0005577306961640716
batch 45 loss: 0.0005576655035838485
batch 50 loss: 0.0005576322902925313
batch 55 loss: 0.0005578270996920764
batch 60 loss: 0.0005576485535129904
batch 65 loss: 0.0005576888332143426
batch 70 loss: 0.0005577161908149719
batch 75 loss: 0.0005577498464845121
batch 80 loss: 0.000557754747569561
batch 85 loss: 0.0005576221039518714
batch 90 loss: 0.0005576273426413537
batch 95 loss: 0.0005577790550887584
batch 100 loss: 0.0005576312425546349
batch 105 loss: 0.0005578112322837114
batch 110 loss: 0.0005576375173404813
batch 115 loss: 0.0005576976691372693
batch 120 loss: 0.0005576057475991548
batch 125 loss: 0.0005577425705268979
batch 130 loss: 0.0005576402531005442
batch 135 loss: 0.0005577111500315368
batch 140 loss: 0.0005576163996011019
batch 145 loss: 0.00055770498001948
batch 150 loss: 0.0005577320465818047
batch 155 loss: 0.0005577621283009649
batch 160 loss: 0.0005576763534918428
batch 165 loss: 0.0005575992981903255
batch 170 loss: 0.0005576533963903785
batch 175 loss: 0.0005577008589170874
batch 180 loss: 0.0005576663883402943
batch 185 loss: 0.0005576291587203741
batch 190 loss: 0.0005577011848799885
batch 195 loss: 0.0005577456555329263
batch 200 loss: 0.0005577050033025444
batch 205 loss: 0.0005576377501711249
batch 210 loss: 0.0005576279480010271
batch 215 loss: 0.0005577207426540554
batch 220 loss: 0.00055763132404536
batch 225 loss: 0.0005576278548687696
batch 230 loss: 0.0005577698349952698
batch 235 loss: 0.0005576815223321318
batch 240 loss: 0.0005576445837505162
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576934257987886
Epoch 21:
batch 5 loss: 0.0005576539784669876
batch 10 loss: 0.0005576005671173335
batch 15 loss: 0.000557712628506124
batch 20 loss: 0.0005576171679422259
batch 25 loss: 0.0005577168427407742
batch 30 loss: 0.0005577357718721032
batch 35 loss: 0.0005575516843236983
batch 40 loss: 0.0005576920229941607
batch 45 loss: 0.0005577071220614016
batch 50 loss: 0.000557686435058713
batch 55 loss: 0.0005577325238846243
batch 60 loss: 0.0005577891482971608
batch 65 loss: 0.0005576146650128067
batch 70 loss: 0.0005576789379119873
batch 75 loss: 0.0005576321622356772
batch 80 loss: 0.0005576438969001174
batch 85 loss: 0.000557736458722502
batch 90 loss: 0.0005576492287218571
batch 95 loss: 0.0005577038857154549
batch 100 loss: 0.0005576767958700657
batch 105 loss: 0.0005576148745603859
batch 110 loss: 0.0005577242351137102
batch 115 loss: 0.000557646993547678
batch 120 loss: 0.0005578821757808328
batch 125 loss: 0.0005577247007749975
batch 130 loss: 0.0005575797404162586
batch 135 loss: 0.0005577908246777952
batch 140 loss: 0.0005576824769377708
batch 145 loss: 0.0005577234551310539
batch 150 loss: 0.0005576946190558374
batch 155 loss: 0.0005576801137067378
batch 160 loss: 0.0005575827090069652
batch 165 loss: 0.0005577213712967932
batch 170 loss: 0.0005577015108428895
batch 175 loss: 0.0005577833042480051
batch 180 loss: 0.0005576835130341351
batch 185 loss: 0.0005576679483056069
batch 190 loss: 0.0005577410454861819
batch 195 loss: 0.0005576761090196669
batch 200 loss: 0.0005577202187851072
batch 205 loss: 0.0005577369942329824
batch 210 loss: 0.000557567342184484
batch 215 loss: 0.0005576225463300944
batch 220 loss: 0.0005576551659032703
batch 225 loss: 0.0005576733266934753
batch 230 loss: 0.0005575964692980051
batch 235 loss: 0.0005576489958912135
batch 240 loss: 0.0005577476462349295
Training Loss: 0.00055768338230943
Validation Loss: 0.0005576934228884057
Epoch 22:
batch 5 loss: 0.000557680951897055
batch 10 loss: 0.0005576786817982793
batch 15 loss: 0.0005577134084887803
batch 20 loss: 0.0005577609175816178
batch 25 loss: 0.0005575994960963726
batch 30 loss: 0.0005576343857683242
batch 35 loss: 0.0005576627096161247
batch 40 loss: 0.0005576358176767826
batch 45 loss: 0.0005577644915319979
batch 50 loss: 0.0005576499155722558
batch 55 loss: 0.0005577341536991299
batch 60 loss: 0.0005576970172114671
batch 65 loss: 0.0005576245486736298
batch 70 loss: 0.0005576925002969801
batch 75 loss: 0.000557744293473661
batch 80 loss: 0.0005576413008384406
batch 85 loss: 0.0005576797062531114
batch 90 loss: 0.0005576350842602551
batch 95 loss: 0.0005576489260420203
batch 100 loss: 0.0005575279123149812
batch 105 loss: 0.0005577664938755334
batch 110 loss: 0.000557681976351887
batch 115 loss: 0.0005576526513323188
batch 120 loss: 0.0005575925344601274
batch 125 loss: 0.000557608250528574
batch 130 loss: 0.0005577191594056785
batch 135 loss: 0.0005576726514846086
batch 140 loss: 0.0005576497060246766
batch 145 loss: 0.000557662732899189
batch 150 loss: 0.0005576617317274213
batch 155 loss: 0.0005576129187829792
batch 160 loss: 0.0005577466334216296
batch 165 loss: 0.0005577515345066786
batch 170 loss: 0.0005577446310780943
batch 175 loss: 0.0005577166564762592
batch 180 loss: 0.0005576174589805305
batch 185 loss: 0.0005577505100518465
batch 190 loss: 0.0005577653995715081
batch 195 loss: 0.0005576587398536503
batch 200 loss: 0.00055758684175089
batch 205 loss: 0.0005576985888183117
batch 210 loss: 0.0005577639560215176
batch 215 loss: 0.0005576734431087971
batch 220 loss: 0.0005577411619015038
batch 225 loss: 0.0005577711970545351
batch 230 loss: 0.0005577389150857925
batch 235 loss: 0.000557697843760252
batch 240 loss: 0.0005576917435973882
Training Loss: 0.0005576833808542385
Validation Loss: 0.0005576934073663627
Epoch 23:
batch 5 loss: 0.0005576102994382381
batch 10 loss: 0.0005576609284617007
batch 15 loss: 0.0005577135249041021
batch 20 loss: 0.0005577043513767422
batch 25 loss: 0.0005577101488597691
batch 30 loss: 0.0005577384843491017
batch 35 loss: 0.000557602453045547
batch 40 loss: 0.0005577069357968866
batch 45 loss: 0.0005576761323027313
batch 50 loss: 0.0005577192874625325
batch 55 loss: 0.0005576484254561365
batch 60 loss: 0.0005577680189162493
batch 65 loss: 0.0005577403004281223
batch 70 loss: 0.0005576244206167757
batch 75 loss: 0.0005577150848694145
batch 80 loss: 0.0005577742587774992
batch 85 loss: 0.0005576462484896183
batch 90 loss: 0.0005577320931479335
batch 95 loss: 0.0005576261435635387
batch 100 loss: 0.0005577255273237824
batch 105 loss: 0.0005578464479185641
batch 110 loss: 0.0005574784358032048
batch 115 loss: 0.0005576913943514227
batch 120 loss: 0.0005576654919423163
batch 125 loss: 0.0005577266216278076
batch 130 loss: 0.0005576810333877801
batch 135 loss: 0.000557734933681786
batch 140 loss: 0.0005576200201176107
batch 145 loss: 0.0005577082163654267
batch 150 loss: 0.0005576749448664486
batch 155 loss: 0.0005576814641244709
batch 160 loss: 0.0005576406721957028
batch 165 loss: 0.000557763478718698
batch 170 loss: 0.0005577235133387148
batch 175 loss: 0.0005576944327913225
batch 180 loss: 0.0005576103925704956
batch 185 loss: 0.0005576405324973166
batch 190 loss: 0.0005577450501732528
batch 195 loss: 0.0005576578318141401
batch 200 loss: 0.0005575857241638005
batch 205 loss: 0.0005575698334723711
batch 210 loss: 0.0005576425348408521
batch 215 loss: 0.0005577721982263029
batch 220 loss: 0.0005576767609454692
batch 225 loss: 0.0005577092757448554
batch 230 loss: 0.0005576642230153084
batch 235 loss: 0.0005576158873736858
batch 240 loss: 0.0005577377742156386
Training Loss: 0.0005576833789139831
Validation Loss: 0.0005576934044559796
Epoch 24:
batch 5 loss: 0.000557642977219075
batch 10 loss: 0.0005577057832852006
batch 15 loss: 0.000557713897433132
batch 20 loss: 0.000557696190662682
batch 25 loss: 0.0005577634554356336
batch 30 loss: 0.0005576858413405717
batch 35 loss: 0.000557632464915514
batch 40 loss: 0.0005577203701250255
batch 45 loss: 0.0005577453412115574
batch 50 loss: 0.0005577370757237077
batch 55 loss: 0.0005576378200203181
batch 60 loss: 0.0005576702649705112
batch 65 loss: 0.0005577451782301068
batch 70 loss: 0.0005577095784246921
batch 75 loss: 0.000557658716570586
batch 80 loss: 0.0005576849216595292
batch 85 loss: 0.0005576542811468243
batch 90 loss: 0.0005577500211074949
batch 95 loss: 0.0005577235366217792
batch 100 loss: 0.0005576331401243806
batch 105 loss: 0.000557649100665003
batch 110 loss: 0.000557668216060847
batch 115 loss: 0.0005577413830906153
batch 120 loss: 0.0005577497067861259
batch 125 loss: 0.0005576716735959053
batch 130 loss: 0.0005577081348747015
batch 135 loss: 0.0005576049792580306
batch 140 loss: 0.0005576666095294059
batch 145 loss: 0.0005576748517341912
batch 150 loss: 0.0005576872848905623
batch 155 loss: 0.0005577228730544448
batch 160 loss: 0.0005575855495408178
batch 165 loss: 0.0005577113945037127
batch 170 loss: 0.0005576621275395155
batch 175 loss: 0.0005577385541982949
batch 180 loss: 0.0005576509516686201
batch 185 loss: 0.0005575893446803093
batch 190 loss: 0.0005575544317252934
batch 195 loss: 0.0005578226177021861
batch 200 loss: 0.0005578814423643052
batch 205 loss: 0.0005577240837737918
batch 210 loss: 0.0005577378673478961
batch 215 loss: 0.0005576696014031768
batch 220 loss: 0.0005576625932008028
batch 225 loss: 0.0005574098788201809
batch 230 loss: 0.0005575744318775833
batch 235 loss: 0.0005576773430220783
batch 240 loss: 0.0005576944793574512
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576934073663627
Epoch 25:
batch 5 loss: 0.000557696120813489
batch 10 loss: 0.0005576980882324278
batch 15 loss: 0.0005576614639721811
batch 20 loss: 0.0005576881347224116
batch 25 loss: 0.0005576962139457464
batch 30 loss: 0.000557843130081892
batch 35 loss: 0.000557783804833889
batch 40 loss: 0.0005576607305556536
batch 45 loss: 0.0005577464704401791
batch 50 loss: 0.0005576959112659097
batch 55 loss: 0.0005577614763751626
batch 60 loss: 0.0005577366333454848
batch 65 loss: 0.000557741429656744
batch 70 loss: 0.0005577207775786519
batch 75 loss: 0.0005578055162914098
batch 80 loss: 0.0005576927913352847
batch 85 loss: 0.0005576519528403878
batch 90 loss: 0.0005575648159720003
batch 95 loss: 0.000557700521312654
batch 100 loss: 0.0005575854680500925
batch 105 loss: 0.0005576532217673958
batch 110 loss: 0.0005577592994086445
batch 115 loss: 0.0005576963536441326
batch 120 loss: 0.0005576690775342286
batch 125 loss: 0.0005575754912570119
batch 130 loss: 0.0005576133960857987
batch 135 loss: 0.0005577963776886464
batch 140 loss: 0.0005576256080530584
batch 145 loss: 0.0005576298688538372
batch 150 loss: 0.0005576707655563951
batch 155 loss: 0.000557779218070209
batch 160 loss: 0.0005576762137934566
batch 165 loss: 0.000557696190662682
batch 170 loss: 0.0005576238036155701
batch 175 loss: 0.0005576147930696606
batch 180 loss: 0.0005576306488364935
batch 185 loss: 0.0005577226052992046
batch 190 loss: 0.0005576777854003012
batch 195 loss: 0.0005576584488153457
batch 200 loss: 0.0005576876341365278
batch 205 loss: 0.0005575953749939799
batch 210 loss: 0.0005576435942202806
batch 215 loss: 0.0005576709168963135
batch 220 loss: 0.0005576881463639438
batch 225 loss: 0.0005576477502472699
batch 230 loss: 0.0005576561437919736
batch 235 loss: 0.0005576596478931606
batch 240 loss: 0.0005576525465585292
Training Loss: 0.0005576833827944938
Validation Loss: 0.0005576934141572565
Epoch 26:
batch 5 loss: 0.0005577319767326117
batch 10 loss: 0.0005577454459853471
batch 15 loss: 0.0005577942123636603
batch 20 loss: 0.0005576364579610527
batch 25 loss: 0.0005576929426752031
batch 30 loss: 0.0005576413124799729
batch 35 loss: 0.0005576404044404625
batch 40 loss: 0.0005577395437285304
batch 45 loss: 0.0005577852367423475
batch 50 loss: 0.0005577128147706389
batch 55 loss: 0.0005578184500336647
batch 60 loss: 0.0005577623145654797
batch 65 loss: 0.00055767489830032
batch 70 loss: 0.0005577027681283653
batch 75 loss: 0.0005576008115895092
batch 80 loss: 0.0005576594383455812
batch 85 loss: 0.0005576751660555601
batch 90 loss: 0.0005577442469075322
batch 95 loss: 0.0005576007999479771
batch 100 loss: 0.0005575588787905872
batch 105 loss: 0.0005576993688009679
batch 110 loss: 0.0005576517782174051
batch 115 loss: 0.0005576377501711249
batch 120 loss: 0.0005576899042353034
batch 125 loss: 0.0005577747710049152
batch 130 loss: 0.0005575948860496282
batch 135 loss: 0.0005576933734118938
batch 140 loss: 0.0005576653173193335
batch 145 loss: 0.0005576130119152367
batch 150 loss: 0.0005575677962042392
batch 155 loss: 0.0005577025935053825
batch 160 loss: 0.0005576874944381415
batch 165 loss: 0.000557798461522907
batch 170 loss: 0.0005576131399720908
batch 175 loss: 0.0005576701485551893
batch 180 loss: 0.0005576673429459333
batch 185 loss: 0.0005576841998845339
batch 190 loss: 0.0005575594725087285
batch 195 loss: 0.0005577185074798763
batch 200 loss: 0.0005576418363489211
batch 205 loss: 0.0005576390656642615
batch 210 loss: 0.0005577015923336148
batch 215 loss: 0.0005576176685281098
batch 220 loss: 0.0005576767143793404
batch 225 loss: 0.0005576537689194084
batch 230 loss: 0.0005578002892434597
batch 235 loss: 0.000557717273477465
batch 240 loss: 0.0005577467731200158
Training Loss: 0.0005576833837646215
Validation Loss: 0.0005576933996053413
Epoch 27:
batch 5 loss: 0.0005577711272053421
batch 10 loss: 0.0005577107309363783
batch 15 loss: 0.0005576231167651712
batch 20 loss: 0.0005576375406235456
batch 25 loss: 0.00055759142851457
batch 30 loss: 0.0005577108473517001
batch 35 loss: 0.0005577547242864966
batch 40 loss: 0.0005576921510510146
batch 45 loss: 0.0005576710333116353
batch 50 loss: 0.0005576642230153084
batch 55 loss: 0.0005578165175393224
batch 60 loss: 0.0005577296949923038
batch 65 loss: 0.0005576982977800072
batch 70 loss: 0.000557603919878602
batch 75 loss: 0.0005576637340709567
batch 80 loss: 0.0005577232921496033
batch 85 loss: 0.0005575464223511517
batch 90 loss: 0.0005576502182520926
batch 95 loss: 0.0005575082148425281
batch 100 loss: 0.0005577587988227605
batch 105 loss: 0.0005576727213338018
batch 110 loss: 0.0005578412790782749
batch 115 loss: 0.0005577428033575416
batch 120 loss: 0.0005576056893914938
batch 125 loss: 0.000557641324121505
batch 130 loss: 0.0005575313232839107
batch 135 loss: 0.0005576702998951078
batch 140 loss: 0.0005576583789661527
batch 145 loss: 0.0005577162955887616
batch 150 loss: 0.0005577176809310914
batch 155 loss: 0.0005576472030952573
batch 160 loss: 0.0005576024181209504
batch 165 loss: 0.0005577337229624391
batch 170 loss: 0.0005576143274083733
batch 175 loss: 0.0005577998585067689
batch 180 loss: 0.0005577388103120029
batch 185 loss: 0.0005577086703851819
batch 190 loss: 0.0005576723953709007
batch 195 loss: 0.0005577320815064013
batch 200 loss: 0.0005577675881795585
batch 205 loss: 0.0005576648167334497
batch 210 loss: 0.0005576487048529089
batch 215 loss: 0.0005577552714385092
batch 220 loss: 0.0005576526164077222
batch 225 loss: 0.0005577881587669253
batch 230 loss: 0.000557580532040447
batch 235 loss: 0.000557674269657582
batch 240 loss: 0.0005576969939284027
Training Loss: 0.0005576833806117065
Validation Loss: 0.0005576934170676395
Epoch 28:
batch 5 loss: 0.0005577897769398988
batch 10 loss: 0.0005577084724791348
batch 15 loss: 0.0005576085648499429
batch 20 loss: 0.0005577456671744585
batch 25 loss: 0.0005576553521677852
batch 30 loss: 0.000557640683837235
batch 35 loss: 0.0005577746196649969
batch 40 loss: 0.0005577438161708415
batch 45 loss: 0.0005576645256951451
batch 50 loss: 0.0005576033378019929
batch 55 loss: 0.000557697203475982
batch 60 loss: 0.0005577128031291068
batch 65 loss: 0.0005575998104177416
batch 70 loss: 0.000557657761964947
batch 75 loss: 0.0005577321397140622
batch 80 loss: 0.0005577568197622895
batch 85 loss: 0.0005577446543611586
batch 90 loss: 0.0005578017560765147
batch 95 loss: 0.0005576615571044385
batch 100 loss: 0.000557611882686615
batch 105 loss: 0.0005577273084782064
batch 110 loss: 0.0005577275995165109
batch 115 loss: 0.000557644572108984
batch 120 loss: 0.0005576796713285149
batch 125 loss: 0.000557656493037939
batch 130 loss: 0.0005577167146839201
batch 135 loss: 0.0005577258882112802
batch 140 loss: 0.0005577807198278606
batch 145 loss: 0.0005576827330514789
batch 150 loss: 0.0005577113595791161
batch 155 loss: 0.0005577256670221686
batch 160 loss: 0.0005575729184783996
batch 165 loss: 0.0005575824645347894
batch 170 loss: 0.0005576730123721063
batch 175 loss: 0.0005576620460487902
batch 180 loss: 0.0005575742456130683
batch 185 loss: 0.0005577002884820104
batch 190 loss: 0.0005576775292865932
batch 195 loss: 0.0005576836410909891
batch 200 loss: 0.0005576000548899174
batch 205 loss: 0.0005576968425884843
batch 210 loss: 0.0005575819290243089
batch 215 loss: 0.0005576679832302034
batch 220 loss: 0.0005575955961830914
batch 225 loss: 0.0005577510106377304
batch 230 loss: 0.0005576696246862411
batch 235 loss: 0.0005576930125243962
batch 240 loss: 0.0005577302305027843
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576934170676395
Epoch 29:
batch 5 loss: 0.000557654129806906
batch 10 loss: 0.0005575622315518558
batch 15 loss: 0.0005576883209869266
batch 20 loss: 0.0005577541422098875
batch 25 loss: 0.0005577191943302751
batch 30 loss: 0.000557568680960685
batch 35 loss: 0.000557570846285671
batch 40 loss: 0.0005576492985710502
batch 45 loss: 0.0005577467498369515
batch 50 loss: 0.0005576774594374001
batch 55 loss: 0.0005577658186666668
batch 60 loss: 0.0005577410571277142
batch 65 loss: 0.0005577178788371384
batch 70 loss: 0.0005576324299909174
batch 75 loss: 0.0005577178904786706
batch 80 loss: 0.0005576043506152928
batch 85 loss: 0.0005576882394962013
batch 90 loss: 0.0005576756200753152
batch 95 loss: 0.0005575685761868953
batch 100 loss: 0.0005577662377618253
batch 105 loss: 0.0005576833034865558
batch 110 loss: 0.0005576702416874469
batch 115 loss: 0.0005578056559897959
batch 120 loss: 0.0005576926516368986
batch 125 loss: 0.0005576694267801941
batch 130 loss: 0.0005576973897404968
batch 135 loss: 0.0005577613948844374
batch 140 loss: 0.0005575523595325649
batch 145 loss: 0.0005577012780122459
batch 150 loss: 0.0005576667841523886
batch 155 loss: 0.0005576860741712153
batch 160 loss: 0.0005575541756115854
batch 165 loss: 0.000557797693181783
batch 170 loss: 0.0005577172851189971
batch 175 loss: 0.0005577195202931762
batch 180 loss: 0.0005575935007072985
batch 185 loss: 0.0005576158873736858
batch 190 loss: 0.0005576758063398302
batch 195 loss: 0.0005576760973781347
batch 200 loss: 0.0005576937925070524
batch 205 loss: 0.0005576730123721063
batch 210 loss: 0.0005577314761467278
batch 215 loss: 0.000557701860088855
batch 220 loss: 0.0005576810566708445
batch 225 loss: 0.0005577523144893349
batch 230 loss: 0.0005577715113759041
batch 235 loss: 0.0005576638272032141
batch 240 loss: 0.0005577283096499741
Training Loss: 0.0005576833924957706
Validation Loss: 0.0005576934296792994
Epoch 30:
batch 5 loss: 0.0005576925235800445
batch 10 loss: 0.0005577500793151558
batch 15 loss: 0.0005576320458203554
batch 20 loss: 0.0005577350850217045
batch 25 loss: 0.0005575497401878238
batch 30 loss: 0.0005576252238824964
batch 35 loss: 0.0005577113945037127
batch 40 loss: 0.0005575884249992668
batch 45 loss: 0.0005576654337346553
batch 50 loss: 0.0005576560506597161
batch 55 loss: 0.0005576749099418521
batch 60 loss: 0.0005577044328674674
batch 65 loss: 0.0005576930707320571
batch 70 loss: 0.0005577535834163427
batch 75 loss: 0.0005576527793891728
batch 80 loss: 0.0005576868774369359
batch 85 loss: 0.0005576418712735176
batch 90 loss: 0.0005577194970101118
batch 95 loss: 0.0005576251540333032
batch 100 loss: 0.0005576830706559121
batch 105 loss: 0.0005576941184699535
batch 110 loss: 0.0005576538504101336
batch 115 loss: 0.0005577566800639034
batch 120 loss: 0.0005575767136178911
batch 125 loss: 0.0005575923714786768
batch 130 loss: 0.0005576719529926776
batch 135 loss: 0.0005578290321864188
batch 140 loss: 0.0005577511503361166
batch 145 loss: 0.0005576101364567875
batch 150 loss: 0.0005577222327701747
batch 155 loss: 0.0005577020463533699
batch 160 loss: 0.0005576701951213181
batch 165 loss: 0.0005577574018388986
batch 170 loss: 0.0005576957017183303
batch 175 loss: 0.0005578819662332534
batch 180 loss: 0.0005576034309342504
batch 185 loss: 0.0005577085423283279
batch 190 loss: 0.0005577420699410141
batch 195 loss: 0.000557714281603694
batch 200 loss: 0.0005577431991696357
batch 205 loss: 0.0005576338153332472
batch 210 loss: 0.0005576741183176637
batch 215 loss: 0.0005576154333539307
batch 220 loss: 0.0005576180410571396
batch 225 loss: 0.0005577672040089965
batch 230 loss: 0.0005575948511250317
batch 235 loss: 0.0005577673553489148
batch 240 loss: 0.000557613663841039
Training Loss: 0.0005576833917681749
Validation Loss: 0.0005576933986352135
Epoch 31:
batch 5 loss: 0.000557644828222692
batch 10 loss: 0.000557705934625119
batch 15 loss: 0.0005576573312282562
batch 20 loss: 0.0005577135132625699
batch 25 loss: 0.0005577858653850853
batch 30 loss: 0.0005576969357207418
batch 35 loss: 0.0005576645955443383
batch 40 loss: 0.0005576692870818079
batch 45 loss: 0.0005576872499659657
batch 50 loss: 0.0005576387397013604
batch 55 loss: 0.0005577243980951607
batch 60 loss: 0.0005575562478043139
batch 65 loss: 0.0005576493334956467
batch 70 loss: 0.0005576543160714209
batch 75 loss: 0.000557623093482107
batch 80 loss: 0.0005576470051892102
batch 85 loss: 0.0005576096358709037
batch 90 loss: 0.0005576171330176294
batch 95 loss: 0.0005575971445068717
batch 100 loss: 0.0005577993113547563
batch 105 loss: 0.0005577430361881852
batch 110 loss: 0.0005576453520916402
batch 115 loss: 0.0005577655276283622
batch 120 loss: 0.0005576891475357115
batch 125 loss: 0.0005576562602072954
batch 130 loss: 0.0005576647818088531
batch 135 loss: 0.0005577245261520148
batch 140 loss: 0.0005576700437813997
batch 145 loss: 0.0005577676696702838
batch 150 loss: 0.0005576189956627786
batch 155 loss: 0.0005576210445724427
batch 160 loss: 0.0005576998461037874
batch 165 loss: 0.0005576803931035101
batch 170 loss: 0.0005576489376835525
batch 175 loss: 0.0005577386007644236
batch 180 loss: 0.0005577595671638847
batch 185 loss: 0.0005577030824497342
batch 190 loss: 0.0005577030242420733
batch 195 loss: 0.0005576314521022141
batch 200 loss: 0.0005576909403316677
batch 205 loss: 0.0005577269941568375
batch 210 loss: 0.00055771543411538
batch 215 loss: 0.0005577900097705424
batch 220 loss: 0.0005577408242970705
batch 225 loss: 0.0005576128489337862
batch 230 loss: 0.000557694782037288
batch 235 loss: 0.0005576753872446715
batch 240 loss: 0.0005576821858994663
Training Loss: 0.0005576833874026003
Validation Loss: 0.0005576934461714699
Epoch 32:
batch 5 loss: 0.0005576833267696202
batch 10 loss: 0.0005576360039412975
batch 15 loss: 0.0005577929667197167
batch 20 loss: 0.0005576554103754461
batch 25 loss: 0.000557634886354208
batch 30 loss: 0.0005576731287874282
batch 35 loss: 0.0005575985996983945
batch 40 loss: 0.0005576522671617568
batch 45 loss: 0.0005576202180236578
batch 50 loss: 0.0005575771676376462
batch 55 loss: 0.0005577056901529432
batch 60 loss: 0.0005577480304054915
batch 65 loss: 0.0005578409531153738
batch 70 loss: 0.0005576576339080929
batch 75 loss: 0.0005576268769800663
batch 80 loss: 0.0005576401483267546
batch 85 loss: 0.0005576478084549308
batch 90 loss: 0.0005576717783696949
batch 95 loss: 0.000557665468659252
batch 100 loss: 0.0005577066331170499
batch 105 loss: 0.0005577167496085167
batch 110 loss: 0.0005576975760050118
batch 115 loss: 0.0005577781354077161
batch 120 loss: 0.0005576852825470268
batch 125 loss: 0.0005577275180257857
batch 130 loss: 0.0005577048286795616
batch 135 loss: 0.000557636737357825
batch 140 loss: 0.0005576862371526658
batch 145 loss: 0.0005576303461566568
batch 150 loss: 0.000557689880952239
batch 155 loss: 0.0005576716852374375
batch 160 loss: 0.0005577368079684675
batch 165 loss: 0.0005578236654400825
batch 170 loss: 0.0005577038857154549
batch 175 loss: 0.0005575457122176886
batch 180 loss: 0.0005577615229412913
batch 185 loss: 0.0005575856543146074
batch 190 loss: 0.0005577121744863689
batch 195 loss: 0.0005576473311521112
batch 200 loss: 0.000557637948077172
batch 205 loss: 0.0005577050033025444
batch 210 loss: 0.0005577454343438148
batch 215 loss: 0.0005576638737693429
batch 220 loss: 0.0005576685420237481
batch 225 loss: 0.0005577132338657975
batch 230 loss: 0.000557750288862735
batch 235 loss: 0.0005576648632995784
batch 240 loss: 0.0005576767143793404
Training Loss: 0.0005576833881301961
Validation Loss: 0.0005576934015455966
Epoch 33:
batch 5 loss: 0.0005576905445195734
batch 10 loss: 0.0005577533622272313
batch 15 loss: 0.0005577040021307767
batch 20 loss: 0.0005575690185651184
batch 25 loss: 0.0005577691830694675
batch 30 loss: 0.0005575751885771752
batch 35 loss: 0.0005576452938839793
batch 40 loss: 0.0005577174015343189
batch 45 loss: 0.0005576940369792282
batch 50 loss: 0.0005576194846071303
batch 55 loss: 0.0005576366209425032
batch 60 loss: 0.0005576908937655389
batch 65 loss: 0.0005575701594352722
batch 70 loss: 0.0005576967727392912
batch 75 loss: 0.0005576978903263808
batch 80 loss: 0.0005576344789005816
batch 85 loss: 0.0005577648757025599
batch 90 loss: 0.000557647692039609
batch 95 loss: 0.0005576876108534634
batch 100 loss: 0.0005577101605013013
batch 105 loss: 0.0005577438627369702
batch 110 loss: 0.0005577323026955128
batch 115 loss: 0.000557632907293737
batch 120 loss: 0.0005576667492277921
batch 125 loss: 0.0005576904048211872
batch 130 loss: 0.0005576647003181278
batch 135 loss: 0.0005576216266490519
batch 140 loss: 0.0005575880524702371
batch 145 loss: 0.0005577043746598064
batch 150 loss: 0.0005577015806920826
batch 155 loss: 0.0005577338044531644
batch 160 loss: 0.0005577172036282718
batch 165 loss: 0.0005576843512244522
batch 170 loss: 0.0005576822441071272
batch 175 loss: 0.0005577839096076786
batch 180 loss: 0.000557728880085051
batch 185 loss: 0.0005576881230808794
batch 190 loss: 0.0005575982737354934
batch 195 loss: 0.0005576759576797485
batch 200 loss: 0.0005577337462455035
batch 205 loss: 0.0005577059695497155
batch 210 loss: 0.0005578144569881261
batch 215 loss: 0.0005576988100074231
batch 220 loss: 0.0005576936877332628
batch 225 loss: 0.0005576164228841662
batch 230 loss: 0.0005576326977461577
batch 235 loss: 0.0005576737341471017
batch 240 loss: 0.0005577197298407555
Training Loss: 0.0005576834007418559
Validation Loss: 0.0005576934141572565
Epoch 34:
batch 5 loss: 0.0005576586117967963
batch 10 loss: 0.000557626539375633
batch 15 loss: 0.0005576569586992264
batch 20 loss: 0.0005577317671850324
batch 25 loss: 0.0005576511844992638
batch 30 loss: 0.0005578113137744367
batch 35 loss: 0.0005576282506808639
batch 40 loss: 0.0005576752941124141
batch 45 loss: 0.0005576477269642055
batch 50 loss: 0.0005577528499998152
batch 55 loss: 0.0005577516159974039
batch 60 loss: 0.0005577115924097597
batch 65 loss: 0.0005577210686169565
batch 70 loss: 0.0005576079012826086
batch 75 loss: 0.0005576755153015256
batch 80 loss: 0.0005576254916377366
batch 85 loss: 0.0005576305091381073
batch 90 loss: 0.0005576229654252529
batch 95 loss: 0.000557681720238179
batch 100 loss: 0.000557611626572907
batch 105 loss: 0.0005576935946010053
batch 110 loss: 0.000557625328656286
batch 115 loss: 0.0005576970172114671
batch 120 loss: 0.0005576806608587504
batch 125 loss: 0.0005577664240263402
batch 130 loss: 0.0005576645256951451
batch 135 loss: 0.000557694782037288
batch 140 loss: 0.0005576975177973509
batch 145 loss: 0.0005575801827944815
batch 150 loss: 0.0005576317664235831
batch 155 loss: 0.0005576277384534478
batch 160 loss: 0.0005577728617936373
batch 165 loss: 0.0005576737807132303
batch 170 loss: 0.0005577538162469864
batch 175 loss: 0.0005577728268690407
batch 180 loss: 0.0005576957832090556
batch 185 loss: 0.0005577042000368237
batch 190 loss: 0.0005576362367719412
batch 195 loss: 0.0005577664123848081
batch 200 loss: 0.0005576434545218944
batch 205 loss: 0.0005576731986366212
batch 210 loss: 0.0005577262840233743
batch 215 loss: 0.0005577891017310322
batch 220 loss: 0.0005576959112659097
batch 225 loss: 0.0005577006144449115
batch 230 loss: 0.0005575208691880107
batch 235 loss: 0.0005577235366217792
batch 240 loss: 0.0005577144445851445
Training Loss: 0.0005576834036522389
Validation Loss: 0.0005576934073663627
Epoch 35:
batch 5 loss: 0.0005577433621510863
batch 10 loss: 0.0005577385891228914
batch 15 loss: 0.0005577672738581896
batch 20 loss: 0.0005577393225394189
batch 25 loss: 0.0005576608004048466
batch 30 loss: 0.0005576906376518309
batch 35 loss: 0.0005576391704380512
batch 40 loss: 0.0005577279138378799
batch 45 loss: 0.0005575521616265177
batch 50 loss: 0.0005575529299676419
batch 55 loss: 0.0005576129187829792
batch 60 loss: 0.0005577050033025444
batch 65 loss: 0.0005576552590355277
batch 70 loss: 0.000557598820887506
batch 75 loss: 0.000557788007427007
batch 80 loss: 0.0005577676463872194
batch 85 loss: 0.0005577171919867397
batch 90 loss: 0.0005576608469709754
batch 95 loss: 0.0005577425705268979
batch 100 loss: 0.000557640241459012
batch 105 loss: 0.0005577270057983696
batch 110 loss: 0.0005576795199885964
batch 115 loss: 0.0005577064352110028
batch 120 loss: 0.0005577336414717138
batch 125 loss: 0.0005576533847488462
batch 130 loss: 0.0005576489493250847
batch 135 loss: 0.0005576547118835151
batch 140 loss: 0.0005577143630944193
batch 145 loss: 0.0005577793228439987
batch 150 loss: 0.0005576221621595324
batch 155 loss: 0.0005577466217800975
batch 160 loss: 0.0005577062955126166
batch 165 loss: 0.0005577584379352629
batch 170 loss: 0.0005576322320848704
batch 175 loss: 0.0005577564123086631
batch 180 loss: 0.0005577127798460424
batch 185 loss: 0.0005576942930929363
batch 190 loss: 0.0005576691939495504
batch 195 loss: 0.0005575682735070586
batch 200 loss: 0.0005576630355790257
batch 205 loss: 0.0005575616378337145
batch 210 loss: 0.0005576328025199473
batch 215 loss: 0.0005575822782702744
batch 220 loss: 0.0005577589618042111
batch 225 loss: 0.0005577432224527001
batch 230 loss: 0.0005577201140113175
batch 235 loss: 0.0005576230585575103
batch 240 loss: 0.0005576532101258635
Training Loss: 0.0005576833963762813
Validation Loss: 0.0005576934830363219
Epoch 36:
batch 5 loss: 0.0005577698815613985
batch 10 loss: 0.000557663815561682
batch 15 loss: 0.0005577545845881105
batch 20 loss: 0.0005576475290581584
batch 25 loss: 0.0005577166914008558
batch 30 loss: 0.0005575776915065945
batch 35 loss: 0.0005576787400059402
batch 40 loss: 0.000557739136274904
batch 45 loss: 0.0005576675524935126
batch 50 loss: 0.0005576995084993541
batch 55 loss: 0.0005577958421781659
batch 60 loss: 0.0005576507304795086
batch 65 loss: 0.000557595188729465
batch 70 loss: 0.000557667575776577
batch 75 loss: 0.0005576342926360666
batch 80 loss: 0.0005577408475801349
batch 85 loss: 0.0005576773430220783
batch 90 loss: 0.000557677773758769
batch 95 loss: 0.0005576731171458959
batch 100 loss: 0.0005578073440119624
batch 105 loss: 0.0005577241070568561
batch 110 loss: 0.0005576985888183117
batch 115 loss: 0.0005577517440542579
batch 120 loss: 0.0005576949333772063
batch 125 loss: 0.000557655212469399
batch 130 loss: 0.0005575627321377397
batch 135 loss: 0.000557726458646357
batch 140 loss: 0.0005576103343628347
batch 145 loss: 0.0005576989147812128
batch 150 loss: 0.0005576614988967776
batch 155 loss: 0.0005577327101491392
batch 160 loss: 0.0005577848758548498
batch 165 loss: 0.000557643233332783
batch 170 loss: 0.0005576799740083516
batch 175 loss: 0.0005576769472099841
batch 180 loss: 0.000557600986212492
batch 185 loss: 0.0005576863884925842
batch 190 loss: 0.000557658018078655
batch 195 loss: 0.0005577245377935469
batch 200 loss: 0.0005576738971285522
batch 205 loss: 0.0005577004165388643
batch 210 loss: 0.0005576747469604015
batch 215 loss: 0.0005576066439971327
batch 220 loss: 0.0005576619994826614
batch 225 loss: 0.0005576224299147725
batch 230 loss: 0.0005577638978138566
batch 235 loss: 0.000557636539451778
batch 240 loss: 0.0005576554220169782
Training Loss: 0.0005576834036522389
Validation Loss: 0.0005576934267689164
Epoch 37:
batch 5 loss: 0.0005577243398874998
batch 10 loss: 0.0005576791591010988
batch 15 loss: 0.0005576475989073515
batch 20 loss: 0.0005576335010118782
batch 25 loss: 0.00055767911253497
batch 30 loss: 0.0005577944102697074
batch 35 loss: 0.0005577388568781316
batch 40 loss: 0.0005575841874815524
batch 45 loss: 0.0005576624302193522
batch 50 loss: 0.0005576710565946997
batch 55 loss: 0.0005577131290920079
batch 60 loss: 0.0005577841657213867
batch 65 loss: 0.000557682488579303
batch 70 loss: 0.0005576684605330228
batch 75 loss: 0.0005576920928433537
batch 80 loss: 0.0005577504984103143
batch 85 loss: 0.0005576019524596631
batch 90 loss: 0.000557668600231409
batch 95 loss: 0.0005576708121225238
batch 100 loss: 0.0005577556788921356
batch 105 loss: 0.0005576640018261969
batch 110 loss: 0.0005576261202804745
batch 115 loss: 0.0005577907315455378
batch 120 loss: 0.0005576342577114701
batch 125 loss: 0.0005576919065788388
batch 130 loss: 0.0005575534072704613
batch 135 loss: 0.0005576968425884843
batch 140 loss: 0.0005577504867687822
batch 145 loss: 0.0005577522213570773
batch 150 loss: 0.000557631510309875
batch 155 loss: 0.0005576819414272905
batch 160 loss: 0.000557631568517536
batch 165 loss: 0.0005576553638093173
batch 170 loss: 0.0005577013711445033
batch 175 loss: 0.0005577208590693772
batch 180 loss: 0.0005576012772507966
batch 185 loss: 0.0005577618954703212
batch 190 loss: 0.0005576253635808826
batch 195 loss: 0.000557664898224175
batch 200 loss: 0.0005576729774475098
batch 205 loss: 0.0005576971685513854
batch 210 loss: 0.0005576197523623705
batch 215 loss: 0.000557671335991472
batch 220 loss: 0.0005576947703957557
batch 225 loss: 0.0005577015690505505
batch 230 loss: 0.0005576896364800632
batch 235 loss: 0.0005576973780989647
batch 240 loss: 0.0005577204981818795
Training Loss: 0.0005576834092304732
Validation Loss: 0.0005576934131871288
Epoch 38:
batch 5 loss: 0.0005577376927249133
batch 10 loss: 0.0005577154690399766
batch 15 loss: 0.0005577132687903941
batch 20 loss: 0.0005576053052209317
batch 25 loss: 0.0005576858296990394
batch 30 loss: 0.00055761500261724
batch 35 loss: 0.000557728239800781
batch 40 loss: 0.0005576535128057003
batch 45 loss: 0.0005576687515713274
batch 50 loss: 0.0005577779724262655
batch 55 loss: 0.0005576240131631493
batch 60 loss: 0.0005578725365921855
batch 65 loss: 0.0005576537339948117
batch 70 loss: 0.0005577232106588781
batch 75 loss: 0.0005576084833592177
batch 80 loss: 0.0005575390765443444
batch 85 loss: 0.0005576983094215393
batch 90 loss: 0.0005576502415351569
batch 95 loss: 0.0005576072609983385
batch 100 loss: 0.0005576241877861321
batch 105 loss: 0.0005577202769927681
batch 110 loss: 0.0005577075062319636
batch 115 loss: 0.0005577676463872194
batch 120 loss: 0.000557662989012897
batch 125 loss: 0.0005576739902608096
batch 130 loss: 0.0005577245494350791
batch 135 loss: 0.0005576224531978369
batch 140 loss: 0.000557631894480437
batch 145 loss: 0.0005577348987571895
batch 150 loss: 0.0005577384843491017
batch 155 loss: 0.0005577552132308483
batch 160 loss: 0.0005576594383455812
batch 165 loss: 0.0005576458876021207
batch 170 loss: 0.0005576296360231936
batch 175 loss: 0.0005577236996032297
batch 180 loss: 0.00055767324520275
batch 185 loss: 0.0005576741532422602
batch 190 loss: 0.0005576288211159408
batch 195 loss: 0.0005577976349741221
batch 200 loss: 0.0005576831055805088
batch 205 loss: 0.0005577083211392164
batch 210 loss: 0.000557698612101376
batch 215 loss: 0.0005576316150836646
batch 220 loss: 0.0005577539210207761
batch 225 loss: 0.0005576989031396806
batch 230 loss: 0.0005576317431405187
batch 235 loss: 0.0005576810101047159
batch 240 loss: 0.0005576425697654486
Training Loss: 0.0005576834232973245
Validation Loss: 0.0005576935509452596
Epoch 39:
batch 5 loss: 0.0005577158997766673
batch 10 loss: 0.000557654513977468
batch 15 loss: 0.0005576657247729599
batch 20 loss: 0.0005576231051236391
batch 25 loss: 0.0005576365510933101
batch 30 loss: 0.0005577437928877771
batch 35 loss: 0.0005576481344178319
batch 40 loss: 0.0005575997638516128
batch 45 loss: 0.0005577296367846429
batch 50 loss: 0.0005577080999501049
batch 55 loss: 0.0005577048636041581
batch 60 loss: 0.00055779597023502
batch 65 loss: 0.0005577389150857925
batch 70 loss: 0.0005576598807238043
batch 75 loss: 0.0005577200092375279
batch 80 loss: 0.0005576090537942946
batch 85 loss: 0.0005576012888923287
batch 90 loss: 0.0005577489151619375
batch 95 loss: 0.0005576976574957371
batch 100 loss: 0.0005577383446507156
batch 105 loss: 0.0005576485302299262
batch 110 loss: 0.0005577730247750878
batch 115 loss: 0.0005577262956649065
batch 120 loss: 0.0005576260504312813
batch 125 loss: 0.0005575976334512234
batch 130 loss: 0.000557748379651457
batch 135 loss: 0.0005576335010118782
batch 140 loss: 0.0005576985771767795
batch 145 loss: 0.0005577615927904844
batch 150 loss: 0.0005576552706770599
batch 155 loss: 0.0005577073083259165
batch 160 loss: 0.0005576412891969085
batch 165 loss: 0.000557648716494441
batch 170 loss: 0.000557731615845114
batch 175 loss: 0.000557717913761735
batch 180 loss: 0.000557662989012897
batch 185 loss: 0.0005576336989179254
batch 190 loss: 0.0005576404510065913
batch 195 loss: 0.0005577140487730503
batch 200 loss: 0.0005576565396040678
batch 205 loss: 0.0005575888906605541
batch 210 loss: 0.0005576481344178319
batch 215 loss: 0.0005577420699410141
batch 220 loss: 0.0005576388677582145
batch 225 loss: 0.0005577092641033232
batch 230 loss: 0.0005577741423621774
batch 235 loss: 0.0005575960618443787
batch 240 loss: 0.0005577438627369702
Training Loss: 0.0005576834342112609
Validation Loss: 0.0005576934015455966
Epoch 40:
batch 5 loss: 0.000557771441526711
batch 10 loss: 0.0005576708936132491
batch 15 loss: 0.0005576408235356212
batch 20 loss: 0.0005576764233410358
batch 25 loss: 0.0005576038034632802
batch 30 loss: 0.0005577556905336678
batch 35 loss: 0.0005576483206823468
batch 40 loss: 0.0005576770636253059
batch 45 loss: 0.0005576760740950704
batch 50 loss: 0.0005577558651566506
batch 55 loss: 0.0005576401832513511
batch 60 loss: 0.0005578254000283778
batch 65 loss: 0.0005577565170824528
batch 70 loss: 0.000557734095491469
batch 75 loss: 0.0005577180651016534
batch 80 loss: 0.0005575887626037002
batch 85 loss: 0.0005577156785875559
batch 90 loss: 0.0005576718482188881
batch 95 loss: 0.0005577512551099062
batch 100 loss: 0.0005577055388130247
batch 105 loss: 0.0005576213356107473
batch 110 loss: 0.0005577082163654267
batch 115 loss: 0.000557642662897706
batch 120 loss: 0.0005576407420448959
batch 125 loss: 0.0005577312200330198
batch 130 loss: 0.0005576514406129718
batch 135 loss: 0.0005575604620389641
batch 140 loss: 0.0005576884839683771
batch 145 loss: 0.0005576146184466779
batch 150 loss: 0.0005577710806392133
batch 155 loss: 0.000557737541384995
batch 160 loss: 0.0005577189847826957
batch 165 loss: 0.000557732896413654
batch 170 loss: 0.0005576555849984288
batch 175 loss: 0.00055764673743397
batch 180 loss: 0.0005576572264544665
batch 185 loss: 0.0005576798343099654
batch 190 loss: 0.0005576491123065353
batch 195 loss: 0.0005576503113843501
batch 200 loss: 0.0005576155963353813
batch 205 loss: 0.0005577010568231344
batch 210 loss: 0.0005576325813308359
batch 215 loss: 0.0005577384261414408
batch 220 loss: 0.0005576823255978525
batch 225 loss: 0.000557632523123175
batch 230 loss: 0.0005576109630055726
batch 235 loss: 0.0005577469011768699
batch 240 loss: 0.0005577018833719194
Training Loss: 0.0005576834269353033
Validation Loss: 0.0005576934452013423
Epoch 41:
batch 5 loss: 0.0005576524185016751
batch 10 loss: 0.0005575806368142367
batch 15 loss: 0.0005577031755819917
batch 20 loss: 0.0005576956318691373
batch 25 loss: 0.000557763664983213
batch 30 loss: 0.0005577301140874624
batch 35 loss: 0.0005576819879934191
batch 40 loss: 0.000557652022689581
batch 45 loss: 0.0005577085888944566
batch 50 loss: 0.0005576598574407399
batch 55 loss: 0.0005577156087383627
batch 60 loss: 0.0005577280302532017
batch 65 loss: 0.0005576485884375871
batch 70 loss: 0.0005576973315328359
batch 75 loss: 0.0005577264702878893
batch 80 loss: 0.0005576464463956654
batch 85 loss: 0.0005575138842687011
batch 90 loss: 0.000557784631382674
batch 95 loss: 0.0005576667957939207
batch 100 loss: 0.0005576569004915654
batch 105 loss: 0.000557687459513545
batch 110 loss: 0.0005576201132498681
batch 115 loss: 0.0005575964110903442
batch 120 loss: 0.0005576258176006377
batch 125 loss: 0.0005576510448008776
batch 130 loss: 0.0005577397067099809
batch 135 loss: 0.0005576192634180188
batch 140 loss: 0.0005576029536314309
batch 145 loss: 0.0005576948402449489
batch 150 loss: 0.0005577464820817113
batch 155 loss: 0.0005576738505624235
batch 160 loss: 0.0005576281226240098
batch 165 loss: 0.0005576444207690656
batch 170 loss: 0.0005578504642471671
batch 175 loss: 0.0005576874827966094
batch 180 loss: 0.0005577149800956249
batch 185 loss: 0.0005577153759077191
batch 190 loss: 0.0005577811854891479
batch 195 loss: 0.0005576489958912135
batch 200 loss: 0.0005576051422394812
batch 205 loss: 0.0005577249103225768
batch 210 loss: 0.0005576508236117661
batch 215 loss: 0.000557689182460308
batch 220 loss: 0.0005577110568992794
batch 225 loss: 0.000557739136274904
batch 230 loss: 0.0005576818250119687
batch 235 loss: 0.0005576968542300165
batch 240 loss: 0.0005577629781328142
Training Loss: 0.000557683409715537
Validation Loss: 0.0005576934529623637
Epoch 42:
batch 5 loss: 0.0005577752133831381
batch 10 loss: 0.0005575946881435812
batch 15 loss: 0.0005576966912485659
batch 20 loss: 0.0005576913710683585
batch 25 loss: 0.0005576270981691777
batch 30 loss: 0.0005575347342528403
batch 35 loss: 0.0005578160518780351
batch 40 loss: 0.0005577128846198321
batch 45 loss: 0.0005576804862357676
batch 50 loss: 0.0005577044794335961
batch 55 loss: 0.0005577133619226515
batch 60 loss: 0.0005576845025643707
batch 65 loss: 0.0005576570518314838
batch 70 loss: 0.0005576954805292189
batch 75 loss: 0.0005578608717769385
batch 80 loss: 0.0005577131872996688
batch 85 loss: 0.0005576910101808607
batch 90 loss: 0.0005575279472395777
batch 95 loss: 0.0005576396128162741
batch 100 loss: 0.0005576327908784151
batch 105 loss: 0.0005577836069278419
batch 110 loss: 0.0005577170522883534
batch 115 loss: 0.0005575251765549183
batch 120 loss: 0.0005577784148044884
batch 125 loss: 0.0005578106036409735
batch 130 loss: 0.000557720719370991
batch 135 loss: 0.000557576003484428
batch 140 loss: 0.0005576359108090401
batch 145 loss: 0.0005576339550316334
batch 150 loss: 0.0005576076218858361
batch 155 loss: 0.0005577108822762966
batch 160 loss: 0.000557668216060847
batch 165 loss: 0.0005577227333560586
batch 170 loss: 0.0005576105322688818
batch 175 loss: 0.000557777239009738
batch 180 loss: 0.0005577230826020241
batch 185 loss: 0.0005577800096943974
batch 190 loss: 0.0005577175528742373
batch 195 loss: 0.0005576834199018776
batch 200 loss: 0.0005577282747253776
batch 205 loss: 0.0005576472147367894
batch 210 loss: 0.0005576375522650778
batch 215 loss: 0.0005576124764047563
batch 220 loss: 0.0005577566102147102
batch 225 loss: 0.000557496037799865
batch 230 loss: 0.0005577383912168443
batch 235 loss: 0.0005577104049734772
batch 240 loss: 0.0005576453288085759
Training Loss: 0.000557683427905431
Validation Loss: 0.0005576935082596417
Epoch 43:
batch 5 loss: 0.0005577001837082207
batch 10 loss: 0.0005577592295594513
batch 15 loss: 0.0005577793228439987
batch 20 loss: 0.0005578107433393597
batch 25 loss: 0.00055760812247172
batch 30 loss: 0.0005575980059802533
batch 35 loss: 0.000557617424055934
batch 40 loss: 0.0005577104282565414
batch 45 loss: 0.0005577857722528279
batch 50 loss: 0.0005576593568548561
batch 55 loss: 0.0005577022559009493
batch 60 loss: 0.0005577534437179565
batch 65 loss: 0.0005576997064054012
batch 70 loss: 0.0005576423252932727
batch 75 loss: 0.0005578000680543482
batch 80 loss: 0.0005575984832830727
batch 85 loss: 0.0005577027215622366
batch 90 loss: 0.0005577358300797641
batch 95 loss: 0.0005576545721851289
batch 100 loss: 0.0005577102769166231
batch 105 loss: 0.0005576312658376991
batch 110 loss: 0.0005575840012170374
batch 115 loss: 0.000557707145344466
batch 120 loss: 0.0005576804862357676
batch 125 loss: 0.000557591300457716
batch 130 loss: 0.0005577139556407929
batch 135 loss: 0.000557646166998893
batch 140 loss: 0.0005576141527853906
batch 145 loss: 0.0005576658644713461
batch 150 loss: 0.0005576804978772998
batch 155 loss: 0.000557727727573365
batch 160 loss: 0.0005576292751356959
batch 165 loss: 0.0005575930117629468
batch 170 loss: 0.00055775735527277
batch 175 loss: 0.0005577526055276394
batch 180 loss: 0.0005577056785114109
batch 185 loss: 0.0005575706250965595
batch 190 loss: 0.0005576629308052361
batch 195 loss: 0.0005576429073698818
batch 200 loss: 0.0005577424424700439
batch 205 loss: 0.0005577336531132459
batch 210 loss: 0.0005577191594056785
batch 215 loss: 0.000557763734832406
batch 220 loss: 0.0005575944553129375
batch 225 loss: 0.0005577633157372475
batch 230 loss: 0.0005576712894253433
batch 235 loss: 0.0005576081806793809
batch 240 loss: 0.0005576241761445999
Training Loss: 0.0005576834507034315
Validation Loss: 0.0005576934306494271
Epoch 44:
batch 5 loss: 0.000557672674767673
batch 10 loss: 0.0005575667368248105
batch 15 loss: 0.0005577000789344311
batch 20 loss: 0.0005577454459853471
batch 25 loss: 0.0005576259107328951
batch 30 loss: 0.0005577035481110215
batch 35 loss: 0.0005576023366302251
batch 40 loss: 0.0005576447700150311
batch 45 loss: 0.0005577186471782625
batch 50 loss: 0.0005578359356150031
batch 55 loss: 0.0005577365052886307
batch 60 loss: 0.0005577078089118003
batch 65 loss: 0.0005576416035182775
batch 70 loss: 0.000557590601965785
batch 75 loss: 0.0005577181465923787
batch 80 loss: 0.0005576735245995223
batch 85 loss: 0.0005576949100941419
batch 90 loss: 0.0005577164469286799
batch 95 loss: 0.0005577259929850697
batch 100 loss: 0.0005576664349064231
batch 105 loss: 0.000557520636357367
batch 110 loss: 0.0005576485535129904
batch 115 loss: 0.0005576841766014695
batch 120 loss: 0.0005577070871368051
batch 125 loss: 0.0005576436407864094
batch 130 loss: 0.0005577365052886307
batch 135 loss: 0.0005576642579399049
batch 140 loss: 0.0005576894734986126
batch 145 loss: 0.0005577357020229101
batch 150 loss: 0.000557780486997217
batch 155 loss: 0.0005577464937232435
batch 160 loss: 0.0005576764466241002
batch 165 loss: 0.0005576065275818109
batch 170 loss: 0.0005577084957621992
batch 175 loss: 0.0005577538278885186
batch 180 loss: 0.0005578252021223307
batch 185 loss: 0.0005577829317189753
batch 190 loss: 0.0005576704279519618
batch 195 loss: 0.0005575632327236235
batch 200 loss: 0.0005577557138167321
batch 205 loss: 0.0005576777155511081
batch 210 loss: 0.0005576786585152149
batch 215 loss: 0.0005576731287874282
batch 220 loss: 0.0005576842930167913
batch 225 loss: 0.0005576659110374749
batch 230 loss: 0.000557626853697002
batch 235 loss: 0.0005576220108196139
batch 240 loss: 0.0005575888557359576
Training Loss: 0.0005576834439125378
Validation Loss: 0.0005576934781856835
Epoch 45:
batch 5 loss: 0.0005577108007855713
batch 10 loss: 0.0005576490890234709
batch 15 loss: 0.0005576110910624265
batch 20 loss: 0.0005576095776632428
batch 25 loss: 0.0005577943753451109
batch 30 loss: 0.0005576594732701779
batch 35 loss: 0.0005576047813519835
batch 40 loss: 0.0005576254334300756
batch 45 loss: 0.0005577079486101866
batch 50 loss: 0.000557692046277225
batch 55 loss: 0.0005577060976065696
batch 60 loss: 0.0005576988914981484
batch 65 loss: 0.0005577186238951981
batch 70 loss: 0.0005577613599598407
batch 75 loss: 0.0005577318253926933
batch 80 loss: 0.0005577355506829918
batch 85 loss: 0.0005577015457674861
batch 90 loss: 0.0005577097646892071
batch 95 loss: 0.0005577075760811567
batch 100 loss: 0.0005577064701355993
batch 105 loss: 0.0005578175187110901
batch 110 loss: 0.000557629601098597
batch 115 loss: 0.0005575697286985815
batch 120 loss: 0.0005576647119596601
batch 125 loss: 0.0005575937684625387
batch 130 loss: 0.000557732256129384
batch 135 loss: 0.0005576445488259196
batch 140 loss: 0.0005577295320108533
batch 145 loss: 0.0005577295902185142
batch 150 loss: 0.0005576381110586226
batch 155 loss: 0.000557667319662869
batch 160 loss: 0.0005576824187301099
batch 165 loss: 0.0005576723022386431
batch 170 loss: 0.0005577068193815648
batch 175 loss: 0.0005576121853664518
batch 180 loss: 0.0005575851304456591
batch 185 loss: 0.0005576305207796395
batch 190 loss: 0.0005576533032581211
batch 195 loss: 0.0005576299270614981
batch 200 loss: 0.0005576798808760941
batch 205 loss: 0.0005578575772233307
batch 210 loss: 0.0005577541887760162
batch 215 loss: 0.0005577967269346118
batch 220 loss: 0.0005576516967266798
batch 225 loss: 0.0005576356779783964
batch 230 loss: 0.0005576251307502389
batch 235 loss: 0.0005576714058406651
batch 240 loss: 0.0005577029194682836
Training Loss: 0.0005576834754416874
Validation Loss: 0.0005576933996053413
Epoch 46:
batch 5 loss: 0.0005576227558776736
batch 10 loss: 0.0005576024414040148
batch 15 loss: 0.0005577256786637008
batch 20 loss: 0.0005575970746576786
batch 25 loss: 0.00055769186001271
batch 30 loss: 0.000557711289729923
batch 35 loss: 0.00055772167397663
batch 40 loss: 0.0005577682051807642
batch 45 loss: 0.0005576963303610682
batch 50 loss: 0.0005577597301453352
batch 55 loss: 0.0005576830939389765
batch 60 loss: 0.000557734165340662
batch 65 loss: 0.0005576421739533543
batch 70 loss: 0.0005577458767220377
batch 75 loss: 0.0005576652358286083
batch 80 loss: 0.0005576956667937338
batch 85 loss: 0.000557640753686428
batch 90 loss: 0.0005576324765570462
batch 95 loss: 0.0005578175419941545
batch 100 loss: 0.0005576555617153644
batch 105 loss: 0.0005576618132181466
batch 110 loss: 0.0005576695082709193
batch 115 loss: 0.0005576709751039744
batch 120 loss: 0.0005577086005359889
batch 125 loss: 0.0005576971336267889
batch 130 loss: 0.0005577027448453009
batch 135 loss: 0.0005576567607931792
batch 140 loss: 0.0005576474010013044
batch 145 loss: 0.0005576650844886899
batch 150 loss: 0.0005576395080424846
batch 155 loss: 0.0005577157600782812
batch 160 loss: 0.0005577331525273621
batch 165 loss: 0.0005576008232310414
batch 170 loss: 0.0005576062947511673
batch 175 loss: 0.0005575597402639687
batch 180 loss: 0.0005576650612056256
batch 185 loss: 0.0005578172043897211
batch 190 loss: 0.00055780706461519
batch 195 loss: 0.000557685166131705
batch 200 loss: 0.0005576395429670811
batch 205 loss: 0.0005577515345066786
batch 210 loss: 0.0005577192059718072
batch 215 loss: 0.0005576770403422415
batch 220 loss: 0.000557678728364408
batch 225 loss: 0.0005576916737481952
batch 230 loss: 0.0005576583207584918
batch 235 loss: 0.0005576082854531706
batch 240 loss: 0.0005576635361649096
Training Loss: 0.0005576834844153685
Validation Loss: 0.0005576934452013423
Epoch 47:
batch 5 loss: 0.0005577601958066225
batch 10 loss: 0.0005576762836426496
batch 15 loss: 0.0005577077972702682
batch 20 loss: 0.0005576865281909704
batch 25 loss: 0.0005576303345151245
batch 30 loss: 0.0005576708936132491
batch 35 loss: 0.0005577426636591554
batch 40 loss: 0.0005577315459959209
batch 45 loss: 0.0005577221978455782
batch 50 loss: 0.0005577777745202183
batch 55 loss: 0.0005577244679443538
batch 60 loss: 0.0005577874137088657
batch 65 loss: 0.0005575934541411698
batch 70 loss: 0.0005576852592639626
batch 75 loss: 0.000557549949735403
batch 80 loss: 0.0005577447824180126
batch 85 loss: 0.0005575775750912726
batch 90 loss: 0.0005577409523539245
batch 95 loss: 0.0005576764699071646
batch 100 loss: 0.0005577409290708601
batch 105 loss: 0.0005576113588176667
batch 110 loss: 0.0005575904040597379
batch 115 loss: 0.0005577008822001516
batch 120 loss: 0.0005576835363171994
batch 125 loss: 0.0005576968425884843
batch 130 loss: 0.0005577531992457807
batch 135 loss: 0.0005575711489655078
batch 140 loss: 0.0005576991010457277
batch 145 loss: 0.0005577586824074387
batch 150 loss: 0.0005576007184572518
batch 155 loss: 0.0005577178089879453
batch 160 loss: 0.000557726004626602
batch 165 loss: 0.0005576663417741657
batch 170 loss: 0.0005576848518103362
batch 175 loss: 0.0005577282165177167
batch 180 loss: 0.0005577186588197946
batch 185 loss: 0.000557716703042388
batch 190 loss: 0.0005575927556492388
batch 195 loss: 0.0005575989023782312
batch 200 loss: 0.0005576937692239881
batch 205 loss: 0.0005576861673034728
batch 210 loss: 0.000557721802033484
batch 215 loss: 0.0005575836286880076
batch 220 loss: 0.0005576508468948304
batch 225 loss: 0.0005576550494879485
batch 230 loss: 0.0005576778668910265
batch 235 loss: 0.0005576868774369359
batch 240 loss: 0.0005577055271714926
Training Loss: 0.000557683440032027
Validation Loss: 0.0005576933957248306
Epoch 48:
batch 5 loss: 0.0005576997064054012
batch 10 loss: 0.0005576636875048279
batch 15 loss: 0.0005576636060141027
batch 20 loss: 0.00055783127900213
batch 25 loss: 0.0005577062023803592
batch 30 loss: 0.0005575687391683459
batch 35 loss: 0.0005576767260208725
batch 40 loss: 0.0005577288335189224
batch 45 loss: 0.0005576681345701217
batch 50 loss: 0.0005577678210102021
batch 55 loss: 0.0005577445728704334
batch 60 loss: 0.0005577208241447807
batch 65 loss: 0.0005575871095061302
batch 70 loss: 0.0005576536641456187
batch 75 loss: 0.000557738624047488
batch 80 loss: 0.0005576603696681559
batch 85 loss: 0.0005576947471126914
batch 90 loss: 0.0005576699040830135
batch 95 loss: 0.0005576426861807704
batch 100 loss: 0.0005577114410698414
batch 105 loss: 0.0005576538736931979
batch 110 loss: 0.0005576146068051457
batch 115 loss: 0.0005576233961619437
batch 120 loss: 0.0005576843512244522
batch 125 loss: 0.0005575810093432665
batch 130 loss: 0.000557720405049622
batch 135 loss: 0.0005576991941779851
batch 140 loss: 0.0005576626630499959
batch 145 loss: 0.0005577157950028777
batch 150 loss: 0.0005577000090852379
batch 155 loss: 0.0005577227915637195
batch 160 loss: 0.0005576773197390139
batch 165 loss: 0.0005577420000918209
batch 170 loss: 0.0005576815572567284
batch 175 loss: 0.0005577835370786488
batch 180 loss: 0.0005576516850851476
batch 185 loss: 0.0005577268777415157
batch 190 loss: 0.0005577473319135606
batch 195 loss: 0.0005575031973421574
batch 200 loss: 0.0005575318587943912
batch 205 loss: 0.0005576415918767452
batch 210 loss: 0.0005577104631811381
batch 215 loss: 0.0005576583673246205
batch 220 loss: 0.0005577704519964755
batch 225 loss: 0.000557760486844927
batch 230 loss: 0.0005575980525463819
batch 235 loss: 0.0005577599513344467
batch 240 loss: 0.0005576861789450049
Training Loss: 0.0005576834933890496
Validation Loss: 0.0005576935693776856
Epoch 49:
batch 5 loss: 0.0005577489617280662
batch 10 loss: 0.0005576592520810664
batch 15 loss: 0.0005575889372266829
batch 20 loss: 0.0005576793919317424
batch 25 loss: 0.0005575836054049432
batch 30 loss: 0.0005577629082836211
batch 35 loss: 0.0005576536525040865
batch 40 loss: 0.0005576996831223368
batch 45 loss: 0.000557719252537936
batch 50 loss: 0.0005576559458859264
batch 55 loss: 0.0005577557254582644
batch 60 loss: 0.0005575882969424129
batch 65 loss: 0.0005576635710895061
batch 70 loss: 0.000557700137142092
batch 75 loss: 0.0005576457246206701
batch 80 loss: 0.0005576284136623144
batch 85 loss: 0.0005577900796197355
batch 90 loss: 0.0005576850846409798
batch 95 loss: 0.0005576908471994102
batch 100 loss: 0.0005576701485551893
batch 105 loss: 0.0005576593568548561
batch 110 loss: 0.0005575979128479957
batch 115 loss: 0.0005575879476964474
batch 120 loss: 0.0005576780531555414
batch 125 loss: 0.0005576845025643707
batch 130 loss: 0.0005577232455834747
batch 135 loss: 0.0005576412891969085
batch 140 loss: 0.0005576647585257888
batch 145 loss: 0.0005576991708949209
batch 150 loss: 0.0005577544565312564
batch 155 loss: 0.0005577515228651464
batch 160 loss: 0.0005577295552939177
batch 165 loss: 0.0005577277392148971
batch 170 loss: 0.0005577174481004477
batch 175 loss: 0.0005577295669354498
batch 180 loss: 0.0005576323834247887
batch 185 loss: 0.0005576612311415374
batch 190 loss: 0.0005577938165515662
batch 195 loss: 0.0005576856201514602
batch 200 loss: 0.00055769186001271
batch 205 loss: 0.0005577060277573764
batch 210 loss: 0.0005576312192715705
batch 215 loss: 0.0005576803116127849
batch 220 loss: 0.0005575750721618534
batch 225 loss: 0.0005577247589826584
batch 230 loss: 0.0005576142692007124
batch 235 loss: 0.0005577680538408458
batch 240 loss: 0.0005577280884608626
Training Loss: 0.0005576835178847735
Validation Loss: 0.0005576934704246621
Epoch 50:
batch 5 loss: 0.000557607610244304
batch 10 loss: 0.000557798205409199
batch 15 loss: 0.0005577057483606041
batch 20 loss: 0.0005576473544351756
batch 25 loss: 0.0005577245028689504
batch 30 loss: 0.0005576677853241563
batch 35 loss: 0.000557616539299488
batch 40 loss: 0.000557656737510115
batch 45 loss: 0.0005577604402787984
batch 50 loss: 0.0005577003001235425
batch 55 loss: 0.0005578181124292314
batch 60 loss: 0.0005576948751695454
batch 65 loss: 0.0005576076218858361
batch 70 loss: 0.0005576149793341756
batch 75 loss: 0.0005578142707236111
batch 80 loss: 0.0005576373660005629
batch 85 loss: 0.0005575216840952635
batch 90 loss: 0.0005577057600021362
batch 95 loss: 0.0005577042931690813
batch 100 loss: 0.0005577194620855153
batch 105 loss: 0.0005576298804953695
batch 110 loss: 0.0005576277151703834
batch 115 loss: 0.0005577191128395498
batch 120 loss: 0.0005577279604040086
batch 125 loss: 0.0005576194962486625
batch 130 loss: 0.0005575921735726297
batch 135 loss: 0.000557714351452887
batch 140 loss: 0.0005576040013693274
batch 145 loss: 0.0005576327093876898
batch 150 loss: 0.0005576787516474724
batch 155 loss: 0.0005576795898377896
batch 160 loss: 0.0005576862720772624
batch 165 loss: 0.0005576829193159938
batch 170 loss: 0.000557622208725661
batch 175 loss: 0.0005577179603278637
batch 180 loss: 0.0005576392053626478
batch 185 loss: 0.0005577354226261377
batch 190 loss: 0.0005577507545240223
batch 195 loss: 0.0005577306612394751
batch 200 loss: 0.0005577984149567783
batch 205 loss: 0.0005576546187512577
batch 210 loss: 0.0005576727096922695
batch 215 loss: 0.0005577188683673739
batch 220 loss: 0.0005577617674134671
batch 225 loss: 0.0005576398223638534
batch 230 loss: 0.0005577503587119281
batch 235 loss: 0.0005576766794547439
batch 240 loss: 0.0005576193565502763
Training Loss: 0.0005576834873257515
Validation Loss: 0.0005576934044559796
Epoch 51:
batch 5 loss: 0.0005576033843681216
batch 10 loss: 0.0005576993804425001
batch 15 loss: 0.0005576101830229163
batch 20 loss: 0.0005576757946982979
batch 25 loss: 0.000557747355196625
batch 30 loss: 0.0005576443742029369
batch 35 loss: 0.0005577233154326677
batch 40 loss: 0.0005577751202508807
batch 45 loss: 0.0005576206953264773
batch 50 loss: 0.0005576358176767826
batch 55 loss: 0.0005577215342782438
batch 60 loss: 0.0005576852709054947
batch 65 loss: 0.0005576790077611804
batch 70 loss: 0.0005576895549893379
batch 75 loss: 0.0005576020805165172
batch 80 loss: 0.0005577653762884438
batch 85 loss: 0.0005576343741267919
batch 90 loss: 0.0005577145260758698
batch 95 loss: 0.000557799160014838
batch 100 loss: 0.0005576877272687852
batch 105 loss: 0.0005577155272476376
batch 110 loss: 0.000557635969016701
batch 115 loss: 0.0005577088799327612
batch 120 loss: 0.0005577051197178662
batch 125 loss: 0.0005576437222771346
batch 130 loss: 0.000557651522103697
batch 135 loss: 0.0005576559691689908
batch 140 loss: 0.0005577698117122054
batch 145 loss: 0.0005577920004725456
batch 150 loss: 0.0005576265975832939
batch 155 loss: 0.0005576440133154392
batch 160 loss: 0.0005577261443249881
batch 165 loss: 0.0005576409748755395
batch 170 loss: 0.0005577478674240411
batch 175 loss: 0.0005575880291871727
batch 180 loss: 0.0005577019997872412
batch 185 loss: 0.0005576573079451918
batch 190 loss: 0.0005576636642217637
batch 195 loss: 0.0005577033618465066
batch 200 loss: 0.0005577821284532547
batch 205 loss: 0.0005576514406129718
batch 210 loss: 0.000557666493114084
batch 215 loss: 0.0005576780997216702
batch 220 loss: 0.0005576539319008589
batch 225 loss: 0.0005577317671850324
batch 230 loss: 0.0005576470284722745
batch 235 loss: 0.000557680323254317
batch 240 loss: 0.0005576209630817174
Training Loss: 0.000557683431058346
Validation Loss: 0.0005576934209481503
Epoch 52:
batch 5 loss: 0.0005577623145654797
batch 10 loss: 0.0005576420109719038
batch 15 loss: 0.0005577487405389548
batch 20 loss: 0.0005576084833592177
batch 25 loss: 0.0005576686700806022
batch 30 loss: 0.0005577345262281596
batch 35 loss: 0.000557679811026901
batch 40 loss: 0.0005576532334089279
batch 45 loss: 0.0005576661671511829
batch 50 loss: 0.0005576925352215767
batch 55 loss: 0.0005576790310442448
batch 60 loss: 0.0005575007642619311
batch 65 loss: 0.0005577073665335774
batch 70 loss: 0.0005576439201831817
batch 75 loss: 0.0005577890668064355
batch 80 loss: 0.0005575932911597193
batch 85 loss: 0.0005576219409704208
batch 90 loss: 0.000557607423979789
batch 95 loss: 0.0005575582850724459
batch 100 loss: 0.0005576153169386088
batch 105 loss: 0.0005578493466600776
batch 110 loss: 0.000557720975484699
batch 115 loss: 0.0005576234543696046
batch 120 loss: 0.0005577701260335744
batch 125 loss: 0.0005576525698415935
batch 130 loss: 0.0005576576571911573
batch 135 loss: 0.000557806680444628
batch 140 loss: 0.0005577311385422945
batch 145 loss: 0.000557670381385833
batch 150 loss: 0.0005576975527219474
batch 155 loss: 0.0005576562252826989
batch 160 loss: 0.0005576582276262343
batch 165 loss: 0.0005576926749199628
batch 170 loss: 0.0005576942698098719
batch 175 loss: 0.0005577183212153614
batch 180 loss: 0.0005576606374233962
batch 185 loss: 0.0005576728028245271
batch 190 loss: 0.0005577813601121307
batch 195 loss: 0.0005577121512033046
batch 200 loss: 0.0005577972857281566
batch 205 loss: 0.0005576644209213555
batch 210 loss: 0.0005576793337240815
batch 215 loss: 0.0005576450494118035
batch 220 loss: 0.0005575937801040709
batch 225 loss: 0.000557671335991472
batch 230 loss: 0.0005577170173637569
batch 235 loss: 0.0005576693220064044
batch 240 loss: 0.0005577685660682619
Training Loss: 0.0005576834492482401
Validation Loss: 0.0005576934616935129
Epoch 53:
batch 5 loss: 0.0005576765397563577
batch 10 loss: 0.000557650183327496
batch 15 loss: 0.0005577251431532204
batch 20 loss: 0.0005577377392910421
batch 25 loss: 0.0005576769472099841
batch 30 loss: 0.0005577283445745707
batch 35 loss: 0.0005577655276283622
batch 40 loss: 0.0005577214993536473
batch 45 loss: 0.0005577380070462823
batch 50 loss: 0.0005576838157139719
batch 55 loss: 0.0005576277617365122
batch 60 loss: 0.00055773607455194
batch 65 loss: 0.0005575754446908831
batch 70 loss: 0.000557672989089042
batch 75 loss: 0.000557698414195329
batch 80 loss: 0.0005575375049374998
batch 85 loss: 0.0005577500560320914
batch 90 loss: 0.0005576520576141775
batch 95 loss: 0.0005576897645369172
batch 100 loss: 0.0005575806018896401
batch 105 loss: 0.0005577033152803779
batch 110 loss: 0.0005576943513005972
batch 115 loss: 0.0005576413823291659
batch 120 loss: 0.0005577264120802284
batch 125 loss: 0.0005576229421421885
batch 130 loss: 0.0005577062838710845
batch 135 loss: 0.0005576833966188133
batch 140 loss: 0.0005575454095378518
batch 145 loss: 0.0005576178431510925
batch 150 loss: 0.0005577597650699318
batch 155 loss: 0.0005576530704274774
batch 160 loss: 0.00055768535239622
batch 165 loss: 0.0005578492535278201
batch 170 loss: 0.0005577275995165109
batch 175 loss: 0.0005576900788582861
batch 180 loss: 0.0005576821626164019
batch 185 loss: 0.0005577296949923038
batch 190 loss: 0.0005577662144787609
batch 195 loss: 0.0005575210088863969
batch 200 loss: 0.0005577721050940454
batch 205 loss: 0.0005575648858211935
batch 210 loss: 0.0005576493800617755
batch 215 loss: 0.0005576871684752404
batch 220 loss: 0.0005576396360993385
batch 225 loss: 0.0005576758645474911
batch 230 loss: 0.000557750416919589
batch 235 loss: 0.0005577137344516814
batch 240 loss: 0.000557723653037101
Training Loss: 0.0005576834749566236
Validation Loss: 0.0005576936974345396
Epoch 54:
batch 5 loss: 0.0005579065647907556
batch 10 loss: 0.0005577662726864218
batch 15 loss: 0.0005576513358391821
batch 20 loss: 0.000557686819229275
batch 25 loss: 0.0005577588803134858
batch 30 loss: 0.0005575847811996937
batch 35 loss: 0.0005576483788900077
batch 40 loss: 0.0005576002062298357
batch 45 loss: 0.0005576880648732185
batch 50 loss: 0.0005577467847615481
batch 55 loss: 0.0005576573428697884
batch 60 loss: 0.000557689752895385
batch 65 loss: 0.0005576800205744803
batch 70 loss: 0.0005577721167355776
batch 75 loss: 0.0005577216157689691
batch 80 loss: 0.0005575624410994351
batch 85 loss: 0.0005576065974310041
batch 90 loss: 0.0005575833376497031
batch 95 loss: 0.0005577136296778917
batch 100 loss: 0.0005576995899900794
batch 105 loss: 0.0005577253177762032
batch 110 loss: 0.0005576626746915281
batch 115 loss: 0.0005576212075538933
batch 120 loss: 0.0005577057483606041
batch 125 loss: 0.0005577155272476376
batch 130 loss: 0.0005577445263043046
batch 135 loss: 0.0005577194155193865
batch 140 loss: 0.0005576941068284214
batch 145 loss: 0.0005576364463195204
batch 150 loss: 0.0005576234078034758
batch 155 loss: 0.0005576392402872443
batch 160 loss: 0.0005577550502493978
batch 165 loss: 0.0005576217779889702
batch 170 loss: 0.0005576669471338391
batch 175 loss: 0.0005577370990067721
batch 180 loss: 0.0005577173782512545
batch 185 loss: 0.0005574981332756579
batch 190 loss: 0.000557698227930814
batch 195 loss: 0.0005576820694841444
batch 200 loss: 0.0005576556432060897
batch 205 loss: 0.0005576282506808639
batch 210 loss: 0.0005577169009484351
batch 215 loss: 0.0005576980416662991
batch 220 loss: 0.0005576383206062019
batch 225 loss: 0.0005577085074037313
batch 230 loss: 0.0005576854338869453
batch 235 loss: 0.0005577512900345027
batch 240 loss: 0.0005577357020229101
Training Loss: 0.0005576834776244747
Validation Loss: 0.000557693464603896
Epoch 55:
batch 5 loss: 0.0005576944560743869
batch 10 loss: 0.0005576738389208913
batch 15 loss: 0.0005576850031502545
batch 20 loss: 0.0005576836410909891
batch 25 loss: 0.0005576635710895061
batch 30 loss: 0.000557643489446491
batch 35 loss: 0.0005577100673690438
batch 40 loss: 0.0005576189956627786
batch 45 loss: 0.0005577447242103517
batch 50 loss: 0.0005576087743975223
batch 55 loss: 0.0005576815223321318
batch 60 loss: 0.0005578322568908333
batch 65 loss: 0.0005575546529144049
batch 70 loss: 0.0005578321870416403
batch 75 loss: 0.0005577525938861072
batch 80 loss: 0.0005578051321208477
batch 85 loss: 0.0005576695781201124
batch 90 loss: 0.0005575823015533388
batch 95 loss: 0.0005576469353400171
batch 100 loss: 0.0005577130592428148
batch 105 loss: 0.0005576398223638534
batch 110 loss: 0.0005577058647759259
batch 115 loss: 0.0005576413357630372
batch 120 loss: 0.0005576438619755208
batch 125 loss: 0.0005575947347097099
batch 130 loss: 0.0005576101364567875
batch 135 loss: 0.0005576899391598999
batch 140 loss: 0.0005577287985943258
batch 145 loss: 0.0005576549447141588
batch 150 loss: 0.0005576981813646853
batch 155 loss: 0.0005577302421443164
batch 160 loss: 0.0005576777970418334
batch 165 loss: 0.0005577335599809885
batch 170 loss: 0.0005576433381065726
batch 175 loss: 0.0005576560855843127
batch 180 loss: 0.0005576487630605698
batch 185 loss: 0.0005576896597631276
batch 190 loss: 0.0005576519411988557
batch 195 loss: 0.0005577375763095915
batch 200 loss: 0.0005576737807132303
batch 205 loss: 0.0005575844435952604
batch 210 loss: 0.000557674840092659
batch 215 loss: 0.0005576422438025475
batch 220 loss: 0.0005577665288001299
batch 225 loss: 0.0005577619536779821
batch 230 loss: 0.0005576951894909144
batch 235 loss: 0.000557668146211654
batch 240 loss: 0.000557766086421907
Training Loss: 0.0005576834703485171
Validation Loss: 0.0005576934112468734
Epoch 56:
batch 5 loss: 0.0005577274598181248
batch 10 loss: 0.0005576739902608096
batch 15 loss: 0.0005577369127422571
batch 20 loss: 0.0005575705785304308
batch 25 loss: 0.0005576791591010988
batch 30 loss: 0.0005577093339525163
batch 35 loss: 0.0005576659692451358
batch 40 loss: 0.0005577143747359514
batch 45 loss: 0.0005576741183176637
batch 50 loss: 0.0005576543277129531
batch 55 loss: 0.000557748693972826
batch 60 loss: 0.0005577077274210751
batch 65 loss: 0.0005576646770350635
batch 70 loss: 0.0005576624302193522
batch 75 loss: 0.0005576296593062579
batch 80 loss: 0.0005576248513534665
batch 85 loss: 0.000557803176343441
batch 90 loss: 0.0005576831405051053
batch 95 loss: 0.0005577306263148784
batch 100 loss: 0.0005576521158218384
batch 105 loss: 0.0005576412891969085
batch 110 loss: 0.0005576695199124515
batch 115 loss: 0.0005577309406362474
batch 120 loss: 0.000557706190738827
batch 125 loss: 0.0005576828960329294
batch 130 loss: 0.0005576631636358797
batch 135 loss: 0.0005577380768954753
batch 140 loss: 0.0005576586816459894
batch 145 loss: 0.0005576754570938647
batch 150 loss: 0.0005576893803663552
batch 155 loss: 0.0005576957715675235
batch 160 loss: 0.0005576184717938304
batch 165 loss: 0.000557700835634023
batch 170 loss: 0.0005576445139013231
batch 175 loss: 0.0005576351308263838
batch 180 loss: 0.0005577897769398988
batch 185 loss: 0.0005576947005465627
batch 190 loss: 0.000557602196931839
batch 195 loss: 0.000557669170666486
batch 200 loss: 0.0005578324431553483
batch 205 loss: 0.0005576960276812315
batch 210 loss: 0.0005575940012931824
batch 215 loss: 0.0005576989031396806
batch 220 loss: 0.0005577389267273248
batch 225 loss: 0.0005576389492489397
batch 230 loss: 0.0005577143398113549
batch 235 loss: 0.0005576507188379765
batch 240 loss: 0.0005576238734647631
Training Loss: 0.0005576834931465177
Validation Loss: 0.0005576936508684109
Epoch 57:
batch 5 loss: 0.0005576956085860729
batch 10 loss: 0.0005577479489147664
batch 15 loss: 0.0005576109513640403
batch 20 loss: 0.0005576698807999491
batch 25 loss: 0.0005576900090090931
batch 30 loss: 0.0005577111034654081
batch 35 loss: 0.0005576034425757825
batch 40 loss: 0.0005576702766120434
batch 45 loss: 0.0005576844094321131
batch 50 loss: 0.0005577361676841974
batch 55 loss: 0.0005576096242293716
batch 60 loss: 0.0005576730123721063
batch 65 loss: 0.0005576880765147507
batch 70 loss: 0.0005577070987783372
batch 75 loss: 0.0005576595198363066
batch 80 loss: 0.0005575637915171683
batch 85 loss: 0.0005577786010690034
batch 90 loss: 0.0005576371331699193
batch 95 loss: 0.0005575381452217698
batch 100 loss: 0.0005576242692768574
batch 105 loss: 0.000557795271743089
batch 110 loss: 0.0005576838622801006
batch 115 loss: 0.000557716574985534
batch 120 loss: 0.000557656493037939
batch 125 loss: 0.0005576816969551146
batch 130 loss: 0.0005576585070230066
batch 135 loss: 0.000557589519303292
batch 140 loss: 0.0005577382282353938
batch 145 loss: 0.0005576858180575073
batch 150 loss: 0.0005577275878749788
batch 155 loss: 0.000557685806415975
batch 160 loss: 0.0005576795432716608
batch 165 loss: 0.0005577446543611586
batch 170 loss: 0.0005576253402978182
batch 175 loss: 0.0005576126975938677
batch 180 loss: 0.0005576822208240628
batch 185 loss: 0.0005576940136961638
batch 190 loss: 0.0005575829651206732
batch 195 loss: 0.0005577497882768512
batch 200 loss: 0.00055773148778826
batch 205 loss: 0.0005576882278546691
batch 210 loss: 0.0005577028030529618
batch 215 loss: 0.0005576569121330977
batch 220 loss: 0.0005577880423516035
batch 225 loss: 0.0005576989846304059
batch 230 loss: 0.000557788775768131
batch 235 loss: 0.0005576734663918614
batch 240 loss: 0.0005577879957854748
Training Loss: 0.0005576834657404107
Validation Loss: 0.0005576934228884057
Epoch 58:
batch 5 loss: 0.0005577047006227076
batch 10 loss: 0.000557716703042388
batch 15 loss: 0.0005577442585490644
batch 20 loss: 0.0005575969000346959
batch 25 loss: 0.0005576161318458616
batch 30 loss: 0.0005577259464189411
batch 35 loss: 0.0005576953641138971
batch 40 loss: 0.0005576221970841289
batch 45 loss: 0.0005575734539888799
batch 50 loss: 0.0005576700670644641
batch 55 loss: 0.0005577108007855713
batch 60 loss: 0.0005576482159085572
batch 65 loss: 0.0005577533855102956
batch 70 loss: 0.000557680381461978
batch 75 loss: 0.0005577138392254711
batch 80 loss: 0.0005577164003625513
batch 85 loss: 0.0005577488685958087
batch 90 loss: 0.0005577503121457994
batch 95 loss: 0.0005577281233854591
batch 100 loss: 0.0005576213821768761
batch 105 loss: 0.0005576061550527811
batch 110 loss: 0.0005576102295890451
batch 115 loss: 0.0005577011965215206
batch 120 loss: 0.0005577526520937681
batch 125 loss: 0.0005576521391049028
batch 130 loss: 0.0005577399278990924
batch 135 loss: 0.0005576834315434098
batch 140 loss: 0.0005577348987571895
batch 145 loss: 0.0005576946306973696
batch 150 loss: 0.0005576548865064979
batch 155 loss: 0.0005577131872996688
batch 160 loss: 0.0005577142117545008
batch 165 loss: 0.0005576563649810851
batch 170 loss: 0.00055756033398211
batch 175 loss: 0.0005576552706770599
batch 180 loss: 0.0005576723837293684
batch 185 loss: 0.0005576879833824932
batch 190 loss: 0.0005577029660344124
batch 195 loss: 0.0005577613017521798
batch 200 loss: 0.0005576787749305368
batch 205 loss: 0.0005577114876359701
batch 210 loss: 0.0005575685179792344
batch 215 loss: 0.0005577007774263621
batch 220 loss: 0.0005576866678893566
batch 225 loss: 0.0005577639793045818
batch 230 loss: 0.0005576066905632615
batch 235 loss: 0.0005577292991802097
batch 240 loss: 0.0005576686235144734
Training Loss: 0.0005576834667105383
Validation Loss: 0.0005576935664673026
Epoch 59:
batch 5 loss: 0.0005576895317062735
batch 10 loss: 0.0005576399969868362
batch 15 loss: 0.0005577972740866243
batch 20 loss: 0.0005577345611527563
batch 25 loss: 0.0005576309165917337
batch 30 loss: 0.0005576069816015661
batch 35 loss: 0.0005577635136432945
batch 40 loss: 0.0005577221512794494
batch 45 loss: 0.0005576165043748915
batch 50 loss: 0.000557655084412545
batch 55 loss: 0.0005576138501055539
batch 60 loss: 0.0005577535717748106
batch 65 loss: 0.000557693827431649
batch 70 loss: 0.0005578040261752903
batch 75 loss: 0.0005576223018579185
batch 80 loss: 0.0005576888332143426
batch 85 loss: 0.0005577131640166044
batch 90 loss: 0.0005576723720878363
batch 95 loss: 0.0005577022559009493
batch 100 loss: 0.0005576574592851102
batch 105 loss: 0.0005576309631578624
batch 110 loss: 0.0005576452123932541
batch 115 loss: 0.0005576876224949956
batch 120 loss: 0.0005576415685936808
batch 125 loss: 0.0005575679359026253
batch 130 loss: 0.0005577363772317767
batch 135 loss: 0.0005577312316745519
batch 140 loss: 0.0005575553164817393
batch 145 loss: 0.0005576108116656542
batch 150 loss: 0.0005576005904003977
batch 155 loss: 0.0005576757248491049
batch 160 loss: 0.0005576970055699348
batch 165 loss: 0.0005577425006777048
batch 170 loss: 0.0005577490315772593
batch 175 loss: 0.0005576845607720316
batch 180 loss: 0.0005577398114837706
batch 185 loss: 0.0005576302763074636
batch 190 loss: 0.0005577704752795399
batch 195 loss: 0.0005576567607931792
batch 200 loss: 0.0005577962030656636
batch 205 loss: 0.0005577264935709536
batch 210 loss: 0.0005576944793574512
batch 215 loss: 0.0005576056777499616
batch 220 loss: 0.0005576430819928646
batch 225 loss: 0.0005576563766226172
batch 230 loss: 0.0005577440606430173
batch 235 loss: 0.0005576308351010085
batch 240 loss: 0.0005577765754424035
Training Loss: 0.0005576834528862189
Validation Loss: 0.0005576934345299378
Epoch 60:
batch 5 loss: 0.0005577365984208882
batch 10 loss: 0.0005576030467636883
batch 15 loss: 0.0005577256437391042
batch 20 loss: 0.0005576311028562486
batch 25 loss: 0.0005576392984949052
batch 30 loss: 0.0005577113828621804
batch 35 loss: 0.0005577555508352816
batch 40 loss: 0.0005576811148785054
batch 45 loss: 0.0005576319410465658
batch 50 loss: 0.0005576428375206888
batch 55 loss: 0.0005576718831434846
batch 60 loss: 0.0005576183553785086
batch 65 loss: 0.0005576100200414657
batch 70 loss: 0.0005576526047661901
batch 75 loss: 0.0005576521274633705
batch 80 loss: 0.0005576067138463259
batch 85 loss: 0.0005576807889156044
batch 90 loss: 0.0005576561787165701
batch 95 loss: 0.0005576559226028622
batch 100 loss: 0.0005576316732913255
batch 105 loss: 0.0005576691473834217
batch 110 loss: 0.0005577599629759788
batch 115 loss: 0.0005575576797127724
batch 120 loss: 0.0005577713833190501
batch 125 loss: 0.0005577111849561333
batch 130 loss: 0.0005575314746238292
batch 135 loss: 0.0005576408933848142
batch 140 loss: 0.000557704665698111
batch 145 loss: 0.0005576247000135482
batch 150 loss: 0.0005576704046688974
batch 155 loss: 0.0005578153301030398
batch 160 loss: 0.0005577917909249663
batch 165 loss: 0.0005578042822889983
batch 170 loss: 0.0005578774376772344
batch 175 loss: 0.0005576525814831257
batch 180 loss: 0.0005577270174399018
batch 185 loss: 0.0005576965981163084
batch 190 loss: 0.0005576928029768169
batch 195 loss: 0.0005576613475568593
batch 200 loss: 0.0005575554096139967
batch 205 loss: 0.0005577236646786332
batch 210 loss: 0.0005576493917033076
batch 215 loss: 0.0005577144678682089
batch 220 loss: 0.0005576689960435033
batch 225 loss: 0.0005576944211497902
batch 230 loss: 0.0005577612435445189
batch 235 loss: 0.0005576871684752404
batch 240 loss: 0.0005577943753451109
Training Loss: 0.0005576834293606226
Validation Loss: 0.000557693784746031
Epoch 61:
batch 5 loss: 0.0005576603347435593
batch 10 loss: 0.0005577364820055664
batch 15 loss: 0.0005576990777626633
batch 20 loss: 0.0005577048403210938
batch 25 loss: 0.0005577937350608409
batch 30 loss: 0.0005577596602961421
batch 35 loss: 0.0005575642921030521
batch 40 loss: 0.0005576554918661714
batch 45 loss: 0.0005577239091508091
batch 50 loss: 0.0005575621966272592
batch 55 loss: 0.0005576781695708632
batch 60 loss: 0.0005576634197495878
batch 65 loss: 0.0005576261784881354
batch 70 loss: 0.0005575897288508713
batch 75 loss: 0.0005576207884587347
batch 80 loss: 0.0005576677736826241
batch 85 loss: 0.0005577007192187011
batch 90 loss: 0.0005577514995820821
batch 95 loss: 0.0005577690666541457
batch 100 loss: 0.000557651452254504
batch 105 loss: 0.0005576371098868549
batch 110 loss: 0.0005577032105065882
batch 115 loss: 0.0005577539559453726
batch 120 loss: 0.0005576969007961452
batch 125 loss: 0.0005575626972131431
batch 130 loss: 0.0005577215575613082
batch 135 loss: 0.0005576889379881322
batch 140 loss: 0.0005576464463956654
batch 145 loss: 0.0005577540025115014
batch 150 loss: 0.0005576571333222091
batch 155 loss: 0.000557630870025605
batch 160 loss: 0.0005577052012085915
batch 165 loss: 0.0005576822673901916
batch 170 loss: 0.0005577386473305523
batch 175 loss: 0.0005577021278440952
batch 180 loss: 0.0005576542811468243
batch 185 loss: 0.000557686691172421
batch 190 loss: 0.0005576032679527998
batch 195 loss: 0.0005577184376306832
batch 200 loss: 0.0005576281459070742
batch 205 loss: 0.000557762011885643
batch 210 loss: 0.0005578296724706889
batch 215 loss: 0.0005577468778938055
batch 220 loss: 0.0005576550960540771
batch 225 loss: 0.0005577594391070306
batch 230 loss: 0.0005577034316956997
batch 235 loss: 0.0005576038849540055
batch 240 loss: 0.0005575940944254398
Training Loss: 0.0005576834419722825
Validation Loss: 0.0005576935082596417
Epoch 62:
batch 5 loss: 0.0005577335832640529
batch 10 loss: 0.0005576938157901168
batch 15 loss: 0.000557638960890472
batch 20 loss: 0.0005576964700594545
batch 25 loss: 0.0005576928611844778
batch 30 loss: 0.0005576384137384593
batch 35 loss: 0.0005576368654146791
batch 40 loss: 0.0005576758994720876
batch 45 loss: 0.0005576228839345276
batch 50 loss: 0.0005577015457674861
batch 55 loss: 0.0005577156087383627
batch 60 loss: 0.000557751301676035
batch 65 loss: 0.0005576723837293684
batch 70 loss: 0.0005576147581450641
batch 75 loss: 0.0005576684721745551
batch 80 loss: 0.0005577066331170499
batch 85 loss: 0.0005577573785558343
batch 90 loss: 0.0005577334319241345
batch 95 loss: 0.0005577587406150996
batch 100 loss: 0.0005577144329436123
batch 105 loss: 0.0005576010677032173
batch 110 loss: 0.0005577481002546847
batch 115 loss: 0.0005577193456701935
batch 120 loss: 0.0005576508585363627
batch 125 loss: 0.0005576718715019525
batch 130 loss: 0.000557590601965785
batch 135 loss: 0.0005576770170591771
batch 140 loss: 0.0005576669122092426
batch 145 loss: 0.0005576662952080369
batch 150 loss: 0.0005575641524046659
batch 155 loss: 0.0005577664589509368
batch 160 loss: 0.0005577524076215923
batch 165 loss: 0.0005576818948611617
batch 170 loss: 0.0005576426396146416
batch 175 loss: 0.0005577603471465409
batch 180 loss: 0.0005576502531766892
batch 185 loss: 0.0005577413365244865
batch 190 loss: 0.0005576391355134547
batch 195 loss: 0.0005576273892074823
batch 200 loss: 0.000557736458722502
batch 205 loss: 0.0005576911731623113
batch 210 loss: 0.000557624432258308
batch 215 loss: 0.000557648076210171
batch 220 loss: 0.0005577601841650903
batch 225 loss: 0.0005576472845859826
batch 230 loss: 0.0005577256670221686
batch 235 loss: 0.0005576661322265863
batch 240 loss: 0.000557663943618536
Training Loss: 0.0005576834557966019
Validation Loss: 0.0005576933996053413
Epoch 63:
batch 5 loss: 0.0005576984956860542
batch 10 loss: 0.0005576081341132522
batch 15 loss: 0.0005577439442276955
batch 20 loss: 0.0005576635710895061
batch 25 loss: 0.0005577587871812284
batch 30 loss: 0.0005577727453783155
batch 35 loss: 0.0005577965523116291
batch 40 loss: 0.0005576044437475503
batch 45 loss: 0.000557658460456878
batch 50 loss: 0.0005577096017077565
batch 55 loss: 0.0005577198811806738
batch 60 loss: 0.0005576516385190188
batch 65 loss: 0.0005577877978794277
batch 70 loss: 0.0005576989729888737
batch 75 loss: 0.0005576239549554884
batch 80 loss: 0.0005577398464083671
batch 85 loss: 0.0005576647818088531
batch 90 loss: 0.0005576135707087814
batch 95 loss: 0.0005576521391049028
batch 100 loss: 0.000557737227063626
batch 105 loss: 0.0005577205447480083
batch 110 loss: 0.0005576380295678973
batch 115 loss: 0.000557678029872477
batch 120 loss: 0.0005576152703724802
batch 125 loss: 0.0005576563766226172
batch 130 loss: 0.0005577134317718447
batch 135 loss: 0.0005576556664891541
batch 140 loss: 0.0005577197181992233
batch 145 loss: 0.0005577190197072924
batch 150 loss: 0.00055770396720618
batch 155 loss: 0.0005575872608460486
batch 160 loss: 0.0005575789487920702
batch 165 loss: 0.0005576711148023606
batch 170 loss: 0.0005576296476647258
batch 175 loss: 0.0005577230826020241
batch 180 loss: 0.0005577121046371758
batch 185 loss: 0.0005576935014687479
batch 190 loss: 0.000557750032749027
batch 195 loss: 0.0005576196359470487
batch 200 loss: 0.0005576695199124515
batch 205 loss: 0.0005577296833507717
batch 210 loss: 0.000557797763030976
batch 215 loss: 0.0005575472139753401
batch 220 loss: 0.0005575971561484038
batch 225 loss: 0.0005577039206400514
batch 230 loss: 0.0005576661555096507
batch 235 loss: 0.0005577713716775178
batch 240 loss: 0.0005576318362727761
Training Loss: 0.0005576834281479629
Validation Loss: 0.0005576934859467049
Epoch 64:
batch 5 loss: 0.0005577184376306832
batch 10 loss: 0.0005577496252954006
batch 15 loss: 0.0005576267023570836
batch 20 loss: 0.0005577276344411076
batch 25 loss: 0.0005576850031502545
batch 30 loss: 0.0005576860043220222
batch 35 loss: 0.0005577576230280101
batch 40 loss: 0.0005577609641477466
batch 45 loss: 0.0005575980176217854
batch 50 loss: 0.00055771543411538
batch 55 loss: 0.0005576795898377896
batch 60 loss: 0.0005576245370320976
batch 65 loss: 0.0005577434320002794
batch 70 loss: 0.0005576781812123954
batch 75 loss: 0.000557750859297812
batch 80 loss: 0.0005576304625719786
batch 85 loss: 0.0005577122559770942
batch 90 loss: 0.0005575846764259041
batch 95 loss: 0.0005576918483711779
batch 100 loss: 0.0005577144911512733
batch 105 loss: 0.0005576624884270131
batch 110 loss: 0.0005576622090302408
batch 115 loss: 0.0005576837575063109
batch 120 loss: 0.0005576630705036223
batch 125 loss: 0.0005575736402533948
batch 130 loss: 0.0005577739211730659
batch 135 loss: 0.0005576142342761159
batch 140 loss: 0.000557598692830652
batch 145 loss: 0.0005577584612183273
batch 150 loss: 0.0005576468189246953
batch 155 loss: 0.0005577607662416995
batch 160 loss: 0.0005577455391176045
batch 165 loss: 0.0005575897754170001
batch 170 loss: 0.0005576351773925126
batch 175 loss: 0.0005577178206294775
batch 180 loss: 0.0005577366449870169
batch 185 loss: 0.0005575674003921449
batch 190 loss: 0.0005576909868977963
batch 195 loss: 0.000557682674843818
batch 200 loss: 0.000557616539299488
batch 205 loss: 0.0005576474242843688
batch 210 loss: 0.0005577418603934347
batch 215 loss: 0.0005577180883847177
batch 220 loss: 0.0005575897521339357
batch 225 loss: 0.0005578527459874749
batch 230 loss: 0.0005576031282544136
batch 235 loss: 0.0005577073199674488
batch 240 loss: 0.0005577285191975534
Training Loss: 0.0005576834424573463
Validation Loss: 0.0005576934393805762
Epoch 65:
batch 5 loss: 0.0005577280302532017
batch 10 loss: 0.0005577918025664985
batch 15 loss: 0.0005575978779233992
batch 20 loss: 0.00055770498001948
batch 25 loss: 0.0005576760740950704
batch 30 loss: 0.0005577039089985191
batch 35 loss: 0.0005576535826548934
batch 40 loss: 0.0005576983327046037
batch 45 loss: 0.0005577200092375279
batch 50 loss: 0.0005575275863520801
batch 55 loss: 0.0005576983676292002
batch 60 loss: 0.0005574693321250379
batch 65 loss: 0.0005577285308390855
batch 70 loss: 0.0005576246068812907
batch 75 loss: 0.0005576809868216515
batch 80 loss: 0.0005578324315138161
batch 85 loss: 0.0005577354459092021
batch 90 loss: 0.0005577106610871851
batch 95 loss: 0.000557593663688749
batch 100 loss: 0.0005576283554546535
batch 105 loss: 0.0005577261792495847
batch 110 loss: 0.0005576622090302408
batch 115 loss: 0.0005576222785748542
batch 120 loss: 0.0005577057134360075
batch 125 loss: 0.0005576489027589559
batch 130 loss: 0.0005577762494795024
batch 135 loss: 0.0005576209397986532
batch 140 loss: 0.0005578371696174145
batch 145 loss: 0.0005577557487413288
batch 150 loss: 0.0005576340365223587
batch 155 loss: 0.0005577666917815805
batch 160 loss: 0.000557769532315433
batch 165 loss: 0.0005576512310653925
batch 170 loss: 0.0005577026749961078
batch 175 loss: 0.0005576674710027873
batch 180 loss: 0.0005576088442467153
batch 185 loss: 0.0005576900439336896
batch 190 loss: 0.0005576963303610682
batch 195 loss: 0.0005577413830906153
batch 200 loss: 0.000557663943618536
batch 205 loss: 0.0005577063304372132
batch 210 loss: 0.000557617680169642
batch 215 loss: 0.0005577201372943819
batch 220 loss: 0.0005576411960646511
batch 225 loss: 0.000557584164198488
batch 230 loss: 0.0005576452822424471
batch 235 loss: 0.0005577936652116477
batch 240 loss: 0.0005576450959779323
Training Loss: 0.0005576834519160911
Validation Loss: 0.0005576935373634721
Epoch 66:
batch 5 loss: 0.0005576481344178319
batch 10 loss: 0.0005576527677476406
batch 15 loss: 0.0005577252130024135
batch 20 loss: 0.0005576644325628877
batch 25 loss: 0.0005575955612584949
batch 30 loss: 0.0005576727329753339
batch 35 loss: 0.0005576413357630372
batch 40 loss: 0.0005576460505835712
batch 45 loss: 0.0005577960168011486
batch 50 loss: 0.000557655154261738
batch 55 loss: 0.0005576122668571771
batch 60 loss: 0.0005575761781074107
batch 65 loss: 0.0005577390315011143
batch 70 loss: 0.0005576631054282188
batch 75 loss: 0.0005577038624323905
batch 80 loss: 0.0005577070405706763
batch 85 loss: 0.0005577513482421636
batch 90 loss: 0.0005576279829256237
batch 95 loss: 0.0005576869705691933
batch 100 loss: 0.000557705166283995
batch 105 loss: 0.0005576322553679347
batch 110 loss: 0.0005576452589593828
batch 115 loss: 0.0005576718715019525
batch 120 loss: 0.0005576482391916216
batch 125 loss: 0.0005576905328780413
batch 130 loss: 0.0005577291478402913
batch 135 loss: 0.0005577468895353377
batch 140 loss: 0.0005577065283432602
batch 145 loss: 0.0005576176103204489
batch 150 loss: 0.0005577611620537937
batch 155 loss: 0.0005577419069595635
batch 160 loss: 0.0005577391013503075
batch 165 loss: 0.0005576819530688226
batch 170 loss: 0.0005576498224399984
batch 175 loss: 0.0005576123017817736
batch 180 loss: 0.0005576512077823282
batch 185 loss: 0.0005577299045398832
batch 190 loss: 0.0005576188093982636
batch 195 loss: 0.0005577551084570587
batch 200 loss: 0.0005577048286795616
batch 205 loss: 0.0005575869116000831
batch 210 loss: 0.0005576076335273683
batch 215 loss: 0.0005577067495323718
batch 220 loss: 0.0005577419768087565
batch 225 loss: 0.0005577563541010022
batch 230 loss: 0.0005577503703534603
batch 235 loss: 0.0005576391005888582
batch 240 loss: 0.0005578106269240379
Training Loss: 0.0005576834274203672
Validation Loss: 0.0005576934151273841
Epoch 67:
batch 5 loss: 0.0005575647228397429
batch 10 loss: 0.0005576693452894688
batch 15 loss: 0.0005577165051363408
batch 20 loss: 0.0005577758187428117
batch 25 loss: 0.000557728041894734
batch 30 loss: 0.0005576891591772438
batch 35 loss: 0.0005576599040068686
batch 40 loss: 0.000557788391597569
batch 45 loss: 0.0005576264928095042
batch 50 loss: 0.0005576489144004882
batch 55 loss: 0.0005577195435762406
batch 60 loss: 0.0005576131865382194
batch 65 loss: 0.0005577165633440017
batch 70 loss: 0.0005575983901508153
batch 75 loss: 0.0005577000672928988
batch 80 loss: 0.0005576771567575633
batch 85 loss: 0.0005577744450420141
batch 90 loss: 0.0005576225696131587
batch 95 loss: 0.0005576525349169969
batch 100 loss: 0.0005577103933319449
batch 105 loss: 0.0005576835479587317
batch 110 loss: 0.0005576478317379952
batch 115 loss: 0.0005577405449002981
batch 120 loss: 0.0005576944560743869
batch 125 loss: 0.0005576936178840697
batch 130 loss: 0.000557676237076521
batch 135 loss: 0.0005576667841523886
batch 140 loss: 0.0005576725117862225
batch 145 loss: 0.0005577257368713617
batch 150 loss: 0.0005576310330070555
batch 155 loss: 0.0005577568663284183
batch 160 loss: 0.0005576414638198912
batch 165 loss: 0.000557611440308392
batch 170 loss: 0.0005576648865826428
batch 175 loss: 0.0005576928379014134
batch 180 loss: 0.0005576882627792657
batch 185 loss: 0.0005576905910857022
batch 190 loss: 0.0005577180185355246
batch 195 loss: 0.0005578009877353907
batch 200 loss: 0.0005577411851845681
batch 205 loss: 0.0005576909519731998
batch 210 loss: 0.0005576498340815305
batch 215 loss: 0.0005576755735091865
batch 220 loss: 0.0005576213705353438
batch 225 loss: 0.0005576885072514415
batch 230 loss: 0.0005577016738243401
batch 235 loss: 0.0005576421855948866
batch 240 loss: 0.0005576438154093922
Training Loss: 0.0005576834354239206
Validation Loss: 0.0005576933986352135
Epoch 68:
batch 5 loss: 0.000557587796356529
batch 10 loss: 0.000557669228874147
batch 15 loss: 0.0005577176227234304
batch 20 loss: 0.000557733851019293
batch 25 loss: 0.0005576972849667073
batch 30 loss: 0.0005576669471338391
batch 35 loss: 0.0005577062722295523
batch 40 loss: 0.0005576834897510708
batch 45 loss: 0.0005576992873102427
batch 50 loss: 0.0005576535710133612
batch 55 loss: 0.000557641708292067
batch 60 loss: 0.0005577735952101648
batch 65 loss: 0.00055775735527277
batch 70 loss: 0.000557647307869047
batch 75 loss: 0.000557715876493603
batch 80 loss: 0.000557662092614919
batch 85 loss: 0.0005576313589699567
batch 90 loss: 0.0005577062373049557
batch 95 loss: 0.0005576108349487185
batch 100 loss: 0.0005575853167101741
batch 105 loss: 0.0005577795556746424
batch 110 loss: 0.000557692744769156
batch 115 loss: 0.0005577124771662057
batch 120 loss: 0.0005575767019763589
batch 125 loss: 0.000557660253252834
batch 130 loss: 0.0005576767493039369
batch 135 loss: 0.0005576715338975191
batch 140 loss: 0.0005576433148235082
batch 145 loss: 0.0005576377385295928
batch 150 loss: 0.0005577430012635887
batch 155 loss: 0.0005577839445322752
batch 160 loss: 0.000557715620379895
batch 165 loss: 0.0005576948751695454
batch 170 loss: 0.0005575782619416713
batch 175 loss: 0.0005576768890023232
batch 180 loss: 0.0005576704046688974
batch 185 loss: 0.0005576662835665048
batch 190 loss: 0.0005578302429057658
batch 195 loss: 0.0005578249576501549
batch 200 loss: 0.0005577522446401417
batch 205 loss: 0.0005575341987423599
batch 210 loss: 0.0005577060510404408
batch 215 loss: 0.0005576738622039557
batch 220 loss: 0.0005576697760261596
batch 225 loss: 0.0005576102645136416
batch 230 loss: 0.0005577650852501392
batch 235 loss: 0.0005576679948717355
batch 240 loss: 0.0005576423485763371
Training Loss: 0.0005576834252375799
Validation Loss: 0.0005576934306494271
Epoch 69:
batch 5 loss: 0.0005575901595875621
batch 10 loss: 0.0005576476687565446
batch 15 loss: 0.0005576575873419643
batch 20 loss: 0.0005576483672484756
batch 25 loss: 0.0005576891242526471
batch 30 loss: 0.0005576550262048841
batch 35 loss: 0.000557738111820072
batch 40 loss: 0.0005577945383265614
batch 45 loss: 0.0005576565745286644
batch 50 loss: 0.0005576643859967589
batch 55 loss: 0.0005577808129601181
batch 60 loss: 0.0005577054456807673
batch 65 loss: 0.0005575594841502607
batch 70 loss: 0.0005576773779466748
batch 75 loss: 0.00055768076563254
batch 80 loss: 0.0005577669828198851
batch 85 loss: 0.0005577233037911356
batch 90 loss: 0.0005575713119469583
batch 95 loss: 0.0005577012430876494
batch 100 loss: 0.0005577067844569683
batch 105 loss: 0.0005577245145104825
batch 110 loss: 0.0005576824769377708
batch 115 loss: 0.0005576722207479179
batch 120 loss: 0.0005576849216595292
batch 125 loss: 0.0005576544790528715
batch 130 loss: 0.0005576141062192619
batch 135 loss: 0.0005577401723712682
batch 140 loss: 0.0005576684721745551
batch 145 loss: 0.0005576550960540771
batch 150 loss: 0.0005576196475885808
batch 155 loss: 0.0005578326294198632
batch 160 loss: 0.0005577227333560586
batch 165 loss: 0.0005576537572778761
batch 170 loss: 0.000557719252537936
batch 175 loss: 0.0005576320691034198
batch 180 loss: 0.0005576918483711779
batch 185 loss: 0.0005577218253165483
batch 190 loss: 0.0005576432682573796
batch 195 loss: 0.0005576128838583827
batch 200 loss: 0.0005577222560532391
batch 205 loss: 0.0005576311377808452
batch 210 loss: 0.000557682290673256
batch 215 loss: 0.0005577231640927494
batch 220 loss: 0.0005576075054705143
batch 225 loss: 0.0005577495554462075
batch 230 loss: 0.0005576490773819387
batch 235 loss: 0.0005577753763645887
batch 240 loss: 0.0005577034200541676
Training Loss: 0.0005576834419722825
Validation Loss: 0.0005576936392268787
Epoch 70:
batch 5 loss: 0.0005575814167968929
batch 10 loss: 0.0005576975643634796
batch 15 loss: 0.0005577478092163801
batch 20 loss: 0.000557719066273421
batch 25 loss: 0.0005576675524935126
batch 30 loss: 0.0005577379488386214
batch 35 loss: 0.0005576635128818452
batch 40 loss: 0.0005576943629421293
batch 45 loss: 0.0005577410687692463
batch 50 loss: 0.0005576514289714396
batch 55 loss: 0.0005577696487307548
batch 60 loss: 0.000557641068007797
batch 65 loss: 0.0005576971801929176
batch 70 loss: 0.0005576812429353595
batch 75 loss: 0.0005576935014687479
batch 80 loss: 0.0005576958879828453
batch 85 loss: 0.0005576935247518123
batch 90 loss: 0.0005578139913268388
batch 95 loss: 0.0005576158408075571
batch 100 loss: 0.0005576522671617568
batch 105 loss: 0.0005576522788032889
batch 110 loss: 0.0005576751427724957
batch 115 loss: 0.0005576361087150872
batch 120 loss: 0.0005577667034231126
batch 125 loss: 0.0005577435833401978
batch 130 loss: 0.0005575438379310071
batch 135 loss: 0.0005576572963036597
batch 140 loss: 0.0005576810915954411
batch 145 loss: 0.0005577007541432977
batch 150 loss: 0.0005576272611506284
batch 155 loss: 0.0005576347233727574
batch 160 loss: 0.0005576731055043638
batch 165 loss: 0.0005576786585152149
batch 170 loss: 0.0005577243166044355
batch 175 loss: 0.0005577910924330354
batch 180 loss: 0.000557691091671586
batch 185 loss: 0.0005576776689849794
batch 190 loss: 0.0005576794617809355
batch 195 loss: 0.0005576399271376431
batch 200 loss: 0.0005576703348197043
batch 205 loss: 0.0005576853407546877
batch 210 loss: 0.0005576795199885964
batch 215 loss: 0.0005575813818722963
batch 220 loss: 0.0005576045601628721
batch 225 loss: 0.000557784887496382
batch 230 loss: 0.0005576650612056256
batch 235 loss: 0.0005577749572694302
batch 240 loss: 0.0005576284718699753
Training Loss: 0.0005576834271778352
Validation Loss: 0.0005576934713947897
Epoch 71:
batch 5 loss: 0.0005576619179919363
batch 10 loss: 0.000557733082678169
batch 15 loss: 0.0005576826515607536
batch 20 loss: 0.0005576673429459333
batch 25 loss: 0.0005576068419031799
batch 30 loss: 0.0005576509982347488
batch 35 loss: 0.000557585246860981
batch 40 loss: 0.0005576594616286456
batch 45 loss: 0.0005576451541855932
batch 50 loss: 0.0005576522089540958
batch 55 loss: 0.0005576417315751314
batch 60 loss: 0.0005577056086622179
batch 65 loss: 0.0005577087984420359
batch 70 loss: 0.0005576213705353438
batch 75 loss: 0.0005575834424234926
batch 80 loss: 0.0005575686460360885
batch 85 loss: 0.0005576571100391447
batch 90 loss: 0.000557718810159713
batch 95 loss: 0.0005576961091719567
batch 100 loss: 0.0005576282739639283
batch 105 loss: 0.0005575891234911978
batch 110 loss: 0.0005576513591222465
batch 115 loss: 0.0005577528150752187
batch 120 loss: 0.0005577461328357458
batch 125 loss: 0.0005576315918006003
batch 130 loss: 0.0005576803698204458
batch 135 loss: 0.0005576397990807891
batch 140 loss: 0.0005576749215833842
batch 145 loss: 0.0005577740143053233
batch 150 loss: 0.0005577216157689691
batch 155 loss: 0.0005577795207500457
batch 160 loss: 0.0005577021045610308
batch 165 loss: 0.0005576253868639469
batch 170 loss: 0.0005577205098234117
batch 175 loss: 0.0005577211850322783
batch 180 loss: 0.0005576626514084637
batch 185 loss: 0.0005578136886470019
batch 190 loss: 0.0005576542695052922
batch 195 loss: 0.0005577527685090899
batch 200 loss: 0.000557749264407903
batch 205 loss: 0.00055765884462744
batch 210 loss: 0.0005577157251536846
batch 215 loss: 0.0005576483905315399
batch 220 loss: 0.0005576425115577877
batch 225 loss: 0.0005576892406679689
batch 230 loss: 0.0005576154100708664
batch 235 loss: 0.0005578565411269665
batch 240 loss: 0.0005578593234531581
Training Loss: 0.0005576834143236434
Validation Loss: 0.0005576934442312146
Epoch 72:
batch 5 loss: 0.0005577095318585635
batch 10 loss: 0.0005577870761044323
batch 15 loss: 0.0005576460273005069
batch 20 loss: 0.0005576601019129157
batch 25 loss: 0.0005577110452577472
batch 30 loss: 0.0005575972143560648
batch 35 loss: 0.0005577455856837332
batch 40 loss: 0.0005577142932452261
batch 45 loss: 0.0005577903124503791
batch 50 loss: 0.0005577327217906713
batch 55 loss: 0.00055754006607458
batch 60 loss: 0.0005576427793130279
batch 65 loss: 0.0005577647942118346
batch 70 loss: 0.0005576099851168692
batch 75 loss: 0.0005578006617724896
batch 80 loss: 0.0005576680530793964
batch 85 loss: 0.0005576833616942167
batch 90 loss: 0.0005574529757723212
batch 95 loss: 0.0005577040486969054
batch 100 loss: 0.0005576847936026752
batch 105 loss: 0.0005577256320975721
batch 110 loss: 0.0005576857016421855
batch 115 loss: 0.0005576463765464724
batch 120 loss: 0.0005576606374233962
batch 125 loss: 0.0005577760515734553
batch 130 loss: 0.000557733851019293
batch 135 loss: 0.0005576181691139936
batch 140 loss: 0.0005576307885348797
batch 145 loss: 0.0005575552931986749
batch 150 loss: 0.00055762481642887
batch 155 loss: 0.0005577154573984444
batch 160 loss: 0.0005577201140113175
batch 165 loss: 0.0005576269351877273
batch 170 loss: 0.0005577592877671122
batch 175 loss: 0.0005576128140091896
batch 180 loss: 0.0005576150491833687
batch 185 loss: 0.0005576584371738136
batch 190 loss: 0.0005575795541517437
batch 195 loss: 0.0005578111275099217
batch 200 loss: 0.0005577907082624734
batch 205 loss: 0.0005576889961957932
batch 210 loss: 0.00055773543426767
batch 215 loss: 0.000557649158872664
batch 220 loss: 0.0005576522438786924
batch 225 loss: 0.0005578173673711717
batch 230 loss: 0.000557675352320075
batch 235 loss: 0.0005576859344728291
batch 240 loss: 0.0005577077856287361
Training Loss: 0.0005576834271778352
Validation Loss: 0.0005576934228884057
Epoch 73:
batch 5 loss: 0.0005576510331593454
batch 10 loss: 0.0005577227333560586
batch 15 loss: 0.0005577929085120558
batch 20 loss: 0.0005576448398642242
batch 25 loss: 0.0005575819290243089
batch 30 loss: 0.0005576093448325991
batch 35 loss: 0.0005576469702646136
batch 40 loss: 0.0005576989031396806
batch 45 loss: 0.0005576836527325213
batch 50 loss: 0.0005576619412750005
batch 55 loss: 0.0005576371564529836
batch 60 loss: 0.0005576623138040304
batch 65 loss: 0.0005576319177635014
batch 70 loss: 0.0005576145951636136
batch 75 loss: 0.0005576179944910109
batch 80 loss: 0.0005577422911301255
batch 85 loss: 0.000557715690229088
batch 90 loss: 0.0005575714050792158
batch 95 loss: 0.0005577168893069028
batch 100 loss: 0.0005577325471676886
batch 105 loss: 0.0005578142357990145
batch 110 loss: 0.0005576326628215611
batch 115 loss: 0.0005576647818088531
batch 120 loss: 0.0005577318253926933
batch 125 loss: 0.000557740917429328
batch 130 loss: 0.0005576940486207605
batch 135 loss: 0.0005577433621510863
batch 140 loss: 0.0005575866205617785
batch 145 loss: 0.0005575933726504445
batch 150 loss: 0.0005577178788371384
batch 155 loss: 0.0005576837225817144
batch 160 loss: 0.0005577042698860169
batch 165 loss: 0.0005575950257480144
batch 170 loss: 0.0005576237454079092
batch 175 loss: 0.0005577158299274743
batch 180 loss: 0.000557626853697002
batch 185 loss: 0.0005576739669777453
batch 190 loss: 0.0005576446419581771
batch 195 loss: 0.0005576880532316864
batch 200 loss: 0.0005577286588959396
batch 205 loss: 0.0005577016621828079
batch 210 loss: 0.0005576562252826989
batch 215 loss: 0.0005577416624873877
batch 220 loss: 0.0005576972966082394
batch 225 loss: 0.0005578223965130747
batch 230 loss: 0.0005577433505095542
batch 235 loss: 0.0005577084026299417
batch 240 loss: 0.0005577915580943227
Training Loss: 0.0005576834184466862
Validation Loss: 0.000557693443261087
Epoch 74:
batch 5 loss: 0.0005576639203354716
batch 10 loss: 0.0005576140363700687
batch 15 loss: 0.000557661394122988
batch 20 loss: 0.0005576280876994133
batch 25 loss: 0.0005576038733124733
batch 30 loss: 0.0005577321862801909
batch 35 loss: 0.0005576619761995972
batch 40 loss: 0.0005576988332904875
batch 45 loss: 0.0005577539210207761
batch 50 loss: 0.000557619275059551
batch 55 loss: 0.0005577079020440579
batch 60 loss: 0.000557697075419128
batch 65 loss: 0.0005576266790740192
batch 70 loss: 0.0005576757597737014
batch 75 loss: 0.0005576559226028622
batch 80 loss: 0.000557705492246896
batch 85 loss: 0.0005576823838055134
batch 90 loss: 0.0005578205455094576
batch 95 loss: 0.000557741429656744
batch 100 loss: 0.0005576122435741127
batch 105 loss: 0.0005577169358730316
batch 110 loss: 0.0005578098585829139
batch 115 loss: 0.0005576324532739818
batch 120 loss: 0.0005576626979745924
batch 125 loss: 0.0005577114527113736
batch 130 loss: 0.0005577696720138192
batch 135 loss: 0.0005576152820140123
batch 140 loss: 0.0005576272960752249
batch 145 loss: 0.0005577872274443507
batch 150 loss: 0.0005577272968366742
batch 155 loss: 0.0005576130817644299
batch 160 loss: 0.0005578558892011642
batch 165 loss: 0.0005577157600782812
batch 170 loss: 0.000557623989880085
batch 175 loss: 0.0005576601717621088
batch 180 loss: 0.0005576905095949769
batch 185 loss: 0.0005576879484578967
batch 190 loss: 0.0005576387979090214
batch 195 loss: 0.0005576917435973882
batch 200 loss: 0.0005576348630711436
batch 205 loss: 0.000557554920669645
batch 210 loss: 0.0005575973424129188
batch 215 loss: 0.0005577996256761253
batch 220 loss: 0.0005576268653385341
batch 225 loss: 0.000557571742683649
batch 230 loss: 0.0005577387055382133
batch 235 loss: 0.000557720789220184
batch 240 loss: 0.0005577569943852722
Training Loss: 0.0005576833927383025
Validation Loss: 0.0005576934015455966
Epoch 75:
batch 5 loss: 0.0005576452589593828
batch 10 loss: 0.0005576381459832192
batch 15 loss: 0.0005577106028795242
batch 20 loss: 0.0005577457603067159
batch 25 loss: 0.000557731103617698
batch 30 loss: 0.0005577194271609188
batch 35 loss: 0.0005575999501161277
batch 40 loss: 0.0005577005911618471
batch 45 loss: 0.000557657063473016
batch 50 loss: 0.0005577164236456155
batch 55 loss: 0.0005576409515924752
batch 60 loss: 0.0005576484370976686
batch 65 loss: 0.0005577494623139501
batch 70 loss: 0.0005576715338975191
batch 75 loss: 0.000557720847427845
batch 80 loss: 0.000557547970674932
batch 85 loss: 0.0005575744318775833
batch 90 loss: 0.0005576479830779135
batch 95 loss: 0.0005578519427217543
batch 100 loss: 0.0005576270981691777
batch 105 loss: 0.0005575496354140342
batch 110 loss: 0.0005577088915742934
batch 115 loss: 0.0005577010568231344
batch 120 loss: 0.0005577293457463383
batch 125 loss: 0.0005576567025855183
batch 130 loss: 0.0005576859810389578
batch 135 loss: 0.0005576633964665234
batch 140 loss: 0.0005576404160819948
batch 145 loss: 0.0005577390664257109
batch 150 loss: 0.0005577329196967184
batch 155 loss: 0.0005576583207584918
batch 160 loss: 0.0005576452589593828
batch 165 loss: 0.0005577760981395841
batch 170 loss: 0.0005577628500759602
batch 175 loss: 0.0005576218594796955
batch 180 loss: 0.0005576801020652055
batch 185 loss: 0.0005576933268457651
batch 190 loss: 0.0005577529082074761
batch 195 loss: 0.0005577465635724366
batch 200 loss: 0.0005576337222009897
batch 205 loss: 0.0005577962030656636
batch 210 loss: 0.0005576498107984662
batch 215 loss: 0.0005575755494646728
batch 220 loss: 0.0005577851203270257
batch 225 loss: 0.0005576379015110433
batch 230 loss: 0.0005576745723374188
batch 235 loss: 0.0005577065632678569
batch 240 loss: 0.0005576546536758542
Training Loss: 0.0005576834121408562
Validation Loss: 0.0005576934316195547
Epoch 76:
batch 5 loss: 0.0005577756441198289
batch 10 loss: 0.0005576460156589746
batch 15 loss: 0.0005576233379542828
batch 20 loss: 0.0005576397408731282
batch 25 loss: 0.0005576960509642958
batch 30 loss: 0.0005577213829383254
batch 35 loss: 0.0005576851428486407
batch 40 loss: 0.0005578230833634734
batch 45 loss: 0.0005576408468186856
batch 50 loss: 0.0005577077157795429
batch 55 loss: 0.0005576339899562299
batch 60 loss: 0.0005576005321927368
batch 65 loss: 0.0005577197414822876
batch 70 loss: 0.0005576260155066848
batch 75 loss: 0.0005577788106165826
batch 80 loss: 0.0005577026167884469
batch 85 loss: 0.0005578166921623052
batch 90 loss: 0.0005576940136961638
batch 95 loss: 0.0005576346302405
batch 100 loss: 0.000557690835557878
batch 105 loss: 0.0005577488918788731
batch 110 loss: 0.0005576684139668942
batch 115 loss: 0.0005577223142609
batch 120 loss: 0.0005576903349719942
batch 125 loss: 0.0005577651550993324
batch 130 loss: 0.0005577330477535725
batch 135 loss: 0.0005576805211603642
batch 140 loss: 0.00055763068376109
batch 145 loss: 0.0005576613475568593
batch 150 loss: 0.0005577123141847551
batch 155 loss: 0.0005576213472522796
batch 160 loss: 0.0005576772731728852
batch 165 loss: 0.0005576717900112272
batch 170 loss: 0.0005575809744186699
batch 175 loss: 0.0005576017661951483
batch 180 loss: 0.0005577185656875372
batch 185 loss: 0.0005577653064392507
batch 190 loss: 0.0005575874238274992
batch 195 loss: 0.0005576336407102644
batch 200 loss: 0.0005576627445407212
batch 205 loss: 0.0005577019182965159
batch 210 loss: 0.0005575825925916433
batch 215 loss: 0.0005576633266173303
batch 220 loss: 0.0005576829193159938
batch 225 loss: 0.0005576211493462324
batch 230 loss: 0.0005577477626502514
batch 235 loss: 0.0005577490897849202
batch 240 loss: 0.0005576640367507934
Training Loss: 0.0005576834060775582
Validation Loss: 0.0005576934219182779
Epoch 77:
batch 5 loss: 0.0005576461437158287
batch 10 loss: 0.0005576678202487528
batch 15 loss: 0.0005576506722718477
batch 20 loss: 0.0005577397067099809
batch 25 loss: 0.0005576004972681403
batch 30 loss: 0.0005577752483077348
batch 35 loss: 0.0005575733026489616
batch 40 loss: 0.0005576878087595105
batch 45 loss: 0.0005577006144449115
batch 50 loss: 0.0005577098461799323
batch 55 loss: 0.0005576704861596227
batch 60 loss: 0.0005578204989433288
batch 65 loss: 0.0005576509283855557
batch 70 loss: 0.0005577565054409206
batch 75 loss: 0.0005576915806159377
batch 80 loss: 0.000557650369592011
batch 85 loss: 0.0005577275878749788
batch 90 loss: 0.0005576304276473821
batch 95 loss: 0.0005577435484156012
batch 100 loss: 0.0005576298688538372
batch 105 loss: 0.0005575953051447869
batch 110 loss: 0.0005577225936576724
batch 115 loss: 0.000557598692830652
batch 120 loss: 0.0005578427342697978
batch 125 loss: 0.0005576602881774307
batch 130 loss: 0.0005577463423833251
batch 135 loss: 0.0005577383562922478
batch 140 loss: 0.0005574556067585945
batch 145 loss: 0.0005577549804002046
batch 150 loss: 0.000557728880085051
batch 155 loss: 0.0005576950963586569
batch 160 loss: 0.0005576114635914564
batch 165 loss: 0.0005577329313382507
batch 170 loss: 0.0005576889147050679
batch 175 loss: 0.00055764903081581
batch 180 loss: 0.0005576521274633705
batch 185 loss: 0.000557698414195329
batch 190 loss: 0.0005576887750066816
batch 195 loss: 0.0005576821626164019
batch 200 loss: 0.0005577533040195704
batch 205 loss: 0.0005578493583016098
batch 210 loss: 0.0005576922092586755
batch 215 loss: 0.0005575870396569371
batch 220 loss: 0.0005575872608460486
batch 225 loss: 0.000557648204267025
batch 230 loss: 0.0005575921153649688
batch 235 loss: 0.0005577222793363035
batch 240 loss: 0.00055770562030375
Training Loss: 0.0005576834072902177
Validation Loss: 0.0005576934384104485
Epoch 78:
batch 5 loss: 0.0005576832802034915
batch 10 loss: 0.0005576405907049775
batch 15 loss: 0.0005576937808655202
batch 20 loss: 0.0005577263305895031
batch 25 loss: 0.0005576716619543731
batch 30 loss: 0.0005577719886787236
batch 35 loss: 0.000557606271468103
batch 40 loss: 0.0005576711380854249
batch 45 loss: 0.0005577396950684488
batch 50 loss: 0.0005577104864642024
batch 55 loss: 0.0005576570983976126
batch 60 loss: 0.0005577257019467652
batch 65 loss: 0.0005576317547820508
batch 70 loss: 0.0005576006951741874
batch 75 loss: 0.0005576463649049401
batch 80 loss: 0.0005576974712312221
batch 85 loss: 0.0005576891358941794
batch 90 loss: 0.0005575939314439893
batch 95 loss: 0.0005576527211815119
batch 100 loss: 0.0005577040486969054
batch 105 loss: 0.000557634315919131
batch 110 loss: 0.0005576950847171247
batch 115 loss: 0.0005578200099989772
batch 120 loss: 0.0005576692405156791
batch 125 loss: 0.0005577040137723088
batch 130 loss: 0.0005576210096478462
batch 135 loss: 0.0005576320691034198
batch 140 loss: 0.0005575742339715362
batch 145 loss: 0.0005577223142609
batch 150 loss: 0.0005576957715675235
batch 155 loss: 0.0005576486699283123
batch 160 loss: 0.0005578119191341103
batch 165 loss: 0.0005577202537097037
batch 170 loss: 0.0005576706491410733
batch 175 loss: 0.0005577453644946218
batch 180 loss: 0.000557607167866081
batch 185 loss: 0.000557624117936939
batch 190 loss: 0.0005577574018388986
batch 195 loss: 0.000557615701109171
batch 200 loss: 0.0005576585768721998
batch 205 loss: 0.0005577433388680219
batch 210 loss: 0.000557743664830923
batch 215 loss: 0.0005576292984187603
batch 220 loss: 0.0005576520692557096
batch 225 loss: 0.0005578610696829855
batch 230 loss: 0.0005576102761551738
batch 235 loss: 0.0005576993455179036
batch 240 loss: 0.0005577226867899299
Training Loss: 0.0005576834121408562
Validation Loss: 0.0005576934374403209
Epoch 79:
batch 5 loss: 0.0005576724768616259
batch 10 loss: 0.0005577337229624391
batch 15 loss: 0.0005576254916377366
batch 20 loss: 0.0005576129304245114
batch 25 loss: 0.0005576347466558218
batch 30 loss: 0.0005576648865826428
batch 35 loss: 0.000557770801242441
batch 40 loss: 0.000557653431314975
batch 45 loss: 0.0005577366682700813
batch 50 loss: 0.0005577251431532204
batch 55 loss: 0.0005577003350481391
batch 60 loss: 0.0005576165858656168
batch 65 loss: 0.0005577081697992981
batch 70 loss: 0.0005576872499659657
batch 75 loss: 0.0005578031879849732
batch 80 loss: 0.0005576987867243588
batch 85 loss: 0.0005577031057327986
batch 90 loss: 0.0005577247124165296
batch 95 loss: 0.0005576289724558592
batch 100 loss: 0.0005576313938945532
batch 105 loss: 0.000557587156072259
batch 110 loss: 0.0005576685187406838
batch 115 loss: 0.0005576185183599591
batch 120 loss: 0.000557581102475524
batch 125 loss: 0.0005577367497608066
batch 130 loss: 0.0005577077856287361
batch 135 loss: 0.0005577438161708415
batch 140 loss: 0.0005577466101385653
batch 145 loss: 0.0005576784024015069
batch 150 loss: 0.0005577306612394751
batch 155 loss: 0.0005577102303504944
batch 160 loss: 0.0005578140029683709
batch 165 loss: 0.0005576237919740379
batch 170 loss: 0.0005576400901190937
batch 175 loss: 0.0005575802875682712
batch 180 loss: 0.0005577897420153022
batch 185 loss: 0.0005577201838605106
batch 190 loss: 0.0005576146766543388
batch 195 loss: 0.0005577120115049184
batch 200 loss: 0.000557673629373312
batch 205 loss: 0.0005577768664807081
batch 210 loss: 0.0005576043156906962
batch 215 loss: 0.0005577213247306645
batch 220 loss: 0.0005577262374572456
batch 225 loss: 0.0005576249910518527
batch 230 loss: 0.0005576333263888955
batch 235 loss: 0.000557724095415324
batch 240 loss: 0.0005575816729106009
Training Loss: 0.0005576834082603455
Validation Loss: 0.0005576934044559796
Epoch 80:
batch 5 loss: 0.0005577223142609
batch 10 loss: 0.0005577245377935469
batch 15 loss: 0.0005576319759711623
batch 20 loss: 0.0005577343283221126
batch 25 loss: 0.0005576938623562455
batch 30 loss: 0.0005575891700573266
batch 35 loss: 0.0005576558411121369
batch 40 loss: 0.0005577560514211655
batch 45 loss: 0.0005575782037340104
batch 50 loss: 0.0005576871801167727
batch 55 loss: 0.0005576922674663364
batch 60 loss: 0.0005576821160502732
batch 65 loss: 0.0005576223717071116
batch 70 loss: 0.0005576863302849233
batch 75 loss: 0.0005576878320425749
batch 80 loss: 0.0005577727919444442
batch 85 loss: 0.0005575771443545819
batch 90 loss: 0.0005575729301199317
batch 95 loss: 0.0005577136529609561
batch 100 loss: 0.0005577096249908209
batch 105 loss: 0.0005576895782724023
batch 110 loss: 0.0005576180526986718
batch 115 loss: 0.0005577375646680594
batch 120 loss: 0.0005577324074693025
batch 125 loss: 0.0005578547134064138
batch 130 loss: 0.0005577244563028216
batch 135 loss: 0.0005577700212597847
batch 140 loss: 0.0005576702300459146
batch 145 loss: 0.0005576307768933475
batch 150 loss: 0.0005576741183176637
batch 155 loss: 0.0005577273783273995
batch 160 loss: 0.0005576771218329668
batch 165 loss: 0.0005576923838816583
batch 170 loss: 0.0005577186238951981
batch 175 loss: 0.0005576955969445408
batch 180 loss: 0.0005576185183599591
batch 185 loss: 0.000557777809444815
batch 190 loss: 0.0005577220697887241
batch 195 loss: 0.000557515665423125
batch 200 loss: 0.0005576895899139344
batch 205 loss: 0.0005575510556809604
batch 210 loss: 0.00055775111541152
batch 215 loss: 0.0005576849915087223
batch 220 loss: 0.0005577052710577845
batch 225 loss: 0.000557681976351887
batch 230 loss: 0.0005576511612161994
batch 235 loss: 0.0005577375995926559
batch 240 loss: 0.0005576129886321723
Training Loss: 0.000557683403409707
Validation Loss: 0.0005576934626636405
Epoch 81:
batch 5 loss: 0.0005576829542405903
batch 10 loss: 0.0005576947587542236
batch 15 loss: 0.0005576798226684332
batch 20 loss: 0.0005576672963798046
batch 25 loss: 0.0005576930940151215
batch 30 loss: 0.0005577417090535164
batch 35 loss: 0.000557745958212763
batch 40 loss: 0.000557658018078655
batch 45 loss: 0.0005576728843152523
batch 50 loss: 0.0005577963660471141
batch 55 loss: 0.0005577641888521612
batch 60 loss: 0.0005577121395617723
batch 65 loss: 0.0005576492636464536
batch 70 loss: 0.0005576838157139719
batch 75 loss: 0.0005577428848482669
batch 80 loss: 0.0005577280186116696
batch 85 loss: 0.0005577361560426652
batch 90 loss: 0.0005576729541644454
batch 95 loss: 0.0005577406147494913
batch 100 loss: 0.0005576987285166978
batch 105 loss: 0.000557592895347625
batch 110 loss: 0.000557712942827493
batch 115 loss: 0.0005575856659561396
batch 120 loss: 0.000557744805701077
batch 125 loss: 0.0005576410330832005
batch 130 loss: 0.0005577115574851632
batch 135 loss: 0.0005576319410465658
batch 140 loss: 0.0005577408825047314
batch 145 loss: 0.0005576335359364748
batch 150 loss: 0.0005578155862167478
batch 155 loss: 0.0005576003575697541
batch 160 loss: 0.0005577099858783185
batch 165 loss: 0.0005575736635364592
batch 170 loss: 0.0005577110103331506
batch 175 loss: 0.0005575619172304869
batch 180 loss: 0.000557626155205071
batch 185 loss: 0.0005576750845648348
batch 190 loss: 0.0005576911033131182
batch 195 loss: 0.0005576447583734989
batch 200 loss: 0.0005577497999183833
batch 205 loss: 0.0005576437688432634
batch 210 loss: 0.0005576267139986157
batch 215 loss: 0.0005576632567681373
batch 220 loss: 0.0005576930823735892
batch 225 loss: 0.0005576771334744989
batch 230 loss: 0.0005577533855102956
batch 235 loss: 0.0005576071795076132
batch 240 loss: 0.0005576220457442105
Training Loss: 0.0005576833932233664
Validation Loss: 0.0005576933986352135
Epoch 82:
batch 5 loss: 0.0005577275063842535
batch 10 loss: 0.0005576695082709193
batch 15 loss: 0.0005576974828727544
batch 20 loss: 0.0005576718249358237
batch 25 loss: 0.0005577988224104047
batch 30 loss: 0.0005577472154982388
batch 35 loss: 0.0005576768191531301
batch 40 loss: 0.000557695038150996
batch 45 loss: 0.0005576891475357115
batch 50 loss: 0.0005576867493800819
batch 55 loss: 0.0005576291936449706
batch 60 loss: 0.000557705550454557
batch 65 loss: 0.0005576489144004882
batch 70 loss: 0.000557659799233079
batch 75 loss: 0.0005575840827077627
batch 80 loss: 0.0005576730472967029
batch 85 loss: 0.0005576389958150685
batch 90 loss: 0.0005576101946644485
batch 95 loss: 0.000557726388797164
batch 100 loss: 0.0005576011259108782
batch 105 loss: 0.0005577200558036566
batch 110 loss: 0.0005576726980507374
batch 115 loss: 0.0005577364470809698
batch 120 loss: 0.0005577802890911699
batch 125 loss: 0.0005576007766649127
batch 130 loss: 0.0005576233961619437
batch 135 loss: 0.0005576668423600495
batch 140 loss: 0.000557537004351616
batch 145 loss: 0.000557743210811168
batch 150 loss: 0.0005577911972068251
batch 155 loss: 0.000557721802033484
batch 160 loss: 0.0005577521515078843
batch 165 loss: 0.0005576742580160499
batch 170 loss: 0.0005577354459092021
batch 175 loss: 0.0005576083902269601
batch 180 loss: 0.0005576216382905841
batch 185 loss: 0.0005575992050580681
batch 190 loss: 0.0005575974355451763
batch 195 loss: 0.0005577246192842722
batch 200 loss: 0.0005577604635618628
batch 205 loss: 0.0005577454925514757
batch 210 loss: 0.0005575901828706265
batch 215 loss: 0.0005576871801167727
batch 220 loss: 0.0005577380303293467
batch 225 loss: 0.0005577344563789666
batch 230 loss: 0.000557619717437774
batch 235 loss: 0.0005576993455179036
batch 240 loss: 0.0005577835720032454
Training Loss: 0.0005576833898279195
Validation Loss: 0.0005576934267689164
Epoch 83:
batch 5 loss: 0.0005577065516263246
batch 10 loss: 0.0005577137460932135
batch 15 loss: 0.0005577398464083671
batch 20 loss: 0.0005577246192842722
batch 25 loss: 0.0005575063405558467
batch 30 loss: 0.0005576974828727544
batch 35 loss: 0.0005576402763836086
batch 40 loss: 0.0005577316274866462
batch 45 loss: 0.0005575865623541176
batch 50 loss: 0.0005576331284828484
batch 55 loss: 0.0005577468196861446
batch 60 loss: 0.0005576154333539307
batch 65 loss: 0.0005578063894063235
batch 70 loss: 0.0005576317198574543
batch 75 loss: 0.0005577688454650343
batch 80 loss: 0.0005576912080869079
batch 85 loss: 0.0005576217779889702
batch 90 loss: 0.0005575878312811255
batch 95 loss: 0.0005576823372393846
batch 100 loss: 0.0005577305098995566
batch 105 loss: 0.0005577038624323905
batch 110 loss: 0.0005576953408308327
batch 115 loss: 0.0005576349445618689
batch 120 loss: 0.0005575853865593672
batch 125 loss: 0.0005577625124715269
batch 130 loss: 0.0005577449570409954
batch 135 loss: 0.0005576753173954785
batch 140 loss: 0.000557568040676415
batch 145 loss: 0.0005577534437179565
batch 150 loss: 0.00055777597008273
batch 155 loss: 0.000557652662973851
batch 160 loss: 0.0005577822914347053
batch 165 loss: 0.000557703513186425
batch 170 loss: 0.0005576667725108563
batch 175 loss: 0.00055760710965842
batch 180 loss: 0.0005577152129262686
batch 185 loss: 0.0005577192991040647
batch 190 loss: 0.0005577023839578032
batch 195 loss: 0.0005576773546636105
batch 200 loss: 0.0005577723379246891
batch 205 loss: 0.0005576802417635917
batch 210 loss: 0.0005575701827183366
batch 215 loss: 0.0005577523028478026
batch 220 loss: 0.0005577967967838049
batch 225 loss: 0.0005576520110480487
batch 230 loss: 0.0005576373543590308
batch 235 loss: 0.000557596585713327
batch 240 loss: 0.0005576555035077035
Training Loss: 0.0005576833905555152
Validation Loss: 0.0005576933957248306
Epoch 84:
batch 5 loss: 0.0005576473544351756
batch 10 loss: 0.0005576908471994102
batch 15 loss: 0.000557672162540257
batch 20 loss: 0.000557741813827306
batch 25 loss: 0.0005578448763117194
batch 30 loss: 0.0005576687981374562
batch 35 loss: 0.0005577033734880388
batch 40 loss: 0.0005576837342232466
batch 45 loss: 0.0005576692870818079
batch 50 loss: 0.000557705934625119
batch 55 loss: 0.0005576400901190937
batch 60 loss: 0.0005577079020440579
batch 65 loss: 0.000557656737510115
batch 70 loss: 0.0005576820578426123
batch 75 loss: 0.0005576303112320602
batch 80 loss: 0.0005576764699071646
batch 85 loss: 0.0005575738032348454
batch 90 loss: 0.0005576580064371228
batch 95 loss: 0.0005576337804086507
batch 100 loss: 0.0005577062489464879
batch 105 loss: 0.0005576275405474007
batch 110 loss: 0.0005576692405156791
batch 115 loss: 0.0005576378549449146
batch 120 loss: 0.0005577026517130434
batch 125 loss: 0.0005578016163781286
batch 130 loss: 0.0005577591829933227
batch 135 loss: 0.0005575727205723524
batch 140 loss: 0.0005577146541327239
batch 145 loss: 0.0005576999625191093
batch 150 loss: 0.0005576465628109872
batch 155 loss: 0.0005576483905315399
batch 160 loss: 0.0005576582276262343
batch 165 loss: 0.0005577565869316458
batch 170 loss: 0.0005575953284278512
batch 175 loss: 0.0005575832328759134
batch 180 loss: 0.0005577054689638317
batch 185 loss: 0.0005577742587774992
batch 190 loss: 0.0005577197298407555
batch 195 loss: 0.0005577601725235582
batch 200 loss: 0.0005576403462328016
batch 205 loss: 0.000557775842025876
batch 210 loss: 0.0005577157018706203
batch 215 loss: 0.0005576128722168505
batch 220 loss: 0.0005576515686698258
batch 225 loss: 0.0005577150965109468
batch 230 loss: 0.0005576550378464162
batch 235 loss: 0.0005577044445089996
batch 240 loss: 0.0005577045027166605
Training Loss: 0.0005576833830370257
Validation Loss: 0.0005576934063962351
Epoch 85:
batch 5 loss: 0.0005576238851062954
batch 10 loss: 0.0005575896939262748
batch 15 loss: 0.000557695736642927
batch 20 loss: 0.0005577210569754243
batch 25 loss: 0.0005578047130256891
batch 30 loss: 0.0005576769821345806
batch 35 loss: 0.0005576861440204084
batch 40 loss: 0.0005576410098001361
batch 45 loss: 0.0005576988798566163
batch 50 loss: 0.0005576537805609405
batch 55 loss: 0.0005576216150075197
batch 60 loss: 0.0005576285882852971
batch 65 loss: 0.0005576271563768387
batch 70 loss: 0.0005576284020207822
batch 75 loss: 0.0005576414288952947
batch 80 loss: 0.0005576574010774493
batch 85 loss: 0.0005577974719926715
batch 90 loss: 0.000557680381461978
batch 95 loss: 0.0005575672839768231
batch 100 loss: 0.0005576952011324465
batch 105 loss: 0.0005576301948167384
batch 110 loss: 0.0005577114061452448
batch 115 loss: 0.0005577074829488992
batch 120 loss: 0.0005577797070145607
batch 125 loss: 0.0005578216398134828
batch 130 loss: 0.0005577061208896339
batch 135 loss: 0.0005577419302426279
batch 140 loss: 0.0005577833973802627
batch 145 loss: 0.000557724165264517
batch 150 loss: 0.0005576971685513854
batch 155 loss: 0.0005576810450293124
batch 160 loss: 0.0005576960160396993
batch 165 loss: 0.0005576145951636136
batch 170 loss: 0.0005577979376539588
batch 175 loss: 0.0005577412783168256
batch 180 loss: 0.0005576763418503105
batch 185 loss: 0.0005576276918873191
batch 190 loss: 0.0005577155505307018
batch 195 loss: 0.0005576057243160904
batch 200 loss: 0.0005576260853558778
batch 205 loss: 0.000557700835634023
batch 210 loss: 0.0005575741990469397
batch 215 loss: 0.0005577063770033419
batch 220 loss: 0.0005577341187745332
batch 225 loss: 0.0005576869356445968
batch 230 loss: 0.0005575848394073546
batch 235 loss: 0.0005576665513217449
batch 240 loss: 0.0005577264353632927
Training Loss: 0.0005576833871600683
Validation Loss: 0.000557693427739044
Epoch 86:
batch 5 loss: 0.0005576917319558561
batch 10 loss: 0.0005576139665208757
batch 15 loss: 0.0005576472729444503
batch 20 loss: 0.0005576464114710689
batch 25 loss: 0.0005575964692980051
batch 30 loss: 0.0005576752009801567
batch 35 loss: 0.0005577043048106134
batch 40 loss: 0.000557741557713598
batch 45 loss: 0.000557757681235671
batch 50 loss: 0.000557654129806906
batch 55 loss: 0.0005576193914748728
batch 60 loss: 0.0005576501484028995
batch 65 loss: 0.0005577054340392351
batch 70 loss: 0.0005576529190875589
batch 75 loss: 0.0005576289957389235
batch 80 loss: 0.0005576474941335618
batch 85 loss: 0.0005576070281676949
batch 90 loss: 0.0005577099858783185
batch 95 loss: 0.0005576215335167944
batch 100 loss: 0.0005577064584940672
batch 105 loss: 0.0005576282390393316
batch 110 loss: 0.0005576376570388675
batch 115 loss: 0.0005578529089689255
batch 120 loss: 0.0005576365976594389
batch 125 loss: 0.0005576330469921232
batch 130 loss: 0.0005577840842306614
batch 135 loss: 0.0005577033618465066
batch 140 loss: 0.0005577338975854218
batch 145 loss: 0.0005576229654252529
batch 150 loss: 0.0005576509633101523
batch 155 loss: 0.0005576546187512577
batch 160 loss: 0.0005576372728683055
batch 165 loss: 0.0005576909519731998
batch 170 loss: 0.0005576740368269384
batch 175 loss: 0.000557711673900485
batch 180 loss: 0.0005576523486524821
batch 185 loss: 0.000557716703042388
batch 190 loss: 0.0005577659234404564
batch 195 loss: 0.000557754235342145
batch 200 loss: 0.0005577868665568531
batch 205 loss: 0.0005576309515163303
batch 210 loss: 0.0005577529082074761
batch 215 loss: 0.0005576676572673023
batch 220 loss: 0.0005577602772973478
batch 225 loss: 0.0005578247713856399
batch 230 loss: 0.0005576372146606446
batch 235 loss: 0.0005577069125138224
batch 240 loss: 0.0005576153984293341
Training Loss: 0.0005576833866750045
Validation Loss: 0.0005576934471415977
Epoch 87:
batch 5 loss: 0.0005576447118073701
batch 10 loss: 0.0005576937808655202
batch 15 loss: 0.0005577110685408115
batch 20 loss: 0.0005575747811235488
batch 25 loss: 0.0005576113937422634
batch 30 loss: 0.0005576997296884656
batch 35 loss: 0.0005575664574280382
batch 40 loss: 0.0005576660856604576
batch 45 loss: 0.0005576673313044012
batch 50 loss: 0.0005575913120992482
batch 55 loss: 0.0005576673895120621
batch 60 loss: 0.0005578154115937651
batch 65 loss: 0.000557660311460495
batch 70 loss: 0.0005577921052463353
batch 75 loss: 0.0005576668540015816
batch 80 loss: 0.0005576708121225238
batch 85 loss: 0.0005577104049734772
batch 90 loss: 0.0005577599862590432
batch 95 loss: 0.0005577301955781877
batch 100 loss: 0.0005577038042247296
batch 105 loss: 0.0005576739087700844
batch 110 loss: 0.000557718554046005
batch 115 loss: 0.0005576820229180157
batch 120 loss: 0.0005575910210609436
batch 125 loss: 0.0005577467498369515
batch 130 loss: 0.0005576688097789883
batch 135 loss: 0.0005575975985266269
batch 140 loss: 0.0005577683681622147
batch 145 loss: 0.0005577520234510303
batch 150 loss: 0.0005576979951001704
batch 155 loss: 0.0005577329080551863
batch 160 loss: 0.0005576987634412945
batch 165 loss: 0.0005576770287007094
batch 170 loss: 0.0005576791591010988
batch 175 loss: 0.0005577427335083484
batch 180 loss: 0.0005576558061875403
batch 185 loss: 0.0005576556432060897
batch 190 loss: 0.0005576724070124328
batch 195 loss: 0.0005577276227995753
batch 200 loss: 0.0005576492636464536
batch 205 loss: 0.0005577256553806365
batch 210 loss: 0.0005576584022492171
batch 215 loss: 0.0005575855146162211
batch 220 loss: 0.000557773548644036
batch 225 loss: 0.0005576714058406651
batch 230 loss: 0.0005576679832302034
batch 235 loss: 0.0005577199161052703
batch 240 loss: 0.0005576077615842223
Training Loss: 0.0005576833854623449
Validation Loss: 0.0005576934228884057
Epoch 88:
batch 5 loss: 0.0005576757597737014
batch 10 loss: 0.000557691790163517
batch 15 loss: 0.0005576419294811785
batch 20 loss: 0.0005576819297857582
batch 25 loss: 0.0005576726165600121
batch 30 loss: 0.0005576534895226359
batch 35 loss: 0.0005577280651777982
batch 40 loss: 0.0005576720344834029
batch 45 loss: 0.000557783362455666
batch 50 loss: 0.0005576271330937743
batch 55 loss: 0.0005577502772212029
batch 60 loss: 0.000557633675634861
batch 65 loss: 0.0005576997296884656
batch 70 loss: 0.0005576853989623487
batch 75 loss: 0.0005576596595346928
batch 80 loss: 0.000557735632173717
batch 85 loss: 0.0005576499388553202
batch 90 loss: 0.0005576952709816397
batch 95 loss: 0.0005577058531343937
batch 100 loss: 0.0005577025702223181
batch 105 loss: 0.0005576551659032703
batch 110 loss: 0.0005577106727287173
batch 115 loss: 0.0005576432566158473
batch 120 loss: 0.0005577090545557439
batch 125 loss: 0.0005577000090852379
batch 130 loss: 0.0005576863419264555
batch 135 loss: 0.000557617493905127
batch 140 loss: 0.0005576409748755395
batch 145 loss: 0.0005575731280259788
batch 150 loss: 0.0005576783092692495
batch 155 loss: 0.0005575803923420608
batch 160 loss: 0.0005577299743890762
batch 165 loss: 0.0005576911149546504
batch 170 loss: 0.0005576734081842005
batch 175 loss: 0.0005577149917371571
batch 180 loss: 0.0005577504518441856
batch 185 loss: 0.000557758763898164
batch 190 loss: 0.000557728426065296
batch 195 loss: 0.0005576664232648909
batch 200 loss: 0.0005576078779995442
batch 205 loss: 0.0005577133153565228
batch 210 loss: 0.0005577351665124297
batch 215 loss: 0.0005578254465945065
batch 220 loss: 0.000557801965624094
batch 225 loss: 0.0005576148629188538
batch 230 loss: 0.000557582697365433
batch 235 loss: 0.0005575766903348267
batch 240 loss: 0.0005576900090090931
Training Loss: 0.0005576833854623449
Validation Loss: 0.0005576933976650859
Epoch 89:
batch 5 loss: 0.0005577081115916372
batch 10 loss: 0.0005577244097366929
batch 15 loss: 0.0005576442694291472
batch 20 loss: 0.0005576375173404813
batch 25 loss: 0.0005575635004788637
batch 30 loss: 0.0005576728261075913
batch 35 loss: 0.0005578013486228883
batch 40 loss: 0.0005576652707532048
batch 45 loss: 0.0005577149800956249
batch 50 loss: 0.0005576146068051457
batch 55 loss: 0.000557692558504641
batch 60 loss: 0.0005577395670115948
batch 65 loss: 0.0005577044794335961
batch 70 loss: 0.0005576732568442821
batch 75 loss: 0.0005576430587098003
batch 80 loss: 0.000557677075266838
batch 85 loss: 0.0005577562609687447
batch 90 loss: 0.0005576777271926403
batch 95 loss: 0.0005576219060458243
batch 100 loss: 0.0005576433148235082
batch 105 loss: 0.0005576764931902289
batch 110 loss: 0.0005577266565524042
batch 115 loss: 0.000557663175277412
batch 120 loss: 0.0005576485767960548
batch 125 loss: 0.0005576679832302034
batch 130 loss: 0.0005576633615419268
batch 135 loss: 0.0005576340132392943
batch 140 loss: 0.0005576579016633332
batch 145 loss: 0.0005577201023697853
batch 150 loss: 0.0005576470866799355
batch 155 loss: 0.0005578129552304745
batch 160 loss: 0.00055769516620785
batch 165 loss: 0.0005576738272793591
batch 170 loss: 0.0005576641531661152
batch 175 loss: 0.0005577551899477839
batch 180 loss: 0.0005577002302743495
batch 185 loss: 0.0005578110576607287
batch 190 loss: 0.0005577416508458555
batch 195 loss: 0.0005575574818067252
batch 200 loss: 0.0005577317322604358
batch 205 loss: 0.0005576704046688974
batch 210 loss: 0.0005576443509198725
batch 215 loss: 0.0005576748866587877
batch 220 loss: 0.0005576295428909361
batch 225 loss: 0.0005576558993197978
batch 230 loss: 0.0005577466101385653
batch 235 loss: 0.0005577155738137662
batch 240 loss: 0.0005576402880251407
Training Loss: 0.0005576833832795576
Validation Loss: 0.0005576934025157243
Epoch 90:
batch 5 loss: 0.0005576364579610527
batch 10 loss: 0.0005576937459409237
batch 15 loss: 0.0005577571806497872
batch 20 loss: 0.0005576942930929363
batch 25 loss: 0.0005576414172537625
batch 30 loss: 0.000557652022689581
batch 35 loss: 0.0005575816147029399
batch 40 loss: 0.0005575983203016221
batch 45 loss: 0.0005575744202360511
batch 50 loss: 0.0005576503230258822
batch 55 loss: 0.0005576539551839232
batch 60 loss: 0.0005577720585279167
batch 65 loss: 0.0005577122676186264
batch 70 loss: 0.0005576014518737793
batch 75 loss: 0.0005577316856943071
batch 80 loss: 0.000557674327865243
batch 85 loss: 0.0005577092990279198
batch 90 loss: 0.0005577124655246734
batch 95 loss: 0.0005577237927354873
batch 100 loss: 0.0005577409523539245
batch 105 loss: 0.0005577415227890015
batch 110 loss: 0.0005576976691372693
batch 115 loss: 0.0005576622555963695
batch 120 loss: 0.0005577337462455035
batch 125 loss: 0.0005577501142397523
batch 130 loss: 0.000557621184270829
batch 135 loss: 0.0005577174481004477
batch 140 loss: 0.0005576656083576381
batch 145 loss: 0.0005576835246756673
batch 150 loss: 0.0005577026167884469
batch 155 loss: 0.0005576646770350635
batch 160 loss: 0.0005576873081736267
batch 165 loss: 0.0005577387171797454
batch 170 loss: 0.0005576032912358641
batch 175 loss: 0.0005576194613240659
batch 180 loss: 0.000557806168217212
batch 185 loss: 0.0005576881929300725
batch 190 loss: 0.0005577199626713991
batch 195 loss: 0.0005576402763836086
batch 200 loss: 0.0005576804163865745
batch 205 loss: 0.0005577503121457994
batch 210 loss: 0.0005576604045927525
batch 215 loss: 0.0005577078671194613
batch 220 loss: 0.0005576647236011922
batch 225 loss: 0.0005577601958066225
batch 230 loss: 0.0005576714291237294
batch 235 loss: 0.0005576778901740908
batch 240 loss: 0.000557573325932026
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576934209481503
Epoch 91:
batch 5 loss: 0.000557741173543036
batch 10 loss: 0.0005576981813646853
batch 15 loss: 0.000557748880237341
batch 20 loss: 0.0005576896481215953
batch 25 loss: 0.0005576690658926964
batch 30 loss: 0.000557667831890285
batch 35 loss: 0.000557730789296329
batch 40 loss: 0.0005575616960413754
batch 45 loss: 0.000557760486844927
batch 50 loss: 0.000557669170666486
batch 55 loss: 0.0005576308933086694
batch 60 loss: 0.0005577022209763526
batch 65 loss: 0.0005577085656113922
batch 70 loss: 0.0005577436415478588
batch 75 loss: 0.0005577504518441856
batch 80 loss: 0.0005576731287874282
batch 85 loss: 0.0005575719405896962
batch 90 loss: 0.0005576946772634983
batch 95 loss: 0.0005576621275395155
batch 100 loss: 0.0005576625582762063
batch 105 loss: 0.0005576992873102427
batch 110 loss: 0.0005577572737820446
batch 115 loss: 0.00055761334951967
batch 120 loss: 0.0005577120231464505
batch 125 loss: 0.0005577372969128192
batch 130 loss: 0.0005577276227995753
batch 135 loss: 0.0005575786926783621
batch 140 loss: 0.0005576147697865963
batch 145 loss: 0.0005576300667598843
batch 150 loss: 0.0005577087285928428
batch 155 loss: 0.0005577757023274898
batch 160 loss: 0.0005577587056905031
batch 165 loss: 0.0005577189498580992
batch 170 loss: 0.0005576491472311318
batch 175 loss: 0.000557761883828789
batch 180 loss: 0.000557632022537291
batch 185 loss: 0.0005576798808760941
batch 190 loss: 0.0005575177608989179
batch 195 loss: 0.0005578020471148192
batch 200 loss: 0.0005576287396252156
batch 205 loss: 0.0005576893803663552
batch 210 loss: 0.0005576439667493105
batch 215 loss: 0.0005576584953814745
batch 220 loss: 0.0005576498806476593
batch 225 loss: 0.0005577203701250255
batch 230 loss: 0.0005576302763074636
batch 235 loss: 0.0005576973780989647
batch 240 loss: 0.0005576715106144547
Training Loss: 0.0005576833820668981
Validation Loss: 0.0005576934093066181
Epoch 92:
batch 5 loss: 0.00055773543426767
batch 10 loss: 0.0005576136871241033
batch 15 loss: 0.0005577399395406246
batch 20 loss: 0.0005576976458542049
batch 25 loss: 0.0005576960276812315
batch 30 loss: 0.0005577037809416652
batch 35 loss: 0.0005576763302087784
batch 40 loss: 0.0005577606847509742
batch 45 loss: 0.0005576561321504414
batch 50 loss: 0.0005577510106377304
batch 55 loss: 0.0005577563075348735
batch 60 loss: 0.0005576077150180936
batch 65 loss: 0.000557625899091363
batch 70 loss: 0.000557946995832026
batch 75 loss: 0.0005577626288868487
batch 80 loss: 0.0005576412426307797
batch 85 loss: 0.0005575963645242154
batch 90 loss: 0.0005575816612690687
batch 95 loss: 0.0005576772149652242
batch 100 loss: 0.0005576111259870231
batch 105 loss: 0.0005575937451794743
batch 110 loss: 0.000557681021746248
batch 115 loss: 0.0005575940012931824
batch 120 loss: 0.0005576899740844965
batch 125 loss: 0.0005575930932536721
batch 130 loss: 0.0005576318129897117
batch 135 loss: 0.0005576082039624453
batch 140 loss: 0.0005577076459303498
batch 145 loss: 0.0005576904397457838
batch 150 loss: 0.0005576948868110776
batch 155 loss: 0.0005577036645263433
batch 160 loss: 0.0005576527444645762
batch 165 loss: 0.0005577126867137849
batch 170 loss: 0.0005575948627665639
batch 175 loss: 0.0005576922674663364
batch 180 loss: 0.0005577646777965128
batch 185 loss: 0.0005576942930929363
batch 190 loss: 0.000557731871958822
batch 195 loss: 0.0005576159222982824
batch 200 loss: 0.0005576093215495348
batch 205 loss: 0.0005577363539487123
batch 210 loss: 0.000557702302467078
batch 215 loss: 0.0005577668314799666
batch 220 loss: 0.0005576668889261782
batch 225 loss: 0.0005577026167884469
batch 230 loss: 0.000557645468506962
batch 235 loss: 0.0005577569012530148
batch 240 loss: 0.0005577298928983509
Training Loss: 0.0005576833796415788
Validation Loss: 0.0005576934131871288
Epoch 93:
batch 5 loss: 0.0005575176910497248
batch 10 loss: 0.0005577522679232061
batch 15 loss: 0.0005578106734901667
batch 20 loss: 0.0005575640476308763
batch 25 loss: 0.0005576478433795273
batch 30 loss: 0.0005575591931119561
batch 35 loss: 0.0005576781113632023
batch 40 loss: 0.0005577530129812658
batch 45 loss: 0.0005578265408985317
batch 50 loss: 0.0005577521747909487
batch 55 loss: 0.000557634187862277
batch 60 loss: 0.0005575230694375932
batch 65 loss: 0.0005577811854891479
batch 70 loss: 0.0005576928379014134
batch 75 loss: 0.0005577367148362101
batch 80 loss: 0.0005576723022386431
batch 85 loss: 0.0005576606956310571
batch 90 loss: 0.0005577027332037687
batch 95 loss: 0.0005577412899583578
batch 100 loss: 0.0005577101488597691
batch 105 loss: 0.0005576054099947214
batch 110 loss: 0.0005577917909249663
batch 115 loss: 0.0005576745606958866
batch 120 loss: 0.0005577408825047314
batch 125 loss: 0.0005576903349719942
batch 130 loss: 0.0005576282273977995
batch 135 loss: 0.0005576819414272905
batch 140 loss: 0.0005576617550104856
batch 145 loss: 0.0005576071445830167
batch 150 loss: 0.0005577075411565602
batch 155 loss: 0.0005576110444962979
batch 160 loss: 0.0005577126983553171
batch 165 loss: 0.0005577143980190158
batch 170 loss: 0.0005577111733146012
batch 175 loss: 0.0005577387288212776
batch 180 loss: 0.0005575831863097846
batch 185 loss: 0.0005576834781095385
batch 190 loss: 0.000557741429656744
batch 195 loss: 0.0005575984367169439
batch 200 loss: 0.0005578214419074357
batch 205 loss: 0.000557671207934618
batch 210 loss: 0.0005577971925958991
batch 215 loss: 0.0005577080650255084
batch 220 loss: 0.0005576205439865589
batch 225 loss: 0.0005576665163971484
batch 230 loss: 0.0005576584720984101
batch 235 loss: 0.0005576728144660592
batch 240 loss: 0.0005575850256718696
Training Loss: 0.0005576833784289193
Validation Loss: 0.0005576934228884057
Epoch 94:
batch 5 loss: 0.0005576109513640403
batch 10 loss: 0.0005576273426413537
batch 15 loss: 0.0005576722091063857
batch 20 loss: 0.0005577067262493074
batch 25 loss: 0.0005576632916927338
batch 30 loss: 0.0005576065625064075
batch 35 loss: 0.0005576924537308514
batch 40 loss: 0.0005577708827331663
batch 45 loss: 0.0005577626870945096
batch 50 loss: 0.0005577436531893909
batch 55 loss: 0.0005576762603595852
batch 60 loss: 0.0005577269708737731
batch 65 loss: 0.0005576939671300352
batch 70 loss: 0.0005577148054726422
batch 75 loss: 0.0005576963420026004
batch 80 loss: 0.0005575726507231593
batch 85 loss: 0.000557645969092846
batch 90 loss: 0.0005576753756031394
batch 95 loss: 0.0005577082629315555
batch 100 loss: 0.0005576058174483478
batch 105 loss: 0.0005576124880462885
batch 110 loss: 0.0005576867726631463
batch 115 loss: 0.0005577359814196825
batch 120 loss: 0.0005576442461460828
batch 125 loss: 0.0005576312309131026
batch 130 loss: 0.0005576925002969801
batch 135 loss: 0.0005575909977778793
batch 140 loss: 0.0005577328032813966
batch 145 loss: 0.0005577550618909299
batch 150 loss: 0.0005576988449320197
batch 155 loss: 0.0005577982752583921
batch 160 loss: 0.0005577421630732715
batch 165 loss: 0.0005577112780883909
batch 170 loss: 0.000557626795489341
batch 175 loss: 0.000557712756562978
batch 180 loss: 0.0005576614639721811
batch 185 loss: 0.0005576690309680998
batch 190 loss: 0.0005576933501288295
batch 195 loss: 0.000557603407651186
batch 200 loss: 0.0005576767725870013
batch 205 loss: 0.0005578011972829699
batch 210 loss: 0.0005577569711022079
batch 215 loss: 0.0005575726157985627
batch 220 loss: 0.0005576686351560056
batch 225 loss: 0.0005578254931606352
batch 230 loss: 0.0005576779018156231
batch 235 loss: 0.0005575482151471079
batch 240 loss: 0.000557701603975147
Training Loss: 0.0005576833757610681
Validation Loss: 0.0005576934025157243
Epoch 95:
batch 5 loss: 0.0005576874013058841
batch 10 loss: 0.0005575923481956124
batch 15 loss: 0.0005576700554229319
batch 20 loss: 0.0005576441180892289
batch 25 loss: 0.0005576818832196296
batch 30 loss: 0.0005576165858656168
batch 35 loss: 0.0005576650612056256
batch 40 loss: 0.0005576690891757607
batch 45 loss: 0.0005576366558670998
batch 50 loss: 0.000557712942827493
batch 55 loss: 0.0005576174007728696
batch 60 loss: 0.0005576637107878923
batch 65 loss: 0.0005576156196184457
batch 70 loss: 0.0005576587864197791
batch 75 loss: 0.0005577830132097005
batch 80 loss: 0.0005576857016421855
batch 85 loss: 0.000557644828222692
batch 90 loss: 0.0005578517797403037
batch 95 loss: 0.0005577204516157508
batch 100 loss: 0.0005577295087277889
batch 105 loss: 0.0005578498239628971
batch 110 loss: 0.0005577446310780943
batch 115 loss: 0.0005576488212682307
batch 120 loss: 0.0005576277617365122
batch 125 loss: 0.0005577273550443351
batch 130 loss: 0.0005577558884397149
batch 135 loss: 0.0005576976691372693
batch 140 loss: 0.0005576803116127849
batch 145 loss: 0.0005577916512265801
batch 150 loss: 0.0005576709983870388
batch 155 loss: 0.0005576035124249757
batch 160 loss: 0.0005577594507485628
batch 165 loss: 0.0005576096940785646
batch 170 loss: 0.0005576355033554137
batch 175 loss: 0.0005576747586019337
batch 180 loss: 0.0005576617550104856
batch 185 loss: 0.0005577480769716203
batch 190 loss: 0.0005577657138928771
batch 195 loss: 0.000557607295922935
batch 200 loss: 0.0005575992399826646
batch 205 loss: 0.0005576659692451358
batch 210 loss: 0.0005576416268013417
batch 215 loss: 0.0005577971343882382
batch 220 loss: 0.0005576572148129344
batch 225 loss: 0.0005577331874519586
batch 230 loss: 0.0005576330819167197
batch 235 loss: 0.0005576180061325431
batch 240 loss: 0.0005576490773819387
Training Loss: 0.0005576833781863873
Validation Loss: 0.0005576934025157243
Epoch 96:
batch 5 loss: 0.0005577284842729569
batch 10 loss: 0.0005576418829150498
batch 15 loss: 0.0005575869814492762
batch 20 loss: 0.0005577163654379546
batch 25 loss: 0.0005577305215410888
batch 30 loss: 0.0005576325696893036
batch 35 loss: 0.0005578124197199941
batch 40 loss: 0.0005577598582021892
batch 45 loss: 0.0005576261086389422
batch 50 loss: 0.0005576814175583423
batch 55 loss: 0.0005576123716309666
batch 60 loss: 0.0005576316849328578
batch 65 loss: 0.0005575662944465876
batch 70 loss: 0.0005576563067734242
batch 75 loss: 0.0005576603696681559
batch 80 loss: 0.0005575741990469397
batch 85 loss: 0.0005576825467869639
batch 90 loss: 0.0005576433846727014
batch 95 loss: 0.000557716318871826
batch 100 loss: 0.0005577821866609156
batch 105 loss: 0.000557644828222692
batch 110 loss: 0.0005576525698415935
batch 115 loss: 0.000557798647787422
batch 120 loss: 0.0005577686242759228
batch 125 loss: 0.0005576786701567471
batch 130 loss: 0.0005576554453000426
batch 135 loss: 0.0005577127449214458
batch 140 loss: 0.0005576210445724427
batch 145 loss: 0.0005576990428380668
batch 150 loss: 0.0005576475989073515
batch 155 loss: 0.0005577721749432385
batch 160 loss: 0.0005576819297857582
batch 165 loss: 0.0005576729658059776
batch 170 loss: 0.000557799485977739
batch 175 loss: 0.0005578135373070836
batch 180 loss: 0.0005577328498475254
batch 185 loss: 0.0005577222676947713
batch 190 loss: 0.0005576035706326366
batch 195 loss: 0.0005577614065259695
batch 200 loss: 0.000557754433248192
batch 205 loss: 0.0005576107185333967
batch 210 loss: 0.0005576408002525568
batch 215 loss: 0.0005576542345806957
batch 220 loss: 0.0005575829185545444
batch 225 loss: 0.0005576816271059215
batch 230 loss: 0.0005576650728471577
batch 235 loss: 0.0005576276103965938
batch 240 loss: 0.0005577028496190906
Training Loss: 0.0005576833738208127
Validation Loss: 0.0005576934073663627
Epoch 97:
batch 5 loss: 0.0005576295196078717
batch 10 loss: 0.00055779431713745
batch 15 loss: 0.0005576810450293124
batch 20 loss: 0.0005577053758315742
batch 25 loss: 0.0005576986004598439
batch 30 loss: 0.0005576704978011548
batch 35 loss: 0.0005576318362727761
batch 40 loss: 0.0005577014177106321
batch 45 loss: 0.0005576809169724583
batch 50 loss: 0.0005576092284172773
batch 55 loss: 0.000557664199732244
batch 60 loss: 0.0005576781230047345
batch 65 loss: 0.0005575674935244024
batch 70 loss: 0.0005576968775130809
batch 75 loss: 0.0005576465395279228
batch 80 loss: 0.0005576686817221344
batch 85 loss: 0.0005576532334089279
batch 90 loss: 0.0005577346659265458
batch 95 loss: 0.0005576189956627786
batch 100 loss: 0.0005575994146056473
batch 105 loss: 0.000557552243117243
batch 110 loss: 0.0005577340722084046
batch 115 loss: 0.0005576816736720502
batch 120 loss: 0.0005577730247750878
batch 125 loss: 0.0005578170297667384
batch 130 loss: 0.0005577391013503075
batch 135 loss: 0.0005576723837293684
batch 140 loss: 0.0005576208117417991
batch 145 loss: 0.0005577509291470051
batch 150 loss: 0.0005576315452344716
batch 155 loss: 0.0005577081115916372
batch 160 loss: 0.0005577760399319232
batch 165 loss: 0.0005576867144554854
batch 170 loss: 0.0005576265510171651
batch 175 loss: 0.0005578190670348704
batch 180 loss: 0.0005576692405156791
batch 185 loss: 0.0005576985771767795
batch 190 loss: 0.0005576866795308888
batch 195 loss: 0.0005577300675213337
batch 200 loss: 0.0005577631760388613
batch 205 loss: 0.000557654199656099
batch 210 loss: 0.0005577452830038965
batch 215 loss: 0.000557630870025605
batch 220 loss: 0.0005577511270530522
batch 225 loss: 0.0005575493793003261
batch 230 loss: 0.0005575788673013449
batch 235 loss: 0.0005576916271820664
batch 240 loss: 0.0005577326752245426
Training Loss: 0.0005576833760036
Validation Loss: 0.0005576934005754689
Epoch 98:
batch 5 loss: 0.0005577257834374905
batch 10 loss: 0.0005577837117016315
batch 15 loss: 0.0005577534320764244
batch 20 loss: 0.000557765131816268
batch 25 loss: 0.0005576164461672306
batch 30 loss: 0.0005576075287535787
batch 35 loss: 0.000557732954621315
batch 40 loss: 0.0005575533490628004
batch 45 loss: 0.0005577097530476749
batch 50 loss: 0.0005575557239353657
batch 55 loss: 0.0005576892290264368
batch 60 loss: 0.0005576949566602707
batch 65 loss: 0.0005576462252065539
batch 70 loss: 0.0005576370400376618
batch 75 loss: 0.0005577036645263433
batch 80 loss: 0.0005576719529926776
batch 85 loss: 0.0005577081697992981
batch 90 loss: 0.0005576407769694924
batch 95 loss: 0.0005576323950663209
batch 100 loss: 0.0005575917544774711
batch 105 loss: 0.0005576472962275147
batch 110 loss: 0.000557712244335562
batch 115 loss: 0.0005577022326178849
batch 120 loss: 0.0005577279720455408
batch 125 loss: 0.0005577047704719007
batch 130 loss: 0.0005577812204137445
batch 135 loss: 0.0005576518946327269
batch 140 loss: 0.0005577045725658536
batch 145 loss: 0.0005576710682362318
batch 150 loss: 0.0005577530362643301
batch 155 loss: 0.0005576286930590868
batch 160 loss: 0.0005576886236667633
batch 165 loss: 0.0005577631061896681
batch 170 loss: 0.0005576248979195953
batch 175 loss: 0.0005576139083132148
batch 180 loss: 0.0005576366209425032
batch 185 loss: 0.0005576827563345433
batch 190 loss: 0.0005577361676841974
batch 195 loss: 0.0005576433148235082
batch 200 loss: 0.0005577721749432385
batch 205 loss: 0.000557691918220371
batch 210 loss: 0.0005576531169936061
batch 215 loss: 0.0005576994153670967
batch 220 loss: 0.0005576877971179784
batch 225 loss: 0.0005577242234721779
batch 230 loss: 0.0005577397532761097
batch 235 loss: 0.0005576905910857022
batch 240 loss: 0.0005576485884375871
Training Loss: 0.0005576833740633447
Validation Loss: 0.0005576934025157243
Epoch 99:
batch 5 loss: 0.0005576601717621088
batch 10 loss: 0.000557655212469399
batch 15 loss: 0.0005577007425017655
batch 20 loss: 0.0005576198222115636
batch 25 loss: 0.0005576763884164393
batch 30 loss: 0.0005576837225817144
batch 35 loss: 0.0005577388568781316
batch 40 loss: 0.000557694782037288
batch 45 loss: 0.0005576381110586226
batch 50 loss: 0.0005578297539614141
batch 55 loss: 0.0005576940253376961
batch 60 loss: 0.0005576222785748542
batch 65 loss: 0.0005575778777711093
batch 70 loss: 0.0005576587282121181
batch 75 loss: 0.0005577248986810446
batch 80 loss: 0.0005576810333877801
batch 85 loss: 0.0005577016738243401
batch 90 loss: 0.0005577241070568561
batch 95 loss: 0.0005577314179390669
batch 100 loss: 0.000557658076286316
batch 105 loss: 0.0005575635354034602
batch 110 loss: 0.0005576976691372693
batch 115 loss: 0.0005577592994086445
batch 120 loss: 0.0005576958530582488
batch 125 loss: 0.0005576912546530366
batch 130 loss: 0.0005576021852903068
batch 135 loss: 0.0005578441778197885
batch 140 loss: 0.0005577418953180314
batch 145 loss: 0.000557568424846977
batch 150 loss: 0.0005576644791290164
batch 155 loss: 0.0005577027914114296
batch 160 loss: 0.0005576623254455626
batch 165 loss: 0.0005577428732067346
batch 170 loss: 0.0005576613708399236
batch 175 loss: 0.000557651452254504
batch 180 loss: 0.0005576032097451388
batch 185 loss: 0.0005577205214649439
batch 190 loss: 0.0005575739429332316
batch 195 loss: 0.0005576482857577503
batch 200 loss: 0.0005577000789344311
batch 205 loss: 0.0005577154923230409
batch 210 loss: 0.0005576993455179036
batch 215 loss: 0.000557727343402803
batch 220 loss: 0.0005577185074798763
batch 225 loss: 0.0005577071220614016
batch 230 loss: 0.0005576468305662274
batch 235 loss: 0.0005577117903158068
batch 240 loss: 0.0005577082745730876
Training Loss: 0.0005576833752760043
Validation Loss: 0.0005576933996053413
Epoch 100:
batch 5 loss: 0.0005576371098868549
batch 10 loss: 0.000557697913609445
batch 15 loss: 0.0005576539319008589
batch 20 loss: 0.0005577042582444846
batch 25 loss: 0.0005577226635068655
batch 30 loss: 0.0005576636525802314
batch 35 loss: 0.0005576819181442261
batch 40 loss: 0.0005576464813202619
batch 45 loss: 0.000557676237076521
batch 50 loss: 0.0005576264229603112
batch 55 loss: 0.0005575794028118252
batch 60 loss: 0.0005577013711445033
batch 65 loss: 0.000557632907293737
batch 70 loss: 0.000557692104484886
batch 75 loss: 0.0005576729774475098
batch 80 loss: 0.0005577025236561895
batch 85 loss: 0.0005576424417085945
batch 90 loss: 0.0005576386698521674
batch 95 loss: 0.0005576339899562299
batch 100 loss: 0.0005577215226367116
batch 105 loss: 0.0005577454576268792
batch 110 loss: 0.0005576218944042921
batch 115 loss: 0.0005575945833697915
batch 120 loss: 0.0005576395662501455
batch 125 loss: 0.0005577169242314995
batch 130 loss: 0.0005575976567342878
batch 135 loss: 0.0005576684023253619
batch 140 loss: 0.0005576629191637039
batch 145 loss: 0.0005576564813964069
batch 150 loss: 0.0005578395910561084
batch 155 loss: 0.0005577045492827892
batch 160 loss: 0.0005577669129706919
batch 165 loss: 0.0005576241062954068
batch 170 loss: 0.0005576984258368611
batch 175 loss: 0.0005576912430115044
batch 180 loss: 0.0005576925468631089
batch 185 loss: 0.0005577469477429986
batch 190 loss: 0.0005577523028478026
batch 195 loss: 0.0005577008589170874
batch 200 loss: 0.000557584420312196
batch 205 loss: 0.0005577202653512358
batch 210 loss: 0.0005577042349614203
batch 215 loss: 0.0005576914409175515
batch 220 loss: 0.0005578386480920017
batch 225 loss: 0.0005576563533395528
batch 230 loss: 0.0005576710100285709
batch 235 loss: 0.0005578449578024447
batch 240 loss: 0.0005576406139880419
Training Loss: 0.0005576833711529617
Validation Loss: 0.0005576934025157243
****************************************************************
*                      SLURM Batch System                      *
*           IN2P3 Computing Centre, Villeurbanne FR            *
****************************************************************
Date: Mon Jul  1 18:33:58 CEST 2024
Job informations can be found using these commands:
Accounting:
sacct -j 17512725
Efficiency:
seff 17512725
****************************************************************
