no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 1400
20240628_074350
Epoch 1:
batch 5 loss: 0.01743971509858966
batch 10 loss: 0.003166598407551646
batch 15 loss: 0.001969083514995873
batch 20 loss: 0.001447902456857264
batch 25 loss: 0.0012303776806220413
batch 30 loss: 0.0010948433540761472
batch 35 loss: 0.000978531246073544
batch 40 loss: 0.0009025699924677611
batch 45 loss: 0.0008313276688568294
batch 50 loss: 0.0007657368201762438
batch 55 loss: 0.0007756951614283025
batch 60 loss: 0.0007107292767614126
batch 65 loss: 0.000696237781085074
batch 70 loss: 0.0006903967703692615
batch 75 loss: 0.0006850972538813948
batch 80 loss: 0.0007147027179598808
batch 85 loss: 0.0008167936932295561
batch 90 loss: 0.0006964691099710762
batch 95 loss: 0.0006836922606453299
batch 100 loss: 0.0006799129187129438
batch 105 loss: 0.0006756414426490664
batch 110 loss: 0.0006711155059747398
batch 115 loss: 0.0006664125248789787
batch 120 loss: 0.000660681293811649
batch 125 loss: 0.0006507112644612789
batch 130 loss: 0.0008252355153672397
batch 135 loss: 0.0007015614421106875
batch 140 loss: 0.0006613853271119297
batch 145 loss: 0.000655339634977281
batch 150 loss: 0.0006446684128604829
batch 155 loss: 0.0006343637825921178
batch 160 loss: 0.00062713852385059
batch 165 loss: 0.0006217207061126828
batch 170 loss: 0.000617376307491213
batch 175 loss: 0.000613109499681741
batch 180 loss: 0.0006089431815780699
batch 185 loss: 0.0006048559909686447
batch 190 loss: 0.0005995413521304727
batch 195 loss: 0.0005905294208787381
batch 200 loss: 0.0005884341662749648
batch 205 loss: 0.0005869728163816034
batch 210 loss: 0.0005859817378222942
batch 215 loss: 0.0005851152003742754
batch 220 loss: 0.0005839512101374567
batch 225 loss: 0.0005829522735439241
batch 230 loss: 0.0007813919102773071
batch 235 loss: 0.0007013972382992506
batch 240 loss: 0.0006241981987841428
Training Loss: 0.0011443153972019596
Validation Loss: 0.000613371332292445
Epoch 2:
batch 5 loss: 0.0006203018827363848
batch 10 loss: 0.0006167322164401412
batch 15 loss: 0.0006127239321358502
batch 20 loss: 0.000608439079951495
batch 25 loss: 0.0006039014901034534
batch 30 loss: 0.0005994368344545365
batch 35 loss: 0.0005958969937637449
batch 40 loss: 0.0005927338381297887
batch 45 loss: 0.0005894432193599642
batch 50 loss: 0.0005861406330950558
batch 55 loss: 0.0005835071671754122
batch 60 loss: 0.0005810752976685762
batch 65 loss: 0.0005796986166387797
batch 70 loss: 0.0005786601570434869
batch 75 loss: 0.0005774219054728747
batch 80 loss: 0.0005763402092270553
batch 85 loss: 0.0005804113810881972
batch 90 loss: 0.0005833443370647729
batch 95 loss: 0.0005811709444969893
batch 100 loss: 0.0005800690036267042
batch 105 loss: 0.0005790638388134539
batch 110 loss: 0.0005782703519798815
batch 115 loss: 0.0005774043267592788
batch 120 loss: 0.0005765589070506394
batch 125 loss: 0.0005758217070251703
batch 130 loss: 0.0005751968128606677
batch 135 loss: 0.000574707833584398
batch 140 loss: 0.0005742369801737368
batch 145 loss: 0.0005737491650506854
batch 150 loss: 0.0005732176825404168
batch 155 loss: 0.0005728384247049689
batch 160 loss: 0.0005728221614845097
batch 165 loss: 0.0005723576177842915
batch 170 loss: 0.0005721349967643619
batch 175 loss: 0.0005718324100598693
batch 180 loss: 0.0005717042949981987
batch 185 loss: 0.0005715426290407777
batch 190 loss: 0.0005712399608455599
batch 195 loss: 0.0005710320197977126
batch 200 loss: 0.0005710434168577194
batch 205 loss: 0.0005708761978894472
batch 210 loss: 0.0005708447541110217
batch 215 loss: 0.0005706344032660127
batch 220 loss: 0.0005704573937691748
batch 225 loss: 0.000570319197140634
batch 230 loss: 0.0005702006630599499
batch 235 loss: 0.0005701562273316086
batch 240 loss: 0.0005699264584109187
Training Loss: 0.0005805758327672568
Validation Loss: 0.0005683491869907205
Epoch 3:
batch 5 loss: 0.0005698147928342223
batch 10 loss: 0.000569737609475851
batch 15 loss: 0.000569433777127415
batch 20 loss: 0.0005694040912203491
batch 25 loss: 0.0005692711099982261
batch 30 loss: 0.0005690499441698193
batch 35 loss: 0.0005689054029062391
batch 40 loss: 0.0005688515491783618
batch 45 loss: 0.0005687439232133328
batch 50 loss: 0.0005686672171577811
batch 55 loss: 0.0005684621166437865
batch 60 loss: 0.0005684697185643018
batch 65 loss: 0.0005682714749127626
batch 70 loss: 0.0005682544899173081
batch 75 loss: 0.0005681489245034754
batch 80 loss: 0.0005681060254573822
batch 85 loss: 0.0005678398068994283
batch 90 loss: 0.0005678560584783555
batch 95 loss: 0.0005677968030795455
batch 100 loss: 0.0005677015753462911
batch 105 loss: 0.000567574251908809
batch 110 loss: 0.0005675664055161178
batch 115 loss: 0.0005675574066117406
batch 120 loss: 0.0005675143329426646
batch 125 loss: 0.0009182187495753169
batch 130 loss: 0.000883678626269102
batch 135 loss: 0.0008429442183114588
batch 140 loss: 0.0006492309155873954
batch 145 loss: 0.0006176905240863562
batch 150 loss: 0.0005966723314486444
batch 155 loss: 0.0005900273448787629
batch 160 loss: 0.0005858564865775407
batch 165 loss: 0.0005824791616760195
batch 170 loss: 0.0005803597276099026
batch 175 loss: 0.0005786905298009515
batch 180 loss: 0.000576953508425504
batch 185 loss: 0.0005754808080382645
batch 190 loss: 0.0005739875370636583
batch 195 loss: 0.0005730415810830891
batch 200 loss: 0.0005722534493543208
batch 205 loss: 0.0005716109066270292
batch 210 loss: 0.0005709265358746051
batch 215 loss: 0.0005706015508621931
batch 220 loss: 0.0005699636065401136
batch 225 loss: 0.0005695620784536004
batch 230 loss: 0.0005690442631021142
batch 235 loss: 0.0005686028627678752
batch 240 loss: 0.0005683494848199189
Training Loss: 0.0005937338666020272
Validation Loss: 0.0005647694779327139
Epoch 4:
batch 5 loss: 0.0005679399706423283
batch 10 loss: 0.0005676811095327138
batch 15 loss: 0.0005673719337210059
batch 20 loss: 0.0005671467282809317
batch 25 loss: 0.0005669064354151488
batch 30 loss: 0.0005668167723342776
batch 35 loss: 0.0005666936747729778
batch 40 loss: 0.0005663840798661113
batch 45 loss: 0.0005664191325195133
batch 50 loss: 0.000566333462484181
batch 55 loss: 0.0005661631003022194
batch 60 loss: 0.0005659925634972751
batch 65 loss: 0.0005658777663484216
batch 70 loss: 0.0005658840178512037
batch 75 loss: 0.0005656060297042131
batch 80 loss: 0.0005657661939039826
batch 85 loss: 0.0005655114888213575
batch 90 loss: 0.0005655094049870968
batch 95 loss: 0.0005655102315358818
batch 100 loss: 0.0005654808832332492
batch 105 loss: 0.000565335969440639
batch 110 loss: 0.000565213244408369
batch 115 loss: 0.0005651391227729618
batch 120 loss: 0.0005652318708598614
batch 125 loss: 0.0005652272142469883
batch 130 loss: 0.0005651483661495149
batch 135 loss: 0.0005648974794894457
batch 140 loss: 0.0005649589002132416
batch 145 loss: 0.0005648396909236908
batch 150 loss: 0.0005647304002195597
batch 155 loss: 0.0005648039048537612
batch 160 loss: 0.0005649955361150205
batch 165 loss: 0.0005651136627420783
batch 170 loss: 0.0005648773163557053
batch 175 loss: 0.0005646708654239774
batch 180 loss: 0.0005647073616273701
batch 185 loss: 0.0005644403048790991
batch 190 loss: 0.0005644681281410157
batch 195 loss: 0.0005643158569000661
batch 200 loss: 0.0005642762524075806
batch 205 loss: 0.0005643601645715535
batch 210 loss: 0.0005643024225719274
batch 215 loss: 0.0005642714910209179
batch 220 loss: 0.0005643335636705161
batch 225 loss: 0.0005642931326292455
batch 230 loss: 0.0005641236901283264
batch 235 loss: 0.0005640359711833298
batch 240 loss: 0.0005642303382046521
Training Loss: 0.0005653820250396772
Validation Loss: 0.0005636990603913243
Epoch 5:
batch 5 loss: 0.0005640039220452308
batch 10 loss: 0.0005640130140818656
batch 15 loss: 0.000564002653118223
batch 20 loss: 0.0005639825598336756
batch 25 loss: 0.000564021326135844
batch 30 loss: 0.0005640572984702885
batch 35 loss: 0.000563959835562855
batch 40 loss: 0.0005640085088089109
batch 45 loss: 0.0017710114712826907
batch 50 loss: 0.0013853619573637843
batch 55 loss: 0.0007432625046931207
batch 60 loss: 0.000682838773354888
batch 65 loss: 0.0006798960850574077
batch 70 loss: 0.0006742619443684816
batch 75 loss: 0.0006618933053687215
batch 80 loss: 0.00065557915950194
batch 85 loss: 0.0006510946084745228
batch 90 loss: 0.0006480735493823886
batch 95 loss: 0.000644408748485148
batch 100 loss: 0.0006422015954740346
batch 105 loss: 0.0006393272313289344
batch 110 loss: 0.0006359460996463895
batch 115 loss: 0.0006326573900878429
batch 120 loss: 0.000630805327091366
batch 125 loss: 0.0006282168789766729
batch 130 loss: 0.0006244124379009008
batch 135 loss: 0.0006230705184862018
batch 140 loss: 0.0006230244063772262
batch 145 loss: 0.0006222024559974671
batch 150 loss: 0.000621382996905595
batch 155 loss: 0.0006210394552908838
batch 160 loss: 0.0006203172379173338
batch 165 loss: 0.0006198260933160781
batch 170 loss: 0.0006193248555064201
batch 175 loss: 0.000618775375187397
batch 180 loss: 0.0006182189099490643
batch 185 loss: 0.000617625704035163
batch 190 loss: 0.0006172331981360912
batch 195 loss: 0.0006165454629808665
batch 200 loss: 0.0006162201170809567
batch 205 loss: 0.0006157740484923124
batch 210 loss: 0.0006158284260891378
batch 215 loss: 0.000615570496302098
batch 220 loss: 0.0006148336105979979
batch 225 loss: 0.0006141645018942655
batch 230 loss: 0.0006135254050604999
batch 235 loss: 0.000612293544691056
batch 240 loss: 0.0006116318050771952
Training Loss: 0.0006610776419014049
Validation Loss: 0.0006076263516054799
Epoch 6:
batch 5 loss: 0.0006108767352998257
batch 10 loss: 0.0006101372884586454
batch 15 loss: 0.0006091546267271042
batch 20 loss: 0.0006083764717914164
batch 25 loss: 0.0006075658253394068
batch 30 loss: 0.0006067520123906433
batch 35 loss: 0.0006057874416001141
batch 40 loss: 0.0006048006704077125
batch 45 loss: 0.0006037831539288163
batch 50 loss: 0.0006026162416674196
batch 55 loss: 0.0006011055200360715
batch 60 loss: 0.0005993484985083342
batch 65 loss: 0.0005976256914436817
batch 70 loss: 0.0005960644572041929
batch 75 loss: 0.0005943094729445875
batch 80 loss: 0.0005923609016463161
batch 85 loss: 0.0005914138164371252
batch 90 loss: 0.0005897712195292115
batch 95 loss: 0.0005882573663257062
batch 100 loss: 0.0005862211110070348
batch 105 loss: 0.0005846608430147171
batch 110 loss: 0.0005833509028889238
batch 115 loss: 0.0005817368510179222
batch 120 loss: 0.0005805318825878203
batch 125 loss: 0.000579223851673305
batch 130 loss: 0.000578452693298459
batch 135 loss: 0.0005777204059995711
batch 140 loss: 0.0005770911462605
batch 145 loss: 0.0005764127126894891
batch 150 loss: 0.0005759904743172228
batch 155 loss: 0.0005762898479588329
batch 160 loss: 0.0005757484002970159
batch 165 loss: 0.0005750888376496732
batch 170 loss: 0.0005745773785747587
batch 175 loss: 0.0005745553644374013
batch 180 loss: 0.0005740687134675682
batch 185 loss: 0.0005736249033361674
batch 190 loss: 0.0005735248676501214
batch 195 loss: 0.0005730806617066265
batch 200 loss: 0.0005723145557567477
batch 205 loss: 0.0005726967588998378
batch 210 loss: 0.0005726697272621095
batch 215 loss: 0.0005724709015339613
batch 220 loss: 0.0005718310247175395
batch 225 loss: 0.0005714780418202281
batch 230 loss: 0.0005710897617973387
batch 235 loss: 0.0005705805611796677
batch 240 loss: 0.0005713724764063955
Training Loss: 0.0005858033973102768
Validation Loss: 0.0005718150030588731
Epoch 7:
batch 5 loss: 0.000570887140929699
batch 10 loss: 0.0005709970486350357
batch 15 loss: 0.0005698543274775148
batch 20 loss: 0.0005695205414667726
batch 25 loss: 0.0005691284313797951
batch 30 loss: 0.0005694401799701155
batch 35 loss: 0.0005694889812730252
batch 40 loss: 0.0005697000073269009
batch 45 loss: 0.0005692836712114513
batch 50 loss: 0.0005703183123841882
batch 55 loss: 0.0005700294743292034
batch 60 loss: 0.0005700095905922353
batch 65 loss: 0.0005689362064003944
batch 70 loss: 0.0005690133315511048
batch 75 loss: 0.0005693597835488617
batch 80 loss: 0.0005702922469936311
batch 85 loss: 0.0005706008756533265
batch 90 loss: 0.0005713900434784591
batch 95 loss: 0.0005734950536862016
batch 100 loss: 0.0005733572994358838
batch 105 loss: 0.0005723336129449308
batch 110 loss: 0.0005724913673475385
batch 115 loss: 0.0005722346366383135
batch 120 loss: 0.0005709353950805962
batch 125 loss: 0.0005703356000594795
batch 130 loss: 0.0005698988097719848
batch 135 loss: 0.0005682376213371754
batch 140 loss: 0.0005689152283594012
batch 145 loss: 0.0005682618939317763
batch 150 loss: 0.0005688842851668596
batch 155 loss: 0.0005691564292646945
batch 160 loss: 0.0005696949665434659
batch 165 loss: 0.000569353683385998
batch 170 loss: 0.000571036257315427
batch 175 loss: 0.0005707482108846307
batch 180 loss: 0.0005702251219190657
batch 185 loss: 0.0005709592951461673
batch 190 loss: 0.0005717035965062678
batch 195 loss: 0.0005703378119505942
batch 200 loss: 0.0005692462669685483
batch 205 loss: 0.000568939174991101
batch 210 loss: 0.0005699105095118284
batch 215 loss: 0.0005692630773410201
batch 220 loss: 0.0005681874812580645
batch 225 loss: 0.000569455383811146
batch 230 loss: 0.0005680290050804615
batch 235 loss: 0.0005671886727213859
batch 240 loss: 0.0005667415214702487
Training Loss: 0.000569954322175666
Validation Loss: 0.0005684283775432656
Epoch 8:
batch 5 loss: 0.0005679562804289162
batch 10 loss: 0.0005672062514349818
batch 15 loss: 0.0005676637054421007
batch 20 loss: 0.0005678068264387548
batch 25 loss: 0.0005664364318363369
batch 30 loss: 0.0005661003640852868
batch 35 loss: 0.0005656808032654225
batch 40 loss: 0.0005669123027473688
batch 45 loss: 0.0005670935846865177
batch 50 loss: 0.0005683353170752526
batch 55 loss: 0.0005676077445968986
batch 60 loss: 0.0005668043275363743
batch 65 loss: 0.0005663134274072945
batch 70 loss: 0.0005699345842003822
batch 75 loss: 0.000567191478330642
batch 80 loss: 0.0005670203478075564
batch 85 loss: 0.0005664746044203639
batch 90 loss: 0.0005660558003000915
batch 95 loss: 0.0005669905454851687
batch 100 loss: 0.0005679238005541265
batch 105 loss: 0.0005664786207489669
batch 110 loss: 0.0005650361417792737
batch 115 loss: 0.0005651816958561539
batch 120 loss: 0.0005654001841321587
batch 125 loss: 0.0005649848026223481
batch 130 loss: 0.0005644923658110201
batch 135 loss: 0.0005662374082021415
batch 140 loss: 0.0005665054777637124
batch 145 loss: 0.000565348460804671
batch 150 loss: 0.0005643803742714226
batch 155 loss: 0.0005642870906740427
batch 160 loss: 0.0005638384143821895
batch 165 loss: 0.0005635045235976577
batch 170 loss: 0.000564177508931607
batch 175 loss: 0.0005647710873745382
batch 180 loss: 0.0005638615926727653
batch 185 loss: 0.000564238999504596
batch 190 loss: 0.0005645132041536272
batch 195 loss: 0.000564814277458936
batch 200 loss: 0.0005639451555907726
batch 205 loss: 0.0005631071981042624
batch 210 loss: 0.0005644319579005241
batch 215 loss: 0.0005638452246785163
batch 220 loss: 0.0005636578192934394
batch 225 loss: 0.0005639289971441031
batch 230 loss: 0.0005635722423903644
batch 235 loss: 0.0005631322041153908
batch 240 loss: 0.0005636944202706218
Training Loss: 0.0005656015828814513
Validation Loss: 0.0005638066718044381
Epoch 9:
batch 5 loss: 0.0005638871574774384
batch 10 loss: 0.0005638454924337566
batch 15 loss: 0.0005633855704218149
batch 20 loss: 0.0005632259184494614
batch 25 loss: 0.0005637474590912461
batch 30 loss: 0.0005640073795802891
batch 35 loss: 0.0005638300208374858
batch 40 loss: 0.0005636398680508136
batch 45 loss: 0.0005637308117002249
batch 50 loss: 0.000564007880166173
batch 55 loss: 0.0005632674554362893
batch 60 loss: 0.0005631301552057266
batch 65 loss: 0.0005630071624182165
batch 70 loss: 0.0005633734981529415
batch 75 loss: 0.0005646945792250335
batch 80 loss: 0.0005642397329211235
batch 85 loss: 0.0005636064219288528
batch 90 loss: 0.0005631639156490565
batch 95 loss: 0.0005638608708977699
batch 100 loss: 0.0005629830295220018
batch 105 loss: 0.0005641339928843081
batch 110 loss: 0.0005634467233903706
batch 115 loss: 0.0005628664162941277
batch 120 loss: 0.0005631954059936106
batch 125 loss: 0.0005633712047711015
batch 130 loss: 0.0005630845320411026
batch 135 loss: 0.0005638850620016455
batch 140 loss: 0.0005642242264002562
batch 145 loss: 0.000563365628477186
batch 150 loss: 0.0005635770270600915
batch 155 loss: 0.0005632312619127333
batch 160 loss: 0.0005646779201924801
batch 165 loss: 0.0005645582568831742
batch 170 loss: 0.0005637638503685593
batch 175 loss: 0.0005633193184621632
batch 180 loss: 0.0005629178485833108
batch 185 loss: 0.0005631172796711325
batch 190 loss: 0.0005632563028484583
batch 195 loss: 0.000563686725217849
batch 200 loss: 0.0005631448118947446
batch 205 loss: 0.0005627869511954486
batch 210 loss: 0.0005627501523122191
batch 215 loss: 0.0005628092796541751
batch 220 loss: 0.0005628361832350492
batch 225 loss: 0.0005633550230413675
batch 230 loss: 0.0005630402243696153
batch 235 loss: 0.0005626495229080319
batch 240 loss: 0.0005624350742436945
Training Loss: 0.0005634608455390359
Validation Loss: 0.0005619425501208752
Epoch 10:
batch 5 loss: 0.0005629509454593062
batch 10 loss: 0.0005629016319289804
batch 15 loss: 0.0005625080317258835
batch 20 loss: 0.0005635728826746345
batch 25 loss: 0.0005639052717015148
batch 30 loss: 0.0005637764697894454
batch 35 loss: 0.000562831002753228
batch 40 loss: 0.0005629004328511656
batch 45 loss: 0.0005624829675070942
batch 50 loss: 0.0005626271013170481
batch 55 loss: 0.0005628461833111942
batch 60 loss: 0.0005626885336823762
batch 65 loss: 0.0005631935317069292
batch 70 loss: 0.0005630514700897038
batch 75 loss: 0.000562643154989928
batch 80 loss: 0.0005633323104120791
batch 85 loss: 0.0005627609556540847
batch 90 loss: 0.0005626782192848622
batch 95 loss: 0.0005626336205750704
batch 100 loss: 0.0005636208923533558
batch 105 loss: 0.0005632547661662101
batch 110 loss: 0.0005629198276437819
batch 115 loss: 0.0005626493948511779
batch 120 loss: 0.000562607729807496
batch 125 loss: 0.0005625611171126366
batch 130 loss: 0.0005625141435302794
batch 135 loss: 0.0005625819670967757
batch 140 loss: 0.0005629302351735533
batch 145 loss: 0.0005636277375742794
batch 150 loss: 0.0005634893313981593
batch 155 loss: 0.0005629014573059976
batch 160 loss: 0.0005623096250928939
batch 165 loss: 0.0005629925988614559
batch 170 loss: 0.0005628794664517045
batch 175 loss: 0.000562696356792003
batch 180 loss: 0.0005620669573545456
batch 185 loss: 0.0005623607663437724
batch 190 loss: 0.0005621623829938471
batch 195 loss: 0.0005624589743092656
batch 200 loss: 0.000563152099493891
batch 205 loss: 0.0005625908845104278
batch 210 loss: 0.0005622798344120383
batch 215 loss: 0.0005621037096716463
batch 220 loss: 0.0005630587227642536
batch 225 loss: 0.0005626316182315349
batch 230 loss: 0.0005626818398013711
batch 235 loss: 0.0005628356011584401
batch 240 loss: 0.0005664033815264702
Training Loss: 0.0005629085028582873
Validation Loss: 0.0005576963817778354
Epoch 11:
batch 5 loss: 0.0005655993358232081
batch 10 loss: 0.0005650123814120889
batch 15 loss: 0.0005640940507873893
batch 20 loss: 0.0005641672760248184
batch 25 loss: 0.0005640306626446545
batch 30 loss: 0.0005633771768771112
batch 35 loss: 0.0005634103203192353
batch 40 loss: 0.0005630180356092751
batch 45 loss: 0.0005626274971291423
batch 50 loss: 0.0005623043747618794
batch 55 loss: 0.000562155689112842
batch 60 loss: 0.0005624504294246436
batch 65 loss: 0.0005623151897452772
batch 70 loss: 0.0005622621509246528
batch 75 loss: 0.00056214650394395
batch 80 loss: 0.0005624848883599043
batch 85 loss: 0.0005623055272735656
batch 90 loss: 0.0005620774812996387
batch 95 loss: 0.0005620255134999752
batch 100 loss: 0.0005620247218757867
batch 105 loss: 0.000562132615596056
batch 110 loss: 0.0005622576456516982
batch 115 loss: 0.0005619246861897409
batch 120 loss: 0.0005619908450171351
batch 125 loss: 0.0005622150609269738
batch 130 loss: 0.0005624545272439718
batch 135 loss: 0.0005625234683975577
batch 140 loss: 0.0005625661462545395
batch 145 loss: 0.0005630015977658332
batch 150 loss: 0.0005624221637845039
batch 155 loss: 0.0005620204610750079
batch 160 loss: 0.0005620021955110132
batch 165 loss: 0.0005617454764433206
batch 170 loss: 0.0005615372094325722
batch 175 loss: 0.0005613679764792323
batch 180 loss: 0.0005615049623884261
batch 185 loss: 0.0005614268477074802
batch 190 loss: 0.0005613948800601065
batch 195 loss: 0.000561376113910228
batch 200 loss: 0.000561632018070668
batch 205 loss: 0.0005619081668555737
batch 210 loss: 0.0005622027558274568
batch 215 loss: 0.0005616759532131255
batch 220 loss: 0.0005622701835818588
batch 225 loss: 0.0005623825825750827
batch 230 loss: 0.0005621547577902674
batch 235 loss: 0.0005621107295155526
batch 240 loss: 0.0005622840486466885
Training Loss: 0.0005624244850575148
Validation Loss: 0.000564152913284488
Epoch 12:
batch 5 loss: 0.0005620446754619479
batch 10 loss: 0.0005617815535515547
batch 15 loss: 0.0005616005742922425
batch 20 loss: 0.0005613468238152564
batch 25 loss: 0.0005612656706944108
batch 30 loss: 0.0005612725275568664
batch 35 loss: 0.000561375345569104
batch 40 loss: 0.0005612268927507102
batch 45 loss: 0.0005614161142148078
batch 50 loss: 0.0005614376394078135
batch 55 loss: 0.0005612237611785531
batch 60 loss: 0.0005613886401988566
batch 65 loss: 0.000561369163915515
batch 70 loss: 0.000561214191839099
batch 75 loss: 0.0005614920170046389
batch 80 loss: 0.0005613580113276839
batch 85 loss: 0.000561613345053047
batch 90 loss: 0.000561497057788074
batch 95 loss: 0.0005615952308289706
batch 100 loss: 0.0005617178860120476
batch 105 loss: 0.0005617758259177208
batch 110 loss: 0.0005616143811494112
batch 115 loss: 0.0005614658468402923
batch 120 loss: 0.0005613568471744656
batch 125 loss: 0.0005621871212497353
batch 130 loss: 0.0005616010050289333
batch 135 loss: 0.0005612599081359804
batch 140 loss: 0.0005615409230813384
batch 145 loss: 0.0005612533306702972
batch 150 loss: 0.0005608532577753067
batch 155 loss: 0.0005613243905827403
batch 160 loss: 0.0005611968110315502
batch 165 loss: 0.0005615088623017073
batch 170 loss: 0.0005614110035821795
batch 175 loss: 0.000561471888795495
batch 180 loss: 0.0005615242058411241
batch 185 loss: 0.0005615412723273039
batch 190 loss: 0.0005617965827696026
batch 195 loss: 0.0005619688774459064
batch 200 loss: 0.000561665988061577
batch 205 loss: 0.0005617735208943486
batch 210 loss: 0.0005619529518298804
batch 215 loss: 0.0005618598195724189
batch 220 loss: 0.0005618933937512338
batch 225 loss: 0.000561751367058605
batch 230 loss: 0.0005619823001325131
batch 235 loss: 0.0005616216221824288
batch 240 loss: 0.00056163928238675
Training Loss: 0.0005615422856256676
Validation Loss: 0.0005610500976520901
Epoch 13:
batch 5 loss: 0.00056153325131163
batch 10 loss: 0.0005613826564513147
batch 15 loss: 0.0005614243214949966
batch 20 loss: 0.0005615433328785002
batch 25 loss: 0.0005618805764243007
batch 30 loss: 0.0005617647781036794
batch 35 loss: 0.0005616478156298399
batch 40 loss: 0.0005615217960439622
batch 45 loss: 0.0005615214467979968
batch 50 loss: 0.0005616043112240731
batch 55 loss: 0.0005613105604425073
batch 60 loss: 0.0005619924981147051
batch 65 loss: 0.0005615245550870896
batch 70 loss: 0.0005614262423478067
batch 75 loss: 0.0005616983748041093
batch 80 loss: 0.0005614696419797838
batch 85 loss: 0.0005614536115899682
batch 90 loss: 0.0005615254398435354
batch 95 loss: 0.0005614705733023584
batch 100 loss: 0.0005611492320895195
batch 105 loss: 0.0005612300126813352
batch 110 loss: 0.0005613448098301888
batch 115 loss: 0.0005611171829514206
batch 120 loss: 0.0005610632826574147
batch 125 loss: 0.0005613958230242134
batch 130 loss: 0.0005614314810372889
batch 135 loss: 0.000561477430164814
batch 140 loss: 0.0005612762528471648
batch 145 loss: 0.000563247746322304
batch 150 loss: 0.0005701145390048623
batch 155 loss: 0.000568958604708314
batch 160 loss: 0.0005660770228132605
batch 165 loss: 0.0005664407624863088
batch 170 loss: 0.0005655015236698091
batch 175 loss: 0.0005650339648127556
batch 180 loss: 0.0005645249504595995
batch 185 loss: 0.000564503762871027
batch 190 loss: 0.0005639673792757094
batch 195 loss: 0.0005634333821944893
batch 200 loss: 0.0005629894090816379
batch 205 loss: 0.0005628026789054275
batch 210 loss: 0.0005633479449898005
batch 215 loss: 0.0005626958096399904
batch 220 loss: 0.0005622460739687085
batch 225 loss: 0.0005620211130008101
batch 230 loss: 0.0005619651172310113
batch 235 loss: 0.0005621762480586767
batch 240 loss: 0.0005622542696073652
Training Loss: 0.0005626142415470288
Validation Loss: 0.0005616538265409569
Epoch 14:
batch 5 loss: 0.000562017154879868
batch 10 loss: 0.000562000134959817
batch 15 loss: 0.0005617723916657269
batch 20 loss: 0.0005618436145596207
batch 25 loss: 0.0005616283160634339
batch 30 loss: 0.0005617463029921055
batch 35 loss: 0.000561499351169914
batch 40 loss: 0.0005615539266727865
batch 45 loss: 0.0005612345295958221
batch 50 loss: 0.0005612551001831889
batch 55 loss: 0.0005612410721369087
batch 60 loss: 0.0005611427477560937
batch 65 loss: 0.000561558362096548
batch 70 loss: 0.0005613287445157767
batch 75 loss: 0.0005611851112917066
batch 80 loss: 0.000561231654137373
batch 85 loss: 0.0005612770328298211
batch 90 loss: 0.0005610632826574147
batch 95 loss: 0.0005610108491964638
batch 100 loss: 0.0005609038285911083
batch 105 loss: 0.0005607309518381953
batch 110 loss: 0.000560842000413686
batch 115 loss: 0.0005609207320958376
batch 120 loss: 0.0005609819781966507
batch 125 loss: 0.0005608818028122187
batch 130 loss: 0.000560791720636189
batch 135 loss: 0.0005607226514257491
batch 140 loss: 0.0005607461673207581
batch 145 loss: 0.0005609731771983207
batch 150 loss: 0.0005607756087556481
batch 155 loss: 0.0005606602528132499
batch 160 loss: 0.0005605863290838898
batch 165 loss: 0.0005607835482805967
batch 170 loss: 0.0005607322324067354
batch 175 loss: 0.0005609199055470526
batch 180 loss: 0.0005609262618236244
batch 185 loss: 0.0005606387043371796
batch 190 loss: 0.0005605095182545483
batch 195 loss: 0.0005604053381830454
batch 200 loss: 0.0005605111713521182
batch 205 loss: 0.0005606945254839957
batch 210 loss: 0.0005606190767139197
batch 215 loss: 0.0005606356309726835
batch 220 loss: 0.0005604434874840081
batch 225 loss: 0.0005603875499218703
batch 230 loss: 0.0005606557009741664
batch 235 loss: 0.000560588005464524
batch 240 loss: 0.0005605854676105082
Training Loss: 0.0005610029792781764
Validation Loss: 0.0005605911050224677
Epoch 15:
batch 5 loss: 0.0005607196129858494
batch 10 loss: 0.0005604074103757739
batch 15 loss: 0.0005605104146525264
batch 20 loss: 0.0005606973776593804
batch 25 loss: 0.0005605122307315469
batch 30 loss: 0.0005604904028587044
batch 35 loss: 0.0005607307655736804
batch 40 loss: 0.0005604496807791292
batch 45 loss: 0.0005609679617919027
batch 50 loss: 0.0005610394407995045
batch 55 loss: 0.0005607679951936007
batch 60 loss: 0.0005607384606264532
batch 65 loss: 0.0005607866682112217
batch 70 loss: 0.0005606913473457098
batch 75 loss: 0.0005606802995316684
batch 80 loss: 0.0005605516373179853
batch 85 loss: 0.0005605246056802571
batch 90 loss: 0.0005607764236629009
batch 95 loss: 0.0005606455844826997
batch 100 loss: 0.0005607598344795406
batch 105 loss: 0.0005612715496681631
batch 110 loss: 0.0005611739004962146
batch 115 loss: 0.0005609328160062433
batch 120 loss: 0.0005611641914583743
batch 125 loss: 0.0005608272971585393
batch 130 loss: 0.0005609017331153155
batch 135 loss: 0.0005607974831946194
batch 140 loss: 0.0005606753751635551
batch 145 loss: 0.0005606573424302042
batch 150 loss: 0.000560777063947171
batch 155 loss: 0.0005614696186967194
batch 160 loss: 0.0005590892280451954
batch 165 loss: 0.0005576639785431325
batch 170 loss: 0.0005576617433689535
batch 175 loss: 0.000557696376927197
batch 180 loss: 0.0005576810101047159
batch 185 loss: 0.000557626283261925
batch 190 loss: 0.0005576573661528528
batch 195 loss: 0.0005577070172876119
batch 200 loss: 0.0005577060277573764
batch 205 loss: 0.0005576574127189815
batch 210 loss: 0.0005576648632995784
batch 215 loss: 0.0005577264353632927
batch 220 loss: 0.0005576115916483104
batch 225 loss: 0.0005577201838605106
batch 230 loss: 0.0005576625932008028
batch 235 loss: 0.0005576849798671901
batch 240 loss: 0.0005577099276706576
Training Loss: 0.0005597088238573633
Validation Loss: 0.0005576934131871288
Epoch 16:
batch 5 loss: 0.00055768076563254
batch 10 loss: 0.0005576511030085385
batch 15 loss: 0.0005576556781306863
batch 20 loss: 0.0005577051895670592
batch 25 loss: 0.0005576314637437463
batch 30 loss: 0.0005577199393883347
batch 35 loss: 0.0005576425348408521
batch 40 loss: 0.0005577240721322596
batch 45 loss: 0.0005576570867560804
batch 50 loss: 0.0005578154232352972
batch 55 loss: 0.0005576053517870605
batch 60 loss: 0.0005575865157879889
batch 65 loss: 0.0005577084608376026
batch 70 loss: 0.0005577866453677416
batch 75 loss: 0.0005576534545980394
batch 80 loss: 0.0005576563999056816
batch 85 loss: 0.0005575800780206919
batch 90 loss: 0.0005576949333772063
batch 95 loss: 0.0005577615345828235
batch 100 loss: 0.0005576411029323935
batch 105 loss: 0.0005576354218646884
batch 110 loss: 0.0005576672032475471
batch 115 loss: 0.0005577030126005412
batch 120 loss: 0.0005577100673690438
batch 125 loss: 0.0005576976342126727
batch 130 loss: 0.0005576382158324122
batch 135 loss: 0.0005576320691034198
batch 140 loss: 0.0005576005321927368
batch 145 loss: 0.0005577287287451327
batch 150 loss: 0.0005576576455496252
batch 155 loss: 0.0005576423602178693
batch 160 loss: 0.0005577138159424067
batch 165 loss: 0.0005577591131441295
batch 170 loss: 0.0005577615695074201
batch 175 loss: 0.000557605316862464
batch 180 loss: 0.0005576459574513137
batch 185 loss: 0.0005577274248935282
batch 190 loss: 0.0005577959469519556
batch 195 loss: 0.0005576676339842379
batch 200 loss: 0.0005577535834163427
batch 205 loss: 0.0005577147705480457
batch 210 loss: 0.0005577108357101678
batch 215 loss: 0.0005577077274210751
batch 220 loss: 0.0005577718373388052
batch 225 loss: 0.0005576902418397367
batch 230 loss: 0.0005575574352405966
batch 235 loss: 0.0005576623254455626
batch 240 loss: 0.0005576861323788763
Training Loss: 0.0005576833810967704
Validation Loss: 0.0005576934170676395
Epoch 17:
batch 5 loss: 0.000557636737357825
batch 10 loss: 0.0005576866446062922
batch 15 loss: 0.0005576721508987248
batch 20 loss: 0.000557639601174742
batch 25 loss: 0.000557673315051943
batch 30 loss: 0.0005576557945460081
batch 35 loss: 0.0005577688454650343
batch 40 loss: 0.0005576993455179036
batch 45 loss: 0.0005576779134571552
batch 50 loss: 0.00055765068391338
batch 55 loss: 0.0005575000192038715
batch 60 loss: 0.0005576425231993198
batch 65 loss: 0.000557823886629194
batch 70 loss: 0.0005576080409809947
batch 75 loss: 0.0005577428033575416
batch 80 loss: 0.0005577292409725487
batch 85 loss: 0.0005575632909312844
batch 90 loss: 0.0005577375181019306
batch 95 loss: 0.0005577187403105199
batch 100 loss: 0.0005576408351771533
batch 105 loss: 0.0005577753996476531
batch 110 loss: 0.0005576059687882662
batch 115 loss: 0.0005576779483817517
batch 120 loss: 0.0005576250492595136
batch 125 loss: 0.0005576113006100058
batch 130 loss: 0.0005576381576247513
batch 135 loss: 0.0005576009047217667
batch 140 loss: 0.0005577559932135046
batch 145 loss: 0.0005577025003731251
batch 150 loss: 0.0005577430943958462
batch 155 loss: 0.0005576852941885591
batch 160 loss: 0.0005577552015893162
batch 165 loss: 0.000557817833032459
batch 170 loss: 0.0005577139323577285
batch 175 loss: 0.0005577323841862381
batch 180 loss: 0.0005576649331487715
batch 185 loss: 0.0005577475996688009
batch 190 loss: 0.0005577506963163614
batch 195 loss: 0.0005577003699727357
batch 200 loss: 0.0005576203227974474
batch 205 loss: 0.0005577183328568935
batch 210 loss: 0.0005577758653089404
batch 215 loss: 0.0005576464929617942
batch 220 loss: 0.0005576788797043263
batch 225 loss: 0.0005576861789450049
batch 230 loss: 0.0005576359573751688
batch 235 loss: 0.0005575464339926839
batch 240 loss: 0.0005577215808443726
Training Loss: 0.0005576833861899407
Validation Loss: 0.0005576934122170011
Epoch 18:
batch 5 loss: 0.0005576111376285553
batch 10 loss: 0.0005577164585702122
batch 15 loss: 0.0005576757132075727
batch 20 loss: 0.0005576842580921948
batch 25 loss: 0.0005576931638643145
batch 30 loss: 0.0005577140138484538
batch 35 loss: 0.0005577607546001673
batch 40 loss: 0.0005577143165282905
batch 45 loss: 0.0005576614988967776
batch 50 loss: 0.0005576705909334124
batch 55 loss: 0.0005576470168307424
batch 60 loss: 0.000557680637575686
batch 65 loss: 0.000557653559371829
batch 70 loss: 0.0005576728028245271
batch 75 loss: 0.0005576890078373253
batch 80 loss: 0.0005577145842835307
batch 85 loss: 0.0005576411029323935
batch 90 loss: 0.0005576671101152897
batch 95 loss: 0.0005577396135777235
batch 100 loss: 0.0005576419527642429
batch 105 loss: 0.0005577764823101461
batch 110 loss: 0.0005577059579081834
batch 115 loss: 0.0005576000898145139
batch 120 loss: 0.0005577463074587285
batch 125 loss: 0.0005576351541094482
batch 130 loss: 0.0005576701485551893
batch 135 loss: 0.0005576436524279416
batch 140 loss: 0.0005576873081736267
batch 145 loss: 0.0005576797411777079
batch 150 loss: 0.00055770727340132
batch 155 loss: 0.0005577424773946405
batch 160 loss: 0.0005576677387580276
batch 165 loss: 0.000557757425121963
batch 170 loss: 0.0005576885072514415
batch 175 loss: 0.0005575897288508713
batch 180 loss: 0.0005577487288974225
batch 185 loss: 0.0005576092866249382
batch 190 loss: 0.0005577057250775397
batch 195 loss: 0.0005576119874604046
batch 200 loss: 0.0005576812080107629
batch 205 loss: 0.0005578363896347582
batch 210 loss: 0.000557666108943522
batch 215 loss: 0.0005577025236561895
batch 220 loss: 0.0005576825584284961
batch 225 loss: 0.0005576984025537967
batch 230 loss: 0.000557620171457529
batch 235 loss: 0.0005576943629421293
batch 240 loss: 0.0005575977149419487
Training Loss: 0.0005576833844922173
Validation Loss: 0.000557693403485852
Epoch 19:
batch 5 loss: 0.0005578165058977902
batch 10 loss: 0.0005576036986894905
batch 15 loss: 0.0005577420582994818
batch 20 loss: 0.0005577287869527936
batch 25 loss: 0.0005576299270614981
batch 30 loss: 0.0005576751194894314
batch 35 loss: 0.0005577099625952541
batch 40 loss: 0.0005576641415245831
batch 45 loss: 0.0005576247931458056
batch 50 loss: 0.0005576200434006751
batch 55 loss: 0.0005577045027166605
batch 60 loss: 0.0005577609292231501
batch 65 loss: 0.00055756097426638
batch 70 loss: 0.0005576775874942541
batch 75 loss: 0.0005577413830906153
batch 80 loss: 0.0005577172967605292
batch 85 loss: 0.000557623547501862
batch 90 loss: 0.0005576998577453196
batch 95 loss: 0.0005577928735874593
batch 100 loss: 0.0005575665389187634
batch 105 loss: 0.0005577556905336678
batch 110 loss: 0.0005576858529821038
batch 115 loss: 0.0005577095318585635
batch 120 loss: 0.0005576958879828453
batch 125 loss: 0.0005576948635280133
batch 130 loss: 0.0005577403237111867
batch 135 loss: 0.000557616981677711
batch 140 loss: 0.0005576440482400358
batch 145 loss: 0.000557720335200429
batch 150 loss: 0.0005577000905759632
batch 155 loss: 0.0005577365052886307
batch 160 loss: 0.000557619018945843
batch 165 loss: 0.0005576647585257888
batch 170 loss: 0.0005576879950240255
batch 175 loss: 0.0005576708703301847
batch 180 loss: 0.0005576399504207075
batch 185 loss: 0.0005577631993219257
batch 190 loss: 0.0005576173542067408
batch 195 loss: 0.0005577953648753464
batch 200 loss: 0.0005576855968683958
batch 205 loss: 0.0005577627453021705
batch 210 loss: 0.0005576379247941077
batch 215 loss: 0.0005577126634307205
batch 220 loss: 0.0005575528484769166
batch 225 loss: 0.000557684397790581
batch 230 loss: 0.000557623989880085
batch 235 loss: 0.000557700835634023
batch 240 loss: 0.0005576223717071116
Training Loss: 0.0005576833859474088
Validation Loss: 0.0005576934063962351
Epoch 20:
batch 5 loss: 0.0005577019066549838
batch 10 loss: 0.0005577661329880356
batch 15 loss: 0.0005576792755164206
batch 20 loss: 0.0005577780306339264
batch 25 loss: 0.0005575776332989335
batch 30 loss: 0.0005576970172114671
batch 35 loss: 0.0005577315925620497
batch 40 loss: 0.0005577021278440952
batch 45 loss: 0.0005576793570071459
batch 50 loss: 0.0005577441770583391
batch 55 loss: 0.0005576720344834029
batch 60 loss: 0.0005576386116445065
batch 65 loss: 0.0005577033851295709
batch 70 loss: 0.0005575848394073546
batch 75 loss: 0.0005576308001764118
batch 80 loss: 0.0005577217089012265
batch 85 loss: 0.0005576695897616446
batch 90 loss: 0.000557632592972368
batch 95 loss: 0.0005576233263127506
batch 100 loss: 0.0005576995434239506
batch 105 loss: 0.0005576798226684332
batch 110 loss: 0.0005577675648964942
batch 115 loss: 0.0005575739312916994
batch 120 loss: 0.0005577614647336304
batch 125 loss: 0.0005576964467763901
batch 130 loss: 0.0005577679141424597
batch 135 loss: 0.0005577478674240411
batch 140 loss: 0.0005576893338002265
batch 145 loss: 0.0005576102412305772
batch 150 loss: 0.0005575974006205797
batch 155 loss: 0.000557798775844276
batch 160 loss: 0.0005578037351369857
batch 165 loss: 0.0005576236639171839
batch 170 loss: 0.0005576244788244366
batch 175 loss: 0.0005576348979957402
batch 180 loss: 0.0005576867028139531
batch 185 loss: 0.0005576557596214116
batch 190 loss: 0.0005577106378041208
batch 195 loss: 0.0005576882744207978
batch 200 loss: 0.000557668344117701
batch 205 loss: 0.0005577632342465222
batch 210 loss: 0.0005576670984737575
batch 215 loss: 0.0005576137104071677
batch 220 loss: 0.0005576638737693429
batch 225 loss: 0.0005576811148785054
batch 230 loss: 0.000557673373259604
batch 235 loss: 0.0005577717674896121
batch 240 loss: 0.0005575477494858206
Training Loss: 0.0005576833929808345
Validation Loss: 0.0005576934306494271
Epoch 21:
batch 5 loss: 0.0005576664465479553
batch 10 loss: 0.0005576277617365122
batch 15 loss: 0.0005576565861701965
batch 20 loss: 0.0005577095085754991
batch 25 loss: 0.0005576817551627755
batch 30 loss: 0.0005577915115281939
batch 35 loss: 0.0005576355499215424
batch 40 loss: 0.0005577287636697292
batch 45 loss: 0.0005576484021730721
batch 50 loss: 0.0005576487514190376
batch 55 loss: 0.0005575298797339201
batch 60 loss: 0.0005576584837399424
batch 65 loss: 0.0005577217903919518
batch 70 loss: 0.0005576841533184052
batch 75 loss: 0.000557632907293737
batch 80 loss: 0.0005576818133704364
batch 85 loss: 0.0005576546187512577
batch 90 loss: 0.0005578191368840635
batch 95 loss: 0.0005576302180998027
batch 100 loss: 0.0005577403004281223
batch 105 loss: 0.0005575977265834808
batch 110 loss: 0.0005577180534601211
batch 115 loss: 0.0005575601477175951
batch 120 loss: 0.0005576664698310196
batch 125 loss: 0.0005576869589276612
batch 130 loss: 0.0005577407660894096
batch 135 loss: 0.0005577265401370823
batch 140 loss: 0.0005576638854108751
batch 145 loss: 0.000557552429381758
batch 150 loss: 0.0005577288451604545
batch 155 loss: 0.000557670381385833
batch 160 loss: 0.0005577816045843065
batch 165 loss: 0.0005576929193921387
batch 170 loss: 0.00055764967110008
batch 175 loss: 0.0005577610223554075
batch 180 loss: 0.0005576023599132895
batch 185 loss: 0.000557676050812006
batch 190 loss: 0.00055774359498173
batch 195 loss: 0.0005575360031798482
batch 200 loss: 0.0005577331292442977
batch 205 loss: 0.0005576688679866492
batch 210 loss: 0.0005576590425334871
batch 215 loss: 0.0005577266798354686
batch 220 loss: 0.0005576881347224116
batch 225 loss: 0.0005577258067205549
batch 230 loss: 0.0005578133859671652
batch 235 loss: 0.0005577695788815618
batch 240 loss: 0.0005577142117545008
Training Loss: 0.0005576833876451323
Validation Loss: 0.0005576934190078948
Epoch 22:
batch 5 loss: 0.0005576012306846678
batch 10 loss: 0.0005576267256401479
batch 15 loss: 0.0005576496361754835
batch 20 loss: 0.0005576751427724957
batch 25 loss: 0.0005577742587774992
batch 30 loss: 0.0005576354218646884
batch 35 loss: 0.0005576652940362692
batch 40 loss: 0.000557756086345762
batch 45 loss: 0.0005576804163865745
batch 50 loss: 0.0005577327217906713
batch 55 loss: 0.000557770801242441
batch 60 loss: 0.0005577004863880575
batch 65 loss: 0.0005576768540777266
batch 70 loss: 0.000557697075419128
batch 75 loss: 0.0005576556664891541
batch 80 loss: 0.0005576375522650778
batch 85 loss: 0.0005577979958616197
batch 90 loss: 0.0005576380877755583
batch 95 loss: 0.0005576892988756299
batch 100 loss: 0.0005576193449087441
batch 105 loss: 0.0005576260853558778
batch 110 loss: 0.0005576899857260287
batch 115 loss: 0.0005576448398642242
batch 120 loss: 0.000557660439517349
batch 125 loss: 0.000557783676777035
batch 130 loss: 0.000557699054479599
batch 135 loss: 0.0005575757706537843
batch 140 loss: 0.0005577063420787454
batch 145 loss: 0.0005577310454100371
batch 150 loss: 0.0005576424999162554
batch 155 loss: 0.0005576805095188319
batch 160 loss: 0.0005577450734563172
batch 165 loss: 0.0005576975992880762
batch 170 loss: 0.0005576349445618689
batch 175 loss: 0.0005577774019911886
batch 180 loss: 0.0005576225346885621
batch 185 loss: 0.0005577191477641463
batch 190 loss: 0.0005577675416134298
batch 195 loss: 0.0005576491355895996
batch 200 loss: 0.0005577232572250068
batch 205 loss: 0.0005576365161687136
batch 210 loss: 0.0005576593102887273
batch 215 loss: 0.0005576329655013979
batch 220 loss: 0.0005576445255428553
batch 225 loss: 0.0005577681004069746
batch 230 loss: 0.0005577270523644984
batch 235 loss: 0.0005575959221459925
batch 240 loss: 0.0005576813593506813
Training Loss: 0.0005576833903129833
Validation Loss: 0.0005576933996053413
Epoch 23:
batch 5 loss: 0.0005577177391387523
batch 10 loss: 0.000557679997291416
batch 15 loss: 0.0005576753290370106
batch 20 loss: 0.0005577033152803779
batch 25 loss: 0.0005576922558248043
batch 30 loss: 0.0005576722091063857
batch 35 loss: 0.00055764279095456
batch 40 loss: 0.0005576828843913972
batch 45 loss: 0.0005575982737354934
batch 50 loss: 0.0005576786352321506
batch 55 loss: 0.000557689683046192
batch 60 loss: 0.0005576393217779696
batch 65 loss: 0.0005577003350481391
batch 70 loss: 0.0005577436066232621
batch 75 loss: 0.000557759020011872
batch 80 loss: 0.0005577683914452791
batch 85 loss: 0.0005574959912337362
batch 90 loss: 0.0005576137220486999
batch 95 loss: 0.0005576019175350666
batch 100 loss: 0.0005576866562478245
batch 105 loss: 0.0005576790659688413
batch 110 loss: 0.0005575852817855775
batch 115 loss: 0.0005577084491960704
batch 120 loss: 0.0005576144438236951
batch 125 loss: 0.0005578136770054698
batch 130 loss: 0.0005577249685302377
batch 135 loss: 0.0005577382631599903
batch 140 loss: 0.0005576878087595105
batch 145 loss: 0.0005576802766881883
batch 150 loss: 0.0005577500443905592
batch 155 loss: 0.0005577506613917649
batch 160 loss: 0.0005576789611950516
batch 165 loss: 0.0005576698575168848
batch 170 loss: 0.0005576711031608283
batch 175 loss: 0.000557761185336858
batch 180 loss: 0.0005577848991379142
batch 185 loss: 0.0005577383679337799
batch 190 loss: 0.0005577068543061614
batch 195 loss: 0.0005576526164077222
batch 200 loss: 0.0005576537805609405
batch 205 loss: 0.000557650497648865
batch 210 loss: 0.0005577621050179005
batch 215 loss: 0.0005576967960223556
batch 220 loss: 0.0005575970164500177
batch 225 loss: 0.0005576630821451545
batch 230 loss: 0.000557618064340204
batch 235 loss: 0.0005576920113526285
batch 240 loss: 0.0005576307070441544
Training Loss: 0.000557683394193494
Validation Loss: 0.0005576934335598101
Epoch 24:
batch 5 loss: 0.0005577267380431295
batch 10 loss: 0.000557629985269159
batch 15 loss: 0.0005576833733357489
batch 20 loss: 0.0005574994604103268
batch 25 loss: 0.0005576477036811411
batch 30 loss: 0.0005576403113082051
batch 35 loss: 0.0005576796247623861
batch 40 loss: 0.0005577103584073484
batch 45 loss: 0.0005576370866037905
batch 50 loss: 0.0005576677154749632
batch 55 loss: 0.0005576079711318016
batch 60 loss: 0.000557730218861252
batch 65 loss: 0.0005576714873313903
batch 70 loss: 0.0005576350726187229
batch 75 loss: 0.0005576828494668007
batch 80 loss: 0.0005577163421548903
batch 85 loss: 0.0005576830124482512
batch 90 loss: 0.0005577839910984039
batch 95 loss: 0.0005576279712840915
batch 100 loss: 0.0005576382507570087
batch 105 loss: 0.0005578461568802595
batch 110 loss: 0.0005576703348197043
batch 115 loss: 0.0005576428025960922
batch 120 loss: 0.0005576179013587534
batch 125 loss: 0.0005576397874392569
batch 130 loss: 0.0005576497525908053
batch 135 loss: 0.000557786610443145
batch 140 loss: 0.0005577183561399579
batch 145 loss: 0.0005576481577008963
batch 150 loss: 0.000557716644834727
batch 155 loss: 0.0005576308467425406
batch 160 loss: 0.0005576180992648005
batch 165 loss: 0.0005577084957621992
batch 170 loss: 0.0005577366566285491
batch 175 loss: 0.0005577108240686357
batch 180 loss: 0.0005577362840995193
batch 185 loss: 0.000557791767641902
batch 190 loss: 0.0005576959927566349
batch 195 loss: 0.0005577644798904658
batch 200 loss: 0.0005575533490628004
batch 205 loss: 0.0005576725816354156
batch 210 loss: 0.0005576576339080929
batch 215 loss: 0.0005577517440542579
batch 220 loss: 0.0005576994153670967
batch 225 loss: 0.0005575900082476437
batch 230 loss: 0.0005576855735853314
batch 235 loss: 0.0005576583091169596
batch 240 loss: 0.0005579049466177821
Training Loss: 0.0005576833966188133
Validation Loss: 0.0005576934199780226
Epoch 25:
batch 5 loss: 0.0005577374249696732
batch 10 loss: 0.0005575666087679565
batch 15 loss: 0.0005577253643423319
batch 20 loss: 0.0005575770861469209
batch 25 loss: 0.0005576717667281628
batch 30 loss: 0.0005577219766564667
batch 35 loss: 0.0005577197181992233
batch 40 loss: 0.0005576604744419456
batch 45 loss: 0.0005577689153142273
batch 50 loss: 0.0005576576804742217
batch 55 loss: 0.0005576978554017841
batch 60 loss: 0.0005577483330853284
batch 65 loss: 0.0005576606490649283
batch 70 loss: 0.0005577207077294589
batch 75 loss: 0.0005576549097895622
batch 80 loss: 0.0005577246192842722
batch 85 loss: 0.0005577332340180874
batch 90 loss: 0.0005576834548264742
batch 95 loss: 0.0005576182040385902
batch 100 loss: 0.0005576582858338952
batch 105 loss: 0.0005577681120485067
batch 110 loss: 0.0005577601958066225
batch 115 loss: 0.0005577050847932697
batch 120 loss: 0.0005577061674557626
batch 125 loss: 0.0005576535011641681
batch 130 loss: 0.0005576149444095791
batch 135 loss: 0.000557722756639123
batch 140 loss: 0.0005576347932219506
batch 145 loss: 0.0005577552481554448
batch 150 loss: 0.0005576361203566193
batch 155 loss: 0.0005576753639616073
batch 160 loss: 0.0005576449679210782
batch 165 loss: 0.0005576216266490519
batch 170 loss: 0.0005577433621510863
batch 175 loss: 0.000557645969092846
batch 180 loss: 0.0005576605908572674
batch 185 loss: 0.0005576486932113766
batch 190 loss: 0.0005576802184805274
batch 195 loss: 0.0005576538736931979
batch 200 loss: 0.0005576883093453943
batch 205 loss: 0.0005577645264565944
batch 210 loss: 0.0005576599272899329
batch 215 loss: 0.0005576897878199816
batch 220 loss: 0.0005577143980190158
batch 225 loss: 0.0005576534080319106
batch 230 loss: 0.0005576055846177042
batch 235 loss: 0.0005577336996793747
batch 240 loss: 0.0005576542927883566
Training Loss: 0.000557683391525643
Validation Loss: 0.0005576934422909592
Epoch 26:
batch 5 loss: 0.0005576888099312783
batch 10 loss: 0.0005576240713708102
batch 15 loss: 0.0005575930117629468
batch 20 loss: 0.0005575369694270193
batch 25 loss: 0.0005577052943408489
batch 30 loss: 0.0005576582392677665
batch 35 loss: 0.0005576711380854249
batch 40 loss: 0.0005576918716542423
batch 45 loss: 0.0005575973074883223
batch 50 loss: 0.0005576734314672649
batch 55 loss: 0.0005576377385295928
batch 60 loss: 0.0005576783907599748
batch 65 loss: 0.0005576849449425936
batch 70 loss: 0.0005577150848694145
batch 75 loss: 0.0005576628260314465
batch 80 loss: 0.0005576201132498681
batch 85 loss: 0.0005577402305789291
batch 90 loss: 0.0005577679956331849
batch 95 loss: 0.0005576788447797298
batch 100 loss: 0.0005577149451710284
batch 105 loss: 0.0005576738039962947
batch 110 loss: 0.0005575709859840572
batch 115 loss: 0.0005576049792580306
batch 120 loss: 0.000557682674843818
batch 125 loss: 0.0005576193099841476
batch 130 loss: 0.0005576459923759102
batch 135 loss: 0.0005577377043664456
batch 140 loss: 0.0005577377043664456
batch 145 loss: 0.0005576959229074419
batch 150 loss: 0.0005578427924774587
batch 155 loss: 0.000557805085554719
batch 160 loss: 0.0005576367024332285
batch 165 loss: 0.0005576833384111524
batch 170 loss: 0.0005576575873419643
batch 175 loss: 0.0005576517782174051
batch 180 loss: 0.0005577419768087565
batch 185 loss: 0.0005576706957072019
batch 190 loss: 0.0005576354800723493
batch 195 loss: 0.0005577508127316833
batch 200 loss: 0.0005577244330197573
batch 205 loss: 0.00055775964865461
batch 210 loss: 0.0005576694500632584
batch 215 loss: 0.0005576981347985566
batch 220 loss: 0.0005577045143581927
batch 225 loss: 0.0005577972508035601
batch 230 loss: 0.0005576634546741843
batch 235 loss: 0.0005577516974881291
batch 240 loss: 0.0005576480063609779
Training Loss: 0.0005576833995291963
Validation Loss: 0.0005576933996053413
Epoch 27:
batch 5 loss: 0.0005576102994382381
batch 10 loss: 0.0005575723829679191
batch 15 loss: 0.0005576061084866524
batch 20 loss: 0.0005577223957516253
batch 25 loss: 0.0005577891250140965
batch 30 loss: 0.0005577143630944193
batch 35 loss: 0.0005576395895332098
batch 40 loss: 0.0005576818133704364
batch 45 loss: 0.0005577226402238011
batch 50 loss: 0.0005576605326496065
batch 55 loss: 0.0005575506831519306
batch 60 loss: 0.0005576922441832722
batch 65 loss: 0.0005576704395934939
batch 70 loss: 0.0005577313015237451
batch 75 loss: 0.000557793048210442
batch 80 loss: 0.0005576940253376961
batch 85 loss: 0.0005577921983785927
batch 90 loss: 0.0005576388328336179
batch 95 loss: 0.0005577495554462075
batch 100 loss: 0.0005576859228312969
batch 105 loss: 0.0005576745490543545
batch 110 loss: 0.0005576260969974101
batch 115 loss: 0.0005576847470365465
batch 120 loss: 0.0005576660856604576
batch 125 loss: 0.0005577808478847146
batch 130 loss: 0.0005577773554250598
batch 135 loss: 0.0005576335126534105
batch 140 loss: 0.0005576772266067565
batch 145 loss: 0.0005576689611189068
batch 150 loss: 0.0005577243864536285
batch 155 loss: 0.0005577303585596382
batch 160 loss: 0.0005575840710662306
batch 165 loss: 0.0005576181458309293
batch 170 loss: 0.0005576284020207822
batch 175 loss: 0.0005576055264100432
batch 180 loss: 0.0005576904281042516
batch 185 loss: 0.000557710649445653
batch 190 loss: 0.0005577117204666138
batch 195 loss: 0.0005578050506301225
batch 200 loss: 0.0005577336065471173
batch 205 loss: 0.0005575151764787734
batch 210 loss: 0.0005575764807872474
batch 215 loss: 0.0005577836418524384
batch 220 loss: 0.0005576693336479365
batch 225 loss: 0.0005576738039962947
batch 230 loss: 0.0005576973664574326
batch 235 loss: 0.0005577323609031737
batch 240 loss: 0.0005577058880589902
Training Loss: 0.0005576834017119836
Validation Loss: 0.0005576933996053413
Epoch 28:
batch 5 loss: 0.0005576602648943663
batch 10 loss: 0.0005577106261625886
batch 15 loss: 0.0005575891467742622
batch 20 loss: 0.0005577184027060866
batch 25 loss: 0.0005577521282248199
batch 30 loss: 0.0005576143739745021
batch 35 loss: 0.0005576997296884656
batch 40 loss: 0.0005576990079134703
batch 45 loss: 0.0005576019058935345
batch 50 loss: 0.0005577784846536815
batch 55 loss: 0.0005576210794970393
batch 60 loss: 0.0005576023599132895
batch 65 loss: 0.0005576177733018995
batch 70 loss: 0.0005577379022724926
batch 75 loss: 0.0005577316624112427
batch 80 loss: 0.0005577701609581709
batch 85 loss: 0.0005577815929427743
batch 90 loss: 0.0005576721974648535
batch 95 loss: 0.0005575561197474598
batch 100 loss: 0.0005577904987148941
batch 105 loss: 0.0005577433039434254
batch 110 loss: 0.0005577351781539619
batch 115 loss: 0.0005577973206527531
batch 120 loss: 0.0005576699040830135
batch 125 loss: 0.000557665282394737
batch 130 loss: 0.0005576784256845713
batch 135 loss: 0.0005577223491854966
batch 140 loss: 0.0005575345014221967
batch 145 loss: 0.0005575894960202276
batch 150 loss: 0.0005576549912802875
batch 155 loss: 0.0005575490766204893
batch 160 loss: 0.0005577544914558529
batch 165 loss: 0.0005578016047365963
batch 170 loss: 0.0005576065625064075
batch 175 loss: 0.000557764119002968
batch 180 loss: 0.0005577632342465222
batch 185 loss: 0.0005577020929194987
batch 190 loss: 0.0005577249103225768
batch 195 loss: 0.0005576101946644485
batch 200 loss: 0.0005577426985837519
batch 205 loss: 0.0005577019415795803
batch 210 loss: 0.0005575972609221935
batch 215 loss: 0.0005576348281465471
batch 220 loss: 0.0005576621857471764
batch 225 loss: 0.000557627493981272
batch 230 loss: 0.0005576924537308514
batch 235 loss: 0.0005577113362960518
batch 240 loss: 0.0005576614523306489
Training Loss: 0.00055768341893175
Validation Loss: 0.0005576934131871288
Epoch 29:
batch 5 loss: 0.0005575810791924596
batch 10 loss: 0.0005575997522100807
batch 15 loss: 0.0005576107301749289
batch 20 loss: 0.0005576865398325026
batch 25 loss: 0.0005577073548920453
batch 30 loss: 0.0005577401258051396
batch 35 loss: 0.0005577253643423319
batch 40 loss: 0.000557622138876468
batch 45 loss: 0.0005576456431299448
batch 50 loss: 0.0005576637922786176
batch 55 loss: 0.0005576220573857427
batch 60 loss: 0.0005576517549343407
batch 65 loss: 0.0005577142233960331
batch 70 loss: 0.0005577030824497342
batch 75 loss: 0.0005576856900006532
batch 80 loss: 0.0005576391238719224
batch 85 loss: 0.0005577183910645545
batch 90 loss: 0.0005577330361120403
batch 95 loss: 0.000557745003607124
batch 100 loss: 0.0005577215575613082
batch 105 loss: 0.0005577221047133208
batch 110 loss: 0.0005576734198257327
batch 115 loss: 0.0005576057475991548
batch 120 loss: 0.0005577080999501049
batch 125 loss: 0.0005576489958912135
batch 130 loss: 0.0005577546777203679
batch 135 loss: 0.0005576448631472886
batch 140 loss: 0.0005576801020652055
batch 145 loss: 0.0005577164120040834
batch 150 loss: 0.0005576200550422072
batch 155 loss: 0.0005577069940045476
batch 160 loss: 0.0005576185067184269
batch 165 loss: 0.0005576476571150125
batch 170 loss: 0.0005577840376645327
batch 175 loss: 0.0005577178904786706
batch 180 loss: 0.0005577305215410888
batch 185 loss: 0.0005577985779382288
batch 190 loss: 0.0005576570285484195
batch 195 loss: 0.0005576992058195174
batch 200 loss: 0.0005577333737164736
batch 205 loss: 0.0005576667492277921
batch 210 loss: 0.0005576275289058685
batch 215 loss: 0.000557662476785481
batch 220 loss: 0.000557750673033297
batch 225 loss: 0.0005577502073720097
batch 230 loss: 0.0005576618830673396
batch 235 loss: 0.0005577093572355807
batch 240 loss: 0.0005575897637754678
Training Loss: 0.0005576834031671751
Validation Loss: 0.0005576933996053413
Epoch 30:
batch 5 loss: 0.000557709881104529
batch 10 loss: 0.0005576809984631836
batch 15 loss: 0.0005576220690272748
batch 20 loss: 0.0005577481701038778
batch 25 loss: 0.0005576421972364187
batch 30 loss: 0.0005577252595685422
batch 35 loss: 0.0005576149211265147
batch 40 loss: 0.0005576899857260287
batch 45 loss: 0.0005577296600677073
batch 50 loss: 0.0005577451549470425
batch 55 loss: 0.0005577183095738292
batch 60 loss: 0.0005576470866799355
batch 65 loss: 0.0005576833616942167
batch 70 loss: 0.0005576950847171247
batch 75 loss: 0.0005576824070885778
batch 80 loss: 0.0005578050157055258
batch 85 loss: 0.0005577981588430703
batch 90 loss: 0.0005576988449320197
batch 95 loss: 0.0005577347939833999
batch 100 loss: 0.0005577092408202589
batch 105 loss: 0.000557604362256825
batch 110 loss: 0.0005575290648266673
batch 115 loss: 0.0005577187286689878
batch 120 loss: 0.0005576419294811785
batch 125 loss: 0.0005577803589403629
batch 130 loss: 0.0005577309522777796
batch 135 loss: 0.0005577832693234086
batch 140 loss: 0.000557671976275742
batch 145 loss: 0.0005575697869062424
batch 150 loss: 0.0005576369352638721
batch 155 loss: 0.000557613093405962
batch 160 loss: 0.0005576595431193709
batch 165 loss: 0.0005577506613917649
batch 170 loss: 0.0005577077507041394
batch 175 loss: 0.0005576180759817362
batch 180 loss: 0.0005577023257501423
batch 185 loss: 0.0005576293799094856
batch 190 loss: 0.0005576602183282375
batch 195 loss: 0.0005576363182626665
batch 200 loss: 0.0005575916497036814
batch 205 loss: 0.0005577140487730503
batch 210 loss: 0.0005576697760261596
batch 215 loss: 0.0005577403120696544
batch 220 loss: 0.0005576706724241375
batch 225 loss: 0.0005576973548159003
batch 230 loss: 0.0005576555384323
batch 235 loss: 0.0005577254923991859
batch 240 loss: 0.000557614432182163
Training Loss: 0.0005576834293606226
Validation Loss: 0.0005576934568428745
Epoch 31:
batch 5 loss: 0.0005577265867032111
batch 10 loss: 0.0005576947703957557
batch 15 loss: 0.0005575419403612613
batch 20 loss: 0.0005576754454523325
batch 25 loss: 0.000557823246344924
batch 30 loss: 0.0005577588104642928
batch 35 loss: 0.0005577233037911356
batch 40 loss: 0.0005576292634941638
batch 45 loss: 0.0005577270640060306
batch 50 loss: 0.0005577448522672057
batch 55 loss: 0.0005576342344284057
batch 60 loss: 0.0005577085772529245
batch 65 loss: 0.0005577397882007062
batch 70 loss: 0.0005576942465268075
batch 75 loss: 0.0005574997048825026
batch 80 loss: 0.0005577531526796519
batch 85 loss: 0.0005576065392233432
batch 90 loss: 0.0005576048046350479
batch 95 loss: 0.0005576363764703274
batch 100 loss: 0.0005575842806138098
batch 105 loss: 0.0005576231167651712
batch 110 loss: 0.0005576929310336709
batch 115 loss: 0.0005577928037382663
batch 120 loss: 0.0005576990661211312
batch 125 loss: 0.0005576996598392725
batch 130 loss: 0.000557751371525228
batch 135 loss: 0.0005577087984420359
batch 140 loss: 0.0005576153402216732
batch 145 loss: 0.0005577129311859607
batch 150 loss: 0.0005576698225922882
batch 155 loss: 0.0005576924071647227
batch 160 loss: 0.0005576356779783964
batch 165 loss: 0.000557656039018184
batch 170 loss: 0.0005575860268436372
batch 175 loss: 0.0005576180992648005
batch 180 loss: 0.0005576813244260847
batch 185 loss: 0.0005577495438046753
batch 190 loss: 0.0005577558884397149
batch 195 loss: 0.000557688344269991
batch 200 loss: 0.0005577078205533326
batch 205 loss: 0.0005576837924309075
batch 210 loss: 0.0005576958996243775
batch 215 loss: 0.0005576896481215953
batch 220 loss: 0.0005576152703724802
batch 225 loss: 0.0005576513358391821
batch 230 loss: 0.0005577298928983509
batch 235 loss: 0.0005576282041147351
batch 240 loss: 0.0005578658427111804
Training Loss: 0.0005576834143236434
Validation Loss: 0.0005576934927375987
Epoch 32:
batch 5 loss: 0.0005576603813096881
batch 10 loss: 0.0005577399511821568
batch 15 loss: 0.000557654770091176
batch 20 loss: 0.0005578219657763839
batch 25 loss: 0.0005576892872340977
batch 30 loss: 0.0005575795192271471
batch 35 loss: 0.0005577198695391417
batch 40 loss: 0.0005576683091931045
batch 45 loss: 0.0005576253985054791
batch 50 loss: 0.000557690521236509
batch 55 loss: 0.0005576395662501455
batch 60 loss: 0.0005577465053647757
batch 65 loss: 0.0005577034433372318
batch 70 loss: 0.0005577082047238946
batch 75 loss: 0.0005576850846409798
batch 80 loss: 0.0005576831870712339
batch 85 loss: 0.0005577541538514197
batch 90 loss: 0.0005577200907282531
batch 95 loss: 0.0005577253527007997
batch 100 loss: 0.0005577527685090899
batch 105 loss: 0.0005575901363044977
batch 110 loss: 0.0005577169009484351
batch 115 loss: 0.000557588564697653
batch 120 loss: 0.0005576802068389952
batch 125 loss: 0.0005576656083576381
batch 130 loss: 0.0005576029769144952
batch 135 loss: 0.0005577596602961421
batch 140 loss: 0.0005576631287112832
batch 145 loss: 0.0005576477386057376
batch 150 loss: 0.0005576534895226359
batch 155 loss: 0.0005576148629188538
batch 160 loss: 0.0005576004507020116
batch 165 loss: 0.000557809229940176
batch 170 loss: 0.0005576214869506657
batch 175 loss: 0.0005576389376074076
batch 180 loss: 0.0005576329887844623
batch 185 loss: 0.0005577389150857925
batch 190 loss: 0.000557639030739665
batch 195 loss: 0.0005577612202614546
batch 200 loss: 0.0005577050149440765
batch 205 loss: 0.0005576852592639626
batch 210 loss: 0.0005576263414695859
batch 215 loss: 0.0005576008465141058
batch 220 loss: 0.0005576514173299074
batch 225 loss: 0.0005578821059316396
batch 230 loss: 0.0005577073083259165
batch 235 loss: 0.000557709950953722
batch 240 loss: 0.0005576428025960922
Training Loss: 0.0005576834356664525
Validation Loss: 0.0005576936275853465
Epoch 33:
batch 5 loss: 0.0005577141069807112
batch 10 loss: 0.0005576310679316521
batch 15 loss: 0.0005577457603067159
batch 20 loss: 0.000557680323254317
batch 25 loss: 0.0005577119183726609
batch 30 loss: 0.0005577076110057533
batch 35 loss: 0.0005576931056566536
batch 40 loss: 0.0005577223957516253
batch 45 loss: 0.0005576191935688258
batch 50 loss: 0.0005575324990786612
batch 55 loss: 0.0005576809751801192
batch 60 loss: 0.000557724735699594
batch 65 loss: 0.0005577305564656854
batch 70 loss: 0.0005577511270530522
batch 75 loss: 0.0005578678566962481
batch 80 loss: 0.0005576823372393846
batch 85 loss: 0.0005577065283432602
batch 90 loss: 0.0005577203701250255
batch 95 loss: 0.0005577539908699691
batch 100 loss: 0.0005576359340921045
batch 105 loss: 0.0005576146999374032
batch 110 loss: 0.0005577232106588781
batch 115 loss: 0.0005576581810601055
batch 120 loss: 0.0005576892173849047
batch 125 loss: 0.0005576495546847582
batch 130 loss: 0.0005575931631028652
batch 135 loss: 0.0005576495663262903
batch 140 loss: 0.0005576318013481796
batch 145 loss: 0.0005575841874815524
batch 150 loss: 0.000557739520445466
batch 155 loss: 0.000557656236924231
batch 160 loss: 0.000557649729307741
batch 165 loss: 0.000557668844703585
batch 170 loss: 0.0005577616742812097
batch 175 loss: 0.0005577870644629002
batch 180 loss: 0.0005576413008384406
batch 185 loss: 0.0005575225222855806
batch 190 loss: 0.0005576661322265863
batch 195 loss: 0.000557760416995734
batch 200 loss: 0.0005576882627792657
batch 205 loss: 0.0005576108233071864
batch 210 loss: 0.0005577609408646822
batch 215 loss: 0.0005576516385190188
batch 220 loss: 0.000557648844551295
batch 225 loss: 0.0005577181000262499
batch 230 loss: 0.0005576042691245675
batch 235 loss: 0.0005576742114499211
batch 240 loss: 0.0005577891017310322
Training Loss: 0.0005576834502183677
Validation Loss: 0.0005576935412439828
Epoch 34:
batch 5 loss: 0.0005575641640461982
batch 10 loss: 0.0005575751769356429
batch 15 loss: 0.0005577881936915218
batch 20 loss: 0.0005576380644924939
batch 25 loss: 0.0005577175412327051
batch 30 loss: 0.000557740917429328
batch 35 loss: 0.0005576078547164798
batch 40 loss: 0.0005577370291575789
batch 45 loss: 0.0005576068768277764
batch 50 loss: 0.000557650753762573
batch 55 loss: 0.0005577002069912851
batch 60 loss: 0.0005576669005677104
batch 65 loss: 0.0005577226751483977
batch 70 loss: 0.0005577126983553171
batch 75 loss: 0.0005576754570938647
batch 80 loss: 0.0005578321404755115
batch 85 loss: 0.0005576651426963508
batch 90 loss: 0.0005575918825343251
batch 95 loss: 0.0005577994510531425
batch 100 loss: 0.0005577233154326677
batch 105 loss: 0.0005576409283094108
batch 110 loss: 0.0005577347590588033
batch 115 loss: 0.0005576279480010271
batch 120 loss: 0.0005577067844569683
batch 125 loss: 0.0005577176925726235
batch 130 loss: 0.0005576308001764118
batch 135 loss: 0.0005577755626291036
batch 140 loss: 0.0005576828727498651
batch 145 loss: 0.0005577241303399205
batch 150 loss: 0.0005576817784458399
batch 155 loss: 0.0005575218470767141
batch 160 loss: 0.0005577131058089435
batch 165 loss: 0.0005576122435741127
batch 170 loss: 0.0005576400784775615
batch 175 loss: 0.0005575435468927025
batch 180 loss: 0.0005577128613367677
batch 185 loss: 0.0005577059695497155
batch 190 loss: 0.0005577321513555944
batch 195 loss: 0.0005577418603934347
batch 200 loss: 0.0005576684488914907
batch 205 loss: 0.0005577768199145794
batch 210 loss: 0.0005577138508670032
batch 215 loss: 0.0005576477851718664
batch 220 loss: 0.0005576492985710502
batch 225 loss: 0.0005576712428592145
batch 230 loss: 0.0005577393108978868
batch 235 loss: 0.0005576416617259384
batch 240 loss: 0.0005577333271503448
Training Loss: 0.0005576834397894951
Validation Loss: 0.000557693528632323
Epoch 35:
batch 5 loss: 0.0005577263073064387
batch 10 loss: 0.0005577444564551115
batch 15 loss: 0.0005577006842941046
batch 20 loss: 0.0005576368072070182
batch 25 loss: 0.0005576648632995784
batch 30 loss: 0.000557623291388154
batch 35 loss: 0.0005576466908678412
batch 40 loss: 0.0005576102994382381
batch 45 loss: 0.0005576898111030459
batch 50 loss: 0.0005576841533184052
batch 55 loss: 0.000557652220595628
batch 60 loss: 0.0005576982162892819
batch 65 loss: 0.0005577230360358954
batch 70 loss: 0.0005575894727371633
batch 75 loss: 0.0005577838514000177
batch 80 loss: 0.000557676050812006
batch 85 loss: 0.0005578516284003854
batch 90 loss: 0.0005576110677793622
batch 95 loss: 0.0005577161209657788
batch 100 loss: 0.000557715364266187
batch 105 loss: 0.0005577316973358392
batch 110 loss: 0.0005576817551627755
batch 115 loss: 0.0005575596704147756
batch 120 loss: 0.0005575427087023855
batch 125 loss: 0.0005576403229497374
batch 130 loss: 0.0005577570991590619
batch 135 loss: 0.0005576842580921948
batch 140 loss: 0.0005576427094638348
batch 145 loss: 0.0005577265867032111
batch 150 loss: 0.0005576135590672493
batch 155 loss: 0.0005576225812546909
batch 160 loss: 0.0005577365052886307
batch 165 loss: 0.0005577651900239289
batch 170 loss: 0.0005575936054810881
batch 175 loss: 0.0005576485185883939
batch 180 loss: 0.0005576066207140685
batch 185 loss: 0.0005577307078056037
batch 190 loss: 0.0005576918716542423
batch 195 loss: 0.0005577471922151745
batch 200 loss: 0.0005576620227657258
batch 205 loss: 0.0005577908828854561
batch 210 loss: 0.0005577421863563359
batch 215 loss: 0.0005577868781983852
batch 220 loss: 0.0005575884599238635
batch 225 loss: 0.000557746144477278
batch 230 loss: 0.0005577092990279198
batch 235 loss: 0.0005577486474066973
batch 240 loss: 0.0005575646180659532
Training Loss: 0.0005576834727738363
Validation Loss: 0.0005576934180377672
Epoch 36:
batch 5 loss: 0.0005575854447670281
batch 10 loss: 0.0005576313706114888
batch 15 loss: 0.0005577541538514197
batch 20 loss: 0.0005576538736931979
batch 25 loss: 0.0005577584845013917
batch 30 loss: 0.0005576564697548747
batch 35 loss: 0.0005576477386057376
batch 40 loss: 0.000557593849953264
batch 45 loss: 0.0005576704628765583
batch 50 loss: 0.0005576342344284057
batch 55 loss: 0.0005577218602411449
batch 60 loss: 0.000557853898499161
batch 65 loss: 0.0005577026400715113
batch 70 loss: 0.0005577246542088687
batch 75 loss: 0.0005575850838795305
batch 80 loss: 0.0005577243631705642
batch 85 loss: 0.0005577457370236516
batch 90 loss: 0.0005576483439654112
batch 95 loss: 0.0005575815332122148
batch 100 loss: 0.0005577719188295305
batch 105 loss: 0.0005577952368184924
batch 110 loss: 0.0005576132563874126
batch 115 loss: 0.0005577516742050648
batch 120 loss: 0.0005577068310230971
batch 125 loss: 0.0005575890885666013
batch 130 loss: 0.000557660567574203
batch 135 loss: 0.0005577027332037687
batch 140 loss: 0.0005576697178184987
batch 145 loss: 0.0005576625932008028
batch 150 loss: 0.0005576130351983011
batch 155 loss: 0.0005576703348197043
batch 160 loss: 0.0005576738738454878
batch 165 loss: 0.0005577217321842909
batch 170 loss: 0.0005576751544140279
batch 175 loss: 0.0005575936753302813
batch 180 loss: 0.0005576360039412975
batch 185 loss: 0.0005575937684625387
batch 190 loss: 0.0005577518837526441
batch 195 loss: 0.000557696761097759
batch 200 loss: 0.0005576953175477683
batch 205 loss: 0.0005576405092142522
batch 210 loss: 0.0005576630821451545
batch 215 loss: 0.0005576939787715673
batch 220 loss: 0.0005576606257818639
batch 225 loss: 0.0005577496951445938
batch 230 loss: 0.0005577822215855121
batch 235 loss: 0.0005577645380981267
batch 240 loss: 0.0005577313480898738
Training Loss: 0.0005576834448826654
Validation Loss: 0.0005576935781088347
Epoch 37:
batch 5 loss: 0.0005576211027801037
batch 10 loss: 0.0005578130832873285
batch 15 loss: 0.0005575914983637631
batch 20 loss: 0.0005576365976594389
batch 25 loss: 0.0005576980183832347
batch 30 loss: 0.0005577095784246921
batch 35 loss: 0.0005575941293500363
batch 40 loss: 0.0005577453644946218
batch 45 loss: 0.0005577370757237077
batch 50 loss: 0.0005575000657700002
batch 55 loss: 0.000557639030739665
batch 60 loss: 0.0005576888215728104
batch 65 loss: 0.000557662220671773
batch 70 loss: 0.0005577038158662617
batch 75 loss: 0.0005577340256422758
batch 80 loss: 0.0005576387047767639
batch 85 loss: 0.0005576450261287391
batch 90 loss: 0.0005575798684731125
batch 95 loss: 0.0005576173309236765
batch 100 loss: 0.0005576482973992825
batch 105 loss: 0.0005577348521910608
batch 110 loss: 0.0005576516152359546
batch 115 loss: 0.0005577684030868113
batch 120 loss: 0.0005577040486969054
batch 125 loss: 0.000557757809292525
batch 130 loss: 0.0005575217423029244
batch 135 loss: 0.0005577244446612895
batch 140 loss: 0.0005577309872023761
batch 145 loss: 0.0005576881230808794
batch 150 loss: 0.000557677005417645
batch 155 loss: 0.0005577135714702308
batch 160 loss: 0.0005577232339419424
batch 165 loss: 0.000557646353263408
batch 170 loss: 0.0005576637224294246
batch 175 loss: 0.0005577760864980518
batch 180 loss: 0.0005577470292337239
batch 185 loss: 0.0005577654344961047
batch 190 loss: 0.0005576978321187199
batch 195 loss: 0.000557707401458174
batch 200 loss: 0.0005575628834776581
batch 205 loss: 0.000557668274268508
batch 210 loss: 0.0005576785653829575
batch 215 loss: 0.0005577105330303311
batch 220 loss: 0.0005578002310357988
batch 225 loss: 0.0005577111151069403
batch 230 loss: 0.0005576955853030085
batch 235 loss: 0.0005576547817327082
batch 240 loss: 0.0005577212781645357
Training Loss: 0.0005576834708335809
Validation Loss: 0.0005576935082596417
Epoch 38:
batch 5 loss: 0.0005576347815804183
batch 10 loss: 0.0005576842580921948
batch 15 loss: 0.000557777879294008
batch 20 loss: 0.0005577491596341133
batch 25 loss: 0.0005577475181780756
batch 30 loss: 0.0005575809511356056
batch 35 loss: 0.0005576641065999866
batch 40 loss: 0.0005577195901423692
batch 45 loss: 0.000557590916287154
batch 50 loss: 0.000557692814618349
batch 55 loss: 0.0005576623836532236
batch 60 loss: 0.0005576852709054947
batch 65 loss: 0.0005577232572250068
batch 70 loss: 0.0005576001829467713
batch 75 loss: 0.0005576896015554667
batch 80 loss: 0.0005576918367296458
batch 85 loss: 0.0005576579598709941
batch 90 loss: 0.0005576208932325244
batch 95 loss: 0.0005576751078478992
batch 100 loss: 0.0005576577852480114
batch 105 loss: 0.0005577049101702869
batch 110 loss: 0.0005576491472311318
batch 115 loss: 0.0005576658411882817
batch 120 loss: 0.0005576017778366804
batch 125 loss: 0.0005575991352088749
batch 130 loss: 0.0005577035946771503
batch 135 loss: 0.000557776540517807
batch 140 loss: 0.0005577357020229101
batch 145 loss: 0.000557701091747731
batch 150 loss: 0.0005578499287366867
batch 155 loss: 0.000557649799156934
batch 160 loss: 0.0005577192874625325
batch 165 loss: 0.0005577141186222434
batch 170 loss: 0.0005577196599915624
batch 175 loss: 0.0005576733266934753
batch 180 loss: 0.000557713897433132
batch 185 loss: 0.0005576430237852037
batch 190 loss: 0.0005575278890319168
batch 195 loss: 0.0005576589377596974
batch 200 loss: 0.0005576815223321318
batch 205 loss: 0.0005577678210102021
batch 210 loss: 0.0005576964584179222
batch 215 loss: 0.000557671720162034
batch 220 loss: 0.0005576876108534634
batch 225 loss: 0.0005576368421316147
batch 230 loss: 0.0005576637689955532
batch 235 loss: 0.0005577964126132429
batch 240 loss: 0.0005576895433478058
Training Loss: 0.0005576834492482401
Validation Loss: 0.0005576935228115569
Epoch 39:
batch 5 loss: 0.0005576139432378113
batch 10 loss: 0.0005577386589720845
batch 15 loss: 0.0005577445379458367
batch 20 loss: 0.0005577325471676886
batch 25 loss: 0.0005577875301241875
batch 30 loss: 0.000557667831890285
batch 35 loss: 0.000557661906350404
batch 40 loss: 0.000557674840092659
batch 45 loss: 0.0005576500436291099
batch 50 loss: 0.0005577361793257296
batch 55 loss: 0.0005577679141424597
batch 60 loss: 0.0005576546653173863
batch 65 loss: 0.0005576300201937557
batch 70 loss: 0.0005576724535785616
batch 75 loss: 0.0005576893803663552
batch 80 loss: 0.0005577491479925812
batch 85 loss: 0.0005576371680945158
batch 90 loss: 0.0005576359340921045
batch 95 loss: 0.0005577128962613642
batch 100 loss: 0.0005576741648837924
batch 105 loss: 0.0005577229545451701
batch 110 loss: 0.0005577120231464505
batch 115 loss: 0.000557613221462816
batch 120 loss: 0.0005576522904448211
batch 125 loss: 0.0005576712661422789
batch 130 loss: 0.0005576658062636852
batch 135 loss: 0.0005575375747866928
batch 140 loss: 0.0005577065516263246
batch 145 loss: 0.0005576760973781347
batch 150 loss: 0.0005576466326601803
batch 155 loss: 0.000557548797223717
batch 160 loss: 0.0005576619296334684
batch 165 loss: 0.0005577324191108346
batch 170 loss: 0.0005576293682679534
batch 175 loss: 0.0005577029893174768
batch 180 loss: 0.0005577472737058997
batch 185 loss: 0.0005577001837082207
batch 190 loss: 0.0005577490082941949
batch 195 loss: 0.0005576860392466187
batch 200 loss: 0.0005577268544584513
batch 205 loss: 0.0005576302530243993
batch 210 loss: 0.0005577397183515132
batch 215 loss: 0.000557614816352725
batch 220 loss: 0.0005576898693107069
batch 225 loss: 0.000557661207858473
batch 230 loss: 0.0005577593808993697
batch 235 loss: 0.0005575895309448242
batch 240 loss: 0.0005578018142841756
Training Loss: 0.0005576834924189218
Validation Loss: 0.0005576937187773486
Epoch 40:
batch 5 loss: 0.0005576884490437805
batch 10 loss: 0.0005577408708631992
batch 15 loss: 0.000557609077077359
batch 20 loss: 0.0005576405907049775
batch 25 loss: 0.0005576968309469521
batch 30 loss: 0.000557637622114271
batch 35 loss: 0.0005577889853157103
batch 40 loss: 0.0005575464223511517
batch 45 loss: 0.0005576231982558965
batch 50 loss: 0.000557597924489528
batch 55 loss: 0.0005576752359047532
batch 60 loss: 0.0005576880997978151
batch 65 loss: 0.0005576152354478836
batch 70 loss: 0.0005577986594289541
batch 75 loss: 0.0005577489966526628
batch 80 loss: 0.0005576915573328733
batch 85 loss: 0.0005576589843258262
batch 90 loss: 0.0005576379131525755
batch 95 loss: 0.000557700137142092
batch 100 loss: 0.0005576708354055882
batch 105 loss: 0.0005577312782406807
batch 110 loss: 0.000557689496781677
batch 115 loss: 0.0005577351665124297
batch 120 loss: 0.0005576549214310944
batch 125 loss: 0.0005577696254476904
batch 130 loss: 0.0005576620227657258
batch 135 loss: 0.000557708682026714
batch 140 loss: 0.0005577355739660561
batch 145 loss: 0.0005577677860856056
batch 150 loss: 0.0005577377276495099
batch 155 loss: 0.0005576572963036597
batch 160 loss: 0.0005576129420660436
batch 165 loss: 0.0005576963536441326
batch 170 loss: 0.0005576999392360449
batch 175 loss: 0.0005576808471232653
batch 180 loss: 0.0005576627678237856
batch 185 loss: 0.0005577269243076443
batch 190 loss: 0.0005577056901529432
batch 195 loss: 0.0005577909760177135
batch 200 loss: 0.0005576095543801784
batch 205 loss: 0.0005575457820668816
batch 210 loss: 0.0005576650961302221
batch 215 loss: 0.0005576519994065166
batch 220 loss: 0.0005576562951318919
batch 225 loss: 0.0005576818133704364
batch 230 loss: 0.0005576211726292968
batch 235 loss: 0.000557716703042388
batch 240 loss: 0.0005577775416895747
Training Loss: 0.0005576834916913261
Validation Loss: 0.0005576935800490901
Epoch 41:
batch 5 loss: 0.0005577161558903754
batch 10 loss: 0.0005575833376497031
batch 15 loss: 0.0005576109047979117
batch 20 loss: 0.0005577478324994445
batch 25 loss: 0.0005576008348725736
batch 30 loss: 0.0005576058407314122
batch 35 loss: 0.0005576602648943663
batch 40 loss: 0.000557725306134671
batch 45 loss: 0.0005576509749516845
batch 50 loss: 0.0005576654220931232
batch 55 loss: 0.0005575789604336023
batch 60 loss: 0.0005576067371293903
batch 65 loss: 0.0005578507902100682
batch 70 loss: 0.0005576672963798046
batch 75 loss: 0.0005578058306127786
batch 80 loss: 0.0005575421149842441
batch 85 loss: 0.000557715364266187
batch 90 loss: 0.0005576329189352691
batch 95 loss: 0.0005577320931479335
batch 100 loss: 0.0005577575881034136
batch 105 loss: 0.0005577941192314029
batch 110 loss: 0.0005577129311859607
batch 115 loss: 0.0005577449104748666
batch 120 loss: 0.0005576702416874469
batch 125 loss: 0.0005576518131420016
batch 130 loss: 0.0005576700903475284
batch 135 loss: 0.0005577671341598034
batch 140 loss: 0.0005576359224505722
batch 145 loss: 0.0005576684838160872
batch 150 loss: 0.0005576725816354156
batch 155 loss: 0.0005576170515269041
batch 160 loss: 0.0005575802875682712
batch 165 loss: 0.0005577025702223181
batch 170 loss: 0.0005577258067205549
batch 175 loss: 0.0005577865638770163
batch 180 loss: 0.0005577079835347831
batch 185 loss: 0.0005576601950451731
batch 190 loss: 0.0005577266681939363
batch 195 loss: 0.0005576547933742404
batch 200 loss: 0.0005576145369559527
batch 205 loss: 0.0005576775409281253
batch 210 loss: 0.000557671720162034
batch 215 loss: 0.000557765201665461
batch 220 loss: 0.0005576794850639999
batch 225 loss: 0.0005577612202614546
batch 230 loss: 0.0005577258067205549
batch 235 loss: 0.000557678344193846
batch 240 loss: 0.0005576286814175547
Training Loss: 0.0005576835261308588
Validation Loss: 0.0005576934335598101
Epoch 42:
batch 5 loss: 0.0005577331059612334
batch 10 loss: 0.0005576803698204458
batch 15 loss: 0.0005578283802606165
batch 20 loss: 0.0005577076110057533
batch 25 loss: 0.0005576241412200034
batch 30 loss: 0.0005576520110480487
batch 35 loss: 0.0005576372495852411
batch 40 loss: 0.0005577007425017655
batch 45 loss: 0.0005576917785219849
batch 50 loss: 0.0005576835596002638
batch 55 loss: 0.000557603023480624
batch 60 loss: 0.0005576285999268294
batch 65 loss: 0.00055763985728845
batch 70 loss: 0.000557673629373312
batch 75 loss: 0.0005577523959800601
batch 80 loss: 0.0005576536874286831
batch 85 loss: 0.0005576634895987809
batch 90 loss: 0.0005577029194682836
batch 95 loss: 0.0005576989613473415
batch 100 loss: 0.0005576398107223213
batch 105 loss: 0.0005577657255344092
batch 110 loss: 0.000557646038942039
batch 115 loss: 0.0005576325696893036
batch 120 loss: 0.0005577424773946405
batch 125 loss: 0.0005576316965743899
batch 130 loss: 0.0005576455965638161
batch 135 loss: 0.0005575409857556223
batch 140 loss: 0.0005576548632234335
batch 145 loss: 0.0005576748750172556
batch 150 loss: 0.000557681976351887
batch 155 loss: 0.0005576883675530552
batch 160 loss: 0.000557822606060654
batch 165 loss: 0.0005576239433139563
batch 170 loss: 0.0005576947238296271
batch 175 loss: 0.0005577544216066599
batch 180 loss: 0.0005578489741310477
batch 185 loss: 0.0005576224415563047
batch 190 loss: 0.0005577262258157134
batch 195 loss: 0.0005576338735409081
batch 200 loss: 0.0005576998461037874
batch 205 loss: 0.0005577129195444286
batch 210 loss: 0.0005576470168307424
batch 215 loss: 0.0005577054223977029
batch 220 loss: 0.0005577498930506408
batch 225 loss: 0.000557611754629761
batch 230 loss: 0.0005577028496190906
batch 235 loss: 0.0005577543983235955
batch 240 loss: 0.0005575953633524478
Training Loss: 0.000557683482717645
Validation Loss: 0.0005576934112468734
Epoch 43:
batch 5 loss: 0.0005576224066317081
batch 10 loss: 0.0005576444556936622
batch 15 loss: 0.0005575819406658411
batch 20 loss: 0.0005576369119808077
batch 25 loss: 0.0005576480063609779
batch 30 loss: 0.0005575625225901603
batch 35 loss: 0.0005577753763645887
batch 40 loss: 0.0005577207775786519
batch 45 loss: 0.0005578011972829699
batch 50 loss: 0.0005577200441621244
batch 55 loss: 0.000557686307001859
batch 60 loss: 0.0005576505325734615
batch 65 loss: 0.000557599263265729
batch 70 loss: 0.000557613221462816
batch 75 loss: 0.0005577321979217231
batch 80 loss: 0.0005578075419180095
batch 85 loss: 0.0005576448398642242
batch 90 loss: 0.0005577056901529432
batch 95 loss: 0.0005577256437391042
batch 100 loss: 0.0005577556439675391
batch 105 loss: 0.0005575699615292251
batch 110 loss: 0.0005576817784458399
batch 115 loss: 0.0005576359457336366
batch 120 loss: 0.0005575499148108065
batch 125 loss: 0.0005577439209446311
batch 130 loss: 0.0005576986353844404
batch 135 loss: 0.0005576980533078313
batch 140 loss: 0.0005575996707193554
batch 145 loss: 0.0005577283794991672
batch 150 loss: 0.0005577539326623082
batch 155 loss: 0.0005576361785642802
batch 160 loss: 0.0005576793337240815
batch 165 loss: 0.0005576093099080026
batch 170 loss: 0.0005577617674134671
batch 175 loss: 0.0005575590184889734
batch 180 loss: 0.0005577288917265833
batch 185 loss: 0.0005578164127655327
batch 190 loss: 0.0005577005678787828
batch 195 loss: 0.0005578042124398053
batch 200 loss: 0.0005577489850111305
batch 205 loss: 0.0005577234434895218
batch 210 loss: 0.0005576361552812159
batch 215 loss: 0.0005578342243097723
batch 220 loss: 0.0005577363423071802
batch 225 loss: 0.0005577049683779478
batch 230 loss: 0.0005575812770985067
batch 235 loss: 0.0005575853050686419
batch 240 loss: 0.0005576665047556162
Training Loss: 0.000557683491933858
Validation Loss: 0.0005576936945241566
Epoch 44:
batch 5 loss: 0.0005576930241659283
batch 10 loss: 0.0005577538628131151
batch 15 loss: 0.000557735248003155
batch 20 loss: 0.0005576641764491796
batch 25 loss: 0.0005576866329647601
batch 30 loss: 0.0005577457370236516
batch 35 loss: 0.0005577104981057346
batch 40 loss: 0.0005575961549766362
batch 45 loss: 0.000557713326998055
batch 50 loss: 0.000557721743825823
batch 55 loss: 0.0005576343741267919
batch 60 loss: 0.0005577984498813749
batch 65 loss: 0.0005576882394962013
batch 70 loss: 0.0005576867493800819
batch 75 loss: 0.0005576087394729256
batch 80 loss: 0.0005577023723162711
batch 85 loss: 0.0005576906376518309
batch 90 loss: 0.0005576763767749071
batch 95 loss: 0.0005576342111453414
batch 100 loss: 0.0005576576455496252
batch 105 loss: 0.00055768305901438
batch 110 loss: 0.0005576062016189098
batch 115 loss: 0.0005575966904871165
batch 120 loss: 0.0005575807648710906
batch 125 loss: 0.0005576412193477153
batch 130 loss: 0.0005576930823735892
batch 135 loss: 0.0005576855386607349
batch 140 loss: 0.0005577029660344124
batch 145 loss: 0.0005577663308940828
batch 150 loss: 0.0005576913245022297
batch 155 loss: 0.0005577387404628098
batch 160 loss: 0.0005576543277129531
batch 165 loss: 0.0005577137810178101
batch 170 loss: 0.000557720148935914
batch 175 loss: 0.00055763857671991
batch 180 loss: 0.0005576651659794152
batch 185 loss: 0.0005576980765908957
batch 190 loss: 0.0005576743627898395
batch 195 loss: 0.0005576315801590681
batch 200 loss: 0.0005577354924753309
batch 205 loss: 0.0005576895317062735
batch 210 loss: 0.0005577565520070493
batch 215 loss: 0.0005577201605774462
batch 220 loss: 0.0005577589850872755
batch 225 loss: 0.000557669228874147
batch 230 loss: 0.0005576423602178693
batch 235 loss: 0.0005575632094405591
batch 240 loss: 0.0005576946074143052
Training Loss: 0.0005576835472311358
Validation Loss: 0.0005576935402738551
Epoch 45:
batch 5 loss: 0.0005576760275289416
batch 10 loss: 0.0005576866678893566
batch 15 loss: 0.000557726772967726
batch 20 loss: 0.0005576902884058654
batch 25 loss: 0.000557737669441849
batch 30 loss: 0.0005577178555540741
batch 35 loss: 0.0005576827912591398
batch 40 loss: 0.0005577138392254711
batch 45 loss: 0.0005577516974881291
batch 50 loss: 0.000557620357722044
batch 55 loss: 0.00055768305901438
batch 60 loss: 0.0005575354443863034
batch 65 loss: 0.0005576559808105231
batch 70 loss: 0.0005577865173108876
batch 75 loss: 0.0005576278432272375
batch 80 loss: 0.000557713897433132
batch 85 loss: 0.0005576790659688413
batch 90 loss: 0.0005576208466663956
batch 95 loss: 0.0005576391587965191
batch 100 loss: 0.0005576001480221748
batch 105 loss: 0.0005576726864092052
batch 110 loss: 0.0005577539908699691
batch 115 loss: 0.0005577808129601181
batch 120 loss: 0.0005576388328336179
batch 125 loss: 0.0005577045259997249
batch 130 loss: 0.0005577073316089809
batch 135 loss: 0.0005576715804636479
batch 140 loss: 0.0005577320232987403
batch 145 loss: 0.0005576965399086475
batch 150 loss: 0.000557720719370991
batch 155 loss: 0.0005577798001468182
batch 160 loss: 0.0005576344905421138
batch 165 loss: 0.0005576513824053109
batch 170 loss: 0.000557678344193846
batch 175 loss: 0.000557771569583565
batch 180 loss: 0.0005576917785219849
batch 185 loss: 0.0005575716961175204
batch 190 loss: 0.0005576860159635544
batch 195 loss: 0.0005577613250352442
batch 200 loss: 0.0005577556323260069
batch 205 loss: 0.0005576840369030834
batch 210 loss: 0.0005576500669121742
batch 215 loss: 0.000557620101608336
batch 220 loss: 0.0005577184492722154
batch 225 loss: 0.0005576106486842036
batch 230 loss: 0.0005576571566052735
batch 235 loss: 0.0005576181341893971
batch 240 loss: 0.0005576402065344155
Training Loss: 0.0005576834543414103
Validation Loss: 0.0005576935509452596
Epoch 46:
batch 5 loss: 0.0005577146774157881
batch 10 loss: 0.0005576779134571552
batch 15 loss: 0.000557671335991472
batch 20 loss: 0.0005575847928412259
batch 25 loss: 0.0005576932919211686
batch 30 loss: 0.0005575921037234366
batch 35 loss: 0.0005576836643740535
batch 40 loss: 0.0005576703930273652
batch 45 loss: 0.0005576905678026378
batch 50 loss: 0.0005576719529926776
batch 55 loss: 0.0005576957948505879
batch 60 loss: 0.0005577029078267515
batch 65 loss: 0.0005577217321842909
batch 70 loss: 0.0005576939205639064
batch 75 loss: 0.0005575778428465128
batch 80 loss: 0.0005576740368269384
batch 85 loss: 0.0005576120223850012
batch 90 loss: 0.000557684141676873
batch 95 loss: 0.0005576889612711966
batch 100 loss: 0.0005578569252975285
batch 105 loss: 0.0005576632102020085
batch 110 loss: 0.0005577046074904501
batch 115 loss: 0.0005576779949478805
batch 120 loss: 0.0005575983319431544
batch 125 loss: 0.0005576163181103766
batch 130 loss: 0.0005576770636253059
batch 135 loss: 0.0005578604526817798
batch 140 loss: 0.0005576537572778761
batch 145 loss: 0.000557731557637453
batch 150 loss: 0.0005576083436608315
batch 155 loss: 0.00055760646937415
batch 160 loss: 0.000557667319662869
batch 165 loss: 0.000557653815485537
batch 170 loss: 0.0005576893105171621
batch 175 loss: 0.0005577808013185858
batch 180 loss: 0.000557769916485995
batch 185 loss: 0.0005576692754402756
batch 190 loss: 0.0005577532574534416
batch 195 loss: 0.0005576840369030834
batch 200 loss: 0.0005576678202487528
batch 205 loss: 0.000557696504984051
batch 210 loss: 0.000557694083545357
batch 215 loss: 0.0005576900206506252
batch 220 loss: 0.0005576717434450984
batch 225 loss: 0.0005576880532316864
batch 230 loss: 0.0005576421623118222
batch 235 loss: 0.0005576283670961857
batch 240 loss: 0.0005578041775152088
Training Loss: 0.0005576834948442411
Validation Loss: 0.0005576935014687479
Epoch 47:
batch 5 loss: 0.0005576516268774867
batch 10 loss: 0.0005576554453000426
batch 15 loss: 0.0005576347815804183
batch 20 loss: 0.0005577944568358362
batch 25 loss: 0.0005576863652095199
batch 30 loss: 0.0005577781586907804
batch 35 loss: 0.000557607738301158
batch 40 loss: 0.0005576225696131587
batch 45 loss: 0.0005576666677370668
batch 50 loss: 0.0005576952942647039
batch 55 loss: 0.00055757473455742
batch 60 loss: 0.0005577062373049557
batch 65 loss: 0.0005576275754719973
batch 70 loss: 0.0005578274372965098
batch 75 loss: 0.0005576754803769291
batch 80 loss: 0.0005576903116889298
batch 85 loss: 0.0005577665753662587
batch 90 loss: 0.0005576569004915654
batch 95 loss: 0.0005576288443990052
batch 100 loss: 0.0005578318145126104
batch 105 loss: 0.0005577562376856803
batch 110 loss: 0.0005576492287218571
batch 115 loss: 0.0005576259805820883
batch 120 loss: 0.0005576357245445252
batch 125 loss: 0.0005576350842602551
batch 130 loss: 0.0005576576106250287
batch 135 loss: 0.0005576630821451545
batch 140 loss: 0.0005577154224738479
batch 145 loss: 0.0005576330702751874
batch 150 loss: 0.0005577554693445563
batch 155 loss: 0.0005576502881012857
batch 160 loss: 0.000557699881028384
batch 165 loss: 0.0005575839546509087
batch 170 loss: 0.0005577463191002608
batch 175 loss: 0.0005576398689299822
batch 180 loss: 0.0005575211485847831
batch 185 loss: 0.0005577633390203118
batch 190 loss: 0.0005577771342359483
batch 195 loss: 0.0005577586591243744
batch 200 loss: 0.0005577268078923225
batch 205 loss: 0.0005576991708949209
batch 210 loss: 0.0005577762611210346
batch 215 loss: 0.0005576647352427244
batch 220 loss: 0.0005576170515269041
batch 225 loss: 0.0005577253876253962
batch 230 loss: 0.0005576652358286083
batch 235 loss: 0.0005576095078140497
batch 240 loss: 0.0005576750263571739
Training Loss: 0.000557683452158623
Validation Loss: 0.0005576934122170011
Epoch 48:
batch 5 loss: 0.0005575511138886213
batch 10 loss: 0.0005576534545980394
batch 15 loss: 0.0005576298222877085
batch 20 loss: 0.0005576491705141962
batch 25 loss: 0.0005577197065576911
batch 30 loss: 0.0005576720810495317
batch 35 loss: 0.0005576726864092052
batch 40 loss: 0.0005576922558248043
batch 45 loss: 0.0005576262949034572
batch 50 loss: 0.00055778386304155
batch 55 loss: 0.0005577345262281596
batch 60 loss: 0.0005577438394539058
batch 65 loss: 0.0005577341769821941
batch 70 loss: 0.0005577294039539992
batch 75 loss: 0.0005575984716415406
batch 80 loss: 0.0005576953990384936
batch 85 loss: 0.000557764119002968
batch 90 loss: 0.000557691662106663
batch 95 loss: 0.0005575882503762841
batch 100 loss: 0.0005577776348218322
batch 105 loss: 0.000557672861032188
batch 110 loss: 0.0005576369818300009
batch 115 loss: 0.0005576319294050336
batch 120 loss: 0.0005577158182859421
batch 125 loss: 0.0005577049450948834
batch 130 loss: 0.000557650753762573
batch 135 loss: 0.0005576832802034915
batch 140 loss: 0.0005576394847594202
batch 145 loss: 0.0005576970637775957
batch 150 loss: 0.0005576051189564168
batch 155 loss: 0.0005576736060902476
batch 160 loss: 0.0005576963303610682
batch 165 loss: 0.000557762326207012
batch 170 loss: 0.0005576298688538372
batch 175 loss: 0.0005576722673140466
batch 180 loss: 0.0005577392759732902
batch 185 loss: 0.000557648844551295
batch 190 loss: 0.0005576034192927182
batch 195 loss: 0.0005577484029345214
batch 200 loss: 0.0005576067720539868
batch 205 loss: 0.0005577309522777796
batch 210 loss: 0.000557605572976172
batch 215 loss: 0.0005576318013481796
batch 220 loss: 0.0005578087526373565
batch 225 loss: 0.0005578032461926341
batch 230 loss: 0.00055772002087906
batch 235 loss: 0.0005576341180130839
batch 240 loss: 0.0005577460746280849
Training Loss: 0.0005576834962994325
Validation Loss: 0.0005576936091529205
Epoch 49:
batch 5 loss: 0.0005577066331170499
batch 10 loss: 0.0005576748633757233
batch 15 loss: 0.0005577930249273777
batch 20 loss: 0.0005576599738560617
batch 25 loss: 0.0005574943963438273
batch 30 loss: 0.0005577589734457433
batch 35 loss: 0.000557735760230571
batch 40 loss: 0.0005576246534474194
batch 45 loss: 0.0005576864467002451
batch 50 loss: 0.0005575731513090432
batch 55 loss: 0.0005576954805292189
batch 60 loss: 0.0005575210321694613
batch 65 loss: 0.0005578026990406215
batch 70 loss: 0.0005576868657954037
batch 75 loss: 0.000557757297065109
batch 80 loss: 0.0005577231175266206
batch 85 loss: 0.0005576760973781347
batch 90 loss: 0.00055776028893888
batch 95 loss: 0.0005576970870606601
batch 100 loss: 0.0005577531061135233
batch 105 loss: 0.0005577198229730129
batch 110 loss: 0.0005577048053964972
batch 115 loss: 0.000557760416995734
batch 120 loss: 0.000557649985421449
batch 125 loss: 0.0005578299285843968
batch 130 loss: 0.0005576303577981889
batch 135 loss: 0.0005576983676292002
batch 140 loss: 0.0005577349103987217
batch 145 loss: 0.0005576815572567284
batch 150 loss: 0.0005576036404818296
batch 155 loss: 0.0005576808005571365
batch 160 loss: 0.0005577002302743495
batch 165 loss: 0.0005576469236984849
batch 170 loss: 0.0005576437804847955
batch 175 loss: 0.000557633675634861
batch 180 loss: 0.0005576990777626633
batch 185 loss: 0.0005576989497058094
batch 190 loss: 0.00055766343139112
batch 195 loss: 0.0005577304167672992
batch 200 loss: 0.0005577666219323873
batch 205 loss: 0.0005575835355557501
batch 210 loss: 0.0005576095194555819
batch 215 loss: 0.0005575706833042205
batch 220 loss: 0.0005576846189796925
batch 225 loss: 0.0005577893811278045
batch 230 loss: 0.0005576937342993915
batch 235 loss: 0.0005577364703640342
batch 240 loss: 0.0005574832553975284
Training Loss: 0.0005576835384999868
Validation Loss: 0.0005576939991442486
Epoch 50:
batch 5 loss: 0.0005576827912591398
batch 10 loss: 0.0005576044670306147
batch 15 loss: 0.0005577861214987933
batch 20 loss: 0.0005576532799750567
batch 25 loss: 0.0005576652125455439
batch 30 loss: 0.0005576018709689378
batch 35 loss: 0.0005576962023042143
batch 40 loss: 0.0005577307660132646
batch 45 loss: 0.0005577695090323686
batch 50 loss: 0.0005576799623668194
batch 55 loss: 0.0005576579947955907
batch 60 loss: 0.0005577542469836771
batch 65 loss: 0.0005577768897637725
batch 70 loss: 0.0005577344563789666
batch 75 loss: 0.0005578584270551801
batch 80 loss: 0.0005575841176323592
batch 85 loss: 0.0005577320116572082
batch 90 loss: 0.0005575985182076692
batch 95 loss: 0.000557768577709794
batch 100 loss: 0.0005576576688326896
batch 105 loss: 0.0005577823612838984
batch 110 loss: 0.0005576693918555975
batch 115 loss: 0.0005575638380832971
batch 120 loss: 0.000557674199808389
batch 125 loss: 0.0005578387645073235
batch 130 loss: 0.0005576910683885216
batch 135 loss: 0.0005576471914537251
batch 140 loss: 0.0005575890420004726
batch 145 loss: 0.0005576253519393503
batch 150 loss: 0.0005576085299253464
batch 155 loss: 0.0005577390897087753
batch 160 loss: 0.000557687331456691
batch 165 loss: 0.000557597610168159
batch 170 loss: 0.0005577531759627163
batch 175 loss: 0.0005575609859079123
batch 180 loss: 0.0005576988216489554
batch 185 loss: 0.0005577613366767765
batch 190 loss: 0.0005576659925282002
batch 195 loss: 0.0005576308467425406
batch 200 loss: 0.0005576223484240472
batch 205 loss: 0.0005576769355684519
batch 210 loss: 0.0005575463874265552
batch 215 loss: 0.0005577161093242467
batch 220 loss: 0.0005576026509515941
batch 225 loss: 0.0005577682517468929
batch 230 loss: 0.0005577239440754056
batch 235 loss: 0.0005576514406129718
batch 240 loss: 0.0005577225005254149
Training Loss: 0.0005576835123065393
Validation Loss: 0.000557693488857088
Epoch 51:
batch 5 loss: 0.0005576146650128067
batch 10 loss: 0.0005576661787927151
batch 15 loss: 0.0005577730713412166
batch 20 loss: 0.000557752640452236
batch 25 loss: 0.0005576624418608845
batch 30 loss: 0.0005577117553912103
batch 35 loss: 0.0005575639544986189
batch 40 loss: 0.0005576962954364717
batch 45 loss: 0.0005576804163865745
batch 50 loss: 0.0005576756200753152
batch 55 loss: 0.0005577338160946965
batch 60 loss: 0.0005575987976044417
batch 65 loss: 0.0005576468887738883
batch 70 loss: 0.000557781511452049
batch 75 loss: 0.0005576450959779323
batch 80 loss: 0.0005576953291893006
batch 85 loss: 0.000557667831890285
batch 90 loss: 0.0005576859111897647
batch 95 loss: 0.0005575680523179471
batch 100 loss: 0.0005576856434345246
batch 105 loss: 0.000557736970949918
batch 110 loss: 0.0005577463307417929
batch 115 loss: 0.0005575845716521144
batch 120 loss: 0.0005576534662395715
batch 125 loss: 0.0005577886593528091
batch 130 loss: 0.000557682488579303
batch 135 loss: 0.0005575959919951856
batch 140 loss: 0.0005577238043770194
batch 145 loss: 0.0005576779134571552
batch 150 loss: 0.0005576254683546722
batch 155 loss: 0.0005576713127084076
batch 160 loss: 0.000557837646920234
batch 165 loss: 0.000557644956279546
batch 170 loss: 0.0005577261676080525
batch 175 loss: 0.0005577076924964785
batch 180 loss: 0.0005576523137278855
batch 185 loss: 0.0005575430113822222
batch 190 loss: 0.0005577019997872412
batch 195 loss: 0.0005576876923441887
batch 200 loss: 0.0005576195893809199
batch 205 loss: 0.0005577108240686357
batch 210 loss: 0.0005577330244705081
batch 215 loss: 0.0005578042822889983
batch 220 loss: 0.000557603978086263
batch 225 loss: 0.0005576486233621836
batch 230 loss: 0.0005577738513238728
batch 235 loss: 0.0005576451192609965
batch 240 loss: 0.0005577742238529026
Training Loss: 0.0005576834977546241
Validation Loss: 0.0005576935557958981
Epoch 52:
batch 5 loss: 0.0005576511262916029
batch 10 loss: 0.0005576793104410171
batch 15 loss: 0.0005577053059823811
batch 20 loss: 0.0005576992058195174
batch 25 loss: 0.0005576033960096538
batch 30 loss: 0.00055768764577806
batch 35 loss: 0.0005576757015660405
batch 40 loss: 0.0005576139548793436
batch 45 loss: 0.0005575909628532827
batch 50 loss: 0.0005577214644290507
batch 55 loss: 0.0005576084833592177
batch 60 loss: 0.0005577359115704894
batch 65 loss: 0.0005577031173743307
batch 70 loss: 0.0005576954805292189
batch 75 loss: 0.0005576820694841444
batch 80 loss: 0.0005576608004048466
batch 85 loss: 0.0005577484378591179
batch 90 loss: 0.000557646993547678
batch 95 loss: 0.0005577181233093143
batch 100 loss: 0.0005577027099207043
batch 105 loss: 0.0005576674593612552
batch 110 loss: 0.0005576738272793591
batch 115 loss: 0.0005576618248596787
batch 120 loss: 0.000557591812685132
batch 125 loss: 0.0005576630937866867
batch 130 loss: 0.0005576680414378643
batch 135 loss: 0.0005576092517003417
batch 140 loss: 0.000557603093329817
batch 145 loss: 0.000557660311460495
batch 150 loss: 0.0005577575066126883
batch 155 loss: 0.0005576681927777827
batch 160 loss: 0.0005577750038355589
batch 165 loss: 0.0005576642579399049
batch 170 loss: 0.0005576671683229506
batch 175 loss: 0.0005577418836764991
batch 180 loss: 0.0005578769138082862
batch 185 loss: 0.0005577873205766081
batch 190 loss: 0.0005577049683779478
batch 195 loss: 0.0005575903924182057
batch 200 loss: 0.0005576795316301286
batch 205 loss: 0.0005576564581133425
batch 210 loss: 0.0005577643867582082
batch 215 loss: 0.000557700137142092
batch 220 loss: 0.0005577052710577845
batch 225 loss: 0.0005576386116445065
batch 230 loss: 0.0005577886593528091
batch 235 loss: 0.0005576254334300756
batch 240 loss: 0.0005576846771873534
Training Loss: 0.0005576834519160911
Validation Loss: 0.0005576934093066181
Epoch 53:
batch 5 loss: 0.0005576092517003417
batch 10 loss: 0.0005576797295361758
batch 15 loss: 0.0005576815339736641
batch 20 loss: 0.0005576553987339139
batch 25 loss: 0.000557763734832406
batch 30 loss: 0.0005577757372520864
batch 35 loss: 0.000557537004351616
batch 40 loss: 0.0005576669587753713
batch 45 loss: 0.0005577077623456717
batch 50 loss: 0.0005577317322604358
batch 55 loss: 0.0005576386232860386
batch 60 loss: 0.0005577554577030242
batch 65 loss: 0.0005577192641794682
batch 70 loss: 0.0005577381700277329
batch 75 loss: 0.000557658914476633
batch 80 loss: 0.0005576684372499586
batch 85 loss: 0.0005576829193159938
batch 90 loss: 0.0005576607887633145
batch 95 loss: 0.0005577057600021362
batch 100 loss: 0.0005576577153988182
batch 105 loss: 0.0005577029311098159
batch 110 loss: 0.000557797122746706
batch 115 loss: 0.000557596911676228
batch 120 loss: 0.0005576570169068873
batch 125 loss: 0.0005576958414167166
batch 130 loss: 0.0005576801486313343
batch 135 loss: 0.0005577096249908209
batch 140 loss: 0.0005578167387284338
batch 145 loss: 0.0005577759584411979
batch 150 loss: 0.0005576491821557283
batch 155 loss: 0.000557720335200429
batch 160 loss: 0.0005575615563429892
batch 165 loss: 0.0005576252355240285
batch 170 loss: 0.0005576794384978712
batch 175 loss: 0.0005576436989940703
batch 180 loss: 0.0005576826049946248
batch 185 loss: 0.0005576072609983385
batch 190 loss: 0.0005576963885687292
batch 195 loss: 0.0005576952593401074
batch 200 loss: 0.0005576706724241375
batch 205 loss: 0.0005577070754952729
batch 210 loss: 0.0005576429190114141
batch 215 loss: 0.0005577204865403473
batch 220 loss: 0.0005576254683546722
batch 225 loss: 0.0005576048977673053
batch 230 loss: 0.0005577901727519929
batch 235 loss: 0.0005577449686825275
batch 240 loss: 0.0005576129886321723
Training Loss: 0.0005576834958143687
Validation Loss: 0.0005576934025157243
Epoch 54:
batch 5 loss: 0.0005576880066655576
batch 10 loss: 0.0005577284493483603
batch 15 loss: 0.0005577053525485098
batch 20 loss: 0.0005576680530793964
batch 25 loss: 0.0005577378673478961
batch 30 loss: 0.0005576650612056256
batch 35 loss: 0.0005578100914135576
batch 40 loss: 0.0005576599971391261
batch 45 loss: 0.0005575762828812003
batch 50 loss: 0.0005576141411438584
batch 55 loss: 0.0005576779018156231
batch 60 loss: 0.0005576113937422634
batch 65 loss: 0.000557678414043039
batch 70 loss: 0.0005576150142587721
batch 75 loss: 0.0005577435251325369
batch 80 loss: 0.0005576384719461203
batch 85 loss: 0.0005576295428909361
batch 90 loss: 0.0005576575873419643
batch 95 loss: 0.0005576813826337456
batch 100 loss: 0.0005576094379648566
batch 105 loss: 0.0005576913361437618
batch 110 loss: 0.0005576243391260504
batch 115 loss: 0.0005576572963036597
batch 120 loss: 0.0005577026517130434
batch 125 loss: 0.0005576175753958523
batch 130 loss: 0.0005577087868005037
batch 135 loss: 0.0005576960975304246
batch 140 loss: 0.0005577276111580432
batch 145 loss: 0.0005577140371315181
batch 150 loss: 0.0005576894269324839
batch 155 loss: 0.0005576907773502171
batch 160 loss: 0.0005575864925049245
batch 165 loss: 0.0005577254109084606
batch 170 loss: 0.0005577226751483977
batch 175 loss: 0.0005577292526140809
batch 180 loss: 0.0005578202195465565
batch 185 loss: 0.0005578043288551271
batch 190 loss: 0.0005576715455390513
batch 195 loss: 0.0005575810442678631
batch 200 loss: 0.0005576664465479553
batch 205 loss: 0.0005577479489147664
batch 210 loss: 0.0005577486474066973
batch 215 loss: 0.0005577015806920826
batch 220 loss: 0.0005576220457442105
batch 225 loss: 0.0005577264935709536
batch 230 loss: 0.000557708996348083
batch 235 loss: 0.0005576094961725175
batch 240 loss: 0.0005577191011980176
Training Loss: 0.0005576834924189218
Validation Loss: 0.0005576934257987886
Epoch 55:
batch 5 loss: 0.0005577049218118191
batch 10 loss: 0.0005577243049629032
batch 15 loss: 0.0005577595322392881
batch 20 loss: 0.000557709636632353
batch 25 loss: 0.0005577447474934161
batch 30 loss: 0.0005577290430665016
batch 35 loss: 0.0005577600095421076
batch 40 loss: 0.0005577466217800975
batch 45 loss: 0.0005576671333983541
batch 50 loss: 0.0005576511961407959
batch 55 loss: 0.000557656108867377
batch 60 loss: 0.0005577472038567066
batch 65 loss: 0.0005576407420448959
batch 70 loss: 0.0005576999159529805
batch 75 loss: 0.0005577196832746267
batch 80 loss: 0.0005576261784881354
batch 85 loss: 0.0005576607654802501
batch 90 loss: 0.0005576230701990426
batch 95 loss: 0.0005575137212872506
batch 100 loss: 0.0005577493575401604
batch 105 loss: 0.0005577100091613829
batch 110 loss: 0.0005576193565502763
batch 115 loss: 0.0005577387637458742
batch 120 loss: 0.00055761793628335
batch 125 loss: 0.0005576179712079466
batch 130 loss: 0.0005576521391049028
batch 135 loss: 0.0005576529540121556
batch 140 loss: 0.0005577073781751097
batch 145 loss: 0.0005576588911935687
batch 150 loss: 0.0005575783085078001
batch 155 loss: 0.0005577283212915063
batch 160 loss: 0.0005577244912274182
batch 165 loss: 0.0005576483556069434
batch 170 loss: 0.0005576937575824559
batch 175 loss: 0.0005575331626459957
batch 180 loss: 0.0005576846189796925
batch 185 loss: 0.0005576326628215611
batch 190 loss: 0.0005577329779043793
batch 195 loss: 0.0005576309049502015
batch 200 loss: 0.0005576892872340977
batch 205 loss: 0.0005577206145972013
batch 210 loss: 0.0005577048636041581
batch 215 loss: 0.0005576521856710315
batch 220 loss: 0.0005577337346039712
batch 225 loss: 0.000557755387853831
batch 230 loss: 0.0005577072384767235
batch 235 loss: 0.0005576412426307797
batch 240 loss: 0.000557805469725281
Training Loss: 0.0005576834766543471
Validation Loss: 0.0005576934015455966
Epoch 56:
batch 5 loss: 0.0005577185424044728
batch 10 loss: 0.0005576540366746485
batch 15 loss: 0.0005577245261520148
batch 20 loss: 0.0005576593102887273
batch 25 loss: 0.0005576364346779883
batch 30 loss: 0.0005576871801167727
batch 35 loss: 0.0005575831746682525
batch 40 loss: 0.0005577957024797797
batch 45 loss: 0.0005576859461143613
batch 50 loss: 0.0005577385891228914
batch 55 loss: 0.0005577196017839015
batch 60 loss: 0.0005576333962380887
batch 65 loss: 0.0005576071212999523
batch 70 loss: 0.0005576422438025475
batch 75 loss: 0.0005576613708399236
batch 80 loss: 0.000557693897280842
batch 85 loss: 0.0005576871568337083
batch 90 loss: 0.0005577218136750162
batch 95 loss: 0.0005576314055360853
batch 100 loss: 0.0005576587980613112
batch 105 loss: 0.0005577268893830478
batch 110 loss: 0.0005576275638304651
batch 115 loss: 0.0005577211151830852
batch 120 loss: 0.0005576442112214863
batch 125 loss: 0.0005576385068707168
batch 130 loss: 0.000557719951029867
batch 135 loss: 0.0005577394971624017
batch 140 loss: 0.0005577973555773496
batch 145 loss: 0.0005576427676714957
batch 150 loss: 0.0005576892988756299
batch 155 loss: 0.0005576904746703804
batch 160 loss: 0.0005576700670644641
batch 165 loss: 0.0005577527917921543
batch 170 loss: 0.0005575109389610589
batch 175 loss: 0.0005576240015216172
batch 180 loss: 0.0005578233627602458
batch 185 loss: 0.0005576792638748885
batch 190 loss: 0.0005576679366640746
batch 195 loss: 0.000557692046277225
batch 200 loss: 0.0005577089381404221
batch 205 loss: 0.0005575771094299853
batch 210 loss: 0.000557630171533674
batch 215 loss: 0.0005577975302003324
batch 220 loss: 0.0005576798575930297
batch 225 loss: 0.0005577742587774992
batch 230 loss: 0.0005577227915637195
batch 235 loss: 0.0005577307427302002
batch 240 loss: 0.0005575861781835556
Training Loss: 0.00055768345555407
Validation Loss: 0.0005576934529623637
Epoch 57:
batch 5 loss: 0.0005577402887865901
batch 10 loss: 0.0005576926283538342
batch 15 loss: 0.0005577377742156386
batch 20 loss: 0.0005576697178184987
batch 25 loss: 0.0005577257485128939
batch 30 loss: 0.0005577107542194426
batch 35 loss: 0.0005576644907705486
batch 40 loss: 0.0005577216856181621
batch 45 loss: 0.000557684013620019
batch 50 loss: 0.0005576781462877989
batch 55 loss: 0.0005575723480433226
batch 60 loss: 0.0005576019175350666
batch 65 loss: 0.000557666807435453
batch 70 loss: 0.0005577002069912851
batch 75 loss: 0.0005576035473495722
batch 80 loss: 0.0005576882977038622
batch 85 loss: 0.0005576627794653177
batch 90 loss: 0.0005577295552939177
batch 95 loss: 0.0005576649098657071
batch 100 loss: 0.0005577861564233899
batch 105 loss: 0.0005578049691393972
batch 110 loss: 0.0005576450261287391
batch 115 loss: 0.0005576172843575478
batch 120 loss: 0.0005578318960033357
batch 125 loss: 0.0005576865747570992
batch 130 loss: 0.0005576980649493635
batch 135 loss: 0.0005576969240792095
batch 140 loss: 0.0005577059462666511
batch 145 loss: 0.000557759462390095
batch 150 loss: 0.0005576341645792127
batch 155 loss: 0.00055767911253497
batch 160 loss: 0.000557662290520966
batch 165 loss: 0.0005576786701567471
batch 170 loss: 0.0005576864816248417
batch 175 loss: 0.0005576803465373814
batch 180 loss: 0.0005576417897827923
batch 185 loss: 0.0005577145959250629
batch 190 loss: 0.0005577393923886121
batch 195 loss: 0.0005576269468292594
batch 200 loss: 0.0005577185889706016
batch 205 loss: 0.0005576544324867427
batch 210 loss: 0.0005577151780016721
batch 215 loss: 0.0005576470866799355
batch 220 loss: 0.0005577314645051956
batch 225 loss: 0.0005576681345701217
batch 230 loss: 0.0005576196941547096
batch 235 loss: 0.0005575486458837986
batch 240 loss: 0.0005576104973442853
Training Loss: 0.0005576834465803889
Validation Loss: 0.0005576935140804077
Epoch 58:
batch 5 loss: 0.0005577649688348174
batch 10 loss: 0.0005577644682489335
batch 15 loss: 0.0005576521740294993
batch 20 loss: 0.0005576279829256237
batch 25 loss: 0.0005577615112997592
batch 30 loss: 0.0005578522919677198
batch 35 loss: 0.0005575971677899361
batch 40 loss: 0.0005577083444222808
batch 45 loss: 0.0005576943862251937
batch 50 loss: 0.0005577069125138224
batch 55 loss: 0.000557708484120667
batch 60 loss: 0.0005577180883847177
batch 65 loss: 0.000557602196931839
batch 70 loss: 0.0005576350842602551
batch 75 loss: 0.0005577371106483042
batch 80 loss: 0.0005576527328230441
batch 85 loss: 0.0005576386814936995
batch 90 loss: 0.0005576954921707511
batch 95 loss: 0.0005576827912591398
batch 100 loss: 0.0005576084135100245
batch 105 loss: 0.0005576338502578437
batch 110 loss: 0.0005575330229476094
batch 115 loss: 0.0005576380877755583
batch 120 loss: 0.0005577339907176793
batch 125 loss: 0.0005576987168751657
batch 130 loss: 0.0005576514406129718
batch 135 loss: 0.0005577106727287173
batch 140 loss: 0.000557725562248379
batch 145 loss: 0.0005575668881647288
batch 150 loss: 0.0005577624076977372
batch 155 loss: 0.000557651522103697
batch 160 loss: 0.0005576707771979272
batch 165 loss: 0.0005577166215516627
batch 170 loss: 0.0005576583789661527
batch 175 loss: 0.0005577249219641089
batch 180 loss: 0.0005577122676186264
batch 185 loss: 0.0005576076684519649
batch 190 loss: 0.0005576713476330042
batch 195 loss: 0.0005576155730523169
batch 200 loss: 0.0005578241427429021
batch 205 loss: 0.0005576637689955532
batch 210 loss: 0.0005577205098234117
batch 215 loss: 0.0005578547250479459
batch 220 loss: 0.0005576619878411293
batch 225 loss: 0.000557591870892793
batch 230 loss: 0.0005576504510827363
batch 235 loss: 0.0005576792405918241
batch 240 loss: 0.0005576665746048093
Training Loss: 0.0005576834640426872
Validation Loss: 0.0005576934141572565
Epoch 59:
batch 5 loss: 0.0005576068535447121
batch 10 loss: 0.0005575749441049993
batch 15 loss: 0.0005576990661211312
batch 20 loss: 0.0005577508942224086
batch 25 loss: 0.000557659612968564
batch 30 loss: 0.0005577844916842878
batch 35 loss: 0.0005577213480137289
batch 40 loss: 0.0005577693227678537
batch 45 loss: 0.000557848799508065
batch 50 loss: 0.0005576743395067751
batch 55 loss: 0.0005576361087150872
batch 60 loss: 0.0005576566560193897
batch 65 loss: 0.0005576213472522796
batch 70 loss: 0.0005576348630711436
batch 75 loss: 0.0005576532217673958
batch 80 loss: 0.0005577377276495099
batch 85 loss: 0.0005577183910645545
batch 90 loss: 0.0005575937684625387
batch 95 loss: 0.0005577377043664456
batch 100 loss: 0.0005576974130235612
batch 105 loss: 0.0005577660747803747
batch 110 loss: 0.000557692174334079
batch 115 loss: 0.000557687203399837
batch 120 loss: 0.0005576022085733712
batch 125 loss: 0.0005575994728133082
batch 130 loss: 0.0005577126750722528
batch 135 loss: 0.000557675736490637
batch 140 loss: 0.000557610287796706
batch 145 loss: 0.0005576266325078904
batch 150 loss: 0.000557737157214433
batch 155 loss: 0.0005576639203354716
batch 160 loss: 0.0005576890194788575
batch 165 loss: 0.000557665282394737
batch 170 loss: 0.0005577631411142648
batch 175 loss: 0.0005575577146373689
batch 180 loss: 0.0005575830466113985
batch 185 loss: 0.0005577413248829544
batch 190 loss: 0.0005576507071964442
batch 195 loss: 0.0005577512085437775
batch 200 loss: 0.0005577863310463726
batch 205 loss: 0.0005576466559432447
batch 210 loss: 0.0005577018018811941
batch 215 loss: 0.0005576408118940889
batch 220 loss: 0.000557781953830272
batch 225 loss: 0.0005576869589276612
batch 230 loss: 0.0005575915216468275
batch 235 loss: 0.000557693699374795
batch 240 loss: 0.0005577236181125045
Training Loss: 0.0005576834419722825
Validation Loss: 0.0005576934083364904
Epoch 60:
batch 5 loss: 0.0005576085415668786
batch 10 loss: 0.0005576784722507
batch 15 loss: 0.0005577191011980176
batch 20 loss: 0.0005577364936470985
batch 25 loss: 0.0005577570525929331
batch 30 loss: 0.00055763527052477
batch 35 loss: 0.000557746912818402
batch 40 loss: 0.0005576788098551333
batch 45 loss: 0.0005577423027716577
batch 50 loss: 0.0005578277166932821
batch 55 loss: 0.0005576148745603859
batch 60 loss: 0.0005576663999818266
batch 65 loss: 0.0005576604045927525
batch 70 loss: 0.000557713396847248
batch 75 loss: 0.0005576698342338205
batch 80 loss: 0.0005577202304266393
batch 85 loss: 0.0005576643743552268
batch 90 loss: 0.0005577453877776862
batch 95 loss: 0.000557674712035805
batch 100 loss: 0.0005577254923991859
batch 105 loss: 0.0005577380885370076
batch 110 loss: 0.0005576817551627755
batch 115 loss: 0.0005576215218752623
batch 120 loss: 0.0005575799732469022
batch 125 loss: 0.0005576093681156635
batch 130 loss: 0.0005575126269832253
batch 135 loss: 0.000557637948077172
batch 140 loss: 0.0005577775766141713
batch 145 loss: 0.0005577386706136167
batch 150 loss: 0.0005578148760832846
batch 155 loss: 0.0005576303228735924
batch 160 loss: 0.0005577493109740316
batch 165 loss: 0.0005577820935286582
batch 170 loss: 0.0005575519986450672
batch 175 loss: 0.0005576250259764493
batch 180 loss: 0.0005576875875703991
batch 185 loss: 0.0005577147239819169
batch 190 loss: 0.0005576454917900264
batch 195 loss: 0.0005576417664997279
batch 200 loss: 0.0005576153751462698
batch 205 loss: 0.0005576923140324652
batch 210 loss: 0.0005576209281571209
batch 215 loss: 0.0005577252479270101
batch 220 loss: 0.0005577128380537033
batch 225 loss: 0.0005575431510806084
batch 230 loss: 0.0005575852817855775
batch 235 loss: 0.0005577833158895374
batch 240 loss: 0.0005578003940172494
Training Loss: 0.0005576834448826654
Validation Loss: 0.0005576937857161586
Epoch 61:
batch 5 loss: 0.0005577055038884282
batch 10 loss: 0.000557770871091634
batch 15 loss: 0.0005576427094638348
batch 20 loss: 0.0005577375646680594
batch 25 loss: 0.0005576507421210409
batch 30 loss: 0.0005577582283876837
batch 35 loss: 0.0005576888448558748
batch 40 loss: 0.0005577170639298856
batch 45 loss: 0.0005576383788138628
batch 50 loss: 0.0005575679242610931
batch 55 loss: 0.0005576972151175141
batch 60 loss: 0.000557556573767215
batch 65 loss: 0.0005576726165600121
batch 70 loss: 0.0005577355273999274
batch 75 loss: 0.0005576621741056442
batch 80 loss: 0.0005577407660894096
batch 85 loss: 0.0005577377043664456
batch 90 loss: 0.0005577371339313686
batch 95 loss: 0.0005576194846071303
batch 100 loss: 0.0005576304509304463
batch 105 loss: 0.000557648146059364
batch 110 loss: 0.0005576251423917711
batch 115 loss: 0.0005575893097557128
batch 120 loss: 0.0005577008705586195
batch 125 loss: 0.0005577439907938242
batch 130 loss: 0.0005576193681918085
batch 135 loss: 0.0005576660973019898
batch 140 loss: 0.0005578016745857895
batch 145 loss: 0.0005577779142186045
batch 150 loss: 0.0005575927323661744
batch 155 loss: 0.0005576047813519835
batch 160 loss: 0.0005576870869845151
batch 165 loss: 0.0005576552473939955
batch 170 loss: 0.0005576526862569153
batch 175 loss: 0.0005577624891884625
batch 180 loss: 0.0005577089148573577
batch 185 loss: 0.0005575941177085042
batch 190 loss: 0.0005577060510404408
batch 195 loss: 0.0005577080184593797
batch 200 loss: 0.0005577290081419051
batch 205 loss: 0.0005576646537519992
batch 210 loss: 0.0005577817442826927
batch 215 loss: 0.000557623733766377
batch 220 loss: 0.0005575971445068717
batch 225 loss: 0.000557828089222312
batch 230 loss: 0.0005576992407441139
batch 235 loss: 0.0005577542004175484
batch 240 loss: 0.0005576162016950548
Training Loss: 0.0005576834611323041
Validation Loss: 0.0005576934636337683
Epoch 62:
batch 5 loss: 0.0005575420800596476
batch 10 loss: 0.0005575826507993042
batch 15 loss: 0.0005576248746365309
batch 20 loss: 0.0005578021984547377
batch 25 loss: 0.0005577377625741065
batch 30 loss: 0.000557602196931839
batch 35 loss: 0.0005575887160375714
batch 40 loss: 0.0005575757473707199
batch 45 loss: 0.0005576409050263465
batch 50 loss: 0.0005576664116233587
batch 55 loss: 0.0005577026749961078
batch 60 loss: 0.0005577160860411823
batch 65 loss: 0.0005575928487814962
batch 70 loss: 0.0005577838979661465
batch 75 loss: 0.000557632464915514
batch 80 loss: 0.0005575867602601647
batch 85 loss: 0.0005577003466896713
batch 90 loss: 0.000557774945627898
batch 95 loss: 0.0005577197298407555
batch 100 loss: 0.0005577029660344124
batch 105 loss: 0.000557746400590986
batch 110 loss: 0.0005576959811151028
batch 115 loss: 0.0005577598116360605
batch 120 loss: 0.0005576009396463632
batch 125 loss: 0.0005576886818744242
batch 130 loss: 0.0005576860741712153
batch 135 loss: 0.0005575472023338079
batch 140 loss: 0.0005577895208261907
batch 145 loss: 0.0005577770643867552
batch 150 loss: 0.0005577996838837862
batch 155 loss: 0.0005576328374445438
batch 160 loss: 0.0005576881230808794
batch 165 loss: 0.0005576583906076848
batch 170 loss: 0.0005577652482315898
batch 175 loss: 0.0005577477742917836
batch 180 loss: 0.0005577150383032859
batch 185 loss: 0.0005576358526013792
batch 190 loss: 0.0005576542927883566
batch 195 loss: 0.0005576799274422228
batch 200 loss: 0.0005575484014116227
batch 205 loss: 0.000557714095339179
batch 210 loss: 0.0005577182513661682
batch 215 loss: 0.0005576312658376991
batch 220 loss: 0.0005577586125582457
batch 225 loss: 0.0005576702067628502
batch 230 loss: 0.0005577355972491205
batch 235 loss: 0.0005577208590693772
batch 240 loss: 0.0005577632924541831
Training Loss: 0.0005576834519160911
Validation Loss: 0.0005576934063962351
Epoch 63:
batch 5 loss: 0.0005577762145549059
batch 10 loss: 0.0005576237803325057
batch 15 loss: 0.0005576971685513854
batch 20 loss: 0.0005576683906838298
batch 25 loss: 0.0005575928604230285
batch 30 loss: 0.0005576831637881697
batch 35 loss: 0.0005576735245995223
batch 40 loss: 0.000557668914552778
batch 45 loss: 0.0005576342809945345
batch 50 loss: 0.0005576813593506813
batch 55 loss: 0.0005579083575867116
batch 60 loss: 0.0005577095900662244
batch 65 loss: 0.0005576674360781908
batch 70 loss: 0.0005576711497269571
batch 75 loss: 0.000557747355196625
batch 80 loss: 0.0005576764582656324
batch 85 loss: 0.0005577970878221095
batch 90 loss: 0.0005575879127718508
batch 95 loss: 0.0005576896248385311
batch 100 loss: 0.0005576358176767826
batch 105 loss: 0.000557624245993793
batch 110 loss: 0.0005576718831434846
batch 115 loss: 0.0005576275521889329
batch 120 loss: 0.000557644630316645
batch 125 loss: 0.0005577457719482482
batch 130 loss: 0.0005577744799666106
batch 135 loss: 0.0005576932220719754
batch 140 loss: 0.0005577875184826553
batch 145 loss: 0.0005577286588959396
batch 150 loss: 0.0005576486815698445
batch 155 loss: 0.0005577607662416995
batch 160 loss: 0.0005576353869400918
batch 165 loss: 0.0005576372146606446
batch 170 loss: 0.0005577286472544074
batch 175 loss: 0.0005576474592089653
batch 180 loss: 0.0005575443850830197
batch 185 loss: 0.0005576533963903785
batch 190 loss: 0.0005576913477852941
batch 195 loss: 0.0005577371106483042
batch 200 loss: 0.0005576925235800445
batch 205 loss: 0.0005576065508648753
batch 210 loss: 0.0005576800205744803
batch 215 loss: 0.0005577268893830478
batch 220 loss: 0.0005577260511927306
batch 225 loss: 0.000557654770091176
batch 230 loss: 0.0005576772033236921
batch 235 loss: 0.0005576475989073515
batch 240 loss: 0.0005576209397986532
Training Loss: 0.0005576834448826654
Validation Loss: 0.0005576935490050043
Epoch 64:
batch 5 loss: 0.0005577039555646479
batch 10 loss: 0.0005576214287430048
batch 15 loss: 0.0005576892173849047
batch 20 loss: 0.0005577189265750348
batch 25 loss: 0.0005576644674874842
batch 30 loss: 0.0005576092633418738
batch 35 loss: 0.0005577522446401417
batch 40 loss: 0.0005575744318775833
batch 45 loss: 0.000557777879294008
batch 50 loss: 0.000557745317928493
batch 55 loss: 0.0005576484254561365
batch 60 loss: 0.0005576177733018995
batch 65 loss: 0.0005576676339842379
batch 70 loss: 0.0005577115109190345
batch 75 loss: 0.0005576255382038653
batch 80 loss: 0.0005577217554673553
batch 85 loss: 0.0005576516734436155
batch 90 loss: 0.0005576122552156448
batch 95 loss: 0.000557682488579303
batch 100 loss: 0.0005576538736931979
batch 105 loss: 0.0005577428382821381
batch 110 loss: 0.000557676434982568
batch 115 loss: 0.000557718425989151
batch 120 loss: 0.0005576560157351196
batch 125 loss: 0.000557639729231596
batch 130 loss: 0.0005576632218435406
batch 135 loss: 0.0005577016738243401
batch 140 loss: 0.0005577176460064947
batch 145 loss: 0.000557636539451778
batch 150 loss: 0.0005575941177085042
batch 155 loss: 0.0005578356795012951
batch 160 loss: 0.0005577202769927681
batch 165 loss: 0.000557681790087372
batch 170 loss: 0.0005576381459832192
batch 175 loss: 0.0005577556439675391
batch 180 loss: 0.0005576775991357863
batch 185 loss: 0.000557699950877577
batch 190 loss: 0.0005577192991040647
batch 195 loss: 0.0005576796596869826
batch 200 loss: 0.0005576801369898022
batch 205 loss: 0.0005575805553235114
batch 210 loss: 0.000557727343402803
batch 215 loss: 0.0005577242118306458
batch 220 loss: 0.0005576815805397928
batch 225 loss: 0.000557608692906797
batch 230 loss: 0.0005577088799327612
batch 235 loss: 0.0005577456438913941
batch 240 loss: 0.0005577433737926185
Training Loss: 0.0005576834410021547
Validation Loss: 0.000557693449081853
Epoch 65:
batch 5 loss: 0.000557640683837235
batch 10 loss: 0.0005577295203693211
batch 15 loss: 0.0005577463307417929
batch 20 loss: 0.0005576967261731625
batch 25 loss: 0.0005577026633545757
batch 30 loss: 0.0005576656316407025
batch 35 loss: 0.0005575973074883223
batch 40 loss: 0.0005576240713708102
batch 45 loss: 0.0005576689727604389
batch 50 loss: 0.0005576889263466001
batch 55 loss: 0.0005577493691816926
batch 60 loss: 0.0005577331292442977
batch 65 loss: 0.0005576852941885591
batch 70 loss: 0.0005576075753197074
batch 75 loss: 0.000557767681311816
batch 80 loss: 0.0005577563657425344
batch 85 loss: 0.00055767095182091
batch 90 loss: 0.0005577136646024883
batch 95 loss: 0.0005576686933636665
batch 100 loss: 0.0005577108706347645
batch 105 loss: 0.0005577520467340946
batch 110 loss: 0.0005575718474574387
batch 115 loss: 0.0005577330128289759
batch 120 loss: 0.0005577468196861446
batch 125 loss: 0.0005577201023697853
batch 130 loss: 0.0005576952826231718
batch 135 loss: 0.0005577379488386214
batch 140 loss: 0.0005576157243922352
batch 145 loss: 0.0005576896481215953
batch 150 loss: 0.0005577716045081615
batch 155 loss: 0.000557554850820452
batch 160 loss: 0.0005576196825131774
batch 165 loss: 0.0005577732226811349
batch 170 loss: 0.0005576632800512015
batch 175 loss: 0.0005576033960096538
batch 180 loss: 0.0005576948286034166
batch 185 loss: 0.0005576558643952012
batch 190 loss: 0.0005577082862146199
batch 195 loss: 0.0005575934774242342
batch 200 loss: 0.0005576316500082612
batch 205 loss: 0.0005576108116656542
batch 210 loss: 0.0005577490315772593
batch 215 loss: 0.0005576993804425001
batch 220 loss: 0.0005576754221692682
batch 225 loss: 0.0005577110452577472
batch 230 loss: 0.0005577450734563172
batch 235 loss: 0.0005575650255195796
batch 240 loss: 0.0005576918134465814
Training Loss: 0.0005576834293606226
Validation Loss: 0.0005576934335598101
Epoch 66:
batch 5 loss: 0.0005577431991696357
batch 10 loss: 0.0005576058989390731
batch 15 loss: 0.0005576897179707885
batch 20 loss: 0.000557687459513545
batch 25 loss: 0.0005575634888373316
batch 30 loss: 0.0005577132804319262
batch 35 loss: 0.0005576716619543731
batch 40 loss: 0.0005576332798227668
batch 45 loss: 0.000557631824631244
batch 50 loss: 0.0005577543983235955
batch 55 loss: 0.0005576255614869296
batch 60 loss: 0.0005575907533057034
batch 65 loss: 0.0005576838739216328
batch 70 loss: 0.0005576414987444878
batch 75 loss: 0.0005576994037255645
batch 80 loss: 0.0005576407886110247
batch 85 loss: 0.00055761857656762
batch 90 loss: 0.0005577635951340198
batch 95 loss: 0.0005576137104071677
batch 100 loss: 0.0005576566327363253
batch 105 loss: 0.0005576510448008776
batch 110 loss: 0.0005576839088462293
batch 115 loss: 0.0005576839321292937
batch 120 loss: 0.0005577869364060461
batch 125 loss: 0.0005576738738454878
batch 130 loss: 0.0005577698117122054
batch 135 loss: 0.0005576820927672088
batch 140 loss: 0.0005578674841672182
batch 145 loss: 0.0005577011848799885
batch 150 loss: 0.0005576768890023232
batch 155 loss: 0.0005577070289291442
batch 160 loss: 0.0005576387164182961
batch 165 loss: 0.0005577662726864218
batch 170 loss: 0.0005577417672611773
batch 175 loss: 0.0005576763534918428
batch 180 loss: 0.0005577075993642211
batch 185 loss: 0.000557670125272125
batch 190 loss: 0.0005576684023253619
batch 195 loss: 0.0005577545729465782
batch 200 loss: 0.0005577161093242467
batch 205 loss: 0.0005575954215601087
batch 210 loss: 0.0005576268769800663
batch 215 loss: 0.0005576258525252342
batch 220 loss: 0.000557759718503803
batch 225 loss: 0.0005577246891334652
batch 230 loss: 0.0005576610914431512
batch 235 loss: 0.0005577047588303686
batch 240 loss: 0.0005576544208452106
Training Loss: 0.0005576834487631761
Validation Loss: 0.0005576933996053413
Epoch 67:
batch 5 loss: 0.0005578332929871976
batch 10 loss: 0.0005576657596975565
batch 15 loss: 0.0005576334311626852
batch 20 loss: 0.0005574693554081023
batch 25 loss: 0.0005576751893386245
batch 30 loss: 0.0005576631985604763
batch 35 loss: 0.00055768599268049
batch 40 loss: 0.0005576312891207635
batch 45 loss: 0.0005578154814429581
batch 50 loss: 0.0005575889023020864
batch 55 loss: 0.0005577614298090339
batch 60 loss: 0.0005576690775342286
batch 65 loss: 0.0005576004856266082
batch 70 loss: 0.0005576790310442448
batch 75 loss: 0.0005578331416472793
batch 80 loss: 0.0005577688571065664
batch 85 loss: 0.0005575850256718696
batch 90 loss: 0.0005575949675403535
batch 95 loss: 0.000557653559371829
batch 100 loss: 0.0005576150259003043
batch 105 loss: 0.0005576361203566193
batch 110 loss: 0.0005576697411015629
batch 115 loss: 0.0005577168893069028
batch 120 loss: 0.0005576634895987809
batch 125 loss: 0.0005577166215516627
batch 130 loss: 0.000557812093757093
batch 135 loss: 0.0005577264819294214
batch 140 loss: 0.0005577410687692463
batch 145 loss: 0.0005578210926614702
batch 150 loss: 0.0005576826515607536
batch 155 loss: 0.000557732058223337
batch 160 loss: 0.0005576717900112272
batch 165 loss: 0.0005576741765253246
batch 170 loss: 0.000557701790239662
batch 175 loss: 0.000557713711168617
batch 180 loss: 0.0005576625582762063
batch 185 loss: 0.0005576065275818109
batch 190 loss: 0.0005576756200753152
batch 195 loss: 0.0005576947121880948
batch 200 loss: 0.0005576913943514227
batch 205 loss: 0.0005575825343839824
batch 210 loss: 0.0005576889147050679
batch 215 loss: 0.0005576994153670967
batch 220 loss: 0.0005577412899583578
batch 225 loss: 0.0005576931056566536
batch 230 loss: 0.0005577088333666325
batch 235 loss: 0.0005576379946433008
batch 240 loss: 0.0005576192052103579
Training Loss: 0.0005576834245099841
Validation Loss: 0.0005576935063193862
Epoch 68:
batch 5 loss: 0.0005576307070441544
batch 10 loss: 0.0005577166564762592
batch 15 loss: 0.0005576579365879297
batch 20 loss: 0.0005577600677497685
batch 25 loss: 0.0005577417789027095
batch 30 loss: 0.0005576680996455252
batch 35 loss: 0.0005576157360337675
batch 40 loss: 0.0005576607072725892
batch 45 loss: 0.0005576120340265334
batch 50 loss: 0.0005576913477852941
batch 55 loss: 0.0005577028845436871
batch 60 loss: 0.0005577459465712309
batch 65 loss: 0.0005577166331931949
batch 70 loss: 0.0005576311610639096
batch 75 loss: 0.0005576365045271813
batch 80 loss: 0.0005577697651460767
batch 85 loss: 0.0005577559000812471
batch 90 loss: 0.0005577488336712122
batch 95 loss: 0.0005576717900112272
batch 100 loss: 0.0005576668540015816
batch 105 loss: 0.000557705364190042
batch 110 loss: 0.000557609717361629
batch 115 loss: 0.0005576933035627007
batch 120 loss: 0.0005576829775236547
batch 125 loss: 0.000557687075342983
batch 130 loss: 0.0005576995317824185
batch 135 loss: 0.0005577178788371384
batch 140 loss: 0.0005577975651249289
batch 145 loss: 0.0005577564937993884
batch 150 loss: 0.0005576755618676543
batch 155 loss: 0.0005577168310992419
batch 160 loss: 0.0005576412891969085
batch 165 loss: 0.000557685096282512
batch 170 loss: 0.0005576057126745581
batch 175 loss: 0.0005576330469921232
batch 180 loss: 0.0005578516284003854
batch 185 loss: 0.000557718612253666
batch 190 loss: 0.000557585817296058
batch 195 loss: 0.0005576977273449302
batch 200 loss: 0.0005576879833824932
batch 205 loss: 0.000557674840092659
batch 210 loss: 0.0005576872965320945
batch 215 loss: 0.0005576717900112272
batch 220 loss: 0.0005575941991992295
batch 225 loss: 0.0005575806251727044
batch 230 loss: 0.0005576263763941824
batch 235 loss: 0.000557712058071047
batch 240 loss: 0.0005576062947511673
Training Loss: 0.0005576834174765584
Validation Loss: 0.0005576934403507039
Epoch 69:
batch 5 loss: 0.0005576284136623144
batch 10 loss: 0.0005576325696893036
batch 15 loss: 0.0005577223142609
batch 20 loss: 0.0005576882278546691
batch 25 loss: 0.0005577664123848081
batch 30 loss: 0.0005577578558586538
batch 35 loss: 0.0005576432216912508
batch 40 loss: 0.0005577932228334248
batch 45 loss: 0.0005576695315539837
batch 50 loss: 0.0005576058756560087
batch 55 loss: 0.0005576967378146946
batch 60 loss: 0.0005576227325946092
batch 65 loss: 0.0005577047239057719
batch 70 loss: 0.0005576755502261221
batch 75 loss: 0.0005576993222348392
batch 80 loss: 0.0005576903466135263
batch 85 loss: 0.0005576787050813436
batch 90 loss: 0.0005576295196078717
batch 95 loss: 0.0005576996481977403
batch 100 loss: 0.0005576107883825898
batch 105 loss: 0.0005577024887315929
batch 110 loss: 0.0005576530587859452
batch 115 loss: 0.0005577687523327767
batch 120 loss: 0.0005578056443482637
batch 125 loss: 0.0005575745250098407
batch 130 loss: 0.0005578332697041333
batch 135 loss: 0.0005577172036282718
batch 140 loss: 0.0005576819297857582
batch 145 loss: 0.0005576846771873534
batch 150 loss: 0.000557658274192363
batch 155 loss: 0.0005576801369898022
batch 160 loss: 0.0005576129769906402
batch 165 loss: 0.0005577163072302937
batch 170 loss: 0.0005576643976382911
batch 175 loss: 0.0005576136987656355
batch 180 loss: 0.000557732826564461
batch 185 loss: 0.0005577140836976469
batch 190 loss: 0.0005576382041908801
batch 195 loss: 0.0005576624418608845
batch 200 loss: 0.0005576522438786924
batch 205 loss: 0.000557662989012897
batch 210 loss: 0.0005576737457886338
batch 215 loss: 0.0005576995899900794
batch 220 loss: 0.000557726772967726
batch 225 loss: 0.0005576777039095759
batch 230 loss: 0.0005576571566052735
batch 235 loss: 0.0005576261319220066
batch 240 loss: 0.0005576974712312221
Training Loss: 0.0005576834254801118
Validation Loss: 0.000557693427739044
Epoch 70:
batch 5 loss: 0.0005577878444455564
batch 10 loss: 0.0005577448871918022
batch 15 loss: 0.0005577157833613455
batch 20 loss: 0.0005576874711550773
batch 25 loss: 0.0005577270523644984
batch 30 loss: 0.0005576356896199286
batch 35 loss: 0.0005577693344093859
batch 40 loss: 0.0005576147814281285
batch 45 loss: 0.0005576202180236578
batch 50 loss: 0.0005577824311330914
batch 55 loss: 0.0005577531526796519
batch 60 loss: 0.0005576051538810134
batch 65 loss: 0.0005576069583185017
batch 70 loss: 0.0005577971576713025
batch 75 loss: 0.0005575878894887865
batch 80 loss: 0.0005577311152592301
batch 85 loss: 0.0005576919764280319
batch 90 loss: 0.000557748565915972
batch 95 loss: 0.0005577166099101305
batch 100 loss: 0.0005577217205427587
batch 105 loss: 0.000557650497648865
batch 110 loss: 0.0005576801486313343
batch 115 loss: 0.0005577542586252093
batch 120 loss: 0.0005576911731623113
batch 125 loss: 0.0005575856659561396
batch 130 loss: 0.0005576776689849794
batch 135 loss: 0.0005576439434662461
batch 140 loss: 0.0005577263305895031
batch 145 loss: 0.0005576166324317456
batch 150 loss: 0.0005577308125793934
batch 155 loss: 0.0005576651776209474
batch 160 loss: 0.000557593465782702
batch 165 loss: 0.0005577224190346896
batch 170 loss: 0.0005577674019150436
batch 175 loss: 0.0005577045492827892
batch 180 loss: 0.0005575806251727044
batch 185 loss: 0.0005576699273660779
batch 190 loss: 0.0005576229770667851
batch 195 loss: 0.0005576233961619437
batch 200 loss: 0.0005575422313995659
batch 205 loss: 0.0005577298114076256
batch 210 loss: 0.0005576670169830322
batch 215 loss: 0.0005576583673246205
batch 220 loss: 0.0005575658287853003
batch 225 loss: 0.0005577821284532547
batch 230 loss: 0.000557708868291229
batch 235 loss: 0.0005576831637881697
batch 240 loss: 0.0005577156553044915
Training Loss: 0.0005576834570092615
Validation Loss: 0.0005576934422909592
Epoch 71:
batch 5 loss: 0.0005576177965849638
batch 10 loss: 0.0005576532683335244
batch 15 loss: 0.0005577832227572799
batch 20 loss: 0.00055762252304703
batch 25 loss: 0.0005576267605647445
batch 30 loss: 0.0005577096715569496
batch 35 loss: 0.0005576666444540024
batch 40 loss: 0.0005577228497713804
batch 45 loss: 0.000557627237867564
batch 50 loss: 0.0005576660390943289
batch 55 loss: 0.0005577022675424814
batch 60 loss: 0.0005576677969656885
batch 65 loss: 0.0005577206960879266
batch 70 loss: 0.0005577517207711935
batch 75 loss: 0.0005576250609010458
batch 80 loss: 0.0005576244438998401
batch 85 loss: 0.000557601626496762
batch 90 loss: 0.0005577751668170095
batch 95 loss: 0.0005576955270953476
batch 100 loss: 0.000557593209668994
batch 105 loss: 0.0005577120813541114
batch 110 loss: 0.0005576484370976686
batch 115 loss: 0.000557749334257096
batch 120 loss: 0.0005576492054387927
batch 125 loss: 0.0005575442337431014
batch 130 loss: 0.0005576709983870388
batch 135 loss: 0.0005576058989390731
batch 140 loss: 0.0005577014177106321
batch 145 loss: 0.0005576852592639626
batch 150 loss: 0.0005576687050051987
batch 155 loss: 0.0005577032337896526
batch 160 loss: 0.0005578085663728416
batch 165 loss: 0.0005576156894676387
batch 170 loss: 0.0005577065865509212
batch 175 loss: 0.0005577749107033014
batch 180 loss: 0.0005577308475039899
batch 185 loss: 0.0005577194970101118
batch 190 loss: 0.0005577379837632179
batch 195 loss: 0.0005577457719482482
batch 200 loss: 0.000557664327789098
batch 205 loss: 0.0005577008123509586
batch 210 loss: 0.0005576279712840915
batch 215 loss: 0.0005577319418080152
batch 220 loss: 0.0005577291245572269
batch 225 loss: 0.0005576651194132865
batch 230 loss: 0.000557696248870343
batch 235 loss: 0.0005576398107223213
batch 240 loss: 0.0005577168776653707
Training Loss: 0.0005576834254801118
Validation Loss: 0.0005576934471415977
Epoch 72:
batch 5 loss: 0.0005577674368396401
batch 10 loss: 0.0005576134310103953
batch 15 loss: 0.0005577457486651838
batch 20 loss: 0.0005577565287239849
batch 25 loss: 0.0005576560040935874
batch 30 loss: 0.0005577363888733089
batch 35 loss: 0.0005576433963142335
batch 40 loss: 0.0005577350384555757
batch 45 loss: 0.000557607808150351
batch 50 loss: 0.000557570974342525
batch 55 loss: 0.0005576814408414065
batch 60 loss: 0.0005576287978328765
batch 65 loss: 0.0005575824994593859
batch 70 loss: 0.0005578239215537906
batch 75 loss: 0.0005577140138484538
batch 80 loss: 0.000557810184545815
batch 85 loss: 0.0005576725234277546
batch 90 loss: 0.0005576877039857209
batch 95 loss: 0.0005576661904342472
batch 100 loss: 0.0005576728144660592
batch 105 loss: 0.0005576007766649127
batch 110 loss: 0.0005576069699600339
batch 115 loss: 0.0005575963528826833
batch 120 loss: 0.0005576725234277546
batch 125 loss: 0.0005575678311288357
batch 130 loss: 0.0005577612435445189
batch 135 loss: 0.0005577837000600994
batch 140 loss: 0.0005577543750405311
batch 145 loss: 0.0005576781695708632
batch 150 loss: 0.0005577535252086818
batch 155 loss: 0.0005577333737164736
batch 160 loss: 0.0005577365984208882
batch 165 loss: 0.0005576650262810289
batch 170 loss: 0.0005576892406679689
batch 175 loss: 0.0005576211144216359
batch 180 loss: 0.0005575399729423225
batch 185 loss: 0.0005577157717198133
batch 190 loss: 0.0005577301839366555
batch 195 loss: 0.0005575844086706638
batch 200 loss: 0.000557621754705906
batch 205 loss: 0.0005577875534072518
batch 210 loss: 0.0005576461204327643
batch 215 loss: 0.0005576703231781721
batch 220 loss: 0.0005577265052124858
batch 225 loss: 0.0005576135823503137
batch 230 loss: 0.0005577408941462636
batch 235 loss: 0.0005577341071330011
batch 240 loss: 0.000557699054479599
Training Loss: 0.0005576834145661753
Validation Loss: 0.0005576934500519808
Epoch 73:
batch 5 loss: 0.000557702814694494
batch 10 loss: 0.0005576264811679721
batch 15 loss: 0.000557675666641444
batch 20 loss: 0.0005577532225288451
batch 25 loss: 0.0005577290430665016
batch 30 loss: 0.0005577064352110028
batch 35 loss: 0.0005576235125772655
batch 40 loss: 0.0005576276453211904
batch 45 loss: 0.0005576997878961265
batch 50 loss: 0.0005577689735218883
batch 55 loss: 0.0005577026051469148
batch 60 loss: 0.0005575840827077627
batch 65 loss: 0.0005575736169703305
batch 70 loss: 0.0005576642113737762
batch 75 loss: 0.0005577213945798576
batch 80 loss: 0.0005576236755587161
batch 85 loss: 0.0005577466450631619
batch 90 loss: 0.0005575707065872848
batch 95 loss: 0.0005575562128797174
batch 100 loss: 0.0005576674011535943
batch 105 loss: 0.0005576582625508308
batch 110 loss: 0.0005577335483394563
batch 115 loss: 0.0005577576928772032
batch 120 loss: 0.0005576552706770599
batch 125 loss: 0.0005576824303716421
batch 130 loss: 0.0005576778086833656
batch 135 loss: 0.00055775799555704
batch 140 loss: 0.0005577507545240223
batch 145 loss: 0.0005577015224844217
batch 150 loss: 0.0005576788447797298
batch 155 loss: 0.0005577628500759602
batch 160 loss: 0.000557641894556582
batch 165 loss: 0.000557681277859956
batch 170 loss: 0.0005577022675424814
batch 175 loss: 0.0005576561437919736
batch 180 loss: 0.0005577584612183273
batch 185 loss: 0.0005577284144237637
batch 190 loss: 0.0005575776449404657
batch 195 loss: 0.0005576594965532422
batch 200 loss: 0.0005577056552283465
batch 205 loss: 0.0005576598574407399
batch 210 loss: 0.0005577185074798763
batch 215 loss: 0.0005576540366746485
batch 220 loss: 0.0005576013587415219
batch 225 loss: 0.0005576512194238603
batch 230 loss: 0.000557788577862084
batch 235 loss: 0.000557685864623636
batch 240 loss: 0.0005577919888310135
Training Loss: 0.0005576834121408562
Validation Loss: 0.0005576934025157243
Epoch 74:
batch 5 loss: 0.0005575591931119561
batch 10 loss: 0.0005577567033469677
batch 15 loss: 0.0005576032097451388
batch 20 loss: 0.0005577227449975908
batch 25 loss: 0.0005576421273872257
batch 30 loss: 0.000557598180603236
batch 35 loss: 0.0005576443276368081
batch 40 loss: 0.0005576029769144952
batch 45 loss: 0.0005576013936661184
batch 50 loss: 0.0005577240022830665
batch 55 loss: 0.0005576835479587317
batch 60 loss: 0.0005576552241109312
batch 65 loss: 0.0005577136413194239
batch 70 loss: 0.0005576720694079995
batch 75 loss: 0.0005576669005677104
batch 80 loss: 0.0005577806616201997
batch 85 loss: 0.0005576102645136416
batch 90 loss: 0.0005577114061452448
batch 95 loss: 0.000557693256996572
batch 100 loss: 0.0005576680880039931
batch 105 loss: 0.0005577422678470611
batch 110 loss: 0.0005575866322033107
batch 115 loss: 0.0005575867253355682
batch 120 loss: 0.0005577897187322378
batch 125 loss: 0.0005577640258707106
batch 130 loss: 0.000557666306849569
batch 135 loss: 0.000557708228006959
batch 140 loss: 0.0005577961914241314
batch 145 loss: 0.0005577461211942137
batch 150 loss: 0.0005577169358730316
batch 155 loss: 0.0005576003226451576
batch 160 loss: 0.0005576802883297205
batch 165 loss: 0.0005576462252065539
batch 170 loss: 0.0005576946889050305
batch 175 loss: 0.0005577315459959209
batch 180 loss: 0.0005577415809966624
batch 185 loss: 0.0005575756076723337
batch 190 loss: 0.0005577119416557252
batch 195 loss: 0.0005577131290920079
batch 200 loss: 0.0005577337462455035
batch 205 loss: 0.000557643105275929
batch 210 loss: 0.000557638332247734
batch 215 loss: 0.0005577407660894096
batch 220 loss: 0.000557795783970505
batch 225 loss: 0.0005576518713496625
batch 230 loss: 0.0005576956551522017
batch 235 loss: 0.0005577588104642928
batch 240 loss: 0.0005576377036049962
Training Loss: 0.0005576834203869415
Validation Loss: 0.0005576933976650859
Epoch 75:
batch 5 loss: 0.0005578156327828765
batch 10 loss: 0.000557642092462629
batch 15 loss: 0.0005576065857894718
batch 20 loss: 0.0005576904164627195
batch 25 loss: 0.0005576122086495161
batch 30 loss: 0.0005576243740506471
batch 35 loss: 0.0005577295902185142
batch 40 loss: 0.0005576054332777858
batch 45 loss: 0.0005576679133810103
batch 50 loss: 0.0005578026291914284
batch 55 loss: 0.0005575654446147382
batch 60 loss: 0.0005577354226261377
batch 65 loss: 0.0005576229305006564
batch 70 loss: 0.0005576711962930858
batch 75 loss: 0.0005576414405368268
batch 80 loss: 0.000557681336067617
batch 85 loss: 0.0005576602416113019
batch 90 loss: 0.0005576944793574512
batch 95 loss: 0.0005576687748543918
batch 100 loss: 0.0005577085656113922
batch 105 loss: 0.0005577738746069371
batch 110 loss: 0.0005576512427069247
batch 115 loss: 0.0005576733499765396
batch 120 loss: 0.0005577391129918396
batch 125 loss: 0.0005575841409154236
batch 130 loss: 0.0005576326395384968
batch 135 loss: 0.0005576653988100588
batch 140 loss: 0.0005576453753747046
batch 145 loss: 0.0005577393691055477
batch 150 loss: 0.0005578081705607474
batch 155 loss: 0.0005576503113843501
batch 160 loss: 0.0005576661555096507
batch 165 loss: 0.000557720463257283
batch 170 loss: 0.0005576896364800632
batch 175 loss: 0.0005577168776653707
batch 180 loss: 0.0005577057716436684
batch 185 loss: 0.0005576801137067378
batch 190 loss: 0.0005577625473961234
batch 195 loss: 0.0005577113013714551
batch 200 loss: 0.0005576910334639251
batch 205 loss: 0.0005575879476964474
batch 210 loss: 0.0005576236289925873
batch 215 loss: 0.0005576992989517748
batch 220 loss: 0.0005576847004704177
batch 225 loss: 0.000557631382253021
batch 230 loss: 0.0005577833508141339
batch 235 loss: 0.0005577253527007997
batch 240 loss: 0.0005577141419053078
Training Loss: 0.0005576834041373028
Validation Loss: 0.0005576933966949582
Epoch 76:
batch 5 loss: 0.0005576563766226172
batch 10 loss: 0.0005577028845436871
batch 15 loss: 0.0005575788556598127
batch 20 loss: 0.0005577165866270661
batch 25 loss: 0.0005578148760832846
batch 30 loss: 0.0005576480994932354
batch 35 loss: 0.0005576462717726827
batch 40 loss: 0.0005576900904998184
batch 45 loss: 0.0005577251780778169
batch 50 loss: 0.0005577526520937681
batch 55 loss: 0.0005576159921474755
batch 60 loss: 0.0005576265743002295
batch 65 loss: 0.0005576505442149937
batch 70 loss: 0.0005577673669904471
batch 75 loss: 0.0005576805095188319
batch 80 loss: 0.0005578515469096601
batch 85 loss: 0.0005576150841079652
batch 90 loss: 0.0005576980882324278
batch 95 loss: 0.0005577249103225768
batch 100 loss: 0.000557779346127063
batch 105 loss: 0.000557725119870156
batch 110 loss: 0.0005576879368163646
batch 115 loss: 0.0005576131516136229
batch 120 loss: 0.0005577425821684301
batch 125 loss: 0.0005576832685619592
batch 130 loss: 0.0005577382165938616
batch 135 loss: 0.0005577640957199037
batch 140 loss: 0.0005576645955443383
batch 145 loss: 0.0005577093688771129
batch 150 loss: 0.0005575864226557315
batch 155 loss: 0.0005576863884925842
batch 160 loss: 0.0005577014409936965
batch 165 loss: 0.0005577061441726982
batch 170 loss: 0.0005576637922786176
batch 175 loss: 0.0005576964234933257
batch 180 loss: 0.0005575509858317673
batch 185 loss: 0.0005576775525696576
batch 190 loss: 0.0005576529656536877
batch 195 loss: 0.0005576694500632584
batch 200 loss: 0.0005577246891334652
batch 205 loss: 0.0005576858296990394
batch 210 loss: 0.0005577216623350977
batch 215 loss: 0.0005576534196734429
batch 220 loss: 0.000557602709159255
batch 225 loss: 0.0005576045601628721
batch 230 loss: 0.0005576523719355464
batch 235 loss: 0.0005577052477747201
batch 240 loss: 0.0005575908697210252
Training Loss: 0.0005576833978314728
Validation Loss: 0.0005576934209481503
Epoch 77:
batch 5 loss: 0.0005577032221481204
batch 10 loss: 0.0005575454677455127
batch 15 loss: 0.0005576736526563764
batch 20 loss: 0.0005576707306317985
batch 25 loss: 0.0005576308234594762
batch 30 loss: 0.0005577628850005567
batch 35 loss: 0.0005577577976509929
batch 40 loss: 0.0005577310919761657
batch 45 loss: 0.0005576542811468243
batch 50 loss: 0.0005576501600444317
batch 55 loss: 0.0005576713243499398
batch 60 loss: 0.0005578619660809636
batch 65 loss: 0.0005576409981586039
batch 70 loss: 0.0005576863652095199
batch 75 loss: 0.0005575668765231967
batch 80 loss: 0.0005576740950345993
batch 85 loss: 0.0005577547824941575
batch 90 loss: 0.0005575477494858206
batch 95 loss: 0.0005577087285928428
batch 100 loss: 0.0005577072617597878
batch 105 loss: 0.0005577155970968306
batch 110 loss: 0.0005576732335612177
batch 115 loss: 0.00055757409427315
batch 120 loss: 0.0005577660165727138
batch 125 loss: 0.0005576648400165141
batch 130 loss: 0.0005576479714363813
batch 135 loss: 0.0005577145842835307
batch 140 loss: 0.0005576204624958336
batch 145 loss: 0.0005577213130891323
batch 150 loss: 0.0005577929900027811
batch 155 loss: 0.0005576741299591958
batch 160 loss: 0.0005577074247412383
batch 165 loss: 0.0005576002062298357
batch 170 loss: 0.0005576526164077222
batch 175 loss: 0.0005576661904342472
batch 180 loss: 0.000557660183403641
batch 185 loss: 0.0005577103584073484
batch 190 loss: 0.0005577385192736984
batch 195 loss: 0.0005577617790549994
batch 200 loss: 0.0005576700554229319
batch 205 loss: 0.0005577232223004103
batch 210 loss: 0.0005577039439231158
batch 215 loss: 0.0005576750030741095
batch 220 loss: 0.0005576013238169253
batch 225 loss: 0.000557681790087372
batch 230 loss: 0.0005577480653300882
batch 235 loss: 0.0005576358293183148
batch 240 loss: 0.0005577013595029712
Training Loss: 0.000557683403409707
Validation Loss: 0.0005576934500519808
Epoch 78:
batch 5 loss: 0.0005576562718488276
batch 10 loss: 0.0005577455624006689
batch 15 loss: 0.0005576834199018776
batch 20 loss: 0.000557687075342983
batch 25 loss: 0.0005577260279096663
batch 30 loss: 0.0005577020347118378
batch 35 loss: 0.0005576803581789136
batch 40 loss: 0.0005577299511060118
batch 45 loss: 0.0005576249677687883
batch 50 loss: 0.00055774359498173
batch 55 loss: 0.0005577492993324995
batch 60 loss: 0.0005575756775215268
batch 65 loss: 0.0005576906842179596
batch 70 loss: 0.0005576649447903038
batch 75 loss: 0.0005577387870289386
batch 80 loss: 0.0005576605792157352
batch 85 loss: 0.0005577114992775023
batch 90 loss: 0.0005576908471994102
batch 95 loss: 0.0005576137569732964
batch 100 loss: 0.000557697715703398
batch 105 loss: 0.0005576305906288326
batch 110 loss: 0.0005577141186222434
batch 115 loss: 0.0005575679475441575
batch 120 loss: 0.0005576447118073701
batch 125 loss: 0.0005576112889684737
batch 130 loss: 0.0005576753639616073
batch 135 loss: 0.0005577055620960891
batch 140 loss: 0.0005576709867455065
batch 145 loss: 0.000557773164473474
batch 150 loss: 0.0005575923481956124
batch 155 loss: 0.0005578304873779417
batch 160 loss: 0.0005576656898483634
batch 165 loss: 0.0005576624185778201
batch 170 loss: 0.0005577523494139314
batch 175 loss: 0.000557692046277225
batch 180 loss: 0.0005577124655246734
batch 185 loss: 0.0005576304509304463
batch 190 loss: 0.0005577448988333345
batch 195 loss: 0.0005576407886110247
batch 200 loss: 0.0005576153402216732
batch 205 loss: 0.0005576825933530927
batch 210 loss: 0.0005577169009484351
batch 215 loss: 0.0005576957133598626
batch 220 loss: 0.0005576549447141588
batch 225 loss: 0.0005576794501394033
batch 230 loss: 0.0005577720236033201
batch 235 loss: 0.0005575864226557315
batch 240 loss: 0.0005577090778388083
Training Loss: 0.0005576834000142601
Validation Loss: 0.000557693464603896
Epoch 79:
batch 5 loss: 0.0005576209165155888
batch 10 loss: 0.0005576915107667446
batch 15 loss: 0.0005576149211265147
batch 20 loss: 0.0005576827097684145
batch 25 loss: 0.0005576217896305025
batch 30 loss: 0.0005576701136305928
batch 35 loss: 0.0005576340248808265
batch 40 loss: 0.000557692360598594
batch 45 loss: 0.0005576815339736641
batch 50 loss: 0.0005577255273237824
batch 55 loss: 0.0005577793694101274
batch 60 loss: 0.000557686376851052
batch 65 loss: 0.0005577128147706389
batch 70 loss: 0.0005576830706559121
batch 75 loss: 0.000557805597782135
batch 80 loss: 0.0005576781695708632
batch 85 loss: 0.0005576910567469895
batch 90 loss: 0.0005577311618253589
batch 95 loss: 0.0005576516850851476
batch 100 loss: 0.0005576787865720689
batch 105 loss: 0.0005576031515374779
batch 110 loss: 0.0005577881471253932
batch 115 loss: 0.0005578056676313281
batch 120 loss: 0.0005576894152909517
batch 125 loss: 0.0005577812320552766
batch 130 loss: 0.0005576276569627225
batch 135 loss: 0.0005576658877544105
batch 140 loss: 0.0005576173774898052
batch 145 loss: 0.000557718041818589
batch 150 loss: 0.0005576383438892663
batch 155 loss: 0.0005577061674557626
batch 160 loss: 0.000557546631898731
batch 165 loss: 0.0005577881122007966
batch 170 loss: 0.0005578301264904439
batch 175 loss: 0.000557669613044709
batch 180 loss: 0.0005576575174927711
batch 185 loss: 0.0005577742238529026
batch 190 loss: 0.000557609018869698
batch 195 loss: 0.0005575172719545663
batch 200 loss: 0.0005576347815804183
batch 205 loss: 0.0005577252944931388
batch 210 loss: 0.0005577550968155265
batch 215 loss: 0.0005576873780228197
batch 220 loss: 0.0005576519062742591
batch 225 loss: 0.0005576889147050679
batch 230 loss: 0.0005576040712185204
batch 235 loss: 0.00055761105613783
batch 240 loss: 0.0005576782510615885
Training Loss: 0.0005576834135960477
Validation Loss: 0.0005576934122170011
Epoch 80:
batch 5 loss: 0.0005577167845331133
batch 10 loss: 0.0005577308125793934
batch 15 loss: 0.0005576564115472137
batch 20 loss: 0.0005576694500632584
batch 25 loss: 0.0005576811963692307
batch 30 loss: 0.0005576468305662274
batch 35 loss: 0.0005577421747148037
batch 40 loss: 0.0005576287163421512
batch 45 loss: 0.0005576447932980954
batch 50 loss: 0.0005577058298513293
batch 55 loss: 0.0005576680647209287
batch 60 loss: 0.0005575802642852068
batch 65 loss: 0.0005575989838689565
batch 70 loss: 0.000557642593048513
batch 75 loss: 0.0005576189956627786
batch 80 loss: 0.0005576115800067782
batch 85 loss: 0.000557670125272125
batch 90 loss: 0.0005576379247941077
batch 95 loss: 0.0005576144787482918
batch 100 loss: 0.0005576402531005442
batch 105 loss: 0.0005576418596319854
batch 110 loss: 0.0005577109986916184
batch 115 loss: 0.0005577167845331133
batch 120 loss: 0.0005576726514846086
batch 125 loss: 0.0005577045609243214
batch 130 loss: 0.0005577262840233743
batch 135 loss: 0.0005577126634307205
batch 140 loss: 0.0005577126867137849
batch 145 loss: 0.0005577299161814153
batch 150 loss: 0.0005578571115620434
batch 155 loss: 0.0005575549439527094
batch 160 loss: 0.0005576511844992638
batch 165 loss: 0.0005576527095399797
batch 170 loss: 0.0005577142699621617
batch 175 loss: 0.0005577856907621026
batch 180 loss: 0.0005576846189796925
batch 185 loss: 0.0005577278207056224
batch 190 loss: 0.0005576992989517748
batch 195 loss: 0.0005577793228439987
batch 200 loss: 0.000557739904616028
batch 205 loss: 0.000557613791897893
batch 210 loss: 0.0005578300566412509
batch 215 loss: 0.0005576652707532048
batch 220 loss: 0.0005577106145210564
batch 225 loss: 0.0005577053758315742
batch 230 loss: 0.0005576840252615511
batch 235 loss: 0.0005576172145083547
batch 240 loss: 0.0005576950497925282
Training Loss: 0.000557683394678558
Validation Loss: 0.0005576934190078948
Epoch 81:
batch 5 loss: 0.0005576286115683615
batch 10 loss: 0.000557762139942497
batch 15 loss: 0.0005576692754402756
batch 20 loss: 0.0005577304866164923
batch 25 loss: 0.0005577846779488028
batch 30 loss: 0.0005576085182838142
batch 35 loss: 0.000557617109734565
batch 40 loss: 0.0005576556315645576
batch 45 loss: 0.0005577428615652025
batch 50 loss: 0.0005576978204771876
batch 55 loss: 0.0005575301474891603
batch 60 loss: 0.0005577141419053078
batch 65 loss: 0.0005576252937316894
batch 70 loss: 0.0005576980649493635
batch 75 loss: 0.0005576431867666543
batch 80 loss: 0.000557765131816268
batch 85 loss: 0.0005576760857366025
batch 90 loss: 0.00055772167397663
batch 95 loss: 0.0005577160860411823
batch 100 loss: 0.0005578289274126291
batch 105 loss: 0.0005576531868427992
batch 110 loss: 0.0005577389383688569
batch 115 loss: 0.000557691662106663
batch 120 loss: 0.0005576373427174985
batch 125 loss: 0.0005577743286266923
batch 130 loss: 0.000557674653828144
batch 135 loss: 0.0005576591822318733
batch 140 loss: 0.0005577430827543139
batch 145 loss: 0.0005576991708949209
batch 150 loss: 0.0005576474242843688
batch 155 loss: 0.0005577142233960331
batch 160 loss: 0.0005577190197072924
batch 165 loss: 0.0005576312658376991
batch 170 loss: 0.000557677960023284
batch 175 loss: 0.0005575348855927587
batch 180 loss: 0.0005577971343882382
batch 185 loss: 0.000557626609224826
batch 190 loss: 0.0005576739553362131
batch 195 loss: 0.0005577113246545195
batch 200 loss: 0.0005575779709033668
batch 205 loss: 0.0005576491123065353
batch 210 loss: 0.0005576774827204644
batch 215 loss: 0.0005576809868216515
batch 220 loss: 0.0005576924420893192
batch 225 loss: 0.0005577486590482295
batch 230 loss: 0.000557649286929518
batch 235 loss: 0.0005576360621489584
batch 240 loss: 0.0005576700787059963
Training Loss: 0.0005576834021970475
Validation Loss: 0.0005576934549026191
Epoch 82:
batch 5 loss: 0.0005577033502049744
batch 10 loss: 0.0005576170631684362
batch 15 loss: 0.0005575808230787515
batch 20 loss: 0.0005576586583629251
batch 25 loss: 0.0005577229312621057
batch 30 loss: 0.0005577247706241905
batch 35 loss: 0.0005576787400059402
batch 40 loss: 0.000557703198865056
batch 45 loss: 0.00055772396735847
batch 50 loss: 0.0005577301140874624
batch 55 loss: 0.0005576792173087597
batch 60 loss: 0.0005577038158662617
batch 65 loss: 0.0005575340474024415
batch 70 loss: 0.000557682744693011
batch 75 loss: 0.000557661836501211
batch 80 loss: 0.000557676237076521
batch 85 loss: 0.0005576899158768356
batch 90 loss: 0.0005576294730417431
batch 95 loss: 0.0005577145493589342
batch 100 loss: 0.0005576990777626633
batch 105 loss: 0.0005578513955697417
batch 110 loss: 0.0005577541538514197
batch 115 loss: 0.0005576408002525568
batch 120 loss: 0.0005577922565862536
batch 125 loss: 0.0005575927905738353
batch 130 loss: 0.0005576468771323562
batch 135 loss: 0.0005577324773184955
batch 140 loss: 0.0005576579598709941
batch 145 loss: 0.0005577226751483977
batch 150 loss: 0.0005576793220825494
batch 155 loss: 0.000557641254272312
batch 160 loss: 0.000557626155205071
batch 165 loss: 0.0005577459931373596
batch 170 loss: 0.0005576920113526285
batch 175 loss: 0.0005576998344622552
batch 180 loss: 0.000557647121604532
batch 185 loss: 0.0005576227442361415
batch 190 loss: 0.0005576246650889516
batch 195 loss: 0.0005577705218456685
batch 200 loss: 0.0005576939205639064
batch 205 loss: 0.0005577016854658723
batch 210 loss: 0.0005576670984737575
batch 215 loss: 0.0005576198222115636
batch 220 loss: 0.0005577015806920826
batch 225 loss: 0.0005576962023042143
batch 230 loss: 0.0005576470633968711
batch 235 loss: 0.0005577023373916745
batch 240 loss: 0.0005577178089879453
Training Loss: 0.0005576833971038771
Validation Loss: 0.0005576934131871288
Epoch 83:
batch 5 loss: 0.0005577645148150623
batch 10 loss: 0.0005576372146606446
batch 15 loss: 0.0005578009411692619
batch 20 loss: 0.000557648204267025
batch 25 loss: 0.0005576901487074793
batch 30 loss: 0.0005576131748966873
batch 35 loss: 0.0005576076684519649
batch 40 loss: 0.0005577105563133955
batch 45 loss: 0.0005577074130997062
batch 50 loss: 0.0005577318952418864
batch 55 loss: 0.0005576921277679503
batch 60 loss: 0.0005575981689617038
batch 65 loss: 0.0005576662952080369
batch 70 loss: 0.0005577637231908739
batch 75 loss: 0.0005575896939262748
batch 80 loss: 0.0005577281350269914
batch 85 loss: 0.0005577269475907087
batch 90 loss: 0.0005577135598286986
batch 95 loss: 0.0005576373427174985
batch 100 loss: 0.0005577044212259352
batch 105 loss: 0.0005575970979407429
batch 110 loss: 0.000557704211678356
batch 115 loss: 0.000557823630515486
batch 120 loss: 0.000557657377794385
batch 125 loss: 0.0005576947238296271
batch 130 loss: 0.0005577240372076631
batch 135 loss: 0.0005576401716098189
batch 140 loss: 0.0005577219533734023
batch 145 loss: 0.0005577514180913568
batch 150 loss: 0.0005577525123953819
batch 155 loss: 0.0005576607771217823
batch 160 loss: 0.0005577236413955688
batch 165 loss: 0.0005576601251959801
batch 170 loss: 0.0005576112191192806
batch 175 loss: 0.0005576518480665982
batch 180 loss: 0.0005577279254794121
batch 185 loss: 0.0005576163763180375
batch 190 loss: 0.0005578003125265241
batch 195 loss: 0.0005576954688876867
batch 200 loss: 0.0005576465278863907
batch 205 loss: 0.0005576852825470268
batch 210 loss: 0.0005577099975198508
batch 215 loss: 0.0005576511262916029
batch 220 loss: 0.0005577232106588781
batch 225 loss: 0.0005575505550950765
batch 230 loss: 0.0005575802875682712
batch 235 loss: 0.0005576194263994694
batch 240 loss: 0.0005576891591772438
Training Loss: 0.0005576833864324726
Validation Loss: 0.0005576934093066181
Epoch 84:
batch 5 loss: 0.0005575577844865621
batch 10 loss: 0.0005578507320024073
batch 15 loss: 0.0005576750379987061
batch 20 loss: 0.0005575722665525973
batch 25 loss: 0.0005576211027801037
batch 30 loss: 0.0005576426396146416
batch 35 loss: 0.0005577040836215019
batch 40 loss: 0.0005577535834163427
batch 45 loss: 0.000557721289806068
batch 50 loss: 0.0005577164702117443
batch 55 loss: 0.0005575831863097846
batch 60 loss: 0.0005577193689532578
batch 65 loss: 0.0005577112548053265
batch 70 loss: 0.0005576992523856461
batch 75 loss: 0.0005576981115154922
batch 80 loss: 0.0005576663999818266
batch 85 loss: 0.0005575691931881011
batch 90 loss: 0.0005576703464612365
batch 95 loss: 0.0005576055147685111
batch 100 loss: 0.000557677773758769
batch 105 loss: 0.0005575736053287983
batch 110 loss: 0.0005577462608925999
batch 115 loss: 0.0005576797178946435
batch 120 loss: 0.0005576325114816427
batch 125 loss: 0.0005577154224738479
batch 130 loss: 0.0005577024072408676
batch 135 loss: 0.0005575862363912165
batch 140 loss: 0.0005576704395934939
batch 145 loss: 0.0005576185532845557
batch 150 loss: 0.0005576898227445781
batch 155 loss: 0.0005577194155193865
batch 160 loss: 0.0005577991367317736
batch 165 loss: 0.0005577239207923412
batch 170 loss: 0.0005578589392825961
batch 175 loss: 0.0005576788680627942
batch 180 loss: 0.0005577023955993354
batch 185 loss: 0.0005578105105087161
batch 190 loss: 0.000557716260664165
batch 195 loss: 0.0005577362258918584
batch 200 loss: 0.0005576103227213025
batch 205 loss: 0.0005576379830017686
batch 210 loss: 0.0005578427342697978
batch 215 loss: 0.0005577323492616415
batch 220 loss: 0.0005576113937422634
batch 225 loss: 0.0005576602765358984
batch 230 loss: 0.0005576117197051645
batch 235 loss: 0.0005577160627581179
batch 240 loss: 0.0005576039082370698
Training Loss: 0.000557683391525643
Validation Loss: 0.0005576934257987886
Epoch 85:
batch 5 loss: 0.0005576697294600308
batch 10 loss: 0.000557684467639774
batch 15 loss: 0.0005576913361437618
batch 20 loss: 0.0005576888914220035
batch 25 loss: 0.0005577438394539058
batch 30 loss: 0.0005575868184678256
batch 35 loss: 0.0005577335832640529
batch 40 loss: 0.0005576963070780039
batch 45 loss: 0.000557530764490366
batch 50 loss: 0.0005577267147600651
batch 55 loss: 0.000557619275059551
batch 60 loss: 0.0005575809977017343
batch 65 loss: 0.0005576705560088157
batch 70 loss: 0.0005576373660005629
batch 75 loss: 0.0005576651426963508
batch 80 loss: 0.0005577030009590089
batch 85 loss: 0.0005576629424467683
batch 90 loss: 0.0005576904164627195
batch 95 loss: 0.0005576852243393659
batch 100 loss: 0.0005576831172220409
batch 105 loss: 0.0005577362375333905
batch 110 loss: 0.0005575908347964287
batch 115 loss: 0.0005576884257607162
batch 120 loss: 0.000557638518512249
batch 125 loss: 0.0005576789262704551
batch 130 loss: 0.0005576724535785616
batch 135 loss: 0.0005576911615207791
batch 140 loss: 0.0005576058989390731
batch 145 loss: 0.0005577088450081646
batch 150 loss: 0.0005577239207923412
batch 155 loss: 0.0005575859337113798
batch 160 loss: 0.0005577699746936559
batch 165 loss: 0.000557684467639774
batch 170 loss: 0.000557749718427658
batch 175 loss: 0.0005576841067522764
batch 180 loss: 0.0005577089497819543
batch 185 loss: 0.0005576491937972605
batch 190 loss: 0.0005577396252192557
batch 195 loss: 0.0005577054223977029
batch 200 loss: 0.000557864282745868
batch 205 loss: 0.0005576345836743712
batch 210 loss: 0.0005576857714913785
batch 215 loss: 0.0005577311269007624
batch 220 loss: 0.0005577436531893909
batch 225 loss: 0.0005577267846092581
batch 230 loss: 0.0005577552248723805
batch 235 loss: 0.0005576303577981889
batch 240 loss: 0.0005576677271164953
Training Loss: 0.0005576833878876642
Validation Loss: 0.0005576934607233852
Epoch 86:
batch 5 loss: 0.0005576335126534105
batch 10 loss: 0.0005575337097980082
batch 15 loss: 0.0005577296833507717
batch 20 loss: 0.0005577324191108346
batch 25 loss: 0.0005575111834332346
batch 30 loss: 0.0005577110918238759
batch 35 loss: 0.0005577953299507499
batch 40 loss: 0.0005576592404395342
batch 45 loss: 0.0005577783682383597
batch 50 loss: 0.0005576434661634267
batch 55 loss: 0.0005575302289798856
batch 60 loss: 0.0005576988216489554
batch 65 loss: 0.0005577096133492887
batch 70 loss: 0.0005576561321504414
batch 75 loss: 0.0005577211966738104
batch 80 loss: 0.0005576367606408894
batch 85 loss: 0.000557767110876739
batch 90 loss: 0.0005576439667493105
batch 95 loss: 0.0005577371222898365
batch 100 loss: 0.0005577295203693211
batch 105 loss: 0.0005576352938078344
batch 110 loss: 0.0005576770869083703
batch 115 loss: 0.0005577325820922852
batch 120 loss: 0.0005576190655119717
batch 125 loss: 0.0005576118011958898
batch 130 loss: 0.0005577260977588594
batch 135 loss: 0.000557720789220184
batch 140 loss: 0.0005576698109507561
batch 145 loss: 0.0005577883683145046
batch 150 loss: 0.0005575867602601647
batch 155 loss: 0.0005577851319685578
batch 160 loss: 0.0005577351432293654
batch 165 loss: 0.0005576540250331163
batch 170 loss: 0.0005576688679866492
batch 175 loss: 0.0005576677853241563
batch 180 loss: 0.0005575759452767671
batch 185 loss: 0.0005576786235906184
batch 190 loss: 0.0005577876814641059
batch 195 loss: 0.0005575654562562704
batch 200 loss: 0.0005576314986683428
batch 205 loss: 0.0005576310213655233
batch 210 loss: 0.0005576858413405717
batch 215 loss: 0.0005578493583016098
batch 220 loss: 0.0005577588570304215
batch 225 loss: 0.000557769148144871
batch 230 loss: 0.000557765201665461
batch 235 loss: 0.000557636225130409
batch 240 loss: 0.0005576307419687509
Training Loss: 0.0005576833893428557
Validation Loss: 0.0005576934054261073
Epoch 87:
batch 5 loss: 0.0005577013595029712
batch 10 loss: 0.0005577874951995909
batch 15 loss: 0.0005577445030212402
batch 20 loss: 0.0005576891708187759
batch 25 loss: 0.0005577200092375279
batch 30 loss: 0.0005576427793130279
batch 35 loss: 0.0005576960626058281
batch 40 loss: 0.000557753536850214
batch 45 loss: 0.0005577445728704334
batch 50 loss: 0.0005577224656008184
batch 55 loss: 0.000557751557789743
batch 60 loss: 0.0005576618132181466
batch 65 loss: 0.0005576683091931045
batch 70 loss: 0.0005577258300036192
batch 75 loss: 0.0005576348747126758
batch 80 loss: 0.0005577938631176948
batch 85 loss: 0.0005576238036155701
batch 90 loss: 0.0005576178897172213
batch 95 loss: 0.0005576950730755925
batch 100 loss: 0.0005577069241553545
batch 105 loss: 0.0005578035488724709
batch 110 loss: 0.0005576114752329886
batch 115 loss: 0.0005576203111559153
batch 120 loss: 0.0005576680414378643
batch 125 loss: 0.0005577041185460985
batch 130 loss: 0.0005576874362304806
batch 135 loss: 0.0005575710441917181
batch 140 loss: 0.0005576452822424471
batch 145 loss: 0.0005576630588620901
batch 150 loss: 0.0005576311377808452
batch 155 loss: 0.0005576802301220596
batch 160 loss: 0.0005576814874075353
batch 165 loss: 0.0005577688571065664
batch 170 loss: 0.000557576457504183
batch 175 loss: 0.0005576634546741843
batch 180 loss: 0.0005577084724791348
batch 185 loss: 0.0005575760849751532
batch 190 loss: 0.0005577016388997435
batch 195 loss: 0.000557696504984051
batch 200 loss: 0.0005577428732067346
batch 205 loss: 0.0005575545830652118
batch 210 loss: 0.0005576127907261253
batch 215 loss: 0.0005576152703724802
batch 220 loss: 0.0005577746429480612
batch 225 loss: 0.0005577836534939706
batch 230 loss: 0.0005576164112426341
batch 235 loss: 0.0005576367140747607
batch 240 loss: 0.0005577252130024135
Training Loss: 0.0005576833893428557
Validation Loss: 0.0005576934160975119
Epoch 88:
batch 5 loss: 0.0005577031057327986
batch 10 loss: 0.0005576283787377178
batch 15 loss: 0.0005576707422733307
batch 20 loss: 0.0005577568081207573
batch 25 loss: 0.0005577157018706203
batch 30 loss: 0.0005577187985181808
batch 35 loss: 0.0005576777155511081
batch 40 loss: 0.000557626667432487
batch 45 loss: 0.0005576769239269197
batch 50 loss: 0.0005576765164732933
batch 55 loss: 0.0005576514755375684
batch 60 loss: 0.0005576622672379017
batch 65 loss: 0.0005575960269197822
batch 70 loss: 0.0005576802068389952
batch 75 loss: 0.0005577732226811349
batch 80 loss: 0.0005577188218012452
batch 85 loss: 0.0005577076226472855
batch 90 loss: 0.0005577034200541676
batch 95 loss: 0.0005576708004809916
batch 100 loss: 0.0005576170282438397
batch 105 loss: 0.0005577401258051396
batch 110 loss: 0.0005575592163950205
batch 115 loss: 0.0005575907183811068
batch 120 loss: 0.0005577315459959209
batch 125 loss: 0.0005577465402893722
batch 130 loss: 0.0005576875410042703
batch 135 loss: 0.0005576471099629998
batch 140 loss: 0.0005576082388870418
batch 145 loss: 0.0005577354459092021
batch 150 loss: 0.0005576543160714209
batch 155 loss: 0.0005576802999712527
batch 160 loss: 0.0005576726514846086
batch 165 loss: 0.0005577386473305523
batch 170 loss: 0.0005576759111136198
batch 175 loss: 0.0005576550611294806
batch 180 loss: 0.0005576697760261596
batch 185 loss: 0.0005577529780566692
batch 190 loss: 0.0005577774252742528
batch 195 loss: 0.0005575967952609062
batch 200 loss: 0.0005576400202699005
batch 205 loss: 0.0005577286239713431
batch 210 loss: 0.0005576801835559308
batch 215 loss: 0.0005577312083914876
batch 220 loss: 0.000557658914476633
batch 225 loss: 0.0005577022908255458
batch 230 loss: 0.0005576939438469708
batch 235 loss: 0.0005576896131969988
batch 240 loss: 0.0005577250150963664
Training Loss: 0.0005576833835220895
Validation Loss: 0.0005576933986352135
Epoch 89:
batch 5 loss: 0.0005575948860496282
batch 10 loss: 0.0005575714749284089
batch 15 loss: 0.0005575476563535631
batch 20 loss: 0.0005576725234277546
batch 25 loss: 0.0005578162614256144
batch 30 loss: 0.0005575093091465533
batch 35 loss: 0.0005576484836637974
batch 40 loss: 0.0005576948169618845
batch 45 loss: 0.000557639601174742
batch 50 loss: 0.000557809870224446
batch 55 loss: 0.000557689368724823
batch 60 loss: 0.0005577545729465782
batch 65 loss: 0.0005576440133154392
batch 70 loss: 0.0005576082621701062
batch 75 loss: 0.0005577914416790009
batch 80 loss: 0.0005576362018473446
batch 85 loss: 0.0005576522671617568
batch 90 loss: 0.0005577714298851788
batch 95 loss: 0.0005577706615440547
batch 100 loss: 0.0005576505092903972
batch 105 loss: 0.0005576472845859826
batch 110 loss: 0.0005576575873419643
batch 115 loss: 0.0005577294970862568
batch 120 loss: 0.0005577606614679098
batch 125 loss: 0.000557719951029867
batch 130 loss: 0.0005577457370236516
batch 135 loss: 0.0005576959461905062
batch 140 loss: 0.0005576947703957557
batch 145 loss: 0.000557747040875256
batch 150 loss: 0.000557711988221854
batch 155 loss: 0.000557603791821748
batch 160 loss: 0.0005576852359808982
batch 165 loss: 0.0005576308933086694
batch 170 loss: 0.0005577788455411792
batch 175 loss: 0.0005576063063926995
batch 180 loss: 0.000557712244335562
batch 185 loss: 0.0005577071337029338
batch 190 loss: 0.00055769351311028
batch 195 loss: 0.0005577208590693772
batch 200 loss: 0.0005576837225817144
batch 205 loss: 0.0005577478674240411
batch 210 loss: 0.0005576930125243962
batch 215 loss: 0.0005576053285039961
batch 220 loss: 0.0005576982861384749
batch 225 loss: 0.0005576223251409828
batch 230 loss: 0.0005576992873102427
batch 235 loss: 0.0005576801835559308
batch 240 loss: 0.0005576494382694364
Training Loss: 0.00055768338230943
Validation Loss: 0.0005576934219182779
Epoch 90:
batch 5 loss: 0.0005576632334850729
batch 10 loss: 0.0005577855277806521
batch 15 loss: 0.0005575768533162773
batch 20 loss: 0.0005576325580477715
batch 25 loss: 0.0005576794035732746
batch 30 loss: 0.0005577109404839575
batch 35 loss: 0.0005575776449404657
batch 40 loss: 0.0005576066789217294
batch 45 loss: 0.000557601370383054
batch 50 loss: 0.0005577152012847364
batch 55 loss: 0.0005577250849455595
batch 60 loss: 0.00055765884462744
batch 65 loss: 0.000557713897433132
batch 70 loss: 0.000557638076134026
batch 75 loss: 0.0005577126052230596
batch 80 loss: 0.0005577588337473572
batch 85 loss: 0.0005576783209107816
batch 90 loss: 0.0005576669936999679
batch 95 loss: 0.0005577347474172711
batch 100 loss: 0.0005576517898589373
batch 105 loss: 0.0005577784730121493
batch 110 loss: 0.000557488261256367
batch 115 loss: 0.0005577095900662244
batch 120 loss: 0.0005578154115937651
batch 125 loss: 0.0005577518371865153
batch 130 loss: 0.0005577704403549432
batch 135 loss: 0.000557720975484699
batch 140 loss: 0.0005576642579399049
batch 145 loss: 0.0005576431984081864
batch 150 loss: 0.0005576706724241375
batch 155 loss: 0.0005577376578003168
batch 160 loss: 0.0005577241536229849
batch 165 loss: 0.0005576355499215424
batch 170 loss: 0.0005576766561716795
batch 175 loss: 0.0005576289142481983
batch 180 loss: 0.0005577206844463945
batch 185 loss: 0.0005576644558459521
batch 190 loss: 0.000557735632173717
batch 195 loss: 0.0005576521274633705
batch 200 loss: 0.0005575902527198195
batch 205 loss: 0.0005577116506174206
batch 210 loss: 0.0005576327559538186
batch 215 loss: 0.0005577134899795056
batch 220 loss: 0.000557771697640419
batch 225 loss: 0.0005576836178079247
batch 230 loss: 0.0005576379364356399
batch 235 loss: 0.0005575761664658785
batch 240 loss: 0.0005578072625212372
Training Loss: 0.0005576833830370257
Validation Loss: 0.0005576933976650859
Epoch 91:
batch 5 loss: 0.0005575836519710719
batch 10 loss: 0.0005576027324423194
batch 15 loss: 0.0005577184027060866
batch 20 loss: 0.000557691475842148
batch 25 loss: 0.0005576804280281067
batch 30 loss: 0.0005577398813329637
batch 35 loss: 0.0005577663308940828
batch 40 loss: 0.0005576612893491983
batch 45 loss: 0.0005576999858021737
batch 50 loss: 0.0005577248171903193
batch 55 loss: 0.0005576653289608658
batch 60 loss: 0.0005577379022724926
batch 65 loss: 0.0005576008115895092
batch 70 loss: 0.0005576894269324839
batch 75 loss: 0.000557740218937397
batch 80 loss: 0.0005577209638431668
batch 85 loss: 0.000557792594190687
batch 90 loss: 0.0005576947587542236
batch 95 loss: 0.000557676691096276
batch 100 loss: 0.0005577287985943258
batch 105 loss: 0.0005576732102781535
batch 110 loss: 0.0005577077041380108
batch 115 loss: 0.0005576446419581771
batch 120 loss: 0.0005576030118390918
batch 125 loss: 0.0005576189025305212
batch 130 loss: 0.0005576904746703804
batch 135 loss: 0.0005575878662057221
batch 140 loss: 0.0005576947121880948
batch 145 loss: 0.0005577397765591741
batch 150 loss: 0.0005576421623118222
batch 155 loss: 0.0005576636642217637
batch 160 loss: 0.0005577280535362661
batch 165 loss: 0.0005577343166805804
batch 170 loss: 0.0005577191128395498
batch 175 loss: 0.0005574808106757701
batch 180 loss: 0.0005576268653385341
batch 185 loss: 0.0005577092058956623
batch 190 loss: 0.0005576772266067565
batch 195 loss: 0.00055757308145985
batch 200 loss: 0.0005577004863880575
batch 205 loss: 0.0005577743751928211
batch 210 loss: 0.0005577021627686917
batch 215 loss: 0.0005577807547524572
batch 220 loss: 0.0005577770527452231
batch 225 loss: 0.00055775634245947
batch 230 loss: 0.0005576972616836429
batch 235 loss: 0.000557586073409766
batch 240 loss: 0.0005575965740717947
Training Loss: 0.0005576833827944938
Validation Loss: 0.0005576934015455966
Epoch 92:
batch 5 loss: 0.0005576848285272718
batch 10 loss: 0.0005577401723712682
batch 15 loss: 0.0005577178788371384
batch 20 loss: 0.0005576247232966125
batch 25 loss: 0.0005576243973337113
batch 30 loss: 0.0005576899973675608
batch 35 loss: 0.000557661394122988
batch 40 loss: 0.0005577411968261004
batch 45 loss: 0.0005576414056122303
batch 50 loss: 0.000557670253328979
batch 55 loss: 0.0005578022915869951
batch 60 loss: 0.0005576378898695111
batch 65 loss: 0.0005577674601227045
batch 70 loss: 0.0005576425697654486
batch 75 loss: 0.0005576946656219661
batch 80 loss: 0.0005576680763624608
batch 85 loss: 0.0005576937692239881
batch 90 loss: 0.0005577094503678382
batch 95 loss: 0.0005577058880589902
batch 100 loss: 0.0005576787865720689
batch 105 loss: 0.0005577957606874406
batch 110 loss: 0.0005576957017183303
batch 115 loss: 0.0005577069940045476
batch 120 loss: 0.0005577469244599342
batch 125 loss: 0.0005577332573011518
batch 130 loss: 0.0005576596595346928
batch 135 loss: 0.0005576504627242684
batch 140 loss: 0.0005577112780883909
batch 145 loss: 0.0005576725350692868
batch 150 loss: 0.0005576309747993946
batch 155 loss: 0.0005577073781751097
batch 160 loss: 0.0005577772506512701
batch 165 loss: 0.0005576773779466748
batch 170 loss: 0.0005577296484261751
batch 175 loss: 0.0005576066323556006
batch 180 loss: 0.0005576629540883004
batch 185 loss: 0.0005576793337240815
batch 190 loss: 0.0005576400086283684
batch 195 loss: 0.0005576323950663209
batch 200 loss: 0.0005577743286266923
batch 205 loss: 0.0005576277733780443
batch 210 loss: 0.0005578219192102551
batch 215 loss: 0.0005575053277425468
batch 220 loss: 0.0005575672257691622
batch 225 loss: 0.0005575177259743213
batch 230 loss: 0.0005577276227995753
batch 235 loss: 0.000557746656704694
batch 240 loss: 0.0005576003692112863
Training Loss: 0.0005576833869175364
Validation Loss: 0.0005576934306494271
Epoch 93:
batch 5 loss: 0.0005575818591751158
batch 10 loss: 0.0005576389143243432
batch 15 loss: 0.0005576484487392009
batch 20 loss: 0.0005577375879511238
batch 25 loss: 0.0005576464580371975
batch 30 loss: 0.0005576671683229506
batch 35 loss: 0.0005576892639510333
batch 40 loss: 0.0005577268311753869
batch 45 loss: 0.0005577845964580775
batch 50 loss: 0.0005577028379775584
batch 55 loss: 0.0005576564581133425
batch 60 loss: 0.0005576505092903972
batch 65 loss: 0.0005576821160502732
batch 70 loss: 0.0005575984250754118
batch 75 loss: 0.0005576217197813093
batch 80 loss: 0.0005576244206167757
batch 85 loss: 0.0005577973555773496
batch 90 loss: 0.0005576234310865402
batch 95 loss: 0.0005576951545663178
batch 100 loss: 0.0005577603471465409
batch 105 loss: 0.0005576482391916216
batch 110 loss: 0.0005577090661972762
batch 115 loss: 0.0005577132804319262
batch 120 loss: 0.0005576415918767452
batch 125 loss: 0.0005576540715992451
batch 130 loss: 0.000557658716570586
batch 135 loss: 0.0005577651085332036
batch 140 loss: 0.0005577341304160655
batch 145 loss: 0.0005577313248068094
batch 150 loss: 0.0005576538504101336
batch 155 loss: 0.0005575736751779914
batch 160 loss: 0.0005577916279435157
batch 165 loss: 0.0005576722440309822
batch 170 loss: 0.0005576785188168287
batch 175 loss: 0.0005577631411142648
batch 180 loss: 0.0005576939787715673
batch 185 loss: 0.0005576020339503884
batch 190 loss: 0.0005577781237661838
batch 195 loss: 0.0005577594973146915
batch 200 loss: 0.0005576860043220222
batch 205 loss: 0.0005577683448791503
batch 210 loss: 0.0005576769006438553
batch 215 loss: 0.00055757014779374
batch 220 loss: 0.0005576473427936435
batch 225 loss: 0.0005575254210270942
batch 230 loss: 0.0005577457370236516
batch 235 loss: 0.0005576726514846086
batch 240 loss: 0.0005577534670010209
Training Loss: 0.0005576833779438554
Validation Loss: 0.0005576934190078948
Epoch 94:
batch 5 loss: 0.0005576875526458025
batch 10 loss: 0.0005576819065026938
batch 15 loss: 0.0005576988100074231
batch 20 loss: 0.0005576450726948679
batch 25 loss: 0.0005577291245572269
batch 30 loss: 0.0005577022675424814
batch 35 loss: 0.0005576732102781535
batch 40 loss: 0.0005576452589593828
batch 45 loss: 0.0005576568190008401
batch 50 loss: 0.0005577159696258605
batch 55 loss: 0.0005575621617026627
batch 60 loss: 0.0005576278432272375
batch 65 loss: 0.0005577348289079964
batch 70 loss: 0.000557706190738827
batch 75 loss: 0.0005577928852289916
batch 80 loss: 0.0005577353294938803
batch 85 loss: 0.0005576679948717355
batch 90 loss: 0.0005576059338636696
batch 95 loss: 0.000557728367857635
batch 100 loss: 0.0005577106378041208
batch 105 loss: 0.0005577399395406246
batch 110 loss: 0.0005577342002652585
batch 115 loss: 0.000557720789220184
batch 120 loss: 0.0005577457719482482
batch 125 loss: 0.0005577168427407742
batch 130 loss: 0.0005575867835432291
batch 135 loss: 0.0005577055737376214
batch 140 loss: 0.0005576182855293154
batch 145 loss: 0.0005576966796070337
batch 150 loss: 0.0005576419876888395
batch 155 loss: 0.0005577713251113892
batch 160 loss: 0.0005576477386057376
batch 165 loss: 0.00055766177829355
batch 170 loss: 0.000557829684112221
batch 175 loss: 0.0005577281583100558
batch 180 loss: 0.0005577736301347613
batch 185 loss: 0.0005576259223744273
batch 190 loss: 0.0005576073657721281
batch 195 loss: 0.0005576507421210409
batch 200 loss: 0.0005575662828050554
batch 205 loss: 0.0005576212075538933
batch 210 loss: 0.0005576782859861851
batch 215 loss: 0.0005577174131758511
batch 220 loss: 0.0005576585885137319
batch 225 loss: 0.0005576050141826272
batch 230 loss: 0.0005577727104537189
batch 235 loss: 0.0005576105904765427
batch 240 loss: 0.0005576605442911386
Training Loss: 0.0005576833750334724
Validation Loss: 0.0005576934083364904
Epoch 95:
batch 5 loss: 0.0005576117895543575
batch 10 loss: 0.000557685608509928
batch 15 loss: 0.000557701289653778
batch 20 loss: 0.0005576675990596414
batch 25 loss: 0.0005576663417741657
batch 30 loss: 0.0005576832569204271
batch 35 loss: 0.0005577293573878706
batch 40 loss: 0.0005577139789238572
batch 45 loss: 0.0005576609284617007
batch 50 loss: 0.0005577100091613829
batch 55 loss: 0.0005577901494689286
batch 60 loss: 0.0005576628725975751
batch 65 loss: 0.0005576310330070555
batch 70 loss: 0.0005576636525802314
batch 75 loss: 0.0005575986346229911
batch 80 loss: 0.0005577702424488962
batch 85 loss: 0.0005576393101364374
batch 90 loss: 0.0005577296833507717
batch 95 loss: 0.0005577269243076443
batch 100 loss: 0.0005577837699092924
batch 105 loss: 0.0005576467141509056
batch 110 loss: 0.000557692430447787
batch 115 loss: 0.0005576823838055134
batch 120 loss: 0.0005576673429459333
batch 125 loss: 0.0005575874703936279
batch 130 loss: 0.0005576845724135637
batch 135 loss: 0.0005577351548708975
batch 140 loss: 0.0005576928379014134
batch 145 loss: 0.0005577112431637942
batch 150 loss: 0.0005577290314249695
batch 155 loss: 0.0005576269933953882
batch 160 loss: 0.0005575742223300039
batch 165 loss: 0.0005576660973019898
batch 170 loss: 0.0005576883559115231
batch 175 loss: 0.0005577808013185858
batch 180 loss: 0.0005577314412221312
batch 185 loss: 0.0005577118019573391
batch 190 loss: 0.0005576019524596631
batch 195 loss: 0.0005575106479227543
batch 200 loss: 0.0005577702424488962
batch 205 loss: 0.0005576561903581023
batch 210 loss: 0.0005577020929194987
batch 215 loss: 0.0005576824652962387
batch 220 loss: 0.0005576171330176294
batch 225 loss: 0.0005576450494118035
batch 230 loss: 0.0005576819647103548
batch 235 loss: 0.0005577246425673366
batch 240 loss: 0.0005577743984758853
Training Loss: 0.0005576833772162596
Validation Loss: 0.0005576934102767458
Epoch 96:
batch 5 loss: 0.0005576610565185547
batch 10 loss: 0.0005576098104938864
batch 15 loss: 0.0005576461437158287
batch 20 loss: 0.0005577542469836771
batch 25 loss: 0.0005576769704930485
batch 30 loss: 0.0005577861331403256
batch 35 loss: 0.0005576011491939426
batch 40 loss: 0.0005576335359364748
batch 45 loss: 0.0005577114061452448
batch 50 loss: 0.0005576570983976126
batch 55 loss: 0.0005576735129579902
batch 60 loss: 0.000557740149088204
batch 65 loss: 0.0005576217663474381
batch 70 loss: 0.0005577225005254149
batch 75 loss: 0.0005577677045948803
batch 80 loss: 0.0005576481111347675
batch 85 loss: 0.0005577175295911729
batch 90 loss: 0.0005576631869189441
batch 95 loss: 0.0005576212774030864
batch 100 loss: 0.0005577598232775927
batch 105 loss: 0.00055763985728845
batch 110 loss: 0.0005577119532972574
batch 115 loss: 0.0005576514871791005
batch 120 loss: 0.0005576460971496999
batch 125 loss: 0.0005577266681939363
batch 130 loss: 0.0005576372146606446
batch 135 loss: 0.0005577438278123736
batch 140 loss: 0.0005577294039539992
batch 145 loss: 0.0005576599738560617
batch 150 loss: 0.0005575274000875652
batch 155 loss: 0.000557647505775094
batch 160 loss: 0.0005577508709393442
batch 165 loss: 0.0005577237345278264
batch 170 loss: 0.0005576634779572487
batch 175 loss: 0.0005578208481892943
batch 180 loss: 0.0005575719987973571
batch 185 loss: 0.0005576997762545943
batch 190 loss: 0.0005577386938966811
batch 195 loss: 0.0005577456206083298
batch 200 loss: 0.0005576558061875403
batch 205 loss: 0.0005577413481660187
batch 210 loss: 0.000557705806568265
batch 215 loss: 0.0005575846065767109
batch 220 loss: 0.0005576406023465097
batch 225 loss: 0.0005576477153226734
batch 230 loss: 0.0005576713941991329
batch 235 loss: 0.0005577438976615667
batch 240 loss: 0.0005577012780122459
Training Loss: 0.0005576833745484085
Validation Loss: 0.0005576933957248306
Epoch 97:
batch 5 loss: 0.000557680067140609
batch 10 loss: 0.0005578441661782563
batch 15 loss: 0.0005577762494795024
batch 20 loss: 0.000557726842816919
batch 25 loss: 0.0005576867726631463
batch 30 loss: 0.0005576849798671901
batch 35 loss: 0.0005576979252509773
batch 40 loss: 0.0005577056552283465
batch 45 loss: 0.0005576373660005629
batch 50 loss: 0.0005577095435000956
batch 55 loss: 0.0005577235831879079
batch 60 loss: 0.0005576515686698258
batch 65 loss: 0.0005577123607508838
batch 70 loss: 0.0005576798343099654
batch 75 loss: 0.0005575810791924596
batch 80 loss: 0.0005576746305450797
batch 85 loss: 0.0005577440024353564
batch 90 loss: 0.0005576858995482326
batch 95 loss: 0.0005575983203016221
batch 100 loss: 0.0005576540250331163
batch 105 loss: 0.0005576272495090961
batch 110 loss: 0.000557641708292067
batch 115 loss: 0.0005576447932980954
batch 120 loss: 0.000557775842025876
batch 125 loss: 0.0005575644318014383
batch 130 loss: 0.0005577264935709536
batch 135 loss: 0.0005575709277763963
batch 140 loss: 0.000557584292255342
batch 145 loss: 0.000557753792963922
batch 150 loss: 0.0005576636409386993
batch 155 loss: 0.0005577217321842909
batch 160 loss: 0.0005577638163231313
batch 165 loss: 0.0005577637115493417
batch 170 loss: 0.0005575848976150155
batch 175 loss: 0.0005576852126978338
batch 180 loss: 0.0005576543277129531
batch 185 loss: 0.0005577550851739943
batch 190 loss: 0.0005576756433583796
batch 195 loss: 0.0005576736875809729
batch 200 loss: 0.0005575643153861165
batch 205 loss: 0.000557738181669265
batch 210 loss: 0.0005576917203143239
batch 215 loss: 0.0005576915922574699
batch 220 loss: 0.0005577321047894656
batch 225 loss: 0.0005576740833930672
batch 230 loss: 0.0005576997296884656
batch 235 loss: 0.0005576047115027905
batch 240 loss: 0.000557719450443983
Training Loss: 0.0005576833760036
Validation Loss: 0.000557693403485852
Epoch 98:
batch 5 loss: 0.0005577903939411044
batch 10 loss: 0.0005577029543928802
batch 15 loss: 0.000557654129806906
batch 20 loss: 0.0005576508236117661
batch 25 loss: 0.0005577271804213524
batch 30 loss: 0.0005576392984949052
batch 35 loss: 0.0005577932228334248
batch 40 loss: 0.0005577135365456342
batch 45 loss: 0.0005576202529482543
batch 50 loss: 0.0005576071562245488
batch 55 loss: 0.0005576758878305555
batch 60 loss: 0.0005577842239290476
batch 65 loss: 0.0005577868665568531
batch 70 loss: 0.0005576851195655763
batch 75 loss: 0.0005576794035732746
batch 80 loss: 0.0005576879600994288
batch 85 loss: 0.0005577694973908365
batch 90 loss: 0.0005576891708187759
batch 95 loss: 0.0005576729192398489
batch 100 loss: 0.0005576767260208725
batch 105 loss: 0.0005578190670348704
batch 110 loss: 0.0005577053525485098
batch 115 loss: 0.0005576711730100215
batch 120 loss: 0.0005576182040385902
batch 125 loss: 0.0005576806957833469
batch 130 loss: 0.0005577291594818234
batch 135 loss: 0.0005576920346356928
batch 140 loss: 0.0005577350268140436
batch 145 loss: 0.0005577113362960518
batch 150 loss: 0.0005576647468842566
batch 155 loss: 0.0005575925461016595
batch 160 loss: 0.0005577482865191996
batch 165 loss: 0.0005577250616624951
batch 170 loss: 0.0005576884374022484
batch 175 loss: 0.0005576248979195953
batch 180 loss: 0.0005576749565079809
batch 185 loss: 0.0005575921735726297
batch 190 loss: 0.000557493360247463
batch 195 loss: 0.0005576353403739631
batch 200 loss: 0.0005576805211603642
batch 205 loss: 0.0005577027564868331
batch 210 loss: 0.0005577014409936965
batch 215 loss: 0.0005576558294706046
batch 220 loss: 0.000557636737357825
batch 225 loss: 0.0005575828603468835
batch 230 loss: 0.0005577269243076443
batch 235 loss: 0.0005576405907049775
batch 240 loss: 0.0005576657480560243
Training Loss: 0.0005576833747909405
Validation Loss: 0.0005576933957248306
Epoch 99:
batch 5 loss: 0.000557695422321558
batch 10 loss: 0.0005577296600677073
batch 15 loss: 0.0005576978204771876
batch 20 loss: 0.0005575935705564916
batch 25 loss: 0.0005577471922151745
batch 30 loss: 0.0005577472154982388
batch 35 loss: 0.0005577117204666138
batch 40 loss: 0.0005576786585152149
batch 45 loss: 0.0005576430587098003
batch 50 loss: 0.0005576726980507374
batch 55 loss: 0.0005576768657192588
batch 60 loss: 0.0005576381576247513
batch 65 loss: 0.0005576986586675048
batch 70 loss: 0.0005575653281994164
batch 75 loss: 0.0005576328840106726
batch 80 loss: 0.0005577103234827519
batch 85 loss: 0.0005576463765464724
batch 90 loss: 0.0005577179486863315
batch 95 loss: 0.0005577331990934909
batch 100 loss: 0.0005577517440542579
batch 105 loss: 0.0005576865631155669
batch 110 loss: 0.0005577034782618285
batch 115 loss: 0.0005575761897489429
batch 120 loss: 0.0005576446186751127
batch 125 loss: 0.0005575949791818858
batch 130 loss: 0.0005577959818765521
batch 135 loss: 0.0005577081348747015
batch 140 loss: 0.0005576769355684519
batch 145 loss: 0.0005577299976721406
batch 150 loss: 0.0005576133728027344
batch 155 loss: 0.0005578168435022235
batch 160 loss: 0.0005575756542384625
batch 165 loss: 0.0005576311959885061
batch 170 loss: 0.000557752011809498
batch 175 loss: 0.0005576903116889298
batch 180 loss: 0.0005576812662184238
batch 185 loss: 0.0005576777155511081
batch 190 loss: 0.0005578127573244273
batch 195 loss: 0.000557698926422745
batch 200 loss: 0.0005577139323577285
batch 205 loss: 0.0005576162948273122
batch 210 loss: 0.0005577146308496595
batch 215 loss: 0.0005576805560849607
batch 220 loss: 0.0005576937342993915
batch 225 loss: 0.0005576090305112302
batch 230 loss: 0.0005576433148235082
batch 235 loss: 0.0005576920579187572
batch 240 loss: 0.0005576830822974444
Training Loss: 0.0005576833764886639
Validation Loss: 0.0005576934054261073
Epoch 100:
batch 5 loss: 0.0005576286348514259
batch 10 loss: 0.0005576361552812159
batch 15 loss: 0.0005576832336373627
batch 20 loss: 0.0005575915449298919
batch 25 loss: 0.0005576523952186107
batch 30 loss: 0.0005577492411248386
batch 35 loss: 0.0005576953524723649
batch 40 loss: 0.0005575317423790693
batch 45 loss: 0.0005576528958044947
batch 50 loss: 0.0005576523719355464
batch 55 loss: 0.0005576006951741874
batch 60 loss: 0.0005577211733907462
batch 65 loss: 0.0005576137220486999
batch 70 loss: 0.0005577764357440173
batch 75 loss: 0.0005576845142059028
batch 80 loss: 0.0005577088333666325
batch 85 loss: 0.000557842361740768
batch 90 loss: 0.0005576886702328921
batch 95 loss: 0.0005577414063736797
batch 100 loss: 0.0005577038857154549
batch 105 loss: 0.0005576797295361758
batch 110 loss: 0.0005577214644290507
batch 115 loss: 0.0005576389958150685
batch 120 loss: 0.0005577543401159346
batch 125 loss: 0.000557671720162034
batch 130 loss: 0.0005576356314122676
batch 135 loss: 0.0005575854913331569
batch 140 loss: 0.0005577771691605449
batch 145 loss: 0.0005577084491960704
batch 150 loss: 0.0005577602540142834
batch 155 loss: 0.0005577430827543139
batch 160 loss: 0.0005576087860390544
batch 165 loss: 0.0005575812887400389
batch 170 loss: 0.0005575965507887304
batch 175 loss: 0.0005576759926043451
batch 180 loss: 0.0005577339557930827
batch 185 loss: 0.0005577138625085353
batch 190 loss: 0.000557692814618349
batch 195 loss: 0.0005576827563345433
batch 200 loss: 0.0005577628035098314
batch 205 loss: 0.0005577593692578375
batch 210 loss: 0.0005576689261943101
batch 215 loss: 0.0005577168078161776
batch 220 loss: 0.000557637179736048
batch 225 loss: 0.0005576813593506813
batch 230 loss: 0.0005576847237534821
batch 235 loss: 0.0005576161667704583
batch 240 loss: 0.0005577571224421263
Training Loss: 0.0005576833762461319
Validation Loss: 0.0005576933996053413
