no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_CNN_maxPool3_5
# of epochs: 50
# of samples: 24000
Batchsize: 50
# of batches: 480
# of nodes in bottleneck 5000
20240711_171516
Epoch 1:
batch 5 loss: 0.005151103436946869
batch 10 loss: 0.0035527967382222416
batch 15 loss: 0.002764919213950634
batch 20 loss: 0.002150253183208406
batch 25 loss: 0.0017136890906840562
batch 30 loss: 0.0014050116064026953
batch 35 loss: 0.0012193186907097698
batch 40 loss: 0.0010843105148524046
batch 45 loss: 0.0009402930620126427
batch 50 loss: 0.0008578799664974212
batch 55 loss: 0.0007939533097669482
batch 60 loss: 0.0007525571389123797
batch 65 loss: 0.0007314412388950587
batch 70 loss: 0.0006858815671876073
batch 75 loss: 0.0006759203970432281
batch 80 loss: 0.0006447586929425597
batch 85 loss: 0.0006412992253899575
batch 90 loss: 0.0006265794858336448
batch 95 loss: 0.0006181185832247138
batch 100 loss: 0.0006102559622377157
batch 105 loss: 0.0006058188970200718
batch 110 loss: 0.0006067370413802564
batch 115 loss: 0.0005994284991174936
batch 120 loss: 0.0005935411783866585
batch 125 loss: 0.0005862172227352857
batch 130 loss: 0.0005788016831502318
batch 135 loss: 0.0005793346324935555
batch 140 loss: 0.0005774933379143476
batch 145 loss: 0.0005759150488302111
batch 150 loss: 0.0005741959437727928
batch 155 loss: 0.0005713612423278392
batch 160 loss: 0.0005718627944588661
batch 165 loss: 0.0005711200530640781
batch 170 loss: 0.0005710933823138475
batch 175 loss: 0.0005666171084158123
batch 180 loss: 0.00056761713931337
batch 185 loss: 0.0005656883586198092
batch 190 loss: 0.0005661153001710773
batch 195 loss: 0.0005637747002765536
batch 200 loss: 0.0005635360139422119
batch 205 loss: 0.000564123725052923
batch 210 loss: 0.0005639273789711296
batch 215 loss: 0.0005632552434690297
batch 220 loss: 0.00056359349982813
batch 225 loss: 0.0005630573723465204
batch 230 loss: 0.0005624445155262947
batch 235 loss: 0.0005636422196403146
batch 240 loss: 0.0005619139294140041
batch 245 loss: 0.0005618250812403858
batch 250 loss: 0.0005612538778223097
batch 255 loss: 0.0005603761062957346
batch 260 loss: 0.0005620510899461805
batch 265 loss: 0.0005620927782729268
batch 270 loss: 0.0005596788134425879
batch 275 loss: 0.0005608355393633246
batch 280 loss: 0.0005598752642981708
batch 285 loss: 0.0005601753597147763
batch 290 loss: 0.0005586064769886434
batch 295 loss: 0.000559233850799501
batch 300 loss: 0.000559278973378241
batch 305 loss: 0.0005591563647612929
batch 310 loss: 0.0005592557019554079
batch 315 loss: 0.0005594648886471987
batch 320 loss: 0.0005588367930613458
batch 325 loss: 0.0005593130947090686
batch 330 loss: 0.0005588994128629566
batch 335 loss: 0.0005592275294475258
batch 340 loss: 0.0005589468404650688
batch 345 loss: 0.0005588583182543517
batch 350 loss: 0.0005593357607722282
batch 355 loss: 0.0005587840685620904
batch 360 loss: 0.0005584601545706392
batch 365 loss: 0.0005589943029917777
batch 370 loss: 0.0005585623672232032
batch 375 loss: 0.000558459956664592
batch 380 loss: 0.0005585934850387275
batch 385 loss: 0.0005588519270531833
batch 390 loss: 0.0005592155852355063
batch 395 loss: 0.00055855184327811
batch 400 loss: 0.0005582213285379112
batch 405 loss: 0.0005584647529758513
batch 410 loss: 0.0005583175923675298
batch 415 loss: 0.0005584164988249541
batch 420 loss: 0.0005584442289546133
batch 425 loss: 0.0005590688553638757
batch 430 loss: 0.0005588110187090933
batch 435 loss: 0.000558090431150049
batch 440 loss: 0.0005589699721895158
batch 445 loss: 0.0005584021331742406
batch 450 loss: 0.0005583546706475317
batch 455 loss: 0.0005584676051512361
batch 460 loss: 0.0005580528290010988
batch 465 loss: 0.0005582523997873067
batch 470 loss: 0.0005582128884270787
batch 475 loss: 0.0005583988619036973
batch 480 loss: 0.000558511889539659
Training Loss: 0.0007345945638614163
Validation Loss: 0.0005576935790789624
Epoch 2:
batch 5 loss: 0.0005586314015090466
batch 10 loss: 0.0005580689874477684
batch 15 loss: 0.0005583706661127508
batch 20 loss: 0.000558545475360006
batch 25 loss: 0.0005584323196671903
batch 30 loss: 0.000558720005210489
batch 35 loss: 0.0005583696532994509
batch 40 loss: 0.0005584302009083331
batch 45 loss: 0.0005586467683315277
batch 50 loss: 0.0005579788354225457
batch 55 loss: 0.0005579798365943134
batch 60 loss: 0.0005582686164416373
batch 65 loss: 0.0005585247999988496
batch 70 loss: 0.0005583616090007127
batch 75 loss: 0.000558280455879867
batch 80 loss: 0.0005583569640293717
batch 85 loss: 0.0005582946701906622
batch 90 loss: 0.000558174669276923
batch 95 loss: 0.0005581737961620093
batch 100 loss: 0.0005579819204285741
batch 105 loss: 0.0005581599776633084
batch 110 loss: 0.0005580754252150655
batch 115 loss: 0.0005581673351116479
batch 120 loss: 0.0005581740872003138
batch 125 loss: 0.0005579086253419518
batch 130 loss: 0.0005579084856435657
batch 135 loss: 0.0005580155178904533
batch 140 loss: 0.0005582183948718011
batch 145 loss: 0.0005580492084845901
batch 150 loss: 0.0005581496981903911
batch 155 loss: 0.0005582719575613737
batch 160 loss: 0.0005581115954555571
batch 165 loss: 0.0005579985911026597
batch 170 loss: 0.0005578666576184332
batch 175 loss: 0.0005582720506936312
batch 180 loss: 0.0005581902107223869
batch 185 loss: 0.0005577804520726203
batch 190 loss: 0.0005579803255386651
batch 195 loss: 0.0005583963706158101
batch 200 loss: 0.0005578936310485005
batch 205 loss: 0.0005582729354500771
batch 210 loss: 0.0005579777294769883
batch 215 loss: 0.0005580241791903973
batch 220 loss: 0.0005579406279139221
batch 225 loss: 0.0005580053082667291
batch 230 loss: 0.0005578685784712433
batch 235 loss: 0.000558087567333132
batch 240 loss: 0.0005579326767474413
batch 245 loss: 0.0005582491285167634
batch 250 loss: 0.0005582438316196203
batch 255 loss: 0.0005578621639870107
batch 260 loss: 0.0005578987067565322
batch 265 loss: 0.0005580424563959241
batch 270 loss: 0.0005580738303251565
batch 275 loss: 0.0005578813725151122
batch 280 loss: 0.0005579427350312471
batch 285 loss: 0.000558041490148753
batch 290 loss: 0.0005578843294642866
batch 295 loss: 0.0005578271578997373
batch 300 loss: 0.0005581171484664082
batch 305 loss: 0.0005578487413004041
batch 310 loss: 0.0005583212827332318
batch 315 loss: 0.0005578670185059309
batch 320 loss: 0.0005576624535024166
batch 325 loss: 0.0005579611519351601
batch 330 loss: 0.0005577001953497529
batch 335 loss: 0.0005579321412369608
batch 340 loss: 0.0005579260294325649
batch 345 loss: 0.0005583002348430455
batch 350 loss: 0.0005578052252531052
batch 355 loss: 0.0005577372736297548
batch 360 loss: 0.0005578430485911667
batch 365 loss: 0.0005576939089223743
batch 370 loss: 0.0005578279262408615
batch 375 loss: 0.0005578789743594826
batch 380 loss: 0.0005576371680945158
batch 385 loss: 0.0005576205789111554
batch 390 loss: 0.0005578267620876432
batch 395 loss: 0.0005577350733801722
batch 400 loss: 0.0005579184275120497
batch 405 loss: 0.0005577353644184768
batch 410 loss: 0.0005579728167504072
batch 415 loss: 0.0005577048170380295
batch 420 loss: 0.0005580369383096695
batch 425 loss: 0.0005577389616519213
batch 430 loss: 0.000557836820371449
batch 435 loss: 0.0005578586016781629
batch 440 loss: 0.0005579129094257951
batch 445 loss: 0.0005578624201007188
batch 450 loss: 0.0005579510587267577
batch 455 loss: 0.0005578372860327363
batch 460 loss: 0.0005577921634539962
batch 465 loss: 0.0005576967261731625
batch 470 loss: 0.0005578527227044106
batch 475 loss: 0.0005578153184615075
batch 480 loss: 0.0005577747710049152
Training Loss: 0.0005580289324522407
Validation Loss: 0.000557693528632323
Epoch 3:
batch 5 loss: 0.0005578124662861228
batch 10 loss: 0.0005575680872425437
batch 15 loss: 0.0005578659824095666
batch 20 loss: 0.0005578171694651246
batch 25 loss: 0.0005577286239713431
batch 30 loss: 0.000557872315403074
batch 35 loss: 0.0005577006377279758
batch 40 loss: 0.0005577098345384002
batch 45 loss: 0.0005575004266574979
batch 50 loss: 0.0005575601477175951
batch 55 loss: 0.0005576012539677322
batch 60 loss: 0.0005577508476562798
batch 65 loss: 0.0005577901494689286
batch 70 loss: 0.0005576124065555632
batch 75 loss: 0.0005580402794294059
batch 80 loss: 0.0005579906282946467
batch 85 loss: 0.0005576914642006159
batch 90 loss: 0.0005577012663707138
batch 95 loss: 0.0005577592295594513
batch 100 loss: 0.0005578892189078033
batch 105 loss: 0.0005576331168413162
batch 110 loss: 0.0005578791489824653
batch 115 loss: 0.0005576173076406121
batch 120 loss: 0.0005577519885264337
batch 125 loss: 0.000557683315128088
batch 130 loss: 0.000557817961089313
batch 135 loss: 0.0005576161202043295
batch 140 loss: 0.0005576934665441513
batch 145 loss: 0.0005577421630732715
batch 150 loss: 0.0005577779025770724
batch 155 loss: 0.0005576835246756673
batch 160 loss: 0.000557716703042388
batch 165 loss: 0.0005579144693911075
batch 170 loss: 0.0005576029419898987
batch 175 loss: 0.0005577807198278606
batch 180 loss: 0.0005578172393143177
batch 185 loss: 0.0005576271563768387
batch 190 loss: 0.0005579784279689193
batch 195 loss: 0.0005578437470830977
batch 200 loss: 0.000557824200950563
batch 205 loss: 0.0005577025935053825
batch 210 loss: 0.0005577618721872568
batch 215 loss: 0.000558016006834805
batch 220 loss: 0.0005578813143074512
batch 225 loss: 0.0005576784955337643
batch 230 loss: 0.0005578454467467964
batch 235 loss: 0.0005577393108978868
batch 240 loss: 0.0005580242141149938
batch 245 loss: 0.0005578173091635108
batch 250 loss: 0.0005580128519795835
batch 255 loss: 0.0005580066121183336
batch 260 loss: 0.0005579564138315618
batch 265 loss: 0.0005578201613388956
batch 270 loss: 0.000557727087289095
batch 275 loss: 0.000557649158872664
batch 280 loss: 0.0005577905802056193
batch 285 loss: 0.0005576683906838298
batch 290 loss: 0.0005577376810833812
batch 295 loss: 0.0005576665513217449
batch 300 loss: 0.0005577084724791348
batch 305 loss: 0.0005576577270403505
batch 310 loss: 0.0005576860974542796
batch 315 loss: 0.0005575523362495005
batch 320 loss: 0.0005575457937084139
batch 325 loss: 0.0005576037685386837
batch 330 loss: 0.0005577158299274743
batch 335 loss: 0.0005575864459387958
batch 340 loss: 0.0005578168202191591
batch 345 loss: 0.0005576572031714023
batch 350 loss: 0.0005577390315011143
batch 355 loss: 0.0005577275180257857
batch 360 loss: 0.0005578961921855807
batch 365 loss: 0.0005577424424700439
batch 370 loss: 0.0005577074130997062
batch 375 loss: 0.0005576235125772655
batch 380 loss: 0.0005576954456046224
batch 385 loss: 0.0005576593102887273
batch 390 loss: 0.0005578578100539743
batch 395 loss: 0.0005578451789915561
batch 400 loss: 0.0005577100324444473
batch 405 loss: 0.0005577946780249476
batch 410 loss: 0.000557609973475337
batch 415 loss: 0.0005577398464083671
batch 420 loss: 0.0005578059935942292
batch 425 loss: 0.0005577433272264898
batch 430 loss: 0.0005577463191002608
batch 435 loss: 0.0005579524207860231
batch 440 loss: 0.0005578226526267826
batch 445 loss: 0.000557653175201267
batch 450 loss: 0.0005576023482717574
batch 455 loss: 0.0005576648632995784
batch 460 loss: 0.0005578198470175266
batch 465 loss: 0.000557603593915701
batch 470 loss: 0.0005578530370257794
batch 475 loss: 0.000557649799156934
batch 480 loss: 0.0005577210686169565
Training Loss: 0.0005577498274457563
