no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_CNN_maxPool2
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 5000
20240630_135640
Epoch 1:
batch 5 loss: 0.009465932194143534
batch 10 loss: 0.0035168709233403205
batch 15 loss: 0.0021060381783172487
batch 20 loss: 0.0016178626101464033
batch 25 loss: 0.00138042860198766
batch 30 loss: 0.001240586582571268
batch 35 loss: 0.001150695327669382
batch 40 loss: 0.0010476184077560901
batch 45 loss: 0.0010037067346274852
batch 50 loss: 0.000923593295738101
batch 55 loss: 0.0008898028521798551
batch 60 loss: 0.0008598811225965619
batch 65 loss: 0.000810049066785723
batch 70 loss: 0.0007706801407039165
batch 75 loss: 0.000742766831535846
batch 80 loss: 0.0007233189651742577
batch 85 loss: 0.0007109635393135249
batch 90 loss: 0.0006907709292136132
batch 95 loss: 0.000677115237340331
batch 100 loss: 0.0006659660954028368
batch 105 loss: 0.000659057090524584
batch 110 loss: 0.0006493140128441155
batch 115 loss: 0.0006442449288442731
batch 120 loss: 0.000641317036934197
batch 125 loss: 0.0006402472034096717
batch 130 loss: 0.0006254096864722669
batch 135 loss: 0.0006221726885996759
batch 140 loss: 0.0006299394415691495
batch 145 loss: 0.0006253930507227779
batch 150 loss: 0.0006268435623496771
batch 155 loss: 0.0006211934378370643
batch 160 loss: 0.0006140559678897261
batch 165 loss: 0.0006129639106802643
batch 170 loss: 0.0006086638779379428
batch 175 loss: 0.0006073706434108316
batch 180 loss: 0.0006045758957043291
batch 185 loss: 0.0006065632682293653
batch 190 loss: 0.0006147804902866483
batch 195 loss: 0.0006368558388203382
batch 200 loss: 0.0006196081638336181
batch 205 loss: 0.0006142504513263703
batch 210 loss: 0.0006103384890593589
batch 215 loss: 0.0006062595755793154
batch 220 loss: 0.0006102578947320581
batch 225 loss: 0.0006011213408783078
batch 230 loss: 0.0005976905231364071
batch 235 loss: 0.0005943146999925375
batch 240 loss: 0.0005916011752560735
Training Loss: 0.0010068969163209355
Validation Loss: 0.000852601332978035
Epoch 2:
batch 5 loss: 0.0006005426752381027
batch 10 loss: 0.0006278494955040514
batch 15 loss: 0.0006105795735493302
batch 20 loss: 0.0006124167586676777
batch 25 loss: 0.0006046562921255827
batch 30 loss: 0.0006032081670127809
batch 35 loss: 0.0005978270433843136
batch 40 loss: 0.0005946156685240567
batch 45 loss: 0.0005935452994890511
batch 50 loss: 0.000595462613273412
batch 55 loss: 0.0005916018038988113
batch 60 loss: 0.0005850730114616454
batch 65 loss: 0.0005845192121341824
batch 70 loss: 0.0005831107147969306
batch 75 loss: 0.0005824702093377709
batch 80 loss: 0.0006360251922160387
batch 85 loss: 0.0006190821877680719
batch 90 loss: 0.0006072046118788422
batch 95 loss: 0.0005993375554680825
batch 100 loss: 0.0005991502897813916
batch 105 loss: 0.0005900013376958669
batch 110 loss: 0.0005900653195567429
batch 115 loss: 0.0005851385300047696
batch 120 loss: 0.0005910209380090237
batch 125 loss: 0.0005824452615343034
batch 130 loss: 0.0005840003723278642
batch 135 loss: 0.0005790436174720526
batch 140 loss: 0.0005786247435025871
batch 145 loss: 0.0005784731358289719
batch 150 loss: 0.0005746583570726216
batch 155 loss: 0.0005754286772571504
batch 160 loss: 0.0005905040539801121
batch 165 loss: 0.0006044660229235887
batch 170 loss: 0.0005955718224868178
batch 175 loss: 0.0005884691607207059
batch 180 loss: 0.0005831547081470489
batch 185 loss: 0.0005786062916740775
batch 190 loss: 0.0005830784910358488
batch 195 loss: 0.0005824941326864064
batch 200 loss: 0.000578106299508363
batch 205 loss: 0.0005788181093521416
batch 210 loss: 0.0005763422814197838
batch 215 loss: 0.000577941385563463
batch 220 loss: 0.0005744429654441774
batch 225 loss: 0.0005751110031269491
batch 230 loss: 0.0005733003607019782
batch 235 loss: 0.0005733536789193749
batch 240 loss: 0.0005725370487198233
Training Loss: 0.0005900724267121404
Validation Loss: 0.000570244321716018
Epoch 3:
batch 5 loss: 0.0005689407582394779
batch 10 loss: 0.0005706541356630624
batch 15 loss: 0.000571307260543108
batch 20 loss: 0.0005696165491826833
batch 25 loss: 0.0005712011712603271
batch 30 loss: 0.0005717337713576853
batch 35 loss: 0.0005705541581846774
batch 40 loss: 0.0005707576870918274
batch 45 loss: 0.0005711339064873755
batch 50 loss: 0.0006647911504842341
batch 55 loss: 0.001784231886267662
batch 60 loss: 0.000986934988759458
batch 65 loss: 0.0008368473732843995
batch 70 loss: 0.0007334344321861863
batch 75 loss: 0.0006887950701639056
batch 80 loss: 0.0006611877237446606
batch 85 loss: 0.000649641954805702
batch 90 loss: 0.0006269894074648619
batch 95 loss: 0.000617974775377661
batch 100 loss: 0.0006165848695673049
batch 105 loss: 0.0006107513210736216
batch 110 loss: 0.0006050676223821938
batch 115 loss: 0.0006041676853783429
batch 120 loss: 0.0005990699748508633
batch 125 loss: 0.0005973867140710354
batch 130 loss: 0.000595139991492033
batch 135 loss: 0.0005927696591243147
batch 140 loss: 0.0005898432456888259
batch 145 loss: 0.0005898497765883803
batch 150 loss: 0.000589302193839103
batch 155 loss: 0.0005845367442816496
batch 160 loss: 0.0005860157310962677
batch 165 loss: 0.0005835304036736488
batch 170 loss: 0.0005836190073750913
batch 175 loss: 0.0005864879116415977
batch 180 loss: 0.0005829563597217202
batch 185 loss: 0.0005845747073180974
batch 190 loss: 0.0005813144613057375
batch 195 loss: 0.0005815976765006781
batch 200 loss: 0.0005796954967081547
batch 205 loss: 0.0005803463282063603
batch 210 loss: 0.0005780295468866825
batch 215 loss: 0.0005773766548372805
batch 220 loss: 0.0005771872587502002
batch 225 loss: 0.0005764644476585091
batch 230 loss: 0.0005771346273832023
batch 235 loss: 0.0005755134974606335
batch 240 loss: 0.0005743092740885913
Training Loss: 0.0006339031531145641
Validation Loss: 0.0007081586712350448
Epoch 4:
batch 5 loss: 0.0005759580293670296
batch 10 loss: 0.000576150743290782
batch 15 loss: 0.0005738895968534052
batch 20 loss: 0.0005735361250117421
batch 25 loss: 0.0005724592134356499
batch 30 loss: 0.0005744350026361644
batch 35 loss: 0.0005728943971917034
batch 40 loss: 0.0005731986602768302
batch 45 loss: 0.0005732138990424574
batch 50 loss: 0.0005709263845346868
batch 55 loss: 0.0005727630807086826
batch 60 loss: 0.0005730553413741291
batch 65 loss: 0.0005727254436351358
batch 70 loss: 0.0005741399014368654
batch 75 loss: 0.0005735648213885724
batch 80 loss: 0.0005720740999095141
batch 85 loss: 0.0005736783146858216
batch 90 loss: 0.0005727143725380301
batch 95 loss: 0.0005713037913665176
batch 100 loss: 0.0005714366212487221
batch 105 loss: 0.0005693332175724208
batch 110 loss: 0.0005694406107068061
batch 115 loss: 0.0005708635784685612
batch 120 loss: 0.0005715651088394225
batch 125 loss: 0.0005698838154785335
batch 130 loss: 0.0005689866840839386
batch 135 loss: 0.0005676281289197505
batch 140 loss: 0.0005694787134416402
batch 145 loss: 0.0005691229249350727
batch 150 loss: 0.0005691644386388361
batch 155 loss: 0.0005676764296367765
batch 160 loss: 0.000566932198125869
batch 165 loss: 0.0005673774285241961
batch 170 loss: 0.0005665301461704076
batch 175 loss: 0.0005675851250998675
batch 180 loss: 0.000565862306393683
batch 185 loss: 0.0005745188798755407
batch 190 loss: 0.0005742506473325193
batch 195 loss: 0.000572224345523864
batch 200 loss: 0.0005706775467842817
batch 205 loss: 0.0005695850937627256
batch 210 loss: 0.0005693444283679128
batch 215 loss: 0.0005672334227710962
batch 220 loss: 0.0005674797692336142
batch 225 loss: 0.0005670105572789907
batch 230 loss: 0.0005671744816936553
batch 235 loss: 0.0005672093364410102
batch 240 loss: 0.0005659815971739591
Training Loss: 0.0005707133083584873
Validation Loss: 0.001001151647263517
Epoch 5:
batch 5 loss: 0.0005667528137564659
batch 10 loss: 0.000567820796277374
batch 15 loss: 0.0005658232141286135
batch 20 loss: 0.0005651406478136778
batch 25 loss: 0.0005646736477501691
batch 30 loss: 0.0005650125094689429
batch 35 loss: 0.0005652330932207405
batch 40 loss: 0.0005646099336445332
batch 45 loss: 0.0005646150442771613
batch 50 loss: 0.0005649246624670923
batch 55 loss: 0.000564380909781903
batch 60 loss: 0.0005646799807436764
batch 65 loss: 0.0005647963494993746
batch 70 loss: 0.0005647778278216719
batch 75 loss: 0.0005651912069879472
batch 80 loss: 0.0005635429522953927
batch 85 loss: 0.0005647930433042347
batch 90 loss: 0.0005637832218781114
batch 95 loss: 0.0005642666365019977
batch 100 loss: 0.0005674415035173297
batch 105 loss: 0.0005665905773639679
batch 110 loss: 0.0005665033706463874
batch 115 loss: 0.0005649284459650517
batch 120 loss: 0.0005652367835864425
batch 125 loss: 0.0005640343180857599
batch 130 loss: 0.0005658065085299314
batch 135 loss: 0.0005678500048816205
batch 140 loss: 0.0005697110900655389
batch 145 loss: 0.0005672213039360941
batch 150 loss: 0.0005692255799658597
batch 155 loss: 0.000569092168007046
batch 160 loss: 0.0005666450248099864
batch 165 loss: 0.0005668955855071545
batch 170 loss: 0.0005647666868753732
batch 175 loss: 0.0005646349396556616
batch 180 loss: 0.0005636316142044962
batch 185 loss: 0.000564802484586835
batch 190 loss: 0.0005645153461955488
batch 195 loss: 0.0005637363879941404
batch 200 loss: 0.0005639543989673257
batch 205 loss: 0.0005644278600811959
batch 210 loss: 0.0005642494186758995
batch 215 loss: 0.0005638473085127771
batch 220 loss: 0.0005631616804748774
batch 225 loss: 0.0005627711885608733
batch 230 loss: 0.0005631978274323046
batch 235 loss: 0.0005631184088997543
batch 240 loss: 0.0005631770007312297
Training Loss: 0.0005652081939236572
Validation Loss: 0.0005663987996134286
Epoch 6:
batch 5 loss: 0.0005638606613501907
batch 10 loss: 0.0005630236235447228
batch 15 loss: 0.0005650346050970256
batch 20 loss: 0.0005635546753183007
batch 25 loss: 0.0005650802166201174
batch 30 loss: 0.0005712747923098504
batch 35 loss: 0.0005841606645844877
batch 40 loss: 0.0005853077629581094
batch 45 loss: 0.0005796760320663452
batch 50 loss: 0.0005734306643716991
batch 55 loss: 0.0005709560820832849
batch 60 loss: 0.0005702134920284152
batch 65 loss: 0.0005685796262696385
batch 70 loss: 0.0005672700703144073
batch 75 loss: 0.0005660600494593382
batch 80 loss: 0.0005652475054375827
batch 85 loss: 0.0005692417267709971
batch 90 loss: 0.0005657927133142949
batch 95 loss: 0.0005649885046295821
batch 100 loss: 0.0005647427286021411
batch 105 loss: 0.0005647238111123442
batch 110 loss: 0.0005633197142742574
batch 115 loss: 0.0005639549111947417
batch 120 loss: 0.0005636337213218212
batch 125 loss: 0.0005634414381347596
batch 130 loss: 0.0005637059570290148
batch 135 loss: 0.000562865543179214
batch 140 loss: 0.0005629885010421277
batch 145 loss: 0.0005628097336739301
batch 150 loss: 0.0005628439015708864
batch 155 loss: 0.0005628213053569198
batch 160 loss: 0.0005622610682621598
batch 165 loss: 0.0005617941380478442
batch 170 loss: 0.0005619244184345007
batch 175 loss: 0.0005644334247335792
batch 180 loss: 0.0005698874942027032
batch 185 loss: 0.0005733600584790111
batch 190 loss: 0.0005716275656595827
batch 195 loss: 0.0005726413335651159
batch 200 loss: 0.0005695853615179658
batch 205 loss: 0.0005666548619046808
batch 210 loss: 0.0005658675567246973
batch 215 loss: 0.0005649849772453308
batch 220 loss: 0.0005633386201225222
batch 225 loss: 0.0005647613084875047
batch 230 loss: 0.0005642664036713541
batch 235 loss: 0.0005636406363919378
batch 240 loss: 0.000564189744181931
Training Loss: 0.0005668713272219369
Validation Loss: 0.0005627921336175252
Epoch 7:
batch 5 loss: 0.0005631765234284103
batch 10 loss: 0.0005623353645205498
batch 15 loss: 0.0005629762774333358
batch 20 loss: 0.000561593915335834
batch 25 loss: 0.0005631520296446979
batch 30 loss: 0.0005619616596959532
batch 35 loss: 0.0005624630488455295
batch 40 loss: 0.0005617551971226931
batch 45 loss: 0.0005618860479444266
batch 50 loss: 0.0005622502882033587
batch 55 loss: 0.0005616666749119759
batch 60 loss: 0.0005613928660750389
batch 65 loss: 0.000561633543111384
batch 70 loss: 0.0005611560074612499
batch 75 loss: 0.0005606796825304627
batch 80 loss: 0.0005609574727714062
batch 85 loss: 0.0005615175934508443
batch 90 loss: 0.0005625209887512028
batch 95 loss: 0.000561911310069263
batch 100 loss: 0.0005620548734441399
batch 105 loss: 0.0005618193070404231
batch 110 loss: 0.0005620859214104712
batch 115 loss: 0.0007371112005785108
batch 120 loss: 0.0010326421936042607
batch 125 loss: 0.0008052925579249859
batch 130 loss: 0.0007119190413504839
batch 135 loss: 0.0006641546147875488
batch 140 loss: 0.0006308633834123612
batch 145 loss: 0.000614447018597275
batch 150 loss: 0.0006031684926711023
batch 155 loss: 0.0005962700932286679
batch 160 loss: 0.0005930853076279163
batch 165 loss: 0.0005866815685294568
batch 170 loss: 0.000582881725858897
batch 175 loss: 0.0005854685907252133
batch 180 loss: 0.0005811903392896056
batch 185 loss: 0.0005794827011413872
batch 190 loss: 0.0005800104117952287
batch 195 loss: 0.0005808003130368889
batch 200 loss: 0.0005805975059047341
batch 205 loss: 0.0005769236362539232
batch 210 loss: 0.0005752615514211356
batch 215 loss: 0.0005727799842134118
batch 220 loss: 0.0005724113900214434
batch 225 loss: 0.0005719922715798021
batch 230 loss: 0.0005708563956432044
batch 235 loss: 0.0005695030558854342
batch 240 loss: 0.0005700723151676357
Training Loss: 0.0005956002969469409
Validation Loss: 0.0005694581535256779
Epoch 8:
batch 5 loss: 0.0005703179282136262
batch 10 loss: 0.000569779530633241
batch 15 loss: 0.0005684892414137721
batch 20 loss: 0.0005677340901456773
batch 25 loss: 0.0005672975210472941
batch 30 loss: 0.0005706818075850606
batch 35 loss: 0.0005697807297110558
batch 40 loss: 0.0005686855060048401
batch 45 loss: 0.0005689679528586566
batch 50 loss: 0.0005661335191689432
batch 55 loss: 0.0005653636762872338
batch 60 loss: 0.0005665443139150738
batch 65 loss: 0.0005658515030518175
batch 70 loss: 0.0005663148825988173
batch 75 loss: 0.0005676485714502632
batch 80 loss: 0.0008269333047792315
batch 85 loss: 0.0012635717401280999
batch 90 loss: 0.0009780158754438162
batch 95 loss: 0.0008822724805213511
batch 100 loss: 0.0007358976639807224
batch 105 loss: 0.000666891667060554
batch 110 loss: 0.0006354737444780767
batch 115 loss: 0.0006140976445749402
batch 120 loss: 0.0006020171451382339
batch 125 loss: 0.000593542936258018
batch 130 loss: 0.0005879628472030163
batch 135 loss: 0.0005834042909555137
batch 140 loss: 0.0005818412755616009
batch 145 loss: 0.0005825792555697263
batch 150 loss: 0.0005794482654891908
batch 155 loss: 0.0005766022717580199
batch 160 loss: 0.0005756393657065928
batch 165 loss: 0.0005767900729551911
batch 170 loss: 0.0005749602802097798
batch 175 loss: 0.0005745752132497728
batch 180 loss: 0.0005738946027122438
batch 185 loss: 0.0005728719756007194
batch 190 loss: 0.0005732099176384509
batch 195 loss: 0.0005731312907300889
batch 200 loss: 0.0005721582099795341
batch 205 loss: 0.0005717301159165799
batch 210 loss: 0.0005702291964553297
batch 215 loss: 0.0005700135021470487
batch 220 loss: 0.0005708453129045666
batch 225 loss: 0.0005705074756406248
batch 230 loss: 0.0005703659262508154
batch 235 loss: 0.0005701121059246361
batch 240 loss: 0.0005689073936082423
Training Loss: 0.0006154184404294938
Validation Loss: 0.0005681132364164417
Epoch 9:
batch 5 loss: 0.00056930665159598
batch 10 loss: 0.0005684493575245142
batch 15 loss: 0.0005692414240911603
batch 20 loss: 0.0005696470732800662
batch 25 loss: 0.0005684213945642113
batch 30 loss: 0.0005688740871846676
batch 35 loss: 0.000567947281524539
batch 40 loss: 0.0005676343105733394
batch 45 loss: 0.0005681803449988365
batch 50 loss: 0.0005672727944329381
batch 55 loss: 0.0005667445831932128
batch 60 loss: 0.0005657641217112541
batch 65 loss: 0.0005666887853294611
batch 70 loss: 0.0005645208060741424
batch 75 loss: 0.0005655461107380688
batch 80 loss: 0.0005652422667481005
batch 85 loss: 0.000564975361339748
batch 90 loss: 0.0005660944618284703
batch 95 loss: 0.0005655113374814392
batch 100 loss: 0.0005655476357787848
batch 105 loss: 0.000564257160294801
batch 110 loss: 0.0005640998599119485
batch 115 loss: 0.0005638003232888878
batch 120 loss: 0.0005632028565742076
batch 125 loss: 0.0005633593769744039
batch 130 loss: 0.0005633013788610697
batch 135 loss: 0.0005632518325001002
batch 140 loss: 0.0005652312887832522
batch 145 loss: 0.000563415139913559
batch 150 loss: 0.0005634662578813731
batch 155 loss: 0.0005628702929243445
batch 160 loss: 0.0005634682136587798
batch 165 loss: 0.0005629656021483243
batch 170 loss: 0.0005624082987196743
batch 175 loss: 0.0005625947960652411
batch 180 loss: 0.000563119468279183
batch 185 loss: 0.0005651666433550417
batch 190 loss: 0.0005642533069476485
batch 195 loss: 0.0005624718382023275
batch 200 loss: 0.0005623898236081004
batch 205 loss: 0.0005614318884909153
batch 210 loss: 0.0005627447040751577
batch 215 loss: 0.0005619456409476697
batch 220 loss: 0.000562593131326139
batch 225 loss: 0.0005619325442239642
batch 230 loss: 0.0005617523682303726
batch 235 loss: 0.0005622230004519224
batch 240 loss: 0.0005610890104435385
Training Loss: 0.0005647170049390601
Validation Loss: 0.0005604571332999815
Epoch 10:
batch 5 loss: 0.0005604114616289735
batch 10 loss: 0.0005602755933068693
batch 15 loss: 0.0005597036564722657
batch 20 loss: 0.0005595156340859831
batch 25 loss: 0.0005606432445347309
batch 30 loss: 0.000559878337662667
batch 35 loss: 0.0005603112047538161
batch 40 loss: 0.0005607293685898184
batch 45 loss: 0.0005598266376182437
batch 50 loss: 0.0005592606961727142
batch 55 loss: 0.0005586869665421546
batch 60 loss: 0.0005586885032244027
batch 65 loss: 0.0005584615282714367
batch 70 loss: 0.0005588388419710099
batch 75 loss: 0.0005610859603621066
batch 80 loss: 0.0005636248737573623
batch 85 loss: 0.000563467713072896
batch 90 loss: 0.0005625515943393112
batch 95 loss: 0.000561876141000539
batch 100 loss: 0.0005617512739263475
batch 105 loss: 0.0005623545614071191
batch 110 loss: 0.0005616125534288585
batch 115 loss: 0.0005615014350041747
batch 120 loss: 0.0005616454058326781
batch 125 loss: 0.0005617356626316905
batch 130 loss: 0.0005616095382720232
batch 135 loss: 0.0005612004664726555
batch 140 loss: 0.0005615653470158577
batch 145 loss: 0.0005610956810414791
batch 150 loss: 0.000561176473274827
batch 155 loss: 0.000561090768314898
batch 160 loss: 0.0005609275889582932
batch 165 loss: 0.0005612371955066919
batch 170 loss: 0.0005611861706711352
batch 175 loss: 0.0005615425063297153
batch 180 loss: 0.0005607038270682097
batch 185 loss: 0.0005609594052657485
batch 190 loss: 0.000561174051836133
batch 195 loss: 0.0005608294275589287
batch 200 loss: 0.0005607813480310142
batch 205 loss: 0.0005609413725323975
batch 210 loss: 0.0005604731035418808
batch 215 loss: 0.0005615143687464297
batch 220 loss: 0.0005607995903119445
batch 225 loss: 0.0005606950609944761
batch 230 loss: 0.000560470880009234
batch 235 loss: 0.0005603919154964388
batch 240 loss: 0.0005601028446108103
Training Loss: 0.0005608522454470707
Validation Loss: 0.0005599132147229587
Epoch 11:
batch 5 loss: 0.0005606481921859085
batch 10 loss: 0.0005603409255854786
batch 15 loss: 0.0005607352359220386
batch 20 loss: 0.000560414872597903
batch 25 loss: 0.0005601443583145738
batch 30 loss: 0.0005600719945505261
batch 35 loss: 0.0005597056122496724
batch 40 loss: 0.0005597413168288767
batch 45 loss: 0.000560540531296283
batch 50 loss: 0.0005603332072496414
batch 55 loss: 0.0005600095144473016
batch 60 loss: 0.0005645602243021131
batch 65 loss: 0.0005650629522278905
batch 70 loss: 0.0005643511889502406
batch 75 loss: 0.0005639463081024588
batch 80 loss: 0.0005628612590953708
batch 85 loss: 0.0005621482036076486
batch 90 loss: 0.0005621842923574149
batch 95 loss: 0.0005614455207251012
batch 100 loss: 0.0005616985028609634
batch 105 loss: 0.0005611292086541653
batch 110 loss: 0.0005605340236797928
batch 115 loss: 0.0005603188532404601
batch 120 loss: 0.0005599655327387154
batch 125 loss: 0.0005601063719950616
batch 130 loss: 0.0005602880613878369
batch 135 loss: 0.0005600936594419182
batch 140 loss: 0.0005599053576588631
batch 145 loss: 0.0005600649281404912
batch 150 loss: 0.0005598867428489029
batch 155 loss: 0.0005598686169832945
batch 160 loss: 0.0005596176371909678
batch 165 loss: 0.0005594357848167419
batch 170 loss: 0.0005597258103080093
batch 175 loss: 0.0005595565540716052
batch 180 loss: 0.0005597211653366685
batch 185 loss: 0.0005592205910943448
batch 190 loss: 0.0005597282201051712
batch 195 loss: 0.0005591949797235429
batch 200 loss: 0.0005595091846771538
batch 205 loss: 0.0005593592068180442
batch 210 loss: 0.0005590999731794
batch 215 loss: 0.0005594570538960397
batch 220 loss: 0.0005598778952844441
batch 225 loss: 0.0005590508342720568
batch 230 loss: 0.0005593933630734683
batch 235 loss: 0.0005593767622485757
batch 240 loss: 0.0005591292050667107
Training Loss: 0.0005604908289872886
Validation Loss: 0.0005590442587466289
Epoch 12:
batch 5 loss: 0.0005592108936980367
batch 10 loss: 0.0005589750711806118
batch 15 loss: 0.0005590823013335467
batch 20 loss: 0.000559450825676322
batch 25 loss: 0.0005593803711235523
batch 30 loss: 0.0005592953646555543
batch 35 loss: 0.0005591735593043268
batch 40 loss: 0.0005590399610809982
batch 45 loss: 0.0005591396591626107
batch 50 loss: 0.0005587304476648569
batch 55 loss: 0.0005579295917414129
batch 60 loss: 0.0005578146665357053
batch 65 loss: 0.0005577516742050648
batch 70 loss: 0.0005577684729360044
batch 75 loss: 0.0005576318711973727
batch 80 loss: 0.0005578260286711156
batch 85 loss: 0.0005577360279858112
batch 90 loss: 0.000557652791030705
batch 95 loss: 0.0005576936528086662
batch 100 loss: 0.0005576546653173863
batch 105 loss: 0.0005576913594268262
batch 110 loss: 0.0005576439667493105
batch 115 loss: 0.0005576608935371041
batch 120 loss: 0.0005577280186116696
batch 125 loss: 0.0005576997296884656
batch 130 loss: 0.0005576854688115418
batch 135 loss: 0.0005577160278335214
batch 140 loss: 0.0005577328382059931
batch 145 loss: 0.0005576538620516658
batch 150 loss: 0.0005577431409619748
batch 155 loss: 0.0005576880415901541
batch 160 loss: 0.0005577267147600651
batch 165 loss: 0.0005575819872319698
batch 170 loss: 0.0005576246534474194
batch 175 loss: 0.0005576469702646136
batch 180 loss: 0.0005576117429882288
batch 185 loss: 0.0005577028612606227
batch 190 loss: 0.0005577179370447993
batch 195 loss: 0.0005576972500421107
batch 200 loss: 0.0005576425697654486
batch 205 loss: 0.0005577901611104608
batch 210 loss: 0.0005577469826675951
batch 215 loss: 0.0005576734314672649
batch 220 loss: 0.0005575625691562891
batch 225 loss: 0.0005577000789344311
batch 230 loss: 0.0006973985233344138
batch 235 loss: 0.0005587418214417994
batch 240 loss: 0.0005578778684139252
Training Loss: 0.000560938028502278
Validation Loss: 0.0006211473606526852
Epoch 13:
batch 5 loss: 0.0005576230119913817
batch 10 loss: 0.0005576627794653177
batch 15 loss: 0.0005576059338636696
batch 20 loss: 0.0005576296825893223
batch 25 loss: 0.0005576223484240472
batch 30 loss: 0.0005576902767643332
batch 35 loss: 0.0005577961797825992
batch 40 loss: 0.0005577190546318889
batch 45 loss: 0.0005577052477747201
batch 50 loss: 0.0005577264935709536
batch 55 loss: 0.0005577907315455378
batch 60 loss: 0.0005577416974119842
batch 65 loss: 0.0005576976924203336
batch 70 loss: 0.0005577363539487123
batch 75 loss: 0.0005576180526986718
batch 80 loss: 0.0005577083909884095
batch 85 loss: 0.0005576793919317424
batch 90 loss: 0.0005576656199991703
batch 95 loss: 0.0005577001720666885
batch 100 loss: 0.000557636993471533
batch 105 loss: 0.0005577655392698944
batch 110 loss: 0.0005576579365879297
batch 115 loss: 0.0005577512551099062
batch 120 loss: 0.0005577924777753652
batch 125 loss: 0.0005576273892074823
batch 130 loss: 0.0005576185649260879
batch 135 loss: 0.0005576679250225425
batch 140 loss: 0.0005576686467975378
batch 145 loss: 0.0005577483330853284
batch 150 loss: 0.0005576202995143831
batch 155 loss: 0.0005577235948294401
batch 160 loss: 0.0005576981231570244
batch 165 loss: 0.0005575673887506128
batch 170 loss: 0.0005576825002208352
batch 175 loss: 0.0005575897172093392
batch 180 loss: 0.0005577979958616197
batch 185 loss: 0.000557618378661573
batch 190 loss: 0.0005577345727942884
batch 195 loss: 0.0005576078547164798
batch 200 loss: 0.0005577005795203149
batch 205 loss: 0.0005577042582444846
batch 210 loss: 0.0005576178431510925
batch 215 loss: 0.0005576198105700314
batch 220 loss: 0.0005575173301622272
batch 225 loss: 0.00055780871771276
batch 230 loss: 0.0005577661097049714
batch 235 loss: 0.0005577210453338921
batch 240 loss: 0.0005577364005148411
Training Loss: 0.0005576851394531938
Validation Loss: 0.0005576983899421369
Epoch 14:
batch 5 loss: 0.0005576759460382164
batch 10 loss: 0.0005576718831434846
batch 15 loss: 0.0005577121861279011
batch 20 loss: 0.0005577584030106664
batch 25 loss: 0.0005576874944381415
batch 30 loss: 0.000557735376060009
batch 35 loss: 0.0005577245028689504
batch 40 loss: 0.000557706702966243
batch 45 loss: 0.0005576305906288326
batch 50 loss: 0.0005576417781412602
batch 55 loss: 0.000557714281603694
batch 60 loss: 0.0005577239207923412
batch 65 loss: 0.0005577529780566692
batch 70 loss: 0.0005576833500526845
batch 75 loss: 0.0005576212075538933
batch 80 loss: 0.0005575348041020334
batch 85 loss: 0.0005577200674451888
batch 90 loss: 0.0005577410338446498
batch 95 loss: 0.0005577052244916559
batch 100 loss: 0.0005576041759923101
batch 105 loss: 0.0005576011841185391
batch 110 loss: 0.0005576897645369172
batch 115 loss: 0.0005577108589932322
batch 120 loss: 0.000557583523914218
batch 125 loss: 0.0005576914409175515
batch 130 loss: 0.0005576760857366025
batch 135 loss: 0.0005576384603045881
batch 140 loss: 0.0005576092633418738
batch 145 loss: 0.0005575704271905124
batch 150 loss: 0.0005578200332820415
batch 155 loss: 0.0005576674710027873
batch 160 loss: 0.0005576573428697884
batch 165 loss: 0.0005577318603172899
batch 170 loss: 0.0005578305921517312
batch 175 loss: 0.0005577794625423848
batch 180 loss: 0.0005577229545451701
batch 185 loss: 0.0005577247589826584
batch 190 loss: 0.0005576587980613112
batch 195 loss: 0.00055755969369784
batch 200 loss: 0.0005577671690843999
batch 205 loss: 0.0005576366675086319
batch 210 loss: 0.0005576657596975565
batch 215 loss: 0.0005577670759521425
batch 220 loss: 0.0005576394847594202
batch 225 loss: 0.000557744293473661
batch 230 loss: 0.0005576218361966311
batch 235 loss: 0.0005577200441621244
batch 240 loss: 0.0005576421273872257
Training Loss: 0.0005576848821268261
Validation Loss: 0.0005576971287761505
Epoch 15:
batch 5 loss: 0.0005577811389230192
batch 10 loss: 0.0005576287047006189
batch 15 loss: 0.0005576663417741657
batch 20 loss: 0.0005577115574851632
batch 25 loss: 0.00055773212807253
batch 30 loss: 0.0005576054565608502
batch 35 loss: 0.0005575970630161464
batch 40 loss: 0.0005576649331487715
batch 45 loss: 0.0005576245835982263
batch 50 loss: 0.0005576653406023979
batch 55 loss: 0.0005578020354732871
batch 60 loss: 0.0005576811265200377
batch 65 loss: 0.0005577849922701716
batch 70 loss: 0.0005576330702751874
batch 75 loss: 0.0005577217321842909
batch 80 loss: 0.0005576294381171465
batch 85 loss: 0.0005576447118073701
batch 90 loss: 0.0005576956784352661
batch 95 loss: 0.0005576947354711592
batch 100 loss: 0.0005576764116995037
batch 105 loss: 0.0005576611030846835
batch 110 loss: 0.0005576754687353968
batch 115 loss: 0.0005576274474151432
batch 120 loss: 0.0005577368079684675
batch 125 loss: 0.0005576881230808794
batch 130 loss: 0.0005577063537202775
batch 135 loss: 0.0005577065399847925
batch 140 loss: 0.0005576779833063484
batch 145 loss: 0.0005576802417635917
batch 150 loss: 0.0005577593110501766
batch 155 loss: 0.0005577300791628659
batch 160 loss: 0.0005577154108323157
batch 165 loss: 0.0005578011274337769
batch 170 loss: 0.0005577569711022079
batch 175 loss: 0.0005575693561695516
batch 180 loss: 0.0005576086696237326
batch 185 loss: 0.0005576630472205579
batch 190 loss: 0.0005577090778388083
batch 195 loss: 0.0005577800329774618
batch 200 loss: 0.0005576434079557658
batch 205 loss: 0.0005577083211392164
batch 210 loss: 0.0005576197407208383
batch 215 loss: 0.0005575902294367551
batch 220 loss: 0.0005576741299591958
batch 225 loss: 0.0005576456664130091
batch 230 loss: 0.0005576655967161059
batch 235 loss: 0.0005577542935498059
batch 240 loss: 0.0005576512892730534
Training Loss: 0.0005576843126618769
Validation Loss: 0.000557693357889851
Epoch 16:
batch 5 loss: 0.0005576944095082581
batch 10 loss: 0.0005577942472882568
batch 15 loss: 0.0005576808354817331
batch 20 loss: 0.0005576926749199628
batch 25 loss: 0.0005576973664574326
batch 30 loss: 0.0005576915573328733
batch 35 loss: 0.0005576814408414065
batch 40 loss: 0.0005578399053774774
batch 45 loss: 0.000557751371525228
batch 50 loss: 0.0005576100200414657
batch 55 loss: 0.0005577241536229849
batch 60 loss: 0.0005577453062869609
batch 65 loss: 0.0005577122326940298
batch 70 loss: 0.0005576661671511829
batch 75 loss: 0.0005577310686931014
batch 80 loss: 0.0005575313698500395
batch 85 loss: 0.0005577030126005412
batch 90 loss: 0.0005575995310209692
batch 95 loss: 0.0005575829418376088
batch 100 loss: 0.0005576207069680095
batch 105 loss: 0.00055775111541152
batch 110 loss: 0.0005582672893069685
batch 115 loss: 0.0005577169242314995
batch 120 loss: 0.0005577109288424254
batch 125 loss: 0.000557649799156934
batch 130 loss: 0.0005575667950324714
batch 135 loss: 0.0005577101139351726
batch 140 loss: 0.0005576527677476406
batch 145 loss: 0.0005576565512456
batch 150 loss: 0.0005576074472628534
batch 155 loss: 0.0005576005671173335
batch 160 loss: 0.0005577265750616789
batch 165 loss: 0.0005578452604822814
batch 170 loss: 0.0005576647934503853
batch 175 loss: 0.0005576147814281285
batch 180 loss: 0.000557610101532191
batch 185 loss: 0.0005576745723374188
batch 190 loss: 0.0005576444789767265
batch 195 loss: 0.0005575949442572891
batch 200 loss: 0.0005577010335400701
batch 205 loss: 0.0005576992873102427
batch 210 loss: 0.0005577108706347645
batch 215 loss: 0.0005576974945142865
batch 220 loss: 0.0005577177973464132
batch 225 loss: 0.0005577611969783902
batch 230 loss: 0.0005576728493906557
batch 235 loss: 0.0005576556199230253
batch 240 loss: 0.0005576676223427058
Training Loss: 0.000557693747881179
Validation Loss: 0.0005576953660541525
Epoch 17:
batch 5 loss: 0.0005576994153670967
batch 10 loss: 0.0005577320698648691
batch 15 loss: 0.0005576153402216732
batch 20 loss: 0.0005576334893703461
batch 25 loss: 0.0005577008822001516
batch 30 loss: 0.0005576936295256018
batch 35 loss: 0.0005576590890996158
batch 40 loss: 0.0005576677271164953
batch 45 loss: 0.0005577070289291442
batch 50 loss: 0.0005577034549787641
batch 55 loss: 0.0005577011499553919
batch 60 loss: 0.0005576811148785054
batch 65 loss: 0.0005576950497925282
batch 70 loss: 0.0005575423012487591
batch 75 loss: 0.0005577101255767047
batch 80 loss: 0.0005576114868745208
batch 85 loss: 0.0005576691590249538
batch 90 loss: 0.0005577397183515132
batch 95 loss: 0.000557642662897706
batch 100 loss: 0.000557664199732244
batch 105 loss: 0.0005576827214099466
batch 110 loss: 0.0005576642695814371
batch 115 loss: 0.0005577372503466904
batch 120 loss: 0.0005576673429459333
batch 125 loss: 0.0005577985546551644
batch 130 loss: 0.0005576306255534291
batch 135 loss: 0.0005576502997428178
batch 140 loss: 0.0005576347815804183
batch 145 loss: 0.0005577347939833999
batch 150 loss: 0.000557753979228437
batch 155 loss: 0.0005576532683335244
batch 160 loss: 0.0005576939787715673
batch 165 loss: 0.000557700521312654
batch 170 loss: 0.0005576985306106508
batch 175 loss: 0.0005577326635830104
batch 180 loss: 0.0005575644550845027
batch 185 loss: 0.0005577026749961078
batch 190 loss: 0.0005576865631155669
batch 195 loss: 0.0005576866795308888
batch 200 loss: 0.0005577694973908365
batch 205 loss: 0.0005577133852057159
batch 210 loss: 0.0005576062016189098
batch 215 loss: 0.0005577908945269882
batch 220 loss: 0.0005577720934525132
batch 225 loss: 0.0005577942938543856
batch 230 loss: 0.0005576700903475284
batch 235 loss: 0.0005576206371188164
batch 240 loss: 0.0005575325572863221
Training Loss: 0.0005576835979203072
Validation Loss: 0.0005576934102767458
Epoch 18:
batch 5 loss: 0.0005576096824370325
batch 10 loss: 0.000557698612101376
batch 15 loss: 0.0005575660266913474
batch 20 loss: 0.0005576590425334871
batch 25 loss: 0.0005577044445089996
batch 30 loss: 0.0005575386574491858
batch 35 loss: 0.0005576897994615138
batch 40 loss: 0.0005578367388807237
batch 45 loss: 0.0005575985764153301
batch 50 loss: 0.000557653047144413
batch 55 loss: 0.0005576542695052922
batch 60 loss: 0.0005576775642111897
batch 65 loss: 0.0005576712894253433
batch 70 loss: 0.0005578139331191778
batch 75 loss: 0.0005576445022597909
batch 80 loss: 0.0005576051888056099
batch 85 loss: 0.0005576486000791192
batch 90 loss: 0.0005576564581133425
batch 95 loss: 0.0005577971809543669
batch 100 loss: 0.0005576641880907119
batch 105 loss: 0.0005577136762440205
batch 110 loss: 0.000557775457855314
batch 115 loss: 0.0005576121504418552
batch 120 loss: 0.0005577044910751283
batch 125 loss: 0.0005577143048867584
batch 130 loss: 0.0005576681345701217
batch 135 loss: 0.000557683443184942
batch 140 loss: 0.0005576667026616633
batch 145 loss: 0.0005576213588938117
batch 150 loss: 0.000557660439517349
batch 155 loss: 0.000557727983687073
batch 160 loss: 0.0005576261086389422
batch 165 loss: 0.000557693443261087
batch 170 loss: 0.0005576886120252311
batch 175 loss: 0.000557847076561302
batch 180 loss: 0.0005577153526246548
batch 185 loss: 0.0005578976823017001
batch 190 loss: 0.0005578023847192526
batch 195 loss: 0.00055761857656762
batch 200 loss: 0.0005576716735959053
batch 205 loss: 0.0005576769239269197
batch 210 loss: 0.0005577689851634204
batch 215 loss: 0.0005575493327341974
batch 220 loss: 0.0005576429539360106
batch 225 loss: 0.0005577082396484911
batch 230 loss: 0.000557701860088855
batch 235 loss: 0.0005577005562372505
batch 240 loss: 0.0005577202653512358
Training Loss: 0.0005576867910955722
Validation Loss: 0.0005577133134162675
Epoch 19:
batch 5 loss: 0.000557587412185967
batch 10 loss: 0.0005576469004154206
batch 15 loss: 0.0005576467490755021
batch 20 loss: 0.0005576927913352847
batch 25 loss: 0.0005576869938522577
batch 30 loss: 0.0005575924646109342
batch 35 loss: 0.0005576937925070524
batch 40 loss: 0.0005576733616180718
batch 45 loss: 0.0005576311377808452
batch 50 loss: 0.0005575422779656947
batch 55 loss: 0.0005576728261075913
batch 60 loss: 0.0005577394040301442
batch 65 loss: 0.0005576684372499586
batch 70 loss: 0.0005576895433478058
batch 75 loss: 0.0005576909054070712
batch 80 loss: 0.0005577128613367677
batch 85 loss: 0.0005575721617788077
batch 90 loss: 0.0005576990894041956
batch 95 loss: 0.0005578023963607847
batch 100 loss: 0.0005576715106144547
batch 105 loss: 0.0005577014293521642
batch 110 loss: 0.000557713396847248
batch 115 loss: 0.0005575965158641338
batch 120 loss: 0.0005578124313615262
batch 125 loss: 0.0005577640957199037
batch 130 loss: 0.0005577821168117226
batch 135 loss: 0.000557648204267025
batch 140 loss: 0.0005576477153226734
batch 145 loss: 0.0005576783092692495
batch 150 loss: 0.0005576765164732933
batch 155 loss: 0.0005576660041697323
batch 160 loss: 0.0005577195319347083
batch 165 loss: 0.0005577137926593423
batch 170 loss: 0.0005577530479058624
batch 175 loss: 0.0005576776806265116
batch 180 loss: 0.0005577398929744959
batch 185 loss: 0.0005578557960689068
batch 190 loss: 0.0005576524301432074
batch 195 loss: 0.0005576207535341382
batch 200 loss: 0.0005576685885898769
batch 205 loss: 0.0005576305673457682
batch 210 loss: 0.0005576782627031207
batch 215 loss: 0.0005577153526246548
batch 220 loss: 0.0005575781338848174
batch 225 loss: 0.0005576781928539276
batch 230 loss: 0.0005576535477302969
batch 235 loss: 0.0005577138625085353
batch 240 loss: 0.0005577567033469677
Training Loss: 0.0005576834560391338
Validation Loss: 0.0005576934131871288
Epoch 20:
batch 5 loss: 0.0005576238851062954
batch 10 loss: 0.0005576391704380512
batch 15 loss: 0.000557716388721019
batch 20 loss: 0.0005576810101047159
batch 25 loss: 0.0005577398696914315
batch 30 loss: 0.0005576697294600308
batch 35 loss: 0.0005577173084020615
batch 40 loss: 0.0005575238144956529
batch 45 loss: 0.0005576698109507561
batch 50 loss: 0.0005576836410909891
batch 55 loss: 0.000557645270600915
batch 60 loss: 0.0005576258641667664
batch 65 loss: 0.0005577651550993324
batch 70 loss: 0.0005576386931352318
batch 75 loss: 0.0005576796131208539
batch 80 loss: 0.0005577253527007997
batch 85 loss: 0.0005577775300480425
batch 90 loss: 0.0005576207651756704
batch 95 loss: 0.0005577324656769634
batch 100 loss: 0.0005577336181886494
batch 105 loss: 0.0005576604162342846
batch 110 loss: 0.0005577445961534977
batch 115 loss: 0.0005577378207817674
batch 120 loss: 0.0005576781928539276
batch 125 loss: 0.0005576723255217075
batch 130 loss: 0.0005576423602178693
batch 135 loss: 0.0005576410214416683
batch 140 loss: 0.0005577022209763526
batch 145 loss: 0.0005576652009040117
batch 150 loss: 0.0005576577270403505
batch 155 loss: 0.0005575763061642647
batch 160 loss: 0.0005576641880907119
batch 165 loss: 0.0005575334304012358
batch 170 loss: 0.0005577268777415157
batch 175 loss: 0.0005577530595473945
batch 180 loss: 0.0005576411960646511
batch 185 loss: 0.0005577164120040834
batch 190 loss: 0.0005577079369686544
batch 195 loss: 0.000557639985345304
batch 200 loss: 0.0005576987634412945
batch 205 loss: 0.0005577601841650903
batch 210 loss: 0.0005577150033786893
batch 215 loss: 0.0005577922100201249
batch 220 loss: 0.0005577906267717481
batch 225 loss: 0.0005576940602622926
batch 230 loss: 0.0005575544200837612
batch 235 loss: 0.0005618687835521996
batch 240 loss: 0.0005578168435022235
Training Loss: 0.0005577721067917689
Validation Loss: 0.0005577408087750276
Epoch 21:
batch 5 loss: 0.0005576860858127475
batch 10 loss: 0.0005576620809733867
batch 15 loss: 0.0005576932104304433
batch 20 loss: 0.0005577022209763526
batch 25 loss: 0.0005577722331508994
batch 30 loss: 0.0005577811854891479
batch 35 loss: 0.0005577155970968306
batch 40 loss: 0.0005576342809945345
batch 45 loss: 0.0005576139665208757
batch 50 loss: 0.0005576314637437463
batch 55 loss: 0.0005577375646680594
batch 60 loss: 0.0005575840710662306
batch 65 loss: 0.0005576564464718104
batch 70 loss: 0.0005576006369665265
batch 75 loss: 0.0005577295320108533
batch 80 loss: 0.0005576531519182026
batch 85 loss: 0.0005576227209530771
batch 90 loss: 0.0005577606265433132
batch 95 loss: 0.0005577020230703056
batch 100 loss: 0.0005576332099735737
batch 105 loss: 0.0005576503113843501
batch 110 loss: 0.0005576421623118222
batch 115 loss: 0.0005576657480560243
batch 120 loss: 0.0005576825933530927
batch 125 loss: 0.0005577034200541676
batch 130 loss: 0.0005577027448453009
batch 135 loss: 0.0005576931871473789
batch 140 loss: 0.0005576837342232466
batch 145 loss: 0.0005576923256739974
batch 150 loss: 0.0005576885072514415
batch 155 loss: 0.0005576740601100028
batch 160 loss: 0.0005576377385295928
batch 165 loss: 0.0005576286930590868
batch 170 loss: 0.0005577162490226328
batch 175 loss: 0.000557700451463461
batch 180 loss: 0.0005576466792263091
batch 185 loss: 0.0005577759118750691
batch 190 loss: 0.0005576428724452853
batch 195 loss: 0.0005577089730650186
batch 200 loss: 0.0005576211027801037
batch 205 loss: 0.0005577366449870169
batch 210 loss: 0.0005576830706559121
batch 215 loss: 0.0005577374366112053
batch 220 loss: 0.0005577989504672587
batch 225 loss: 0.0005576862604357302
batch 230 loss: 0.0005577159929089249
batch 235 loss: 0.0005576873081736267
batch 240 loss: 0.0005576279247179628
Training Loss: 0.000557683403409707
Validation Loss: 0.0005576934597532575
Epoch 22:
batch 5 loss: 0.000557776668574661
batch 10 loss: 0.0005574937327764929
batch 15 loss: 0.0005577922216616571
batch 20 loss: 0.0005576985189691186
batch 25 loss: 0.0005577280884608626
batch 30 loss: 0.0005576498457230628
batch 35 loss: 0.0005576103460043669
batch 40 loss: 0.0005577313830144703
batch 45 loss: 0.0005577506381087005
batch 50 loss: 0.0005576568772085011
batch 55 loss: 0.0005576855037361383
batch 60 loss: 0.0005576868308708072
batch 65 loss: 0.0005576786585152149
batch 70 loss: 0.0005576990079134703
batch 75 loss: 0.0005577522329986096
batch 80 loss: 0.0005577435367740691
batch 85 loss: 0.0005577181349508464
batch 90 loss: 0.000557648844551295
batch 95 loss: 0.0005577081348747015
batch 100 loss: 0.0005576682393439114
batch 105 loss: 0.000557773932814598
batch 110 loss: 0.0005577573436312377
batch 115 loss: 0.0005576198338530958
batch 120 loss: 0.0005577072268351912
batch 125 loss: 0.0005576131283305585
batch 130 loss: 0.0005576886120252311
batch 135 loss: 0.0005577182164415717
batch 140 loss: 0.0005577431293204427
batch 145 loss: 0.0005576169234700501
batch 150 loss: 0.0005577169242314995
batch 155 loss: 0.000557721802033484
batch 160 loss: 0.0005576200899668037
batch 165 loss: 0.0005576443043537437
batch 170 loss: 0.000557687389664352
batch 175 loss: 0.0005576695664785802
batch 180 loss: 0.0005576311959885061
batch 185 loss: 0.0005576990079134703
batch 190 loss: 0.0005575513816438616
batch 195 loss: 0.0005577856325544416
batch 200 loss: 0.000557596143335104
batch 205 loss: 0.0005576378898695111
batch 210 loss: 0.0005576665047556162
batch 215 loss: 0.0005577142583206296
batch 220 loss: 0.0005576816387474537
batch 225 loss: 0.000558265671133995
batch 230 loss: 0.0005575895076617599
batch 235 loss: 0.0005576933152042329
batch 240 loss: 0.0005576868075877428
Training Loss: 0.0005576953088166192
Validation Loss: 0.0005576934403507039
Epoch 23:
batch 5 loss: 0.0005577003350481391
batch 10 loss: 0.0005576094030402601
batch 15 loss: 0.00055771607439965
batch 20 loss: 0.0005575960036367178
batch 25 loss: 0.0005577317322604358
batch 30 loss: 0.0005576513707637787
batch 35 loss: 0.0005578591022640467
batch 40 loss: 0.000557718425989151
batch 45 loss: 0.0005576689029112459
batch 50 loss: 0.0005576870171353221
batch 55 loss: 0.0005576872266829014
batch 60 loss: 0.0005577470757998527
batch 65 loss: 0.0005575246294029057
batch 70 loss: 0.0005576101131737233
batch 75 loss: 0.0005577772855758667
batch 80 loss: 0.000557651708368212
batch 85 loss: 0.000557730218861252
batch 90 loss: 0.0005575259798206389
batch 95 loss: 0.000557601556647569
batch 100 loss: 0.00055769911268726
batch 105 loss: 0.0005577057017944753
batch 110 loss: 0.0005577746080234647
batch 115 loss: 0.000557680381461978
batch 120 loss: 0.0005576821626164019
batch 125 loss: 0.0005576851312071085
batch 130 loss: 0.0005578245152719318
batch 135 loss: 0.0005577367963269352
batch 140 loss: 0.0005576404742896557
batch 145 loss: 0.0005576283670961857
batch 150 loss: 0.000557627878151834
batch 155 loss: 0.0005576924770139158
batch 160 loss: 0.0005576015450060367
batch 165 loss: 0.0005578015348874033
batch 170 loss: 0.0005576016730628908
batch 175 loss: 0.0005576353869400918
batch 180 loss: 0.0005577756790444254
batch 185 loss: 0.0005577059695497155
batch 190 loss: 0.0005576701951213181
batch 195 loss: 0.0005576843628659845
batch 200 loss: 0.0005576942698098719
batch 205 loss: 0.0005576089024543762
batch 210 loss: 0.0005576635478064418
batch 215 loss: 0.0005578098236583174
batch 220 loss: 0.0005576916853897273
batch 225 loss: 0.0005576452822424471
batch 230 loss: 0.0005576549330726266
batch 235 loss: 0.0005577227449975908
batch 240 loss: 0.0005576693103648723
Training Loss: 0.0005576835127916031
Validation Loss: 0.000557693811909606
Epoch 24:
batch 5 loss: 0.0005576801835559308
batch 10 loss: 0.0005575940711423754
batch 15 loss: 0.0005577207542955875
batch 20 loss: 0.0005576869589276612
batch 25 loss: 0.0005576537339948117
batch 30 loss: 0.0005576703464612365
batch 35 loss: 0.0005577714298851788
batch 40 loss: 0.0005577165051363408
batch 45 loss: 0.0005576633499003947
batch 50 loss: 0.0005575893213972449
batch 55 loss: 0.0005577030358836054
batch 60 loss: 0.0005577001022174955
batch 65 loss: 0.0005575936054810881
batch 70 loss: 0.0005576751544140279
batch 75 loss: 0.0005576826049946248
batch 80 loss: 0.0005577198928222061
batch 85 loss: 0.0005577560164965689
batch 90 loss: 0.0005576629308052361
batch 95 loss: 0.0005577250150963664
batch 100 loss: 0.0005575620802119374
batch 105 loss: 0.0005577788571827114
batch 110 loss: 0.0005577371222898365
batch 115 loss: 0.0005576013005338609
batch 120 loss: 0.0005576179712079466
batch 125 loss: 0.00055774588836357
batch 130 loss: 0.0005575752002187073
batch 135 loss: 0.0005577062489464879
batch 140 loss: 0.0005575960269197822
batch 145 loss: 0.0005577034782618285
batch 150 loss: 0.0005578278331086039
batch 155 loss: 0.0005577216972596944
batch 160 loss: 0.0005576817784458399
batch 165 loss: 0.0005576733383350074
batch 170 loss: 0.0005576195544563234
batch 175 loss: 0.0005577292875386774
batch 180 loss: 0.0005577014526352286
batch 185 loss: 0.0005576047115027905
batch 190 loss: 0.0005576817085966468
batch 195 loss: 0.0005575817893259227
batch 200 loss: 0.0005575830233283341
batch 205 loss: 0.0005575385992415249
batch 210 loss: 0.0005574781098403037
batch 215 loss: 0.0007508648093789815
batch 220 loss: 0.0005698490189388394
batch 225 loss: 0.000557645782828331
batch 230 loss: 0.0005577191011980176
batch 235 loss: 0.0005577991949394345
batch 240 loss: 0.0005577278672717512
Training Loss: 0.0005619503717753105
Validation Loss: 0.0005576934597532575
Epoch 25:
batch 5 loss: 0.000557634700089693
batch 10 loss: 0.0005578231648541987
batch 15 loss: 0.0005576247465796768
batch 20 loss: 0.000557684013620019
batch 25 loss: 0.0005576777039095759
batch 30 loss: 0.0005577160976827144
batch 35 loss: 0.0005576839903369546
batch 40 loss: 0.0005577096133492887
batch 45 loss: 0.0005576174706220626
batch 50 loss: 0.0005576277733780443
batch 55 loss: 0.0005576392635703087
batch 60 loss: 0.00055767388548702
batch 65 loss: 0.0005576760740950704
batch 70 loss: 0.000557692814618349
batch 75 loss: 0.0005576271913014352
batch 80 loss: 0.0005577484262175858
batch 85 loss: 0.0005576848983764648
batch 90 loss: 0.0005576671333983541
batch 95 loss: 0.0005576459225267172
batch 100 loss: 0.0005577457486651838
batch 105 loss: 0.0005577683332376182
batch 110 loss: 0.0005577358650043606
batch 115 loss: 0.0005576755502261221
batch 120 loss: 0.0005576601601205766
batch 125 loss: 0.0005576314986683428
batch 130 loss: 0.0005576085532084107
batch 135 loss: 0.0005577032454311848
batch 140 loss: 0.0005576725117862225
batch 145 loss: 0.0005577334435656667
batch 150 loss: 0.000557797309011221
batch 155 loss: 0.000557629344984889
batch 160 loss: 0.0005576365743763745
batch 165 loss: 0.0005576219409704208
batch 170 loss: 0.0005576284951530397
batch 175 loss: 0.0005576731404289603
batch 180 loss: 0.000557665922679007
batch 185 loss: 0.0005577545845881105
batch 190 loss: 0.0005576941068284214
batch 195 loss: 0.0005577449686825275
batch 200 loss: 0.000557609077077359
batch 205 loss: 0.0005576290772296488
batch 210 loss: 0.0005577046191319823
batch 215 loss: 0.0005578296724706889
batch 220 loss: 0.000557616085279733
batch 225 loss: 0.0005576781812123954
batch 230 loss: 0.0005576999858021737
batch 235 loss: 0.0005577316274866462
batch 240 loss: 0.0005576692405156791
Training Loss: 0.0005576834114132605
Validation Loss: 0.0005576934267689164
Epoch 26:
batch 5 loss: 0.0005576353752985597
batch 10 loss: 0.000557728367857635
batch 15 loss: 0.0005577013129368424
batch 20 loss: 0.0005577264353632927
batch 25 loss: 0.0005576650495640933
batch 30 loss: 0.0005576188443228603
batch 35 loss: 0.0005576459923759102
batch 40 loss: 0.0005577599047683179
batch 45 loss: 0.0005576776922680438
batch 50 loss: 0.0005576601251959801
batch 55 loss: 0.0005576765397563577
batch 60 loss: 0.0005575967952609062
batch 65 loss: 0.0005575086455792188
batch 70 loss: 0.0005576566676609218
batch 75 loss: 0.0005577646079473197
batch 80 loss: 0.0005577794043347239
batch 85 loss: 0.0005577513948082924
batch 90 loss: 0.0005577413248829544
batch 95 loss: 0.0005577883799560368
batch 100 loss: 0.000557680067140609
batch 105 loss: 0.0005576563184149563
batch 110 loss: 0.0005577348172664642
batch 115 loss: 0.0005576584138907492
batch 120 loss: 0.0005576326278969645
batch 125 loss: 0.000557690067216754
batch 130 loss: 0.0005576407886110247
batch 135 loss: 0.0005577702657319605
batch 140 loss: 0.0005577205331064761
batch 145 loss: 0.0005577120813541114
batch 150 loss: 0.0005577119765803218
batch 155 loss: 0.0005576701601967216
batch 160 loss: 0.0005576607305556536
batch 165 loss: 0.0005577300675213337
batch 170 loss: 0.0005577508127316833
batch 175 loss: 0.0005576848983764648
batch 180 loss: 0.0005576005321927368
batch 185 loss: 0.0005575774470344186
batch 190 loss: 0.0005576585070230066
batch 195 loss: 0.000557659869082272
batch 200 loss: 0.000557693059090525
batch 205 loss: 0.0005576882627792657
batch 210 loss: 0.0005576524534262716
batch 215 loss: 0.0005577105563133955
batch 220 loss: 0.0005577851901762187
batch 225 loss: 0.0005576379946433008
batch 230 loss: 0.0005576212774030864
batch 235 loss: 0.0005576493684202432
batch 240 loss: 0.0005576807423494756
Training Loss: 0.0005576833905555152
Validation Loss: 0.0005576935072895139
Epoch 27:
batch 5 loss: 0.0005578115466050804
batch 10 loss: 0.0005577169358730316
batch 15 loss: 0.0005577375181019306
batch 20 loss: 0.0005578598356805742
batch 25 loss: 0.0005577119765803218
batch 30 loss: 0.0005575903225690127
batch 35 loss: 0.0005577193456701935
batch 40 loss: 0.0005576502298936248
batch 45 loss: 0.0005575587041676044
batch 50 loss: 0.0005575735121965408
batch 55 loss: 0.0005576488678343594
batch 60 loss: 0.000557699566707015
batch 65 loss: 0.0005577552830800415
batch 70 loss: 0.0005576343392021954
batch 75 loss: 0.0005576990311965347
batch 80 loss: 0.0005577110918238759
batch 85 loss: 0.0005575913237407804
batch 90 loss: 0.0005575485061854124
batch 95 loss: 0.0005578103591687977
batch 100 loss: 0.0005575741757638753
batch 105 loss: 0.000557547842618078
batch 110 loss: 0.0005576073541305959
batch 115 loss: 0.0005576591473072767
batch 120 loss: 0.0005577430129051208
batch 125 loss: 0.0005577204981818795
batch 130 loss: 0.0005577879841439426
batch 135 loss: 0.0005577775533311069
batch 140 loss: 0.0005577209522016346
batch 145 loss: 0.0005576207884587347
batch 150 loss: 0.0005577061092481017
batch 155 loss: 0.00055758684175089
batch 160 loss: 0.0005576023133471609
batch 165 loss: 0.0005577485775575042
batch 170 loss: 0.0005577029194682836
batch 175 loss: 0.0005576994037255645
batch 180 loss: 0.0005576409050263465
batch 185 loss: 0.0005576469469815492
batch 190 loss: 0.000557583209592849
batch 195 loss: 0.0005576182738877833
batch 200 loss: 0.00055772919440642
batch 205 loss: 0.0005576575174927711
batch 210 loss: 0.0005577820120379329
batch 215 loss: 0.0005577056785114109
batch 220 loss: 0.0005577232339419424
batch 225 loss: 0.0005577763891778887
batch 230 loss: 0.0005576370633207262
batch 235 loss: 0.0005577989039011299
batch 240 loss: 0.0005576751311309636
Training Loss: 0.0005576835047880498
Validation Loss: 0.0005576934296792994
Epoch 28:
batch 5 loss: 0.0005576940136961638
batch 10 loss: 0.0005576418247073889
batch 15 loss: 0.0005576829658821225
batch 20 loss: 0.0005576380528509617
batch 25 loss: 0.0005576428258791566
batch 30 loss: 0.0005576261435635387
batch 35 loss: 0.0005575569230131805
batch 40 loss: 0.0005577372387051583
batch 45 loss: 0.0005577127332799137
batch 50 loss: 0.000557641324121505
batch 55 loss: 0.0005576081806793809
batch 60 loss: 0.0005577143281698226
batch 65 loss: 0.0005577098461799323
batch 70 loss: 0.0005577458068728447
batch 75 loss: 0.0005577153177000582
batch 80 loss: 0.0005576699622906744
batch 85 loss: 0.0005576413706876338
batch 90 loss: 0.0005576474941335618
batch 95 loss: 0.0005577629315666854
batch 100 loss: 0.0005576762137934566
batch 105 loss: 0.0005577175295911729
batch 110 loss: 0.0005576600902713835
batch 115 loss: 0.0005576126975938677
batch 120 loss: 0.0005577169125899672
batch 125 loss: 0.0005577531759627163
batch 130 loss: 0.000557644700165838
batch 135 loss: 0.0005576185649260879
batch 140 loss: 0.0005577690084464848
batch 145 loss: 0.0005576494033448399
batch 150 loss: 0.0005576785304583609
batch 155 loss: 0.0005576069932430982
batch 160 loss: 0.0005576272960752249
batch 165 loss: 0.0005571945337578654
batch 170 loss: 0.0005542079685255885
batch 175 loss: 0.0005554308416321873
batch 180 loss: 0.0005578093114309013
batch 185 loss: 0.00055765948491171
batch 190 loss: 0.0005576064810156822
batch 195 loss: 0.0005576911964453757
batch 200 loss: 0.0005577307078056037
batch 205 loss: 0.0005577158881351352
batch 210 loss: 0.0005575840477831662
batch 215 loss: 0.0005575958522967995
batch 220 loss: 0.0005576748168095947
batch 225 loss: 0.0005576939205639064
batch 230 loss: 0.0005578178213909268
batch 235 loss: 0.0005577048170380295
batch 240 loss: 0.0005577450967393816
Training Loss: 0.0005575496497234174
Validation Loss: 0.0005576934510221084
Epoch 29:
batch 5 loss: 0.0005576709052547812
batch 10 loss: 0.0005577054573222995
batch 15 loss: 0.0005577545962296426
batch 20 loss: 0.0005576828029006719
batch 25 loss: 0.0005577906151302159
batch 30 loss: 0.0005577135947532952
batch 35 loss: 0.0005576108931563794
batch 40 loss: 0.0005577928153797984
batch 45 loss: 0.0005576871917583048
batch 50 loss: 0.0005575982504524291
batch 55 loss: 0.0005576402647420764
batch 60 loss: 0.0005577259580604732
batch 65 loss: 0.0005575832212343812
batch 70 loss: 0.0005576541530899704
batch 75 loss: 0.0005577387055382133
batch 80 loss: 0.0005575985996983945
batch 85 loss: 0.0005576422205194831
batch 90 loss: 0.0005575992050580681
batch 95 loss: 0.0005575370858423411
batch 100 loss: 0.0005576346302405
batch 105 loss: 0.0005576354684308171
batch 110 loss: 0.0005576716270297766
batch 115 loss: 0.0005576831405051053
batch 120 loss: 0.0005577517906203866
batch 125 loss: 0.0005576714291237294
batch 130 loss: 0.0005577238858677447
batch 135 loss: 0.0005575659452006221
batch 140 loss: 0.0005575517192482948
batch 145 loss: 0.0005577488569542766
batch 150 loss: 0.0005576806841418147
batch 155 loss: 0.0005577403004281223
batch 160 loss: 0.0005578985554166138
batch 165 loss: 0.0005576586234383285
batch 170 loss: 0.000557663629297167
batch 175 loss: 0.0005577898817136883
batch 180 loss: 0.0005576933152042329
batch 185 loss: 0.0005577286705374717
batch 190 loss: 0.0005575743387453258
batch 195 loss: 0.0005577163654379546
batch 200 loss: 0.0005577056435868144
batch 205 loss: 0.000557717599440366
batch 210 loss: 0.0005576787749305368
batch 215 loss: 0.0005577234202064574
batch 220 loss: 0.000557675224263221
batch 225 loss: 0.000557756272610277
batch 230 loss: 0.0005576528259553015
batch 235 loss: 0.0005576824187301099
batch 240 loss: 0.000557701603975147
Training Loss: 0.0005576833995291963
Validation Loss: 0.0005576934500519808
Epoch 30:
batch 5 loss: 0.0005577921401709318
batch 10 loss: 0.0005576677154749632
batch 15 loss: 0.000557778007350862
batch 20 loss: 0.0005576730356551707
batch 25 loss: 0.0005576550378464162
batch 30 loss: 0.000557638774625957
batch 35 loss: 0.0005577193689532578
batch 40 loss: 0.0005576467607170344
batch 45 loss: 0.0005576526396907866
batch 50 loss: 0.0005577120697125793
batch 55 loss: 0.0005577695439569653
batch 60 loss: 0.0005577270174399018
batch 65 loss: 0.0005575798451900483
batch 70 loss: 0.0005577739444561303
batch 75 loss: 0.0005577210919000209
batch 80 loss: 0.0005576571449637413
batch 85 loss: 0.0005576386814936995
batch 90 loss: 0.0005576831637881697
batch 95 loss: 0.000557652406860143
batch 100 loss: 0.0005575787159614265
batch 105 loss: 0.0005577263189479708
batch 110 loss: 0.0005577411968261004
batch 115 loss: 0.0005577724310569465
batch 120 loss: 0.0005576111376285553
batch 125 loss: 0.0005577554111368954
batch 130 loss: 0.0005577254574745894
batch 135 loss: 0.0005578545853495598
batch 140 loss: 0.0005576425115577877
batch 145 loss: 0.0005576116847805679
batch 150 loss: 0.0005577491014264524
batch 155 loss: 0.0005576981930062175
batch 160 loss: 0.0005577279138378799
batch 165 loss: 0.0005576839088462293
batch 170 loss: 0.0005577093572355807
batch 175 loss: 0.0005576726282015443
batch 180 loss: 0.0005575931631028652
batch 185 loss: 0.0005575993447564543
batch 190 loss: 0.0005577257368713617
batch 195 loss: 0.0005576857132837176
batch 200 loss: 0.0005575940362177789
batch 205 loss: 0.0005576046649366617
batch 210 loss: 0.0005576845956966281
batch 215 loss: 0.0005576752359047532
batch 220 loss: 0.0005575511953793466
batch 225 loss: 0.0005576331750489771
batch 230 loss: 0.0005576861323788763
batch 235 loss: 0.0005576777271926403
batch 240 loss: 0.0005576941184699535
Training Loss: 0.0005576834121408562
Validation Loss: 0.0005576934005754689
Epoch 31:
batch 5 loss: 0.0005576558643952012
batch 10 loss: 0.0005576406023465097
batch 15 loss: 0.0005577151430770754
batch 20 loss: 0.0005577251547947526
batch 25 loss: 0.0005578925833106041
batch 30 loss: 0.0005576743860729039
batch 35 loss: 0.0005576084367930889
batch 40 loss: 0.0005576799158006907
batch 45 loss: 0.0005577243398874998
batch 50 loss: 0.0005576012539677322
batch 55 loss: 0.0005576555733568967
batch 60 loss: 0.0005577732692472637
batch 65 loss: 0.0005576756549999118
batch 70 loss: 0.0005575644900090992
batch 75 loss: 0.0005577134783379733
batch 80 loss: 0.0005576694617047906
batch 85 loss: 0.0005577114294283092
batch 90 loss: 0.0005577694159001112
batch 95 loss: 0.0005576565163210034
batch 100 loss: 0.0005576746654696763
batch 105 loss: 0.0005576229421421885
batch 110 loss: 0.0005577007890678942
batch 115 loss: 0.000557661592029035
batch 120 loss: 0.0005576421623118222
batch 125 loss: 0.0005577033618465066
batch 130 loss: 0.0005576466792263091
batch 135 loss: 0.000557724735699594
batch 140 loss: 0.0005577088450081646
batch 145 loss: 0.000557691790163517
batch 150 loss: 0.0005577183561399579
batch 155 loss: 0.0005576413357630372
batch 160 loss: 0.0005577062140218914
batch 165 loss: 0.0005577032687142491
batch 170 loss: 0.0005577567033469677
batch 175 loss: 0.0005577137926593423
batch 180 loss: 0.0005576455034315587
batch 185 loss: 0.0005577017436735332
batch 190 loss: 0.0005576038500294089
batch 195 loss: 0.0005576138035394251
batch 200 loss: 0.0005575566086918115
batch 205 loss: 0.0005577260279096663
batch 210 loss: 0.0005576640134677291
batch 215 loss: 0.0005576658179052175
batch 220 loss: 0.0005578582524321974
batch 225 loss: 0.0005576929193921387
batch 230 loss: 0.0005576983676292002
batch 235 loss: 0.0005575962597504259
batch 240 loss: 0.0005576558876782655
Training Loss: 0.0005576834012269198
Validation Loss: 0.0005576934151273841
Epoch 32:
batch 5 loss: 0.0005576918949373067
batch 10 loss: 0.0005577223026193678
batch 15 loss: 0.000557768833823502
batch 20 loss: 0.0005577218835242093
batch 25 loss: 0.0005576530005782842
batch 30 loss: 0.0005575743969529867
batch 35 loss: 0.000557609717361629
batch 40 loss: 0.0005576425231993198
batch 45 loss: 0.0005576649797149003
batch 50 loss: 0.0005576544324867427
batch 55 loss: 0.0005576444556936622
batch 60 loss: 0.0005575969815254211
batch 65 loss: 0.0005576841533184052
batch 70 loss: 0.0005575919640250504
batch 75 loss: 0.0005577029893174768
batch 80 loss: 0.0005577857606112957
batch 85 loss: 0.0005577389267273248
batch 90 loss: 0.000557785842102021
batch 95 loss: 0.0005576548865064979
batch 100 loss: 0.0005576715688221156
batch 105 loss: 0.000557592639233917
batch 110 loss: 0.0005576798226684332
batch 115 loss: 0.0005576429306529462
batch 120 loss: 0.0005577823962084949
batch 125 loss: 0.0005578423268161714
batch 130 loss: 0.0005576162948273122
batch 135 loss: 0.0005576330120675265
batch 140 loss: 0.0005576813593506813
batch 145 loss: 0.0005576767609454692
batch 150 loss: 0.0005577076459303498
batch 155 loss: 0.0005577285541221499
batch 160 loss: 0.0005577128380537033
batch 165 loss: 0.00055764279095456
batch 170 loss: 0.0005576641415245831
batch 175 loss: 0.0005577208939939737
batch 180 loss: 0.0005576270632445812
batch 185 loss: 0.0005576478550210595
batch 190 loss: 0.0005578199285082519
batch 195 loss: 0.0005576552706770599
batch 200 loss: 0.0005575829884037376
batch 205 loss: 0.000557660881895572
batch 210 loss: 0.0005577237694524229
batch 215 loss: 0.0005576651310548186
batch 220 loss: 0.0005576258874498308
batch 225 loss: 0.0005576624418608845
batch 230 loss: 0.0005577676580287516
batch 235 loss: 0.000557724165264517
batch 240 loss: 0.0005577555624768138
Training Loss: 0.0005576834271778352
Validation Loss: 0.0005576934180377672
Epoch 33:
batch 5 loss: 0.0005577167146839201
batch 10 loss: 0.0005577398813329637
batch 15 loss: 0.0005577314295805991
batch 20 loss: 0.0005576681112870574
batch 25 loss: 0.0005577068543061614
batch 30 loss: 0.0005577003699727357
batch 35 loss: 0.0005576178198680281
batch 40 loss: 0.0005576974363066256
batch 45 loss: 0.0005577775067649781
batch 50 loss: 0.0005577680538408458
batch 55 loss: 0.0005576750845648348
batch 60 loss: 0.0005576748633757233
batch 65 loss: 0.0005576610565185547
batch 70 loss: 0.0005576719180680811
batch 75 loss: 0.0005575831397436559
batch 80 loss: 0.0005578084033913911
batch 85 loss: 0.0005576874129474163
batch 90 loss: 0.0005577650270424783
batch 95 loss: 0.0005576857482083141
batch 100 loss: 0.0005577169009484351
batch 105 loss: 0.0005576870520599186
batch 110 loss: 0.0005576550494879485
batch 115 loss: 0.0005576099152676761
batch 120 loss: 0.0005576596595346928
batch 125 loss: 0.0005577781354077161
batch 130 loss: 0.0005576597177423537
batch 135 loss: 0.0005577324191108346
batch 140 loss: 0.0005577475531026721
batch 145 loss: 0.0005576919647864997
batch 150 loss: 0.0005577336531132459
batch 155 loss: 0.0005577479139901697
batch 160 loss: 0.0005576730938628316
batch 165 loss: 0.0005577625590376556
batch 170 loss: 0.0005576419527642429
batch 175 loss: 0.0005576049210503697
batch 180 loss: 0.0005577066214755178
batch 185 loss: 0.0005577096599154174
batch 190 loss: 0.0005575731629505754
batch 195 loss: 0.0005575960385613143
batch 200 loss: 0.0005576180177740752
batch 205 loss: 0.0005576744792051613
batch 210 loss: 0.0005577630014158786
batch 215 loss: 0.0005576969357207418
batch 220 loss: 0.0005575887043960392
batch 225 loss: 0.0005576413357630372
batch 230 loss: 0.000557601940818131
batch 235 loss: 0.0005575273302383721
batch 240 loss: 0.0005576674942858517
Training Loss: 0.0005576834169914946
Validation Loss: 0.0005576933957248306
Epoch 34:
batch 5 loss: 0.0005577480187639594
batch 10 loss: 0.0005576312309131026
batch 15 loss: 0.0005576547235250473
batch 20 loss: 0.0005576368886977435
batch 25 loss: 0.0005576645373366774
batch 30 loss: 0.0005576935596764088
batch 35 loss: 0.0005576335359364748
batch 40 loss: 0.0005576754454523325
batch 45 loss: 0.0005577103118412196
batch 50 loss: 0.0005575987743213773
batch 55 loss: 0.0005577322212047875
batch 60 loss: 0.0005576290888711811
batch 65 loss: 0.0005577794625423848
batch 70 loss: 0.0005576646304689348
batch 75 loss: 0.0005576009163632989
batch 80 loss: 0.0005577780888415873
batch 85 loss: 0.0005576613126322627
batch 90 loss: 0.0005577280651777982
batch 95 loss: 0.0005576194380410016
batch 100 loss: 0.0005577428615652025
batch 105 loss: 0.0005576079827733337
batch 110 loss: 0.000557692814618349
batch 115 loss: 0.000557816622313112
batch 120 loss: 0.0005576878786087037
batch 125 loss: 0.0005577510339207948
batch 130 loss: 0.0005576419411227107
batch 135 loss: 0.0005575709510594606
batch 140 loss: 0.0005576619645580649
batch 145 loss: 0.0005577231873758137
batch 150 loss: 0.0005576879717409611
batch 155 loss: 0.0005575910676270723
batch 160 loss: 0.0005576443974860013
batch 165 loss: 0.0005576214985921979
batch 170 loss: 0.0005577758303843439
batch 175 loss: 0.0005576675641350449
batch 180 loss: 0.0005577077390626073
batch 185 loss: 0.0005576508003287018
batch 190 loss: 0.000557745702099055
batch 195 loss: 0.0005576798226684332
batch 200 loss: 0.0005577455274760723
batch 205 loss: 0.0005577887408435344
batch 210 loss: 0.0005576770636253059
batch 215 loss: 0.0005576908122748136
batch 220 loss: 0.0005576709867455065
batch 225 loss: 0.0005576619878411293
batch 230 loss: 0.0005576967960223556
batch 235 loss: 0.0005576995317824185
batch 240 loss: 0.0005576509749516845
Training Loss: 0.0005576831730043826
Validation Loss: 0.0005576927593210712
Epoch 35:
batch 5 loss: 0.0005576883209869266
batch 10 loss: 0.0005576799041591585
batch 15 loss: 0.0005576201248914004
batch 20 loss: 0.0005577409756369889
batch 25 loss: 0.0005576832452788949
batch 30 loss: 0.0005576395313255488
batch 35 loss: 0.0005577610223554075
batch 40 loss: 0.0005576156079769134
batch 45 loss: 0.0005577500676736235
batch 50 loss: 0.0005576556199230253
batch 55 loss: 0.0005576563416980207
batch 60 loss: 0.0005577579606324435
batch 65 loss: 0.0005577102303504944
batch 70 loss: 0.0005576843162998557
batch 75 loss: 0.0005576955969445408
batch 80 loss: 0.0005576731520704925
batch 85 loss: 0.000557628448586911
batch 90 loss: 0.0005577910575084388
batch 95 loss: 0.0005578297539614141
batch 100 loss: 0.0005576906492933631
batch 105 loss: 0.0005576369469054044
batch 110 loss: 0.0005577016272582114
batch 115 loss: 0.00055768535239622
batch 120 loss: 0.000557708612177521
batch 125 loss: 0.000557604432106018
batch 130 loss: 0.000557679298799485
batch 135 loss: 0.0005576720694079995
batch 140 loss: 0.0005576603696681559
batch 145 loss: 0.0005576472030952573
batch 150 loss: 0.0005576183903031051
batch 155 loss: 0.0005577027564868331
batch 160 loss: 0.0005578012554906308
batch 165 loss: 0.0005577267496846616
batch 170 loss: 0.000557663117069751
batch 175 loss: 0.0005576259340159595
batch 180 loss: 0.0005576365278102457
batch 185 loss: 0.0005576748517341912
batch 190 loss: 0.0005576972267590463
batch 195 loss: 0.0005576587049290538
batch 200 loss: 0.0005576560855843127
batch 205 loss: 0.0005576573661528528
batch 210 loss: 0.0005576748284511268
batch 215 loss: 0.0005576828960329294
batch 220 loss: 0.0005577364470809698
batch 225 loss: 0.0005575780523940921
batch 230 loss: 0.0005576327093876898
batch 235 loss: 0.0005576355732046067
batch 240 loss: 0.0005577707896009087
Training Loss: 0.0005576828771154396
Validation Loss: 0.000557693089164483
Epoch 36:
batch 5 loss: 0.0005575936753302813
batch 10 loss: 0.0005576722789555788
batch 15 loss: 0.0005576734663918614
batch 20 loss: 0.0005577807896770537
batch 25 loss: 0.0005576845374889672
batch 30 loss: 0.0005577667616307735
batch 35 loss: 0.0005576913477852941
batch 40 loss: 0.000557714351452887
batch 45 loss: 0.0005576051073148847
batch 50 loss: 0.000557675096206367
batch 55 loss: 0.0005576449329964817
batch 60 loss: 0.0005576297175139189
batch 65 loss: 0.0005577121744863689
batch 70 loss: 0.0005576705094426871
batch 75 loss: 0.000557619845494628
batch 80 loss: 0.0005578176584094763
batch 85 loss: 0.0005577752483077348
batch 90 loss: 0.0005576309515163303
batch 95 loss: 0.0005575579707510769
batch 100 loss: 0.0005576624069362878
batch 105 loss: 0.0005576809751801192
batch 110 loss: 0.0005576031748205423
batch 115 loss: 0.0005576787050813436
batch 120 loss: 0.0005577088333666325
batch 125 loss: 0.0005577408941462636
batch 130 loss: 0.0005576885072514415
batch 135 loss: 0.0005577780655585229
batch 140 loss: 0.0005576978554017841
batch 145 loss: 0.0005577384727075696
batch 150 loss: 0.000557727855630219
batch 155 loss: 0.0005576658877544105
batch 160 loss: 0.0005575682036578656
batch 165 loss: 0.0005576155497692525
batch 170 loss: 0.0005576899740844965
batch 175 loss: 0.0005577050149440765
batch 180 loss: 0.0005576452822424471
batch 185 loss: 0.0005577930132858455
batch 190 loss: 0.0005576762836426496
batch 195 loss: 0.0005576723022386431
batch 200 loss: 0.0005577322677709162
batch 205 loss: 0.0005577456438913941
batch 210 loss: 0.0005577812204137445
batch 215 loss: 0.0005576078547164798
batch 220 loss: 0.000557580147869885
batch 225 loss: 0.0005576939322054387
batch 230 loss: 0.0005576527677476406
batch 235 loss: 0.0005576765164732933
batch 240 loss: 0.000557675282470882
Training Loss: 0.0005576833190085987
Validation Loss: 0.0005576936304957296
Epoch 37:
batch 5 loss: 0.0005575990420766175
batch 10 loss: 0.0005577774485573172
batch 15 loss: 0.0005576343508437275
batch 20 loss: 0.0005576436989940703
batch 25 loss: 0.0005576134426519275
batch 30 loss: 0.000557741872034967
batch 35 loss: 0.0005577735719271004
batch 40 loss: 0.0005576233728788793
batch 45 loss: 0.0005577411386184395
batch 50 loss: 0.0005575661780312657
batch 55 loss: 0.000557689112611115
batch 60 loss: 0.0005576911149546504
batch 65 loss: 0.0005577039206400514
batch 70 loss: 0.0005577368312515319
batch 75 loss: 0.0005576095660217107
batch 80 loss: 0.0005576904048211872
batch 85 loss: 0.0005576666095294059
batch 90 loss: 0.0005577103816904128
batch 95 loss: 0.0005576988100074231
batch 100 loss: 0.0005576271913014352
batch 105 loss: 0.0005576581694185734
batch 110 loss: 0.0005577045725658536
batch 115 loss: 0.0005576828029006719
batch 120 loss: 0.000557828031014651
batch 125 loss: 0.0005575705203227699
batch 130 loss: 0.0005577460047788918
batch 135 loss: 0.0005575939198024571
batch 140 loss: 0.0005577159463427961
batch 145 loss: 0.0005576725117862225
batch 150 loss: 0.0005578167736530304
batch 155 loss: 0.0005577749107033014
batch 160 loss: 0.0005577569827437401
batch 165 loss: 0.0005576559691689908
batch 170 loss: 0.0005577348289079964
batch 175 loss: 0.000557727343402803
batch 180 loss: 0.000557619018945843
batch 185 loss: 0.0005576185183599591
batch 190 loss: 0.0005576576688326896
batch 195 loss: 0.0005576533731073141
batch 200 loss: 0.0005577771691605449
batch 205 loss: 0.0005576230585575103
batch 210 loss: 0.0005577717907726765
batch 215 loss: 0.0005576105904765427
batch 220 loss: 0.0005576246650889516
batch 225 loss: 0.0005576806492172181
batch 230 loss: 0.0005576275987550616
batch 235 loss: 0.0005576990428380668
batch 240 loss: 0.0005576495779678226
Training Loss: 0.0005576831264382539
Validation Loss: 0.0005576934539324915
Epoch 38:
batch 5 loss: 0.0005576282273977995
batch 10 loss: 0.0005576302064582705
batch 15 loss: 0.0005576639086939394
batch 20 loss: 0.0005576991941779851
batch 25 loss: 0.0005576929892413318
batch 30 loss: 0.0005577918607741594
batch 35 loss: 0.0005576565512456
batch 40 loss: 0.0005576838972046972
batch 45 loss: 0.0005577861680649221
batch 50 loss: 0.0005576073192059994
batch 55 loss: 0.0005575991235673428
batch 60 loss: 0.0005577351432293654
batch 65 loss: 0.000557634059805423
batch 70 loss: 0.0005577406496740878
batch 75 loss: 0.0005577554227784276
batch 80 loss: 0.0005576344206929207
batch 85 loss: 0.00055757180089131
batch 90 loss: 0.0005576699739322066
batch 95 loss: 0.0005577081348747015
batch 100 loss: 0.000557614048011601
batch 105 loss: 0.0005577512085437775
batch 110 loss: 0.0005577245494350791
batch 115 loss: 0.0005576468422077597
batch 120 loss: 0.0005576027208007873
batch 125 loss: 0.0005576828727498651
batch 130 loss: 0.0005576597643084825
batch 135 loss: 0.0005576457013376057
batch 140 loss: 0.0005577016388997435
batch 145 loss: 0.0005576861905865372
batch 150 loss: 0.000557721289806068
batch 155 loss: 0.0005576157942414284
batch 160 loss: 0.0005576267139986157
batch 165 loss: 0.00055765132419765
batch 170 loss: 0.0005577335134148598
batch 175 loss: 0.0005577618139795959
batch 180 loss: 0.000557661522179842
batch 185 loss: 0.0005576512077823282
batch 190 loss: 0.0005577589734457433
batch 195 loss: 0.0005576491239480674
batch 200 loss: 0.0005577049450948834
batch 205 loss: 0.0005576769588515162
batch 210 loss: 0.0005576045135967433
batch 215 loss: 0.0005576847703196109
batch 220 loss: 0.0005577328032813966
batch 225 loss: 0.0005575912189669907
batch 230 loss: 0.0005577628966420889
batch 235 loss: 0.000557753595057875
batch 240 loss: 0.0005575943388976157
Training Loss: 0.0005576779563853052
Validation Loss: 0.0005622308856497208
Epoch 39:
batch 5 loss: 0.0005574257345870137
batch 10 loss: 0.0005573400063440204
batch 15 loss: 0.0005569931818172336
batch 20 loss: 0.0005562576232478022
batch 25 loss: 0.0005549857392907143
batch 30 loss: 0.000552195415366441
batch 35 loss: 0.0005507625173777342
batch 40 loss: 0.0005489162169396878
batch 45 loss: 0.0005472656572237611
batch 50 loss: 0.000545936613343656
batch 55 loss: 0.0005448110052384437
batch 60 loss: 0.0005433285725302995
batch 65 loss: 0.0005425306619144977
batch 70 loss: 0.0005399778136052191
batch 75 loss: 0.0005368054728023708
batch 80 loss: 0.0005352521198801697
batch 85 loss: 0.0005327721242792905
batch 90 loss: 0.0005301074939779937
batch 95 loss: 0.0005288790911436081
batch 100 loss: 0.0005271485191769898
batch 105 loss: 0.0005239563412033022
batch 110 loss: 0.0005236891214735806
batch 115 loss: 0.0005201085237786174
batch 120 loss: 0.0005215583951212466
batch 125 loss: 0.0005188417155295611
batch 130 loss: 0.0005165873561054468
batch 135 loss: 0.0005115422303788364
batch 140 loss: 0.000509057380259037
batch 145 loss: 0.0005058164591901004
batch 150 loss: 0.0005037447204813361
batch 155 loss: 0.0004979788442142308
batch 160 loss: 0.0004999161465093493
batch 165 loss: 0.0004936855169944465
batch 170 loss: 0.00048470929032191635
batch 175 loss: 0.0004779731039889157
batch 180 loss: 0.00047879843623377383
batch 185 loss: 0.0004800309019628912
batch 190 loss: 0.0004729364882223308
batch 195 loss: 0.00046528863022103905
batch 200 loss: 0.0004634687153156847
batch 205 loss: 0.00045309294946491716
batch 210 loss: 0.0004494858323596418
batch 215 loss: 0.0004466367361601442
batch 220 loss: 0.00044200220727361737
batch 225 loss: 0.0004431754758115858
batch 230 loss: 0.0004391104739625007
batch 235 loss: 0.00043447442003525795
batch 240 loss: 0.00043965684017166495
Training Loss: 0.0005084794756839983
Validation Loss: 0.000433292970895612
Epoch 40:
batch 5 loss: 0.0004301810113247484
batch 10 loss: 0.0004238047869876027
batch 15 loss: 0.0004384385363664478
batch 20 loss: 0.0004293543111998588
batch 25 loss: 0.00042822688119485973
batch 30 loss: 0.0004205479461234063
batch 35 loss: 0.00042310572462156415
batch 40 loss: 0.00041748484363779426
batch 45 loss: 0.0004174773523118347
batch 50 loss: 0.0004140785255003721
batch 55 loss: 0.00040554388542659583
batch 60 loss: 0.0004083908977918327
batch 65 loss: 0.00040427655912935736
batch 70 loss: 0.00040040481253527104
batch 75 loss: 0.00039635741850361226
batch 80 loss: 0.00039520764257758854
batch 85 loss: 0.0003914580214768648
batch 90 loss: 0.00039094568928703665
batch 95 loss: 0.00038528895238414406
batch 100 loss: 0.00038754395791329443
batch 105 loss: 0.00038331603282131254
batch 110 loss: 0.00038367355591617525
batch 115 loss: 0.0003789591370150447
batch 120 loss: 0.00037972561549395325
batch 125 loss: 0.00037710114847868683
batch 130 loss: 0.0003738492145203054
batch 135 loss: 0.00037150224670767784
batch 140 loss: 0.0003718760621268302
batch 145 loss: 0.0003689097589813173
batch 150 loss: 0.0003687418415211141
batch 155 loss: 0.00036758774658665063
batch 160 loss: 0.000363353721331805
batch 165 loss: 0.00036522096488624813
batch 170 loss: 0.00036354897893033924
batch 175 loss: 0.0003619844967033714
batch 180 loss: 0.0003620470641180873
batch 185 loss: 0.00035717065329663453
batch 190 loss: 0.00035513268085196614
batch 195 loss: 0.0003544274833984673
batch 200 loss: 0.0003511132672429085
batch 205 loss: 0.00034881983883678914
batch 210 loss: 0.00035252157831564543
batch 215 loss: 0.0003491063893307
batch 220 loss: 0.00034513517748564483
batch 225 loss: 0.00034261992550455034
batch 230 loss: 0.00034239044762216506
batch 235 loss: 0.0003386128810234368
batch 240 loss: 0.00033860169933177533
Training Loss: 0.0003817743200973685
Validation Loss: 0.0003370551077144531
Epoch 41:
batch 5 loss: 0.00033717859769240023
batch 10 loss: 0.00033476618700660764
batch 15 loss: 0.00033608332742005584
batch 20 loss: 0.00032843746012076733
batch 25 loss: 0.00033394909114576877
batch 30 loss: 0.0003292280191089958
batch 35 loss: 0.00033264585654251276
batch 40 loss: 0.00033237640745937825
batch 45 loss: 0.00032821245258674026
batch 50 loss: 0.0003294732596259564
batch 55 loss: 0.00032390980632044373
batch 60 loss: 0.00032639316632412376
batch 65 loss: 0.00032339984900318084
batch 70 loss: 0.0003211686795111746
batch 75 loss: 0.0003242211998440325
batch 80 loss: 0.00031919534085318447
batch 85 loss: 0.0003145486058201641
batch 90 loss: 0.0003125385963357985
batch 95 loss: 0.0003087063261773437
batch 100 loss: 0.000315385905560106
batch 105 loss: 0.0003101852198597044
batch 110 loss: 0.00031374270911328496
batch 115 loss: 0.00031179510406218467
batch 120 loss: 0.00030652518616989253
batch 125 loss: 0.00030179606983438134
batch 130 loss: 0.0003031922737136483
batch 135 loss: 0.000303488242207095
batch 140 loss: 0.00029848560807295145
batch 145 loss: 0.0002986281644552946
batch 150 loss: 0.00029811892309226094
batch 155 loss: 0.0002984893857501447
batch 160 loss: 0.00029889651923440395
batch 165 loss: 0.0002938214398454875
batch 170 loss: 0.000296027451986447
batch 175 loss: 0.00029044036054983737
batch 180 loss: 0.00028593478491529823
batch 185 loss: 0.0002873383811675012
batch 190 loss: 0.00028857290744781493
batch 195 loss: 0.00029164430452510715
batch 200 loss: 0.0002904324268456548
batch 205 loss: 0.00028542111977003515
batch 210 loss: 0.0002900757477618754
batch 215 loss: 0.00028964842786081133
batch 220 loss: 0.00029020403162576257
batch 225 loss: 0.00029179299017414453
batch 230 loss: 0.0002864758367650211
batch 235 loss: 0.0002821478759869933
batch 240 loss: 0.0002811304235365242
Training Loss: 0.00030783895939142286
Validation Loss: 0.0002842461113080693
Epoch 42:
batch 5 loss: 0.00028536919853650033
batch 10 loss: 0.00028207036666572094
batch 15 loss: 0.0002794446365442127
batch 20 loss: 0.0002796681015752256
batch 25 loss: 0.0002829364500939846
batch 30 loss: 0.00028297879034653304
batch 35 loss: 0.0002808004734106362
batch 40 loss: 0.0002818569715600461
batch 45 loss: 0.00028174478211440146
batch 50 loss: 0.00028338368865661324
batch 55 loss: 0.0002784050418995321
batch 60 loss: 0.00027461691061034796
batch 65 loss: 0.00027558606234379113
batch 70 loss: 0.00027796014910563827
batch 75 loss: 0.0002732488908804953
batch 80 loss: 0.00027878721011802553
batch 85 loss: 0.0002770625869743526
batch 90 loss: 0.0002738465613219887
batch 95 loss: 0.0002745326899457723
batch 100 loss: 0.00027535964618436993
batch 105 loss: 0.0002746214624494314
batch 110 loss: 0.000276132149156183
batch 115 loss: 0.0002767291793134063
batch 120 loss: 0.00027551958919502795
batch 125 loss: 0.0002759656053967774
batch 130 loss: 0.00027416032389737666
batch 135 loss: 0.0002709322783630341
batch 140 loss: 0.00027098787832073866
batch 145 loss: 0.00027160230674780903
batch 150 loss: 0.00027546289493329823
batch 155 loss: 0.0002654793672263622
batch 160 loss: 0.0002698970551136881
batch 165 loss: 0.0002678787219338119
batch 170 loss: 0.0002737320493906736
batch 175 loss: 0.00027040702407248317
batch 180 loss: 0.0002692440350074321
batch 185 loss: 0.00026832857402041553
batch 190 loss: 0.0002674187533557415
batch 195 loss: 0.00027299333014525474
batch 200 loss: 0.0002700453333090991
batch 205 loss: 0.00026649910723790525
batch 210 loss: 0.00027059249696321787
batch 215 loss: 0.0002701685938518494
batch 220 loss: 0.0002666683867573738
batch 225 loss: 0.0002629042603075504
batch 230 loss: 0.00026695566484704616
batch 235 loss: 0.0002602310909423977
batch 240 loss: 0.0002586852002423257
Training Loss: 0.00027374795669553954
Validation Loss: 0.00026716631788682813
Epoch 43:
batch 5 loss: 0.0002592066884972155
batch 10 loss: 0.00025631528114899993
batch 15 loss: 0.0002535801497288048
batch 20 loss: 0.0002526421216316521
batch 25 loss: 0.00025283775757998226
batch 30 loss: 0.00025258418172597886
batch 35 loss: 0.0002560746332164854
batch 40 loss: 0.0002563615213148296
batch 45 loss: 0.0002523777715396136
batch 50 loss: 0.0002502459683455527
batch 55 loss: 0.00025274315848946574
batch 60 loss: 0.00025159540236927567
batch 65 loss: 0.00025132535374723376
batch 70 loss: 0.0002483479038346559
batch 75 loss: 0.00024936760310083627
batch 80 loss: 0.0002467298327246681
batch 85 loss: 0.00024548422952648253
batch 90 loss: 0.0002451016276609153
batch 95 loss: 0.00024312818713951857
batch 100 loss: 0.00024402027192991227
batch 105 loss: 0.00023995185329113157
batch 110 loss: 0.00023874880862422286
batch 115 loss: 0.00023718837765045463
batch 120 loss: 0.00023634390090592204
batch 125 loss: 0.00023286435753107072
batch 130 loss: 0.00023166864993982018
batch 135 loss: 0.0002313946228241548
batch 140 loss: 0.00023007890558801591
batch 145 loss: 0.00022839754237793385
batch 150 loss: 0.0002253962738905102
batch 155 loss: 0.00022544289531651885
batch 160 loss: 0.00022564507962670178
batch 165 loss: 0.00022896192967891693
batch 170 loss: 0.00022518154582940042
batch 175 loss: 0.0002227670367574319
batch 180 loss: 0.00021866109746042638
batch 185 loss: 0.0002165198646252975
batch 190 loss: 0.000225165078882128
batch 195 loss: 0.00022746441245544702
batch 200 loss: 0.0002227184100775048
batch 205 loss: 0.00021737457427661866
batch 210 loss: 0.00022471991542261094
batch 215 loss: 0.0002172260166844353
batch 220 loss: 0.0002181845047743991
batch 225 loss: 0.0002126859180862084
batch 230 loss: 0.00020974764775019139
batch 235 loss: 0.00020921217219438404
batch 240 loss: 0.00020843120582867414
Training Loss: 0.0002355877550750544
Validation Loss: 0.00021294104396171558
Epoch 44:
batch 5 loss: 0.00020880660158582032
batch 10 loss: 0.0002092306676786393
batch 15 loss: 0.00020954358915332706
batch 20 loss: 0.00020936519431415945
batch 25 loss: 0.00020737021986860782
batch 30 loss: 0.00020564116712193937
batch 35 loss: 0.00020678953733295203
batch 40 loss: 0.0002048925729468465
batch 45 loss: 0.00020925849676132203
batch 50 loss: 0.00020618869457393884
batch 55 loss: 0.00020260179589968175
batch 60 loss: 0.00020154072262812406
batch 65 loss: 0.0002018461877014488
batch 70 loss: 0.00020425234106369317
batch 75 loss: 0.0002043353917542845
batch 80 loss: 0.0002076361532090232
batch 85 loss: 0.00020331571577116846
batch 90 loss: 0.00020205225446261464
batch 95 loss: 0.00019985420512966812
batch 100 loss: 0.000199836929095909
batch 105 loss: 0.00019766795448958874
batch 110 loss: 0.00019717960094567388
batch 115 loss: 0.00019670811016112567
batch 120 loss: 0.00019688515458256007
batch 125 loss: 0.00019887940725311638
batch 130 loss: 0.000197315463447012
batch 135 loss: 0.00019712771172635256
batch 140 loss: 0.00020090939360670746
batch 145 loss: 0.00019983540405519307
batch 150 loss: 0.00019912593415938318
batch 155 loss: 0.00019760303548537194
batch 160 loss: 0.00019699910480994732
batch 165 loss: 0.00019614285556599498
batch 170 loss: 0.00019598259532358498
batch 175 loss: 0.00019505048694554717
batch 180 loss: 0.00019653499184641988
batch 185 loss: 0.00019559486827347428
batch 190 loss: 0.0001949807832716033
batch 195 loss: 0.0001953348866663873
batch 200 loss: 0.00019531092548277229
batch 205 loss: 0.00019360017613507808
batch 210 loss: 0.00019068169058300555
batch 215 loss: 0.0001927027537021786
batch 220 loss: 0.0001920847309520468
batch 225 loss: 0.00019147597486153246
batch 230 loss: 0.00018952169048134238
batch 235 loss: 0.00018987595685757697
batch 240 loss: 0.00019133322057314218
Training Loss: 0.00019951673542285183
Validation Loss: 0.00019290492249031863
Epoch 45:
batch 5 loss: 0.0001904059201478958
batch 10 loss: 0.00019179528462700545
batch 15 loss: 0.00019649077439680694
batch 20 loss: 0.00019262983405496926
batch 25 loss: 0.0001926832046592608
batch 30 loss: 0.00019099973724223673
batch 35 loss: 0.0001921033632243052
batch 40 loss: 0.00019044475629925728
batch 45 loss: 0.00019059177429880947
batch 50 loss: 0.0001906152960145846
batch 55 loss: 0.00019023825298063456
batch 60 loss: 0.0001896363915875554
batch 65 loss: 0.00018978665175382048
batch 70 loss: 0.0001892229658551514
batch 75 loss: 0.00018937691347673536
batch 80 loss: 0.0001879098330391571
batch 85 loss: 0.00018767845176625997
batch 90 loss: 0.00018833983922377228
batch 95 loss: 0.0001894391345558688
batch 100 loss: 0.00019103732774965464
batch 105 loss: 0.00019064564548898488
batch 110 loss: 0.00018993975827470423
batch 115 loss: 0.00018931841186713426
batch 120 loss: 0.00018831500201486052
batch 125 loss: 0.00018862213182728737
batch 130 loss: 0.00018836446688510477
batch 135 loss: 0.00018936536216642707
batch 140 loss: 0.00018910533981397748
batch 145 loss: 0.0001876143302069977
batch 150 loss: 0.00018683731323108078
batch 155 loss: 0.00018421908607706426
batch 160 loss: 0.0001856194925494492
batch 165 loss: 0.0001841218676418066
batch 170 loss: 0.0001849709980888292
batch 175 loss: 0.0001873493252787739
batch 180 loss: 0.00018764221749734133
batch 185 loss: 0.00018553182890173047
batch 190 loss: 0.0001856245071394369
batch 195 loss: 0.0001841630320996046
batch 200 loss: 0.0001888460828922689
batch 205 loss: 0.00018987019138876348
batch 210 loss: 0.00018844478472601622
batch 215 loss: 0.00018594627908896655
batch 220 loss: 0.0001853901310823858
batch 225 loss: 0.00018346902215853332
batch 230 loss: 0.00018570509273558856
batch 235 loss: 0.0001843664824264124
batch 240 loss: 0.00018363730050623416
Training Loss: 0.00018842648314603138
Validation Loss: 0.00018406651870463974
Epoch 46:
batch 5 loss: 0.00018281413649674506
batch 10 loss: 0.00018094758852384984
batch 15 loss: 0.00018086258496623485
batch 20 loss: 0.000185764255002141
batch 25 loss: 0.00018784396816045045
batch 30 loss: 0.0001871568470960483
batch 35 loss: 0.00018487458291929215
batch 40 loss: 0.00018467010813765227
batch 45 loss: 0.00018492877425160258
batch 50 loss: 0.00018426842289045453
batch 55 loss: 0.00018283043173141777
batch 60 loss: 0.00018382520065642893
batch 65 loss: 0.00018158300954382867
batch 70 loss: 0.00018281597876921297
batch 75 loss: 0.00018183997890446334
batch 80 loss: 0.0001808828266803175
batch 85 loss: 0.00018382007256150245
batch 90 loss: 0.00018256329058203847
batch 95 loss: 0.00018185985391028225
batch 100 loss: 0.00018001914722844958
batch 105 loss: 0.00018085635092575104
batch 110 loss: 0.00018178223690483718
batch 115 loss: 0.0001819640165194869
batch 120 loss: 0.00018208186666015535
batch 125 loss: 0.00018119513406418265
batch 130 loss: 0.00017882844258565456
batch 135 loss: 0.00017945709405466914
batch 140 loss: 0.00018021278374362736
batch 145 loss: 0.00017878700455185026
batch 150 loss: 0.00017701937758829445
batch 155 loss: 0.00017721888143569232
batch 160 loss: 0.00017909371526911855
batch 165 loss: 0.00017882274987641723
batch 170 loss: 0.00018320347298868002
batch 175 loss: 0.00018224957748316228
batch 180 loss: 0.0001802707149181515
batch 185 loss: 0.00018083057657349854
batch 190 loss: 0.00018123867630492896
batch 195 loss: 0.0001775655400706455
batch 200 loss: 0.00017716937290970236
batch 205 loss: 0.0001766182278515771
batch 210 loss: 0.00017640484729781748
batch 215 loss: 0.00017591206706129013
batch 220 loss: 0.00017748478276189417
batch 225 loss: 0.0001768718531820923
batch 230 loss: 0.0001779937738319859
batch 235 loss: 0.00017928609158843756
batch 240 loss: 0.00017783414805307984
Training Loss: 0.00018092550908477278
Validation Loss: 0.00017962168640224264
Epoch 47:
batch 5 loss: 0.00017879266815725713
batch 10 loss: 0.00017850477597676219
batch 15 loss: 0.00017788484110496937
batch 20 loss: 0.00017862414242699742
batch 25 loss: 0.00017668912187218666
batch 30 loss: 0.00017563945439178497
batch 35 loss: 0.00017627431079745293
batch 40 loss: 0.00017586957255844027
batch 45 loss: 0.0001769993075868115
batch 50 loss: 0.00017734650464262813
batch 55 loss: 0.0001777668308932334
batch 60 loss: 0.0001766322646290064
batch 65 loss: 0.00017603363667149097
batch 70 loss: 0.00017557854298502206
batch 75 loss: 0.0001771625829860568
batch 80 loss: 0.00017737857124302536
batch 85 loss: 0.0001763039326760918
batch 90 loss: 0.00017488549929112196
batch 95 loss: 0.00017483246338088066
batch 100 loss: 0.00017608993221074343
batch 105 loss: 0.000175587713601999
batch 110 loss: 0.0001769763504853472
batch 115 loss: 0.00017564433510415257
batch 120 loss: 0.00017562048160471022
batch 125 loss: 0.00017525169241707771
batch 130 loss: 0.00017401253571733833
batch 135 loss: 0.00017618910933379084
batch 140 loss: 0.00017526659648865462
batch 145 loss: 0.00017428846331313252
batch 150 loss: 0.00017332207644358278
batch 155 loss: 0.00017688563675619662
batch 160 loss: 0.00017492566257715225
batch 165 loss: 0.00017746321391314267
batch 170 loss: 0.0001759399223374203
batch 175 loss: 0.00017598661943338812
batch 180 loss: 0.0001738279330311343
batch 185 loss: 0.00017302030755672604
batch 190 loss: 0.0001744124892866239
batch 195 loss: 0.00017517500673420728
batch 200 loss: 0.0001715888298349455
batch 205 loss: 0.0001714613230433315
batch 210 loss: 0.00017288555100094526
batch 215 loss: 0.00017304164357483386
batch 220 loss: 0.00017208343197125942
batch 225 loss: 0.00017167191253975034
batch 230 loss: 0.00017247105133719743
batch 235 loss: 0.00017239661538042127
batch 240 loss: 0.0001735230296617374
Training Loss: 0.00017533767689504505
Validation Loss: 0.00017407522658080172
Epoch 48:
batch 5 loss: 0.0001718666375381872
batch 10 loss: 0.00017273791600018739
batch 15 loss: 0.0001715135993435979
batch 20 loss: 0.00017093335336539894
batch 25 loss: 0.000172698829555884
batch 30 loss: 0.00017126657476183028
batch 35 loss: 0.0001722572691505775
batch 40 loss: 0.00017364023078698666
batch 45 loss: 0.00017414058092981576
batch 50 loss: 0.00017259292653761804
batch 55 loss: 0.0001743987260852009
batch 60 loss: 0.00017279023304581643
batch 65 loss: 0.00017244902555830778
batch 70 loss: 0.00017122424324043096
batch 75 loss: 0.00016973245947156103
batch 80 loss: 0.00016921866917982698
batch 85 loss: 0.0001706686831312254
batch 90 loss: 0.00017132073699031026
batch 95 loss: 0.00016969296266324818
batch 100 loss: 0.00017015293706208469
batch 105 loss: 0.00017107761523220687
batch 110 loss: 0.0001715007092570886
batch 115 loss: 0.000171521125594154
batch 120 loss: 0.00017080717661883683
batch 125 loss: 0.0001712761848466471
batch 130 loss: 0.00016997518250718712
batch 135 loss: 0.00016987022245302797
batch 140 loss: 0.00017110638145823032
batch 145 loss: 0.0001696172636002302
batch 150 loss: 0.00017274553538300098
batch 155 loss: 0.00017232080863323064
batch 160 loss: 0.00017096403462346643
batch 165 loss: 0.00017170692153740675
batch 170 loss: 0.00016981036751531064
batch 175 loss: 0.00016867614176589995
batch 180 loss: 0.00017121892888098954
batch 185 loss: 0.00017276935395784676
batch 190 loss: 0.00017388602427672594
batch 195 loss: 0.00017255585116799922
batch 200 loss: 0.00017011557647492737
batch 205 loss: 0.00016934554150793701
batch 210 loss: 0.00016827938961796462
batch 215 loss: 0.0001696523540886119
batch 220 loss: 0.00017089398752432317
batch 225 loss: 0.00017082187987398355
batch 230 loss: 0.00016955385217443109
batch 235 loss: 0.0001696658116998151
batch 240 loss: 0.00016928959521465004
Training Loss: 0.00017117334191425472
Validation Loss: 0.00016882827015554842
Epoch 49:
batch 5 loss: 0.00016791727975942194
batch 10 loss: 0.00016763968160375952
batch 15 loss: 0.00016789757646620273
batch 20 loss: 0.0001684159826254472
batch 25 loss: 0.0001682286965660751
batch 30 loss: 0.0001701257424429059
batch 35 loss: 0.00016785062034614385
batch 40 loss: 0.00016690862830728292
batch 45 loss: 0.00016725933237466962
batch 50 loss: 0.00016764119500294327
batch 55 loss: 0.00016649789176881313
batch 60 loss: 0.00016671768680680542
batch 65 loss: 0.0001665363961365074
batch 70 loss: 0.00016841541801113636
batch 75 loss: 0.00016860752657521516
batch 80 loss: 0.00016751359216868878
batch 85 loss: 0.00016861137410160155
batch 90 loss: 0.00016775833501014858
batch 95 loss: 0.00016771339578554035
batch 100 loss: 0.00016883061616681516
batch 105 loss: 0.00016979335632640868
batch 110 loss: 0.0001677756430581212
batch 115 loss: 0.00016869748942553996
batch 120 loss: 0.00016817288124002517
batch 125 loss: 0.0001680491113802418
batch 130 loss: 0.0001670178462518379
batch 135 loss: 0.00016828435473144055
batch 140 loss: 0.0001668709854129702
batch 145 loss: 0.00016639199457131327
batch 150 loss: 0.00016671046323608608
batch 155 loss: 0.000169117990299128
batch 160 loss: 0.00016816678689792752
batch 165 loss: 0.0001692575140623376
batch 170 loss: 0.0001701275381492451
batch 175 loss: 0.00016961412620730698
batch 180 loss: 0.0001690769335255027
batch 185 loss: 0.00016796982672531157
batch 190 loss: 0.00016699632105883212
batch 195 loss: 0.00017012137395795436
batch 200 loss: 0.0001680952263996005
batch 205 loss: 0.00016757656412664802
batch 210 loss: 0.00016856142901815473
batch 215 loss: 0.00016815407725516706
batch 220 loss: 0.00016694532241672276
batch 225 loss: 0.0001671672915108502
batch 230 loss: 0.00016651171899866312
batch 235 loss: 0.00016543428355362267
batch 240 loss: 0.00016582050011493266
Training Loss: 0.00016790762329037533
Validation Loss: 0.0002279900625580922
Epoch 50:
batch 5 loss: 0.00016445434303022922
batch 10 loss: 0.00016510950226802378
batch 15 loss: 0.00016547013365197927
batch 20 loss: 0.00016850409447215496
batch 25 loss: 0.00016667913587298244
batch 30 loss: 0.00016417126171290873
batch 35 loss: 0.0001647928904276341
batch 40 loss: 0.00016925567761063575
batch 45 loss: 0.00016899003530852498
batch 50 loss: 0.00016621134418528526
batch 55 loss: 0.00016619376256130636
batch 60 loss: 0.00016497911128681152
batch 65 loss: 0.0001649947604164481
batch 70 loss: 0.000164947408484295
batch 75 loss: 0.00016676954983267932
batch 80 loss: 0.00016406119393650443
batch 85 loss: 0.00016314771492034196
batch 90 loss: 0.00016528181149624288
batch 95 loss: 0.00016550336440559476
batch 100 loss: 0.0001644621865125373
batch 105 loss: 0.00016828646475914867
batch 110 loss: 0.0001681874506175518
batch 115 loss: 0.00016634679050184787
batch 120 loss: 0.0001658291817875579
batch 125 loss: 0.00016671534685883672
batch 130 loss: 0.00016644886636640877
batch 135 loss: 0.000165342865511775
batch 140 loss: 0.00016542223456781358
batch 145 loss: 0.00017127672326751054
batch 150 loss: 0.00017007642600219697
batch 155 loss: 0.00016893285210244357
batch 160 loss: 0.00016780855075921864
batch 165 loss: 0.00016495098825544118
batch 170 loss: 0.000163992220768705
batch 175 loss: 0.00016276456299237906
batch 180 loss: 0.00016430210089311004
batch 185 loss: 0.00016483365907333792
batch 190 loss: 0.00016474085568916053
batch 195 loss: 0.00016433630953542887
batch 200 loss: 0.00016486248059663922
batch 205 loss: 0.0001643563009565696
batch 210 loss: 0.0001652428531087935
batch 215 loss: 0.00016377893334720283
batch 220 loss: 0.00016552182205487043
batch 225 loss: 0.00016493175353389234
batch 230 loss: 0.00016399019805248826
batch 235 loss: 0.00016316013934556395
batch 240 loss: 0.00016354756662622093
Training Loss: 0.00016570762042344238
Validation Loss: 0.00016435343907990804
Epoch 51:
batch 5 loss: 0.00016431896074209362
batch 10 loss: 0.00016388125368393958
batch 15 loss: 0.00016586334095336497
batch 20 loss: 0.0001683171052718535
batch 25 loss: 0.00016603987605776638
batch 30 loss: 0.00016515867318958044
batch 35 loss: 0.00016358588763978332
batch 40 loss: 0.00016423740598838776
batch 45 loss: 0.00016364062612410634
batch 50 loss: 0.00016387995274271815
batch 55 loss: 0.00016175125783775003
batch 60 loss: 0.00016216841759160162
batch 65 loss: 0.0001620814553461969
batch 70 loss: 0.0001609025988727808
batch 75 loss: 0.00016310778446495532
batch 80 loss: 0.00016303972515743225
batch 85 loss: 0.00016214291972573846
batch 90 loss: 0.00016356908308807762
batch 95 loss: 0.00016456469020340592
batch 100 loss: 0.00016229155589826406
batch 105 loss: 0.00016148120921570807
batch 110 loss: 0.00016200027021113784
batch 115 loss: 0.00016154895711224527
batch 120 loss: 0.00016106053080875427
batch 125 loss: 0.00016084001399576663
batch 130 loss: 0.00016187953006010502
batch 135 loss: 0.00016185648273676635
batch 140 loss: 0.00016118833445943892
batch 145 loss: 0.00016199529345612974
batch 150 loss: 0.00016061957576312124
batch 155 loss: 0.0001594385103089735
batch 160 loss: 0.00016318419075105338
batch 165 loss: 0.00016282878641504794
batch 170 loss: 0.00016210636240430177
batch 175 loss: 0.00016148131107911466
batch 180 loss: 0.00016201894031837584
batch 185 loss: 0.00015919009456411004
batch 190 loss: 0.0001601924479473382
batch 195 loss: 0.00016248574247583746
batch 200 loss: 0.0001606643054401502
batch 205 loss: 0.00016053198196459563
batch 210 loss: 0.0001608289632713422
batch 215 loss: 0.00016391203680541367
batch 220 loss: 0.00016444117063656448
batch 225 loss: 0.00016276799142360686
batch 230 loss: 0.00016181901737581937
batch 235 loss: 0.00016186725115403534
batch 240 loss: 0.00016177516663447024
Training Loss: 0.00016251139665352337
Validation Loss: 0.00016069446986269516
Epoch 52:
batch 5 loss: 0.00016398504958488048
batch 10 loss: 0.00016184225678443908
batch 15 loss: 0.00016101011133287101
batch 20 loss: 0.0001600417832378298
batch 25 loss: 0.00015986245416570454
batch 30 loss: 0.00015889343048911542
batch 35 loss: 0.00015882356674410403
batch 40 loss: 0.0001593756634974852
batch 45 loss: 0.0001603133190656081
batch 50 loss: 0.00016521349898539482
batch 55 loss: 0.0001615435059648007
batch 60 loss: 0.00016226799052674324
batch 65 loss: 0.00015987755032256245
batch 70 loss: 0.00016051857091952114
batch 75 loss: 0.00015879754500929265
batch 80 loss: 0.00015864570159465074
batch 85 loss: 0.00015770679165143519
batch 90 loss: 0.00015886865730863065
batch 95 loss: 0.00015877284458838404
batch 100 loss: 0.00016102673253044487
batch 105 loss: 0.00016477089084219188
batch 110 loss: 0.00016322259325534104
batch 115 loss: 0.00016196026699617506
batch 120 loss: 0.00016247560852207245
batch 125 loss: 0.0001604577759280801
batch 130 loss: 0.0001597692462382838
batch 135 loss: 0.00016019544564187526
batch 140 loss: 0.00015991428517736495
batch 145 loss: 0.00016022142954170705
batch 150 loss: 0.00016175889468286186
batch 155 loss: 0.00015904616157058626
batch 160 loss: 0.00015847226604819297
batch 165 loss: 0.00015813105565030128
batch 170 loss: 0.00015673143498133867
batch 175 loss: 0.0001575590402353555
batch 180 loss: 0.000157273854711093
batch 185 loss: 0.00015752005274407565
batch 190 loss: 0.00016108195122797043
batch 195 loss: 0.0001608113758265972
batch 200 loss: 0.00016044132935348897
batch 205 loss: 0.00015980789321474732
batch 210 loss: 0.00016152745520230382
batch 215 loss: 0.0001615775196114555
batch 220 loss: 0.00016002452757675202
batch 225 loss: 0.00015737295616418122
batch 230 loss: 0.00015684816462453456
batch 235 loss: 0.00015484533505514265
batch 240 loss: 0.0001561504992423579
Training Loss: 0.00015994492362854848
Validation Loss: 0.00015921209002650965
Epoch 53:
batch 5 loss: 0.0001545903884107247
batch 10 loss: 0.00015245777030941098
batch 15 loss: 0.00015177493041846902
batch 20 loss: 0.0001504660031059757
batch 25 loss: 0.000151123057003133
batch 30 loss: 0.0001506669825175777
batch 35 loss: 0.00015055080875754357
batch 40 loss: 0.00014986953465268016
batch 45 loss: 0.00015142018673941493
batch 50 loss: 0.0001486214401666075
batch 55 loss: 0.00014620957081206142
batch 60 loss: 0.0001474046817747876
batch 65 loss: 0.0001471902767661959
batch 70 loss: 0.00014721971820108593
batch 75 loss: 0.00015686387778259813
batch 80 loss: 0.00015736092173028736
batch 85 loss: 0.00015181349590420723
batch 90 loss: 0.00014910213358234614
batch 95 loss: 0.0001467729452997446
batch 100 loss: 0.00014723311469424515
batch 105 loss: 0.00014473738556262106
batch 110 loss: 0.0001439934188965708
batch 115 loss: 0.00014494462811853737
batch 120 loss: 0.00014380576903931795
batch 125 loss: 0.0001431654498446733
batch 130 loss: 0.00014340025663841515
batch 135 loss: 0.0001426387607352808
batch 140 loss: 0.0001435755635611713
batch 145 loss: 0.00014187268388923257
batch 150 loss: 0.0001394107093801722
batch 155 loss: 0.00013826856156811117
batch 160 loss: 0.00013871208066120743
batch 165 loss: 0.0001393457263475284
batch 170 loss: 0.00013829704548697918
batch 175 loss: 0.00013965752441436052
batch 180 loss: 0.00013942484511062503
batch 185 loss: 0.00013860425096936523
batch 190 loss: 0.00014053630002308637
batch 195 loss: 0.0001401263289153576
batch 200 loss: 0.00014116913953330368
batch 205 loss: 0.00013767704076599329
batch 210 loss: 0.0001372386614093557
batch 215 loss: 0.00013648118474520742
batch 220 loss: 0.0001362133596558124
batch 225 loss: 0.00013718318077735603
batch 230 loss: 0.00014135033707134425
batch 235 loss: 0.00013821380271110683
batch 240 loss: 0.00013699401170015336
Training Loss: 0.00014449478846169465
Validation Loss: 0.00014069055371995393
Epoch 54:
batch 5 loss: 0.00013688064354937523
batch 10 loss: 0.00013696037349291145
batch 15 loss: 0.00013533717719838023
batch 20 loss: 0.00013395948917604983
batch 25 loss: 0.00013400727766565979
batch 30 loss: 0.00013518491468857973
batch 35 loss: 0.0001354991807602346
batch 40 loss: 0.00013392282126005738
batch 45 loss: 0.00013499294291250408
batch 50 loss: 0.00013557357597164809
batch 55 loss: 0.00013536472979467363
batch 60 loss: 0.00013437727466225624
batch 65 loss: 0.0001324644748819992
batch 70 loss: 0.00013419189199339598
batch 75 loss: 0.0001336691901087761
batch 80 loss: 0.00013516432663891466
batch 85 loss: 0.00013365888153202832
batch 90 loss: 0.00013408215309027583
batch 95 loss: 0.0001335553970420733
batch 100 loss: 0.00013390733511187136
batch 105 loss: 0.00013319545250851662
batch 110 loss: 0.000133846202515997
batch 115 loss: 0.00013348437787499278
batch 120 loss: 0.00013441320043057204
batch 125 loss: 0.00013299490965437143
batch 130 loss: 0.0001335481065325439
batch 135 loss: 0.00013435480941552668
batch 140 loss: 0.00013428540260065346
batch 145 loss: 0.00013466834498103707
batch 150 loss: 0.00013253074430394918
batch 155 loss: 0.00013222498528193684
batch 160 loss: 0.00013166634016670287
batch 165 loss: 0.00013079400814604015
batch 170 loss: 0.00013183381815906614
batch 175 loss: 0.00013213433267083018
batch 180 loss: 0.0001318467897363007
batch 185 loss: 0.0001318332797382027
batch 190 loss: 0.00013114152534399182
batch 195 loss: 0.00013258965336717665
batch 200 loss: 0.00013146826822776347
batch 205 loss: 0.0001311747095314786
batch 210 loss: 0.00013284982996992768
batch 215 loss: 0.00013309360365383328
batch 220 loss: 0.00013447853561956435
batch 225 loss: 0.00013203636917751281
batch 230 loss: 0.00013438887835945935
batch 235 loss: 0.00013257454556878657
batch 240 loss: 0.00013157510838937014
Training Loss: 0.0001335370871553702
Validation Loss: 0.00013111626491687882
Epoch 55:
batch 5 loss: 0.0001307235361309722
batch 10 loss: 0.00013033835566602647
batch 15 loss: 0.00013056765019427984
batch 20 loss: 0.00013032342831138523
batch 25 loss: 0.0001295064022997394
batch 30 loss: 0.0001300832227570936
batch 35 loss: 0.00013194657803978772
batch 40 loss: 0.00013586680870503187
batch 45 loss: 0.00013306873734109104
batch 50 loss: 0.00013166632561478764
batch 55 loss: 0.00013062756625004113
batch 60 loss: 0.00013139430666342378
batch 65 loss: 0.00013177934160921723
batch 70 loss: 0.00013095880567561835
batch 75 loss: 0.00013296233955770732
batch 80 loss: 0.00013073518057353793
batch 85 loss: 0.00013072211004327983
batch 90 loss: 0.00013070304121356457
batch 95 loss: 0.00012848136539105325
batch 100 loss: 0.0001283787947613746
batch 105 loss: 0.0001287959166802466
batch 110 loss: 0.00013366247003432363
batch 115 loss: 0.0001331735140411183
batch 120 loss: 0.0001323655276792124
batch 125 loss: 0.00013120546645950526
batch 130 loss: 0.00013259623374324293
batch 135 loss: 0.0001353273168206215
batch 140 loss: 0.00013196227664593607
batch 145 loss: 0.0001318192866165191
batch 150 loss: 0.0001330903673078865
batch 155 loss: 0.00013394136040005832
batch 160 loss: 0.0001309080544160679
batch 165 loss: 0.00013088129926472903
batch 170 loss: 0.00013030059635639192
batch 175 loss: 0.000130718553555198
batch 180 loss: 0.00013024132640566678
batch 185 loss: 0.00013008898531552404
batch 190 loss: 0.00012918772408738733
batch 195 loss: 0.00012894851679448038
batch 200 loss: 0.00012892319064121692
batch 205 loss: 0.00012778894742950797
batch 210 loss: 0.0001285914797335863
batch 215 loss: 0.00012852760555688293
batch 220 loss: 0.00012862785661127418
batch 225 loss: 0.0001296647940762341
batch 230 loss: 0.00013050939596723766
batch 235 loss: 0.0001286499056732282
batch 240 loss: 0.00012987058726139368
Training Loss: 0.0001308583844244519
Validation Loss: 0.00013035805386607535
Epoch 56:
batch 5 loss: 0.00012876144610345363
batch 10 loss: 0.00012756074720527978
batch 15 loss: 0.0001284109224798158
batch 20 loss: 0.00012736967182718216
batch 25 loss: 0.0001296964648645371
batch 30 loss: 0.0001312189648160711
batch 35 loss: 0.000130196736427024
batch 40 loss: 0.00012955953134223818
batch 45 loss: 0.00012784775462932886
batch 50 loss: 0.0001275241345865652
batch 55 loss: 0.00012676747865043582
batch 60 loss: 0.00012650229618884623
batch 65 loss: 0.00012744063569698484
batch 70 loss: 0.0001262509817024693
batch 75 loss: 0.00012703633983619512
batch 80 loss: 0.0001250018976861611
batch 85 loss: 0.00012696414487436413
batch 90 loss: 0.00012937662249896676
batch 95 loss: 0.00012761083198711276
batch 100 loss: 0.0001281129923881963
batch 105 loss: 0.00012740010570269077
batch 110 loss: 0.000126508908579126
batch 115 loss: 0.0001264257385628298
batch 120 loss: 0.00012688826536759733
batch 125 loss: 0.00012632810685317964
batch 130 loss: 0.00012570055259857326
batch 135 loss: 0.00012583406060002745
batch 140 loss: 0.00012734218616969884
batch 145 loss: 0.0001255861308891326
batch 150 loss: 0.00012586403463501483
batch 155 loss: 0.00012651098077185453
batch 160 loss: 0.0001251995883649215
batch 165 loss: 0.00012530216190498322
batch 170 loss: 0.00012679364590439945
batch 175 loss: 0.00012882835289929062
batch 180 loss: 0.00012795827351510524
batch 185 loss: 0.00012741464015562088
batch 190 loss: 0.00012682011583819985
batch 195 loss: 0.00012588957033585758
batch 200 loss: 0.00012526026112027466
batch 205 loss: 0.0001272305176826194
batch 210 loss: 0.00012506245984695853
batch 215 loss: 0.00012557555455714465
batch 220 loss: 0.0001253383728908375
batch 225 loss: 0.00012757507211063056
batch 230 loss: 0.0001264863385586068
batch 235 loss: 0.00012808587052859366
batch 240 loss: 0.0001302630640566349
Training Loss: 0.000127180906808159
Validation Loss: 0.0001310198075467876
Epoch 57:
batch 5 loss: 0.00012857013789471238
batch 10 loss: 0.00012967176735401154
batch 15 loss: 0.00012775983195751907
batch 20 loss: 0.0001264116319362074
batch 25 loss: 0.00012566443474497645
batch 30 loss: 0.00012449162313714622
batch 35 loss: 0.00012447453918866812
batch 40 loss: 0.00012516151764430106
batch 45 loss: 0.000125311579904519
batch 50 loss: 0.00012642773217521607
batch 55 loss: 0.00012573879794217647
batch 60 loss: 0.00012615153973456473
batch 65 loss: 0.00012484803446568548
batch 70 loss: 0.00012501733435783536
batch 75 loss: 0.0001240003970451653
batch 80 loss: 0.0001282647659536451
batch 85 loss: 0.00012386933813104405
batch 90 loss: 0.0001247372478246689
batch 95 loss: 0.00012591835693456232
batch 100 loss: 0.00012479509168770165
batch 105 loss: 0.0001243121427251026
batch 110 loss: 0.00012369897158350795
batch 115 loss: 0.00012321982067078351
batch 120 loss: 0.00012484117469284682
batch 125 loss: 0.00012412655923981218
batch 130 loss: 0.0001253682712558657
batch 135 loss: 0.00012441379367373885
batch 140 loss: 0.00012495478149503468
batch 145 loss: 0.00012565399229060858
batch 150 loss: 0.000124261507880874
batch 155 loss: 0.0001255301118362695
batch 160 loss: 0.00012884010502602904
batch 165 loss: 0.0001267302839551121
batch 170 loss: 0.00012466734042391182
batch 175 loss: 0.00012390350602800026
batch 180 loss: 0.00012500764860305934
batch 185 loss: 0.00012421523570083082
batch 190 loss: 0.00012295737396925688
batch 195 loss: 0.0001226975757163018
batch 200 loss: 0.00012363298155833037
batch 205 loss: 0.00012689881259575486
batch 210 loss: 0.00012785129074472935
batch 215 loss: 0.00012541130126919597
batch 220 loss: 0.00012364366848487406
batch 225 loss: 0.0001232310081832111
batch 230 loss: 0.00012365864240564405
batch 235 loss: 0.00012280183436814697
batch 240 loss: 0.00012316428619669751
Training Loss: 0.00012514541088724703
Validation Loss: 0.000128404026569721
Epoch 58:
batch 5 loss: 0.0001257088704733178
batch 10 loss: 0.00012392084463499486
batch 15 loss: 0.00012394455261528493
batch 20 loss: 0.00012376417289488016
batch 25 loss: 0.0001224157662363723
batch 30 loss: 0.00012094098783563823
batch 35 loss: 0.00012205107777845115
batch 40 loss: 0.000124088057782501
batch 45 loss: 0.00012171299895271658
batch 50 loss: 0.00012295127380639314
batch 55 loss: 0.0001227495784405619
batch 60 loss: 0.00012279046059120445
batch 65 loss: 0.0001221658429130912
batch 70 loss: 0.0001233749368111603
batch 75 loss: 0.00012206531391711906
batch 80 loss: 0.00012333712948020548
batch 85 loss: 0.00012396393285598605
batch 90 loss: 0.00012241924123372882
batch 95 loss: 0.00012209813721710815
batch 100 loss: 0.0001232173919561319
batch 105 loss: 0.00012204831728013232
batch 110 loss: 0.00012294694315642118
batch 115 loss: 0.00012192588037578389
batch 120 loss: 0.00012181387864984572
batch 125 loss: 0.00012267660204088316
batch 130 loss: 0.00012252448796061798
batch 135 loss: 0.0001221447775606066
batch 140 loss: 0.0001223413259140216
batch 145 loss: 0.00012238563504070045
batch 150 loss: 0.0001232612004969269
batch 155 loss: 0.00012517197756096722
batch 160 loss: 0.00012389703770168125
batch 165 loss: 0.0001245905863470398
batch 170 loss: 0.0001256033341633156
batch 175 loss: 0.00012592501007020475
batch 180 loss: 0.00012396433157846332
batch 185 loss: 0.0001225675019668415
batch 190 loss: 0.00012222700024722144
batch 195 loss: 0.0001222514096298255
batch 200 loss: 0.000122220155026298
batch 205 loss: 0.00012192654830869287
batch 210 loss: 0.0001224406762048602
batch 215 loss: 0.00012210354179842398
batch 220 loss: 0.00012079870502930134
batch 225 loss: 0.00012133408017689362
batch 230 loss: 0.00012276948982616886
batch 235 loss: 0.00012195515737403184
batch 240 loss: 0.00012247341801412402
Training Loss: 0.00012287436620681547
Validation Loss: 0.00012199696878572771
Epoch 59:
batch 5 loss: 0.00012152603012509644
batch 10 loss: 0.00012093745463062077
batch 15 loss: 0.00012171154958195984
batch 20 loss: 0.00012207536055939273
batch 25 loss: 0.00012193938891869038
batch 30 loss: 0.00012081324675818905
batch 35 loss: 0.0001222648483235389
batch 40 loss: 0.0001210624395753257
batch 45 loss: 0.00012160070327809081
batch 50 loss: 0.00012040817528031766
batch 55 loss: 0.00012120730098104104
batch 60 loss: 0.00012080308661097661
batch 65 loss: 0.00012010716600343584
batch 70 loss: 0.00012027218181174249
batch 75 loss: 0.00011936348018934951
batch 80 loss: 0.00011954508227063343
batch 85 loss: 0.00012093709665350616
batch 90 loss: 0.00012039480207022279
batch 95 loss: 0.00012013744708383456
batch 100 loss: 0.00012064007605658845
batch 105 loss: 0.00012217286421218888
batch 110 loss: 0.00012037531414534896
batch 115 loss: 0.00012008308403892443
batch 120 loss: 0.00011994612286798656
batch 125 loss: 0.00012066955969203264
batch 130 loss: 0.00012013814557576552
batch 135 loss: 0.00012065542978234589
batch 140 loss: 0.0001203087842441164
batch 145 loss: 0.000120054317812901
batch 150 loss: 0.00011994417436653749
batch 155 loss: 0.00012152851559221745
batch 160 loss: 0.00011916232760995626
batch 165 loss: 0.00012294681364437564
batch 170 loss: 0.00012313386396272107
batch 175 loss: 0.00012045795010635629
batch 180 loss: 0.00011962143762502819
batch 185 loss: 0.0001198848636704497
batch 190 loss: 0.0001209110050695017
batch 195 loss: 0.00011958372633671387
batch 200 loss: 0.00011986050958512351
batch 205 loss: 0.00012046535412082448
batch 210 loss: 0.0001216767486766912
batch 215 loss: 0.00012202505167806521
batch 220 loss: 0.00012142727646278218
batch 225 loss: 0.00012075566191924736
batch 230 loss: 0.00012301937968004495
batch 235 loss: 0.00012068009527865797
batch 240 loss: 0.00012047013151459396
Training Loss: 0.00012082719637570941
Validation Loss: 0.0001199340250119955
Epoch 60:
batch 5 loss: 0.00012059189903084189
batch 10 loss: 0.00012217556650284678
batch 15 loss: 0.0001204322645207867
batch 20 loss: 0.00011966459569521249
batch 25 loss: 0.00012035728577757255
batch 30 loss: 0.00012037015258101747
batch 35 loss: 0.00012039476132486016
batch 40 loss: 0.00012177656317362562
batch 45 loss: 0.00012012127408524975
batch 50 loss: 0.00012115019490011036
batch 55 loss: 0.00011866062268381939
batch 60 loss: 0.00011831114679807797
batch 65 loss: 0.0001199129517772235
batch 70 loss: 0.00011967691825702787
batch 75 loss: 0.00012019744026474655
batch 80 loss: 0.00011955892550759017
batch 85 loss: 0.00011975591914961114
batch 90 loss: 0.00011938152456423268
batch 95 loss: 0.00012051036028424278
batch 100 loss: 0.00012003209994873031
batch 105 loss: 0.00011881645477842539
batch 110 loss: 0.00011887644795933738
batch 115 loss: 0.00011922968551516532
batch 120 loss: 0.00012238861381774768
batch 125 loss: 0.00012027615739498287
batch 130 loss: 0.00011908823362318798
batch 135 loss: 0.00011958662507822737
batch 140 loss: 0.00011806570837507025
batch 145 loss: 0.00011827023990917951
batch 150 loss: 0.00011844187538372353
batch 155 loss: 0.00011911016335943713
batch 160 loss: 0.00011916334478883073
batch 165 loss: 0.00011923804122488945
batch 170 loss: 0.00011913037160411477
batch 175 loss: 0.00011948339524678886
batch 180 loss: 0.0001184722554171458
batch 185 loss: 0.0001190537543152459
batch 190 loss: 0.00011970364139415323
batch 195 loss: 0.00011943806457566097
batch 200 loss: 0.00011978277761954814
batch 205 loss: 0.00011860370577778667
batch 210 loss: 0.00011881464743055404
batch 215 loss: 0.00011932277702726423
batch 220 loss: 0.00011849553557112813
batch 225 loss: 0.00011931550834560767
batch 230 loss: 0.00011795361497206614
batch 235 loss: 0.00011892947659362108
batch 240 loss: 0.00011849668662762269
Training Loss: 0.0001195537556365404
Validation Loss: 0.00011957092938246206
Epoch 61:
batch 5 loss: 0.00011747586977435276
batch 10 loss: 0.00011845104745589197
batch 15 loss: 0.00011875651398440823
batch 20 loss: 0.00012022745504509657
batch 25 loss: 0.00011841028899652883
batch 30 loss: 0.00011794268357334659
batch 35 loss: 0.00011898786469828337
batch 40 loss: 0.0001180851279059425
batch 45 loss: 0.00011905571154784411
batch 50 loss: 0.00011805309914052487
batch 55 loss: 0.00011822749074781313
batch 60 loss: 0.00011707625817507506
batch 65 loss: 0.00011747911339625716
batch 70 loss: 0.00011740212939912454
batch 75 loss: 0.00011885539861395955
batch 80 loss: 0.00012035843974445015
batch 85 loss: 0.00011885751882800832
batch 90 loss: 0.00011900728422915563
batch 95 loss: 0.00012101806205464528
batch 100 loss: 0.00011946050653932616
batch 105 loss: 0.00011867444700328634
batch 110 loss: 0.0001194410360767506
batch 115 loss: 0.00011795638274634258
batch 120 loss: 0.00011705087672453374
batch 125 loss: 0.00011837063793791458
batch 130 loss: 0.00011941649281652645
batch 135 loss: 0.0001186449997476302
batch 140 loss: 0.00011849364236695691
batch 145 loss: 0.00011972814099863172
batch 150 loss: 0.00011975211091339589
batch 155 loss: 0.00011968975741183385
batch 160 loss: 0.00011973531072726474
batch 165 loss: 0.0001197322373627685
batch 170 loss: 0.00011994265369139612
batch 175 loss: 0.00011895703937625512
batch 180 loss: 0.00011802392837125807
batch 185 loss: 0.0001172655203845352
batch 190 loss: 0.00011711101105902345
batch 195 loss: 0.0001190561320981942
batch 200 loss: 0.00011774962622439489
batch 205 loss: 0.00011740750924218445
batch 210 loss: 0.0001183967964607291
batch 215 loss: 0.00011741996713681147
batch 220 loss: 0.00011701236508088186
batch 225 loss: 0.00011724606156349182
batch 230 loss: 0.00011754997831303626
batch 235 loss: 0.00011725605581887066
batch 240 loss: 0.00011793230514740571
Training Loss: 0.0001185041851385904
Validation Loss: 0.0001193186161496366
Epoch 62:
batch 5 loss: 0.00011772023572120816
batch 10 loss: 0.00011793525918619707
batch 15 loss: 0.00011682596959872171
batch 20 loss: 0.00011760718916775659
batch 25 loss: 0.000117919547483325
batch 30 loss: 0.00011663587938528507
batch 35 loss: 0.00011698924063239247
batch 40 loss: 0.00011741122871171683
batch 45 loss: 0.00011610324872890487
batch 50 loss: 0.00011611829395405949
batch 55 loss: 0.00011617755953921005
batch 60 loss: 0.00011734163563232869
batch 65 loss: 0.00011688483937177807
batch 70 loss: 0.00011782624060288071
batch 75 loss: 0.00011661971802823246
batch 80 loss: 0.00011591873480938375
batch 85 loss: 0.00011700033501256257
batch 90 loss: 0.00011620116711128504
batch 95 loss: 0.00011621797311818227
batch 100 loss: 0.00011772168654715643
batch 105 loss: 0.00011738404427887871
batch 110 loss: 0.00011725354270311072
batch 115 loss: 0.00011663027398753912
batch 120 loss: 0.00011557257821550592
batch 125 loss: 0.00011643936159089208
batch 130 loss: 0.0001170517600257881
batch 135 loss: 0.0001191998046124354
batch 140 loss: 0.00011835465702461078
batch 145 loss: 0.00011724341165972874
batch 150 loss: 0.00011772597063099965
batch 155 loss: 0.00011659601441351696
batch 160 loss: 0.00011764583323383703
batch 165 loss: 0.00011648496292764321
batch 170 loss: 0.00011710374092217535
batch 175 loss: 0.00011782229703385382
batch 180 loss: 0.00011719836038537323
batch 185 loss: 0.00011713066342053935
batch 190 loss: 0.00011791505239671096
batch 195 loss: 0.00011614687100518495
batch 200 loss: 0.00011774032609537243
batch 205 loss: 0.00011715377040673047
batch 210 loss: 0.00011637471179710701
batch 215 loss: 0.00011689620296237991
batch 220 loss: 0.00011743043141905218
batch 225 loss: 0.00011791811120929197
batch 230 loss: 0.00011993100488325581
batch 235 loss: 0.000118383472727146
batch 240 loss: 0.0001182500462164171
Training Loss: 0.00011721152626099259
Validation Loss: 0.00011872109983717868
Epoch 63:
batch 5 loss: 0.00011670599633362144
batch 10 loss: 0.00011751721322070808
batch 15 loss: 0.00011752348946174607
batch 20 loss: 0.00011715009313775226
batch 25 loss: 0.0001164271729066968
batch 30 loss: 0.00011707050143741071
batch 35 loss: 0.00011645463528111576
batch 40 loss: 0.00011596751428442076
batch 45 loss: 0.00011577663535717874
batch 50 loss: 0.00011555316305020825
batch 55 loss: 0.00011630550143308938
batch 60 loss: 0.00011514296202221885
batch 65 loss: 0.00011471380275906995
batch 70 loss: 0.00011593728413572535
batch 75 loss: 0.00011652690591290593
batch 80 loss: 0.00011589867499424144
batch 85 loss: 0.0001156718804850243
batch 90 loss: 0.00011583532468648628
batch 95 loss: 0.00011540938867256045
batch 100 loss: 0.00011573180672712624
batch 105 loss: 0.00011571140785235912
batch 110 loss: 0.00011696771980496123
batch 115 loss: 0.00011928675085073336
batch 120 loss: 0.00012005561438854784
batch 125 loss: 0.00011694030399667099
batch 130 loss: 0.00011693701962940395
batch 135 loss: 0.0001164622968644835
batch 140 loss: 0.00011538989492692054
batch 145 loss: 0.00011637789575615897
batch 150 loss: 0.00011811713775387034
batch 155 loss: 0.00011700180330080912
batch 160 loss: 0.0001168878807220608
batch 165 loss: 0.00011702497577061876
batch 170 loss: 0.00011686913203448057
batch 175 loss: 0.00011563699808903039
batch 180 loss: 0.00011590138165047392
batch 185 loss: 0.00011587420740397648
batch 190 loss: 0.00011570102360565215
batch 195 loss: 0.00011542310676304623
batch 200 loss: 0.00011477387452032417
batch 205 loss: 0.00011500389809953049
batch 210 loss: 0.00011478541564429179
batch 215 loss: 0.00011589785572141409
batch 220 loss: 0.000115819982602261
batch 225 loss: 0.00011515943915583194
batch 230 loss: 0.0001149413117673248
batch 235 loss: 0.00011635308910626919
batch 240 loss: 0.00011651373642962426
Training Loss: 0.00011627364792730078
Validation Loss: 0.00011596203718606072
Epoch 64:
batch 5 loss: 0.00011548654147190973
batch 10 loss: 0.00011570678325369954
batch 15 loss: 0.00011720975453499705
batch 20 loss: 0.0001157644554041326
batch 25 loss: 0.00011584818566916511
batch 30 loss: 0.0001154525889432989
batch 35 loss: 0.0001152625511167571
batch 40 loss: 0.00011490765318740159
batch 45 loss: 0.0001144472524174489
batch 50 loss: 0.00011593206727411598
batch 55 loss: 0.00011539657134562731
batch 60 loss: 0.00011640975426416845
batch 65 loss: 0.00011798799387179315
batch 70 loss: 0.00011639556178124621
batch 75 loss: 0.00011665733036352322
batch 80 loss: 0.00011463236733106896
batch 85 loss: 0.00011529155017342418
batch 90 loss: 0.00011634040274657309
batch 95 loss: 0.00011478942760732025
batch 100 loss: 0.00011508566240081564
batch 105 loss: 0.00011555460660019889
batch 110 loss: 0.00011543885339051485
batch 115 loss: 0.0001151547854533419
batch 120 loss: 0.00011527559981914238
batch 125 loss: 0.00011395566107239575
batch 130 loss: 0.0001143742716521956
batch 135 loss: 0.00011589974601520226
batch 140 loss: 0.00011689007369568572
batch 145 loss: 0.00011543038999661803
batch 150 loss: 0.00011631884990492836
batch 155 loss: 0.00011542498250491918
batch 160 loss: 0.00011585237371036783
batch 165 loss: 0.00011482799163786694
batch 170 loss: 0.00011457993095973507
batch 175 loss: 0.00011494764912640676
batch 180 loss: 0.0001156264916062355
batch 185 loss: 0.00011508505122037604
batch 190 loss: 0.0001143972433055751
batch 195 loss: 0.00011528184259077535
batch 200 loss: 0.00011530745396157727
batch 205 loss: 0.00011449200828792528
batch 210 loss: 0.00011463450500741601
batch 215 loss: 0.00011493672063807025
batch 220 loss: 0.0001150550611782819
batch 225 loss: 0.00011456235515652224
batch 230 loss: 0.00011533730285009369
batch 235 loss: 0.00011434754414949567
batch 240 loss: 0.00011621668236330151
Training Loss: 0.00011542105172945109
Validation Loss: 0.00011575609884554676
Epoch 65:
batch 5 loss: 0.00011556091194506735
batch 10 loss: 0.00011487746814964339
batch 15 loss: 0.00011461139365565032
batch 20 loss: 0.00011440968810347841
batch 25 loss: 0.0001147417293395847
batch 30 loss: 0.00011413025058573111
batch 35 loss: 0.0001140285690780729
batch 40 loss: 0.0001141079337685369
batch 45 loss: 0.000115664501208812
batch 50 loss: 0.00011417733476264403
batch 55 loss: 0.000115502234257292
batch 60 loss: 0.00011536927049746737
batch 65 loss: 0.00011536093079484999
batch 70 loss: 0.0001147370770922862
batch 75 loss: 0.0001144511203165166
batch 80 loss: 0.0001156268292106688
batch 85 loss: 0.00011589110508793964
batch 90 loss: 0.00011454313062131405
batch 95 loss: 0.00011390086147002875
batch 100 loss: 0.00011384120443835855
batch 105 loss: 0.00011420230293879285
batch 110 loss: 0.00011427631397964433
batch 115 loss: 0.00011492032062960788
batch 120 loss: 0.00011355347669450566
batch 125 loss: 0.00011529526673257351
batch 130 loss: 0.00011624679318629205
batch 135 loss: 0.00011481552355689928
batch 140 loss: 0.0001141897329944186
batch 145 loss: 0.00011602868762565777
batch 150 loss: 0.00011425236443756148
batch 155 loss: 0.00011489457974676043
batch 160 loss: 0.00011473392805783078
batch 165 loss: 0.00011506902374094352
batch 170 loss: 0.00011419403745094314
batch 175 loss: 0.00011504274007165805
batch 180 loss: 0.00011511493503348901
batch 185 loss: 0.00011427343561081216
batch 190 loss: 0.00011448337754700333
batch 195 loss: 0.00011432937462814152
batch 200 loss: 0.00011372324515832589
batch 205 loss: 0.00011347432591719553
batch 210 loss: 0.00011348568950779736
batch 215 loss: 0.00011505852598929778
batch 220 loss: 0.000115135962550994
batch 225 loss: 0.0001139711806899868
batch 230 loss: 0.00011411199811846018
batch 235 loss: 0.00011490040342323481
batch 240 loss: 0.00011344097438268364
Training Loss: 0.00011464066801636363
Validation Loss: 0.00011419465869645743
Epoch 66:
batch 5 loss: 0.00011322860955260694
batch 10 loss: 0.000114158567157574
batch 15 loss: 0.00011461221292847767
batch 20 loss: 0.00011450827441876754
batch 25 loss: 0.00011338847980368882
batch 30 loss: 0.00011389249557396397
batch 35 loss: 0.00011443836556281894
batch 40 loss: 0.00011478565720608458
batch 45 loss: 0.00011403074458939954
batch 50 loss: 0.00011377073969924823
batch 55 loss: 0.00011314776202198118
batch 60 loss: 0.00011454274645075201
batch 65 loss: 0.0001139636427978985
batch 70 loss: 0.0001141253174864687
batch 75 loss: 0.00011537648679222911
batch 80 loss: 0.00011717895104084164
batch 85 loss: 0.00011686644284054637
batch 90 loss: 0.00011534900550032034
batch 95 loss: 0.00011515313381096348
batch 100 loss: 0.00011440795205999165
batch 105 loss: 0.00011444032716099173
batch 110 loss: 0.00011550430062925443
batch 115 loss: 0.00011401511001167819
batch 120 loss: 0.00011361781653249637
batch 125 loss: 0.00011437760986154899
batch 130 loss: 0.00011414569889893756
batch 135 loss: 0.00011478513479232789
batch 140 loss: 0.00011587554472498595
batch 145 loss: 0.0001145982532761991
batch 150 loss: 0.00011425039847381412
batch 155 loss: 0.00011478207597974688
batch 160 loss: 0.00011399147770134732
batch 165 loss: 0.00011358517658663914
batch 170 loss: 0.00011404942633816973
batch 175 loss: 0.0001152532800915651
batch 180 loss: 0.00011359189520590007
batch 185 loss: 0.000112977709795814
batch 190 loss: 0.00011300106416456401
batch 195 loss: 0.00011295282893115654
batch 200 loss: 0.0001136343227699399
batch 205 loss: 0.00011411790474085138
batch 210 loss: 0.00011553210788406432
batch 215 loss: 0.00011423355899751187
batch 220 loss: 0.00011324284423608333
batch 225 loss: 0.0001135475205956027
batch 230 loss: 0.00011409937287680805
batch 235 loss: 0.00011412347375880927
batch 240 loss: 0.00011383478704374284
Training Loss: 0.00011431430436156612
Validation Loss: 0.00011565716934759014
Epoch 67:
batch 5 loss: 0.00011343969526933506
batch 10 loss: 0.00011329504777677358
batch 15 loss: 0.00011481263354653492
batch 20 loss: 0.0001152495518908836
batch 25 loss: 0.00011510945914778858
batch 30 loss: 0.00011476889631012455
batch 35 loss: 0.00011454316700110212
batch 40 loss: 0.00011539166152942925
batch 45 loss: 0.00011584861786104739
batch 50 loss: 0.00011432549508754164
batch 55 loss: 0.00011316149029880763
batch 60 loss: 0.00011404107353882865
batch 65 loss: 0.0001134128324338235
batch 70 loss: 0.00011309531691949815
batch 75 loss: 0.0001130968943471089
batch 80 loss: 0.00011291181581327692
batch 85 loss: 0.00011166106414748356
batch 90 loss: 0.00011290123802609742
batch 95 loss: 0.0001128251024056226
batch 100 loss: 0.00011276808654656634
batch 105 loss: 0.00011415163171477616
batch 110 loss: 0.00011476481158751994
batch 115 loss: 0.00011358752963133156
batch 120 loss: 0.00011333420407027006
batch 125 loss: 0.00011270644172327593
batch 130 loss: 0.00011313837167108432
batch 135 loss: 0.00011400342336855828
batch 140 loss: 0.0001145452872151509
batch 145 loss: 0.00011570462229428812
batch 150 loss: 0.00011477841908345
batch 155 loss: 0.00011590585199883208
batch 160 loss: 0.0001140617110650055
batch 165 loss: 0.00011394702305551618
batch 170 loss: 0.00011326340172672645
batch 175 loss: 0.00011339786724420264
batch 180 loss: 0.00011235295387450606
batch 185 loss: 0.00011251311952946708
batch 190 loss: 0.00011312710266793146
batch 195 loss: 0.00011205173213966191
batch 200 loss: 0.00011289088433841243
batch 205 loss: 0.0001126219125580974
batch 210 loss: 0.00011342264187987893
batch 215 loss: 0.0001129358890466392
batch 220 loss: 0.00011266727378824725
batch 225 loss: 0.00011250131065025926
batch 230 loss: 0.00011221788008697331
batch 235 loss: 0.00011145251482957974
batch 240 loss: 0.00011296851735096425
Training Loss: 0.00011357653062683918
Validation Loss: 0.00011433458979202745
Epoch 68:
batch 5 loss: 0.00011312350252410397
batch 10 loss: 0.0001136620543547906
batch 15 loss: 0.00011282354971626773
batch 20 loss: 0.00011161015281686559
batch 25 loss: 0.00011201791203347966
batch 30 loss: 0.00011236426798859611
batch 35 loss: 0.0001127486364566721
batch 40 loss: 0.00011387342965463176
batch 45 loss: 0.00011433015606598929
batch 50 loss: 0.00011461701506050303
batch 55 loss: 0.0001128909716499038
batch 60 loss: 0.00011226946371607482
batch 65 loss: 0.00011219817533856258
batch 70 loss: 0.00011322814680170268
batch 75 loss: 0.00011247235961491242
batch 80 loss: 0.00011302555649308487
batch 85 loss: 0.00011196627019671723
batch 90 loss: 0.00011211536620976403
batch 95 loss: 0.00011289970425423234
batch 100 loss: 0.00011190826626261696
batch 105 loss: 0.00011258385638939217
batch 110 loss: 0.00011239416344324126
batch 115 loss: 0.00011223818437429145
batch 120 loss: 0.00011283626372460276
batch 125 loss: 0.00011322145874146373
batch 130 loss: 0.00011236778227612376
batch 135 loss: 0.0001133988902438432
batch 140 loss: 0.00011318119650240987
batch 145 loss: 0.00011344058002578094
batch 150 loss: 0.0001126628863858059
batch 155 loss: 0.0001115891252993606
batch 160 loss: 0.000113242982479278
batch 165 loss: 0.00011222484754398465
batch 170 loss: 0.00011433662148192525
batch 175 loss: 0.00011429510486777872
batch 180 loss: 0.00011471479228930548
batch 185 loss: 0.00011402304226066917
batch 190 loss: 0.00011307822278467938
batch 195 loss: 0.00011203834728803485
batch 200 loss: 0.00011277223820798099
batch 205 loss: 0.00011217990977456793
batch 210 loss: 0.00011303550563752651
batch 215 loss: 0.00011308788962196558
batch 220 loss: 0.0001114393409807235
batch 225 loss: 0.00011205320915905759
batch 230 loss: 0.00011243185872444883
batch 235 loss: 0.00011412147723603994
batch 240 loss: 0.00011645117483567446
Training Loss: 0.00011294970645394641
Validation Loss: 0.00011439941857436983
Epoch 69:
batch 5 loss: 0.00011540202976902946
batch 10 loss: 0.00011464836716186255
batch 15 loss: 0.00011370293359505012
batch 20 loss: 0.00011386724945623427
batch 25 loss: 0.00011350270215189084
batch 30 loss: 0.00011295253643766046
batch 35 loss: 0.00011343783844495193
batch 40 loss: 0.00011377381160855293
batch 45 loss: 0.00011212105746380985
batch 50 loss: 0.00011208236392121763
batch 55 loss: 0.00011264729837421328
batch 60 loss: 0.0001123921072576195
batch 65 loss: 0.00011250721436226741
batch 70 loss: 0.00011293951392872259
batch 75 loss: 0.00011329448752803729
batch 80 loss: 0.00011393060121918097
batch 85 loss: 0.00011296618904452771
batch 90 loss: 0.00011154974781675264
batch 95 loss: 0.00011256468133069575
batch 100 loss: 0.00011269345413893461
batch 105 loss: 0.00011190420773345977
batch 110 loss: 0.0001129614218370989
batch 115 loss: 0.00011193217360414565
batch 120 loss: 0.00011210478551220149
batch 125 loss: 0.00011295066215097905
batch 130 loss: 0.0001134670659666881
batch 135 loss: 0.00011331134592182935
batch 140 loss: 0.00011302346538286657
batch 145 loss: 0.00011255176796112209
batch 150 loss: 0.0001121918176067993
batch 155 loss: 0.00011271112161921337
batch 160 loss: 0.0001125593451433815
batch 165 loss: 0.00011163093004142865
batch 170 loss: 0.00011143665906274692
batch 175 loss: 0.00011217572609893978
batch 180 loss: 0.00011319880577502772
batch 185 loss: 0.00011230825475649908
batch 190 loss: 0.00011261612962698564
batch 195 loss: 0.00011191716912435367
batch 200 loss: 0.00011270137474639341
batch 205 loss: 0.00011317330499878153
batch 210 loss: 0.00011347056861268356
batch 215 loss: 0.00011214882542844861
batch 220 loss: 0.0001117748353863135
batch 225 loss: 0.00011264007771387697
batch 230 loss: 0.0001123960901168175
batch 235 loss: 0.00011151273502036929
batch 240 loss: 0.00011214742116862907
Training Loss: 0.00011274779735686025
Validation Loss: 0.00011415435813736016
Epoch 70:
batch 5 loss: 0.00011204745969735086
batch 10 loss: 0.00011111227504443378
batch 15 loss: 0.00011188408971065655
batch 20 loss: 0.00011629191430984065
batch 25 loss: 0.00011382094962755218
batch 30 loss: 0.00011277533776592464
batch 35 loss: 0.00011243778280913829
batch 40 loss: 0.00011251577961957082
batch 45 loss: 0.00011239070299779996
batch 50 loss: 0.00011171132209710776
batch 55 loss: 0.00011319985060254112
batch 60 loss: 0.00011142261791974306
batch 65 loss: 0.0001121419612900354
batch 70 loss: 0.00011171489459229633
batch 75 loss: 0.00011268191301496699
batch 80 loss: 0.00011295065050944685
batch 85 loss: 0.00011272440169705079
batch 90 loss: 0.0001123472276958637
batch 95 loss: 0.00011181852896697819
batch 100 loss: 0.00011127582693006843
batch 105 loss: 0.00011108816688647494
batch 110 loss: 0.00011087350721936673
batch 115 loss: 0.00011185970215592534
batch 120 loss: 0.00011220546875847503
batch 125 loss: 0.00011234583216719329
batch 130 loss: 0.00011206727795070038
batch 135 loss: 0.00011228484945604577
batch 140 loss: 0.00011298522149445489
batch 145 loss: 0.00011323515354888514
batch 150 loss: 0.00011288201931165531
batch 155 loss: 0.00011247586662648246
batch 160 loss: 0.00011090451444033534
batch 165 loss: 0.00011306155502097682
batch 170 loss: 0.0001119071283028461
batch 175 loss: 0.00011209650547243655
batch 180 loss: 0.00011225680646020919
batch 185 loss: 0.00011162809823872522
batch 190 loss: 0.00011174892861163244
batch 195 loss: 0.0001120782588259317
batch 200 loss: 0.00011211379460291937
batch 205 loss: 0.0001108466080040671
batch 210 loss: 0.00011100212868768722
batch 215 loss: 0.0001114446422434412
batch 220 loss: 0.0001112439829739742
batch 225 loss: 0.00011148216726724059
batch 230 loss: 0.00011106387828476726
batch 235 loss: 0.00011047044245060533
batch 240 loss: 0.00011112699867226184
Training Loss: 0.00011208427064654339
Validation Loss: 0.00011241356817966637
Epoch 71:
batch 5 loss: 0.00011188257049070671
batch 10 loss: 0.00011145378957735374
batch 15 loss: 0.0001116703380830586
batch 20 loss: 0.00011095427471445874
batch 25 loss: 0.00011168356431880965
batch 30 loss: 0.00011300809856038541
batch 35 loss: 0.00011208032228751108
batch 40 loss: 0.00011169075005454943
batch 45 loss: 0.00011129270278615877
batch 50 loss: 0.0001118571191909723
batch 55 loss: 0.00011047673324355855
batch 60 loss: 0.00011330193665344268
batch 65 loss: 0.00011225355847273022
batch 70 loss: 0.00011144866584800184
batch 75 loss: 0.00011144993477500975
batch 80 loss: 0.00011170992511324584
batch 85 loss: 0.00011139692651340738
batch 90 loss: 0.00011058019445044919
batch 95 loss: 0.00011049235909013077
batch 100 loss: 0.00011137417750433088
batch 105 loss: 0.00011163100716657937
batch 110 loss: 0.00011061511468142271
batch 115 loss: 0.00011096095840912312
batch 120 loss: 0.00011013737239409238
batch 125 loss: 0.00011098036920884625
batch 130 loss: 0.00011106217280030251
batch 135 loss: 0.000110844244773034
batch 140 loss: 0.00011060164542868734
batch 145 loss: 0.00011046744912164286
batch 150 loss: 0.00011023461556760594
batch 155 loss: 0.00011043155391234904
batch 160 loss: 0.00011019945959560573
batch 165 loss: 0.00010936876205960288
batch 170 loss: 0.00011138204718008637
batch 175 loss: 0.00011167716002091765
batch 180 loss: 0.0001111783247324638
batch 185 loss: 0.00011056075745727867
batch 190 loss: 0.00011065869039157405
batch 195 loss: 0.00011254701821599156
batch 200 loss: 0.00011172262602485717
batch 205 loss: 0.00011200342269148677
batch 210 loss: 0.0001106346637243405
batch 215 loss: 0.00011051377659896389
batch 220 loss: 0.00011080601107096299
batch 225 loss: 0.00010916632018052042
batch 230 loss: 0.00011156771797686816
batch 235 loss: 0.00011065262806368992
batch 240 loss: 0.00011062602861784398
Training Loss: 0.00011115187207906274
Validation Loss: 0.00011093673529103399
Epoch 72:
batch 5 loss: 0.00011016942735295743
batch 10 loss: 0.00011077456292696297
batch 15 loss: 0.00011069679749198258
batch 20 loss: 0.00011193005484528839
batch 25 loss: 0.00011150869104312733
batch 30 loss: 0.00011064987484132871
batch 35 loss: 0.00011178947606822476
batch 40 loss: 0.00011079520336352289
batch 45 loss: 0.00011018377408618107
batch 50 loss: 0.00010993060859618709
batch 55 loss: 0.00011111132625956089
batch 60 loss: 0.00011033841874450446
batch 65 loss: 0.00011086756712757051
batch 70 loss: 0.00011075497750425711
batch 75 loss: 0.00010972592135658488
batch 80 loss: 0.00010978408536175266
batch 85 loss: 0.00010980499209836125
batch 90 loss: 0.00011029450106434525
batch 95 loss: 0.00010987062414642423
batch 100 loss: 0.00011096281668869779
batch 105 loss: 0.0001108922777348198
batch 110 loss: 0.00011078151874244213
batch 115 loss: 0.00011043762933695688
batch 120 loss: 0.00011110752529930323
batch 125 loss: 0.00011102137505076825
batch 130 loss: 0.00011061602126574143
batch 135 loss: 0.00010986842826241628
batch 140 loss: 0.00011008793517248705
batch 145 loss: 0.00011006256245309487
batch 150 loss: 0.0001102247610106133
batch 155 loss: 0.00011021943500963972
batch 160 loss: 0.00011058800300816074
batch 165 loss: 0.00011027984583051875
batch 170 loss: 0.00011027915606973693
batch 175 loss: 0.00010917872277786956
batch 180 loss: 0.0001103434871765785
batch 185 loss: 0.00011156232649227605
batch 190 loss: 0.00011091377818956971
batch 195 loss: 0.00010998797952197493
batch 200 loss: 0.00011032492475351318
batch 205 loss: 0.00011051253823097795
batch 210 loss: 0.00010982915264321492
batch 215 loss: 0.00010968659771606326
batch 220 loss: 0.00011091933702118695
batch 225 loss: 0.00011148593039251864
batch 230 loss: 0.00011079491960117594
batch 235 loss: 0.00010994272888638079
batch 240 loss: 0.00011039324890589341
Training Loss: 0.00011050595515674408
Validation Loss: 0.00011166640845961713
Epoch 73:
batch 5 loss: 0.00011044700513593853
batch 10 loss: 0.0001099744695238769
batch 15 loss: 0.00011074075300712138
batch 20 loss: 0.0001099693268770352
batch 25 loss: 0.00011046592844650149
batch 30 loss: 0.00010977693018503487
batch 35 loss: 0.00011075360234826803
batch 40 loss: 0.00011032741313101723
batch 45 loss: 0.00011068839812651276
batch 50 loss: 0.00010984669934259728
batch 55 loss: 0.00010969288559863344
batch 60 loss: 0.0001094220278901048
batch 65 loss: 0.00010991839080816135
batch 70 loss: 0.00010963473905576394
batch 75 loss: 0.00010969079739879817
batch 80 loss: 0.0001085557189071551
batch 85 loss: 0.00010899847402470186
batch 90 loss: 0.00010950436117127538
batch 95 loss: 0.00010952685406664386
batch 100 loss: 0.00011055602954002097
batch 105 loss: 0.00010956977494060993
batch 110 loss: 0.00010989865841111168
batch 115 loss: 0.00011007640277966857
batch 120 loss: 0.00011085794831160456
batch 125 loss: 0.00011128530168207363
batch 130 loss: 0.00011187248601345346
batch 135 loss: 0.00011141683207824827
batch 140 loss: 0.00011095169757027178
batch 145 loss: 0.00010941011569229886
batch 150 loss: 0.00011005179694620893
batch 155 loss: 0.00010968300339300185
batch 160 loss: 0.0001085076160961762
batch 165 loss: 0.0001098155407817103
batch 170 loss: 0.00010980249790009111
batch 175 loss: 0.0001115695689804852
batch 180 loss: 0.00011023661063518375
batch 185 loss: 0.00010957144986605272
batch 190 loss: 0.00010958352504530922
batch 195 loss: 0.00011035426869057118
batch 200 loss: 0.00011059544194722549
batch 205 loss: 0.00010959213832393289
batch 210 loss: 0.0001106348296161741
batch 215 loss: 0.00010990444279741496
batch 220 loss: 0.00010885031661018729
batch 225 loss: 0.0001104289767681621
batch 230 loss: 0.00010961519874399527
batch 235 loss: 0.00010897949105128645
batch 240 loss: 0.00010988904250552878
Training Loss: 0.00011003116205756668
Validation Loss: 0.00011021461020087979
Epoch 74:
batch 5 loss: 0.00010955121833831072
batch 10 loss: 0.00010839241149369627
batch 15 loss: 0.0001088731994968839
batch 20 loss: 0.00010924768284894525
batch 25 loss: 0.00011020246165571734
batch 30 loss: 0.00010947504051728174
batch 35 loss: 0.00010900117340497672
batch 40 loss: 0.00010974455362884327
batch 45 loss: 0.00010917582985712216
batch 50 loss: 0.00010891501879086717
batch 55 loss: 0.00010989861330017448
batch 60 loss: 0.0001082223272533156
batch 65 loss: 0.00010903637594310567
batch 70 loss: 0.00010909267002716661
batch 75 loss: 0.00011030027526430786
batch 80 loss: 0.00010980962833855301
batch 85 loss: 0.000109904381679371
batch 90 loss: 0.00011037477815989405
batch 95 loss: 0.00011064457648899407
batch 100 loss: 0.00010983876563841477
batch 105 loss: 0.00011044398415833711
batch 110 loss: 0.00010963555832859129
batch 115 loss: 0.00011026371212210506
batch 120 loss: 0.00011110252235084772
batch 125 loss: 0.00010951604490401223
batch 130 loss: 0.00010929979180218652
batch 135 loss: 0.00010978880891343579
batch 140 loss: 0.00011029167071683333
batch 145 loss: 0.00011030506138922646
batch 150 loss: 0.00010894477454712615
batch 155 loss: 0.00010991199087584391
batch 160 loss: 0.00010950435535050929
batch 165 loss: 0.00010875327425310388
batch 170 loss: 0.00010837743320735171
batch 175 loss: 0.00011003320687450469
batch 180 loss: 0.00010957375052385032
batch 185 loss: 0.00010969581489916891
batch 190 loss: 0.0001118817410315387
batch 195 loss: 0.00011069138563470914
batch 200 loss: 0.00011101034469902515
batch 205 loss: 0.0001096749329008162
batch 210 loss: 0.00011017864453606307
batch 215 loss: 0.00010925972601398826
batch 220 loss: 0.0001099604822229594
batch 225 loss: 0.00010925016977125779
batch 230 loss: 0.00010860496986424551
batch 235 loss: 0.00010973673488479108
batch 240 loss: 0.00010869115940295159
Training Loss: 0.00010966839642302754
Validation Loss: 0.00011011682217940689
Epoch 75:
batch 5 loss: 0.0001088902892661281
batch 10 loss: 0.00011001576640410349
batch 15 loss: 0.00010901522036874667
batch 20 loss: 0.00010999869409715757
batch 25 loss: 0.00010951909207506106
batch 30 loss: 0.00010980352089973167
batch 35 loss: 0.00010950515134027228
batch 40 loss: 0.00010822993936017155
batch 45 loss: 0.00010868778917938471
batch 50 loss: 0.00010864917712751775
batch 55 loss: 0.00010915923776337877
batch 60 loss: 0.00010939052590401843
batch 65 loss: 0.000109221700404305
batch 70 loss: 0.00010921135835815222
batch 75 loss: 0.00010992178285960109
batch 80 loss: 0.00010939898202195763
batch 85 loss: 0.0001101790796383284
batch 90 loss: 0.00010926578688668087
batch 95 loss: 0.00010917268373304978
batch 100 loss: 0.00010891330312006176
batch 105 loss: 0.00010885083174798638
batch 110 loss: 0.00010923645604634657
batch 115 loss: 0.00010872783459490166
batch 120 loss: 0.00010869825928239152
batch 125 loss: 0.00010847418452613056
batch 130 loss: 0.00010872752318391576
batch 135 loss: 0.00010814304696395993
batch 140 loss: 0.00010847194207599386
batch 145 loss: 0.00010951008007396013
batch 150 loss: 0.0001084475326933898
batch 155 loss: 0.00010850371472770348
batch 160 loss: 0.00010895293089561165
batch 165 loss: 0.00010902416979661212
batch 170 loss: 0.0001087040946003981
batch 175 loss: 0.00011002890241798013
batch 180 loss: 0.00010968723654514179
batch 185 loss: 0.00011032218899345025
batch 190 loss: 0.00011004911648342386
batch 195 loss: 0.00010993710748152807
batch 200 loss: 0.00010906432435149327
batch 205 loss: 0.00010925039678113535
batch 210 loss: 0.00010857650631805882
batch 215 loss: 0.00010944069363176823
batch 220 loss: 0.00010935600876109675
batch 225 loss: 0.00010887921816902236
batch 230 loss: 0.00010853919229703024
batch 235 loss: 0.0001082180955563672
batch 240 loss: 0.00010907068062806501
Training Loss: 0.00010914669480068065
Validation Loss: 0.00010905966325177966
Epoch 76:
batch 5 loss: 0.00010895910090766848
batch 10 loss: 0.00010924592497758567
batch 15 loss: 0.00010888000397244469
batch 20 loss: 0.00010856361623154954
batch 25 loss: 0.00010934421443380416
batch 30 loss: 0.00010929203272098675
batch 35 loss: 0.00010836058208951727
batch 40 loss: 0.00010830008250195533
batch 45 loss: 0.00010930478747468441
batch 50 loss: 0.00010885744704864919
batch 55 loss: 0.00010836232686415315
batch 60 loss: 0.00010957316699204967
batch 65 loss: 0.00010808290535351262
batch 70 loss: 0.00010830300743691623
batch 75 loss: 0.00010892791615333409
batch 80 loss: 0.00010997476638294757
