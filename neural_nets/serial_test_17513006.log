no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
CL_maxPool
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 5000
20240630_135405
Epoch 1:
batch 5 loss: 0.02288923319429159
batch 10 loss: 0.007250963617116213
batch 15 loss: 0.0046371988020837305
batch 20 loss: 0.0034441870171576737
batch 25 loss: 0.002736181952059269
batch 30 loss: 0.002456172090023756
batch 35 loss: 0.0022098063956946133
batch 40 loss: 0.001992038916796446
batch 45 loss: 0.0017741028917953373
batch 50 loss: 0.001625013304874301
batch 55 loss: 0.001536933472380042
batch 60 loss: 0.0014875623397529124
batch 65 loss: 0.0014433856820687651
batch 70 loss: 0.0014280025148764252
batch 75 loss: 0.0013403157936409116
batch 80 loss: 0.0012661721790209412
batch 85 loss: 0.0013437690446153282
batch 90 loss: 0.0012629005592316388
batch 95 loss: 0.0011830827686935663
batch 100 loss: 0.0011686955345794558
batch 105 loss: 0.0010916468687355517
batch 110 loss: 0.001096019078977406
batch 115 loss: 0.0010616096202284097
batch 120 loss: 0.0010427639121189714
batch 125 loss: 0.0010019674431532622
batch 130 loss: 0.0009796456666663288
batch 135 loss: 0.0009145790128968656
batch 140 loss: 0.0008743205806240439
batch 145 loss: 0.0008932067430578172
batch 150 loss: 0.0008565615629777312
batch 155 loss: 0.000999187829438597
batch 160 loss: 0.0012228729901835322
batch 165 loss: 0.0011098557617515326
batch 170 loss: 0.0009709426551125944
batch 175 loss: 0.0009980150614865124
batch 180 loss: 0.001087186811491847
batch 185 loss: 0.001178424130193889
batch 190 loss: 0.0010833884822204708
batch 195 loss: 0.0010101881343871355
batch 200 loss: 0.0009269412490539253
batch 205 loss: 0.0009062447701580822
batch 210 loss: 0.0008609865442849695
batch 215 loss: 0.0008490091306157411
batch 220 loss: 0.000798803789075464
batch 225 loss: 0.0007601528894156217
batch 230 loss: 0.0007487811963073909
batch 235 loss: 0.0007386078243143857
batch 240 loss: 0.0007123840739950537
Training Loss: 0.001901041914243251
Validation Loss: 0.0006828096452712392
Epoch 2:
batch 5 loss: 0.0007041821954771876
batch 10 loss: 0.000697428360581398
batch 15 loss: 0.0006685713771730662
batch 20 loss: 0.0006911718286573887
batch 25 loss: 0.0006747671868652105
batch 30 loss: 0.0006620552507229149
batch 35 loss: 0.0006637611309997738
batch 40 loss: 0.0006542200339026749
batch 45 loss: 0.000643902481533587
batch 50 loss: 0.0006229683640412986
batch 55 loss: 0.000642247125506401
batch 60 loss: 0.0006310570053756237
batch 65 loss: 0.0006337208324111998
batch 70 loss: 0.0006259950692765415
batch 75 loss: 0.0006402933388017118
batch 80 loss: 0.0006482364144176245
batch 85 loss: 0.0006323694484308362
batch 90 loss: 0.0006286245421506465
batch 95 loss: 0.0006922907079569996
batch 100 loss: 0.0007047292427159845
batch 105 loss: 0.0007736481609754264
batch 110 loss: 0.0007837948156520725
batch 115 loss: 0.0007258961210027337
batch 120 loss: 0.0007097991881892085
batch 125 loss: 0.0006817749352194368
batch 130 loss: 0.0006701526581309736
batch 135 loss: 0.000658720766659826
batch 140 loss: 0.000656903104390949
batch 145 loss: 0.0006526704411953688
batch 150 loss: 0.0006391254486516118
batch 155 loss: 0.0006397302495315671
batch 160 loss: 0.0006358139798976481
batch 165 loss: 0.0006228709011338651
batch 170 loss: 0.0006417899741791189
batch 175 loss: 0.0006376862293109298
batch 180 loss: 0.0006374726886861027
batch 185 loss: 0.0006271601188927889
batch 190 loss: 0.0006235172972083092
batch 195 loss: 0.0006196157541126013
batch 200 loss: 0.0006156043498776853
batch 205 loss: 0.0006094638956710696
batch 210 loss: 0.000606429111212492
batch 215 loss: 0.0006043365225195885
batch 220 loss: 0.000594962143804878
batch 225 loss: 0.00059786211932078
batch 230 loss: 0.0006025540642440319
batch 235 loss: 0.00059615463251248
batch 240 loss: 0.000605301046743989
Training Loss: 0.0006506958886651167
Validation Loss: 0.0006350995749623205
Epoch 3:
batch 5 loss: 0.0006284384406171739
batch 10 loss: 0.0006243602256290615
batch 15 loss: 0.0006447662017308176
batch 20 loss: 0.0006308435695245862
batch 25 loss: 0.0006138762691989541
batch 30 loss: 0.000612646306399256
batch 35 loss: 0.0006234709755517543
batch 40 loss: 0.0006145026767626405
batch 45 loss: 0.0006102349259890616
batch 50 loss: 0.0006053940975107253
batch 55 loss: 0.000606191426049918
batch 60 loss: 0.0005987281212583185
batch 65 loss: 0.0005974692874588072
batch 70 loss: 0.0006006647483445704
batch 75 loss: 0.0007151033263653517
batch 80 loss: 0.0006975374184548855
batch 85 loss: 0.0007282506674528122
batch 90 loss: 0.0007020362303592264
batch 95 loss: 0.000672580732498318
batch 100 loss: 0.0006616946426220238
batch 105 loss: 0.0006424135528504849
batch 110 loss: 0.0006291887373663485
batch 115 loss: 0.0006388166686519981
batch 120 loss: 0.0006225014920346438
batch 125 loss: 0.0006250617327168584
batch 130 loss: 0.0006253620726056397
batch 135 loss: 0.000623848952818662
batch 140 loss: 0.0006196609465405345
batch 145 loss: 0.0006199788418598473
batch 150 loss: 0.0006179316435009242
batch 155 loss: 0.0006126615800894797
batch 160 loss: 0.0006101120146922767
batch 165 loss: 0.0005978394416160882
batch 170 loss: 0.000603880756534636
batch 175 loss: 0.0005939509137533605
batch 180 loss: 0.0005922877928242088
batch 185 loss: 0.0006316037499345839
batch 190 loss: 0.0007019629469141364
batch 195 loss: 0.00073660584166646
batch 200 loss: 0.0006949061644263566
batch 205 loss: 0.0006652493961155414
batch 210 loss: 0.0006433660280890762
batch 215 loss: 0.0006421980680897832
batch 220 loss: 0.0006224293960258364
batch 225 loss: 0.0006186376209370792
batch 230 loss: 0.0006115014781244099
batch 235 loss: 0.0006029409589245916
batch 240 loss: 0.0006049259100109339
Training Loss: 0.0006341794789477718
Validation Loss: 0.0005772041496432697
Epoch 4:
batch 5 loss: 0.0005898157134652138
batch 10 loss: 0.0005979270441457629
batch 15 loss: 0.0005907423561438918
batch 20 loss: 0.0005882128607481718
batch 25 loss: 0.0005960200098343194
batch 30 loss: 0.0006921353167854249
batch 35 loss: 0.0007131767342798412
batch 40 loss: 0.0007132059312425554
batch 45 loss: 0.0006573361693881452
batch 50 loss: 0.0006314437603577971
batch 55 loss: 0.0006214680732227861
batch 60 loss: 0.0006264771451242268
batch 65 loss: 0.0006097262492403388
batch 70 loss: 0.0006119927973486483
batch 75 loss: 0.0006015563034452498
batch 80 loss: 0.0006000170134939254
batch 85 loss: 0.0005923134507611394
batch 90 loss: 0.00058794638607651
batch 95 loss: 0.000591892923694104
batch 100 loss: 0.0005884741898626089
batch 105 loss: 0.0005857569864019752
batch 110 loss: 0.0005861058714799583
batch 115 loss: 0.0005835173884406686
batch 120 loss: 0.0005930616171099246
batch 125 loss: 0.0006112373434007168
batch 130 loss: 0.0005930598708800972
batch 135 loss: 0.0005970725789666176
batch 140 loss: 0.0005962448543868959
batch 145 loss: 0.0005938008660450577
batch 150 loss: 0.0005942012532614172
batch 155 loss: 0.0005893979454413056
batch 160 loss: 0.0005937675363384187
batch 165 loss: 0.0005868549807928503
batch 170 loss: 0.0005907962098717689
batch 175 loss: 0.0005839914316311479
batch 180 loss: 0.0005881304852664471
batch 185 loss: 0.0005866010673344135
batch 190 loss: 0.0005797905032522976
batch 195 loss: 0.0005821700440719723
batch 200 loss: 0.0005884939688257873
batch 205 loss: 0.0005811241338960826
batch 210 loss: 0.0005794821190647781
batch 215 loss: 0.0005771080031991005
batch 220 loss: 0.000574840884655714
batch 225 loss: 0.0005727702286094427
batch 230 loss: 0.0005760437226854265
batch 235 loss: 0.0010749649838544429
batch 240 loss: 0.001366441394202411
Training Loss: 0.0006272647646255791
Validation Loss: 0.0087754153025647
Epoch 5:
batch 5 loss: 0.0018457405269145966
batch 10 loss: 0.0017544360365718604
batch 15 loss: 0.001680161920376122
batch 20 loss: 0.0015373122179880739
batch 25 loss: 0.001440479513257742
batch 30 loss: 0.0013864246429875493
batch 35 loss: 0.001360806031152606
batch 40 loss: 0.0012940468732267618
batch 45 loss: 0.001252770354039967
batch 50 loss: 0.0011974709574133157
batch 55 loss: 0.0011907171923667193
batch 60 loss: 0.001149948639795184
batch 65 loss: 0.0011327438754960895
batch 70 loss: 0.0010800610529258848
batch 75 loss: 0.0010721316328272223
batch 80 loss: 0.0010137642035260796
batch 85 loss: 0.0010020601213909686
batch 90 loss: 0.000972114282194525
batch 95 loss: 0.0009117841138504446
batch 100 loss: 0.0008959336555562913
batch 105 loss: 0.0008620371227152646
batch 110 loss: 0.0008448233478702605
batch 115 loss: 0.0008065797737799585
batch 120 loss: 0.0008129834779538215
batch 125 loss: 0.0007803169661201537
batch 130 loss: 0.0007368998718447983
batch 135 loss: 0.0007213653181679547
batch 140 loss: 0.0007150412653572858
batch 145 loss: 0.0006896720035001636
batch 150 loss: 0.0007226804853416979
batch 155 loss: 0.0007502910098992288
batch 160 loss: 0.0007328310515731573
batch 165 loss: 0.0007114734849892557
batch 170 loss: 0.0006942772073671222
batch 175 loss: 0.0007081861840561032
batch 180 loss: 0.0007334816618822515
batch 185 loss: 0.0007272344199009239
batch 190 loss: 0.0007003021310083568
batch 195 loss: 0.0006793101551011205
batch 200 loss: 0.0006588688469491899
batch 205 loss: 0.0006465481012128294
batch 210 loss: 0.0006323357694782317
batch 215 loss: 0.0006329101510345936
batch 220 loss: 0.0006224551354534924
batch 225 loss: 0.0006086249952204526
batch 230 loss: 0.0006171722081489861
batch 235 loss: 0.000632068992126733
batch 240 loss: 0.0006221476593054831
Training Loss: 0.0009369547216920183
Validation Loss: 0.000589066234533675
Epoch 6:
batch 5 loss: 0.0006007786840200424
batch 10 loss: 0.000594866112805903
batch 15 loss: 0.0005915983347222209
batch 20 loss: 0.0006899127969518304
batch 25 loss: 0.0007821085047908128
batch 30 loss: 0.0007755429483950138
batch 35 loss: 0.000724602909758687
batch 40 loss: 0.0006814185646362603
batch 45 loss: 0.0006474891328252852
batch 50 loss: 0.0006208289065398276
batch 55 loss: 0.0006033685058355331
batch 60 loss: 0.0005942365271039308
batch 65 loss: 0.0005946557852439582
batch 70 loss: 0.0005892591550946235
batch 75 loss: 0.0005884736310690641
batch 80 loss: 0.000584622286260128
batch 85 loss: 0.0005818355712108314
batch 90 loss: 0.0005821352242492139
batch 95 loss: 0.000582204561214894
batch 100 loss: 0.000579077540896833
batch 105 loss: 0.0005783928325399756
batch 110 loss: 0.0005777023616246879
batch 115 loss: 0.0005803130799904466
batch 120 loss: 0.0005758683662861585
batch 125 loss: 0.0005774105433374643
batch 130 loss: 0.0005787174217402935
batch 135 loss: 0.0005748277180828154
batch 140 loss: 0.0005746593582443893
batch 145 loss: 0.0005744785419665277
batch 150 loss: 0.0005759665044024586
batch 155 loss: 0.000573955790605396
batch 160 loss: 0.0005785012617707253
batch 165 loss: 0.0005755962338298559
batch 170 loss: 0.0005781976389698685
batch 175 loss: 0.0005716109182685614
batch 180 loss: 0.000574301986489445
batch 185 loss: 0.0005718252621591091
batch 190 loss: 0.0005703406874090433
batch 195 loss: 0.0005731829791329801
batch 200 loss: 0.000630162877496332
batch 205 loss: 0.0007373947184532881
batch 210 loss: 0.0007722668349742889
batch 215 loss: 0.0007722782320342958
batch 220 loss: 0.0007273552706465125
batch 225 loss: 0.0007053481880575418
batch 230 loss: 0.0007237939862534404
batch 235 loss: 0.0007005502586252988
batch 240 loss: 0.0007000428275205195
Training Loss: 0.0006248762159278461
Validation Loss: 0.0006736850307788699
Epoch 7:
batch 5 loss: 0.0006855699233710766
batch 10 loss: 0.0006729205721057952
batch 15 loss: 0.000677064701449126
batch 20 loss: 0.0006643054774031043
batch 25 loss: 0.0006584378541447222
batch 30 loss: 0.0006483510020188987
batch 35 loss: 0.0006434720591641963
batch 40 loss: 0.0006345933419652283
batch 45 loss: 0.0006337223923765123
batch 50 loss: 0.0006279417080804706
batch 55 loss: 0.0006149791064672172
batch 60 loss: 0.0005893546855077148
batch 65 loss: 0.0005830555572174489
batch 70 loss: 0.0005788366077467799
batch 75 loss: 0.0005779293947853148
batch 80 loss: 0.0005793190095573664
batch 85 loss: 0.0005760417203418911
batch 90 loss: 0.0005760192289017141
batch 95 loss: 0.0005749466363340616
batch 100 loss: 0.0005728372489102185
batch 105 loss: 0.0005742599372752011
batch 110 loss: 0.0005738847074098885
batch 115 loss: 0.0005742853158153593
batch 120 loss: 0.000572922732681036
batch 125 loss: 0.0005722894566133618
batch 130 loss: 0.0005706486641429365
batch 135 loss: 0.0005717725842259824
batch 140 loss: 0.0005705449846573174
batch 145 loss: 0.000568025978282094
batch 150 loss: 0.0005688559263944626
batch 155 loss: 0.0005679921829141677
batch 160 loss: 0.0005669633741490543
batch 165 loss: 0.0005681896349415183
batch 170 loss: 0.0005668897647410631
batch 175 loss: 0.0005667839548550546
batch 180 loss: 0.000568049494177103
batch 185 loss: 0.0005683143623173237
batch 190 loss: 0.0005675484659150243
batch 195 loss: 0.0005676742643117904
batch 200 loss: 0.0005646801553666591
batch 205 loss: 0.0005668503581546247
batch 210 loss: 0.0005679543479345739
batch 215 loss: 0.0005662823910824955
batch 220 loss: 0.0005671555874869228
batch 225 loss: 0.0005678866873495281
batch 230 loss: 0.0005654916749335825
batch 235 loss: 0.0005727109615691006
batch 240 loss: 0.0005831225775182247
Training Loss: 0.0005899526823971731
Validation Loss: 0.0006051318642372887
Epoch 8:
batch 5 loss: 0.0005982644157484174
batch 10 loss: 0.0006058214232325554
batch 15 loss: 0.0005985643365420401
batch 20 loss: 0.0005925575736910105
batch 25 loss: 0.000587706221267581
batch 30 loss: 0.0005833655945025384
batch 35 loss: 0.0005765006644651294
batch 40 loss: 0.0005756539525464177
batch 45 loss: 0.0005761024309322238
batch 50 loss: 0.0005723348818719387
batch 55 loss: 0.0005736357648856938
batch 60 loss: 0.0005690875346772373
batch 65 loss: 0.0005699204979464412
batch 70 loss: 0.0005674354499205947
batch 75 loss: 0.000567370920907706
batch 80 loss: 0.0005676588276401162
batch 85 loss: 0.0005667552468366921
batch 90 loss: 0.0005690805613994598
batch 95 loss: 0.0005642208503559232
batch 100 loss: 0.0005653958534821868
batch 105 loss: 0.0005656107561662794
batch 110 loss: 0.00056721072178334
batch 115 loss: 0.0005652721505612135
batch 120 loss: 0.0005656785564497113
batch 125 loss: 0.0005649238592013717
batch 130 loss: 0.0005677787004970015
batch 135 loss: 0.0005652174353599548
batch 140 loss: 0.0005650280509144068
batch 145 loss: 0.0005639573326334357
batch 150 loss: 0.0005633912747725845
batch 155 loss: 0.000565777171868831
batch 160 loss: 0.0005653815576806664
batch 165 loss: 0.0005644526449032128
batch 170 loss: 0.0005621938733384013
batch 175 loss: 0.0005659257061779499
batch 180 loss: 0.0005648685502819717
batch 185 loss: 0.0005644408985972405
batch 190 loss: 0.0005646686418913305
batch 195 loss: 0.0005633638938888907
batch 200 loss: 0.0005637220223434269
batch 205 loss: 0.000563190458342433
batch 210 loss: 0.0005655683809891344
batch 215 loss: 0.0005628846934996546
batch 220 loss: 0.0005640553194098174
batch 225 loss: 0.0005617523798719049
batch 230 loss: 0.0005643495125696063
batch 235 loss: 0.0005625015706755221
batch 240 loss: 0.0005627792794257403
Training Loss: 0.0005698620499363945
Validation Loss: 0.0005649072409141809
Epoch 9:
batch 5 loss: 0.0005627520033158362
batch 10 loss: 0.0005647787591442465
batch 15 loss: 0.0005640762741677463
batch 20 loss: 0.0005623721168376506
batch 25 loss: 0.0005630184081383049
batch 30 loss: 0.0005631869542412459
batch 35 loss: 0.0005633452092297375
batch 40 loss: 0.0005627255421131849
batch 45 loss: 0.000562446960248053
batch 50 loss: 0.0005634899251163005
batch 55 loss: 0.0005638516275212168
batch 60 loss: 0.0005601210054010153
batch 65 loss: 0.0005622360622510314
batch 70 loss: 0.0005626010126434267
batch 75 loss: 0.000560713792219758
batch 80 loss: 0.0005603384575806558
batch 85 loss: 0.0005601091426797212
batch 90 loss: 0.0005610271939076484
batch 95 loss: 0.0005626598605886102
batch 100 loss: 0.0005621333257295191
batch 105 loss: 0.0005619726027362049
batch 110 loss: 0.0005580130964517593
batch 115 loss: 0.0005578111275099217
batch 120 loss: 0.0005577610689215362
batch 125 loss: 0.0005603295750916005
batch 130 loss: 0.0006119576282799244
batch 135 loss: 0.0006011597230099142
batch 140 loss: 0.000557707145344466
batch 145 loss: 0.0005576148745603859
batch 150 loss: 0.0005577090196311474
batch 155 loss: 0.0005576479714363813
batch 160 loss: 0.0005575715564191342
batch 165 loss: 0.0005576576106250287
batch 170 loss: 0.000557723711244762
batch 175 loss: 0.0005576888564974069
batch 180 loss: 0.0005576864467002451
batch 185 loss: 0.0005576864932663739
batch 190 loss: 0.000557611882686615
batch 195 loss: 0.000557691021822393
batch 200 loss: 0.0005577736767008901
batch 205 loss: 0.0005576428142376244
batch 210 loss: 0.0005576719529926776
batch 215 loss: 0.0005576471448875964
batch 220 loss: 0.0005582846002653241
batch 225 loss: 0.0005577320349402726
batch 230 loss: 0.0005576535942964256
batch 235 loss: 0.0005577740143053233
batch 240 loss: 0.0005577124306000769
Training Loss: 0.0005618516105945066
Validation Loss: 0.0005576934054261073
Epoch 10:
batch 5 loss: 0.0005577213363721967
batch 10 loss: 0.0005577035713940859
batch 15 loss: 0.0005576806608587504
batch 20 loss: 0.0005577497649937868
batch 25 loss: 0.0005576432100497186
batch 30 loss: 0.0005577109288424254
batch 35 loss: 0.0005575704853981733
batch 40 loss: 0.0005576677736826241
batch 45 loss: 0.0005577313713729382
batch 50 loss: 0.000557640299666673
batch 55 loss: 0.0005577414063736797
batch 60 loss: 0.0005575814167968929
batch 65 loss: 0.0005577143863774836
batch 70 loss: 0.000557687773834914
batch 75 loss: 0.000557632907293737
batch 80 loss: 0.0005577146075665951
batch 85 loss: 0.0005576933035627007
batch 90 loss: 0.0005575756425969302
batch 95 loss: 0.0005576661555096507
batch 100 loss: 0.0005577484495006502
batch 105 loss: 0.0005577122559770942
batch 110 loss: 0.0005577396834269166
batch 115 loss: 0.0005576236872002482
batch 120 loss: 0.0005576753756031394
batch 125 loss: 0.0005576763185672462
batch 130 loss: 0.0005577198229730129
batch 135 loss: 0.000557589577510953
batch 140 loss: 0.0005576915107667446
batch 145 loss: 0.0005577153759077191
batch 150 loss: 0.0005576890078373253
batch 155 loss: 0.0005576995434239506
batch 160 loss: 0.0005576777504757047
batch 165 loss: 0.0005576482275500893
batch 170 loss: 0.0005577302072197199
batch 175 loss: 0.0005576061783358454
batch 180 loss: 0.0005577643518336117
batch 185 loss: 0.0005575989605858922
batch 190 loss: 0.0005576380644924939
batch 195 loss: 0.0005576649098657071
batch 200 loss: 0.0005576775525696576
batch 205 loss: 0.0005577269126661122
batch 210 loss: 0.0005576215567998588
batch 215 loss: 0.0005578659242019057
batch 220 loss: 0.0005577627569437027
batch 225 loss: 0.0005576788797043263
batch 230 loss: 0.000557782023679465
batch 235 loss: 0.0005578079028055072
batch 240 loss: 0.0005576275289058685
Training Loss: 0.0005576872354140505
Validation Loss: 0.0005576934083364904
Epoch 11:
batch 5 loss: 0.000557665922679007
batch 10 loss: 0.0005577377858571708
batch 15 loss: 0.0005576854688115418
batch 20 loss: 0.0005576498690061272
batch 25 loss: 0.0005576222785748542
batch 30 loss: 0.0005576545721851289
batch 35 loss: 0.0005577818723395467
batch 40 loss: 0.0005576851195655763
batch 45 loss: 0.0005575988441705703
batch 50 loss: 0.0005576287629082799
batch 55 loss: 0.0005576652009040117
batch 60 loss: 0.0005576340015977621
batch 65 loss: 0.0005576734896749258
batch 70 loss: 0.0005576114053837955
batch 75 loss: 0.0005576676339842379
batch 80 loss: 0.0005577075760811567
batch 85 loss: 0.0005577343748882413
batch 90 loss: 0.0005577464355155826
batch 95 loss: 0.0005576483090408146
batch 100 loss: 0.0005576577736064791
batch 105 loss: 0.0005576101597398519
batch 110 loss: 0.0005576406721957028
batch 115 loss: 0.0005576091702096164
batch 120 loss: 0.0005577271338552236
batch 125 loss: 0.000557608692906797
batch 130 loss: 0.0005577165051363408
batch 135 loss: 0.0005576972267590463
batch 140 loss: 0.0005577629432082176
batch 145 loss: 0.0005577672738581896
batch 150 loss: 0.0005577078904025256
batch 155 loss: 0.0005576982395723463
batch 160 loss: 0.0005577629315666854
batch 165 loss: 0.00055772983469069
batch 170 loss: 0.0005576364463195204
batch 175 loss: 0.0005577268660999834
batch 180 loss: 0.0005578611860983073
batch 185 loss: 0.0005577206262387336
batch 190 loss: 0.0005576592520810664
batch 195 loss: 0.0005576447234489024
batch 200 loss: 0.000557687843684107
batch 205 loss: 0.0005577235948294401
batch 210 loss: 0.0005579095683060587
batch 215 loss: 0.0005576845607720316
batch 220 loss: 0.0005576364696025848
batch 225 loss: 0.0005576497060246766
batch 230 loss: 0.0005576894618570805
batch 235 loss: 0.000557690137065947
batch 240 loss: 0.0005577301257289946
Training Loss: 0.0005576905403965308
Validation Loss: 0.0005576933996053413
Epoch 12:
batch 5 loss: 0.0005577493924647569
batch 10 loss: 0.0005577401840128005
batch 15 loss: 0.000557620800100267
batch 20 loss: 0.0005577425821684301
batch 25 loss: 0.0005576784024015069
batch 30 loss: 0.0005576245719566941
batch 35 loss: 0.0005577471922151745
batch 40 loss: 0.0005576869705691933
batch 45 loss: 0.000557676306925714
batch 50 loss: 0.0005576654104515911
batch 55 loss: 0.0005576186347752809
batch 60 loss: 0.0005576895899139344
batch 65 loss: 0.0005576773663051426
batch 70 loss: 0.0005576964467763901
batch 75 loss: 0.0005576875875703991
batch 80 loss: 0.0005576327559538186
batch 85 loss: 0.000557554850820452
batch 90 loss: 0.0005576831172220409
batch 95 loss: 0.0005575722898356616
batch 100 loss: 0.0005577499861828983
batch 105 loss: 0.0005577744217589497
batch 110 loss: 0.0005577181349508464
batch 115 loss: 0.0005576462484896183
batch 120 loss: 0.000557645270600915
batch 125 loss: 0.0005577028612606227
batch 130 loss: 0.0005577091244049371
batch 135 loss: 0.0005577076226472855
batch 140 loss: 0.0005576129537075758
batch 145 loss: 0.0005576802184805274
batch 150 loss: 0.0005576880648732185
batch 155 loss: 0.0005576147814281285
batch 160 loss: 0.0005577018833719194
batch 165 loss: 0.000557825225405395
batch 170 loss: 0.0005576636176556349
batch 175 loss: 0.0005577969481237232
batch 180 loss: 0.0005577542819082737
batch 185 loss: 0.000557666807435453
batch 190 loss: 0.0005576354917138815
batch 195 loss: 0.0005576357594691217
batch 200 loss: 0.0005576891009695828
batch 205 loss: 0.0005576943513005972
batch 210 loss: 0.0005575804971158504
batch 215 loss: 0.000557771697640419
batch 220 loss: 0.0005577102536335587
batch 225 loss: 0.0005575862247496844
batch 230 loss: 0.0005576822557486593
batch 235 loss: 0.0005577816162258386
batch 240 loss: 0.0005576458992436528
Training Loss: 0.0005576836677695004
Validation Loss: 0.0005576934801259388
Epoch 13:
batch 5 loss: 0.000557721930090338
batch 10 loss: 0.0005575683782808483
batch 15 loss: 0.000557649985421449
batch 20 loss: 0.0005577751435339451
batch 25 loss: 0.000557715306058526
batch 30 loss: 0.000557694339659065
batch 35 loss: 0.0005576095310971141
batch 40 loss: 0.0005577837000600994
batch 45 loss: 0.0005576967378146946
batch 50 loss: 0.0005576751194894314
batch 55 loss: 0.0005577322095632553
batch 60 loss: 0.0005576231516897679
batch 65 loss: 0.0005576871102675795
batch 70 loss: 0.0005576910451054573
batch 75 loss: 0.0005577343283221126
batch 80 loss: 0.0005575682036578656
batch 85 loss: 0.0005576610099524259
batch 90 loss: 0.0005576585652306675
batch 95 loss: 0.0005576314055360853
batch 100 loss: 0.0005576953524723649
batch 105 loss: 0.0005575819057412445
batch 110 loss: 0.0005576715804636479
batch 115 loss: 0.0005577374598942697
batch 120 loss: 0.0005577531410381198
batch 125 loss: 0.0005576545372605324
batch 130 loss: 0.0005575874587520957
batch 135 loss: 0.0005575829069130123
batch 140 loss: 0.0005576311377808452
batch 145 loss: 0.0005576542462222278
batch 150 loss: 0.0005576869589276612
batch 155 loss: 0.0005578383919782936
batch 160 loss: 0.0005576727329753339
batch 165 loss: 0.0005576765513978898
batch 170 loss: 0.0005576809751801192
batch 175 loss: 0.0005576117197051645
batch 180 loss: 0.0005577079253271222
batch 185 loss: 0.0005576546071097254
batch 190 loss: 0.0005578178213909268
batch 195 loss: 0.0005577660165727138
batch 200 loss: 0.0005576606956310571
batch 205 loss: 0.0005575810908339917
batch 210 loss: 0.0005577337811701
batch 215 loss: 0.0005576575291343034
batch 220 loss: 0.0005577448639087379
batch 225 loss: 0.0005577982636168599
batch 230 loss: 0.0005577124306000769
batch 235 loss: 0.0005576737341471017
batch 240 loss: 0.0005577010102570057
Training Loss: 0.0005576834172340265
Validation Loss: 0.0005576950905378907
Epoch 14:
batch 5 loss: 0.0005577729432843625
batch 10 loss: 0.0005576618597842753
batch 15 loss: 0.0005576938041485846
batch 20 loss: 0.0005577056552283465
batch 25 loss: 0.0005578058655373753
batch 30 loss: 0.0005575813702307642
batch 35 loss: 0.000557654199656099
batch 40 loss: 0.0005576155846938491
batch 45 loss: 0.0005576492520049214
batch 50 loss: 0.0005577852949500084
batch 55 loss: 0.0005576314870268106
batch 60 loss: 0.000557703897356987
batch 65 loss: 0.000557633803691715
batch 70 loss: 0.0005575471790507436
batch 75 loss: 0.0005576435243710876
batch 80 loss: 0.0005596938193775713
batch 85 loss: 0.0005577877396717668
batch 90 loss: 0.0005575814633630216
batch 95 loss: 0.0005575791816227138
batch 100 loss: 0.000557810568716377
batch 105 loss: 0.0005576138035394251
batch 110 loss: 0.0005576600204221904
batch 115 loss: 0.0005577413714490831
batch 120 loss: 0.0005576003342866898
batch 125 loss: 0.0005576699273660779
batch 130 loss: 0.0005576281459070742
batch 135 loss: 0.0005577243398874998
batch 140 loss: 0.0005578121053986251
batch 145 loss: 0.0005575167015194893
batch 150 loss: 0.0005575952236540616
batch 155 loss: 0.0005576796131208539
batch 160 loss: 0.0005576855270192027
batch 165 loss: 0.0005576439551077783
batch 170 loss: 0.000557688728440553
batch 175 loss: 0.0005578393931500614
batch 180 loss: 0.0005577201140113175
batch 185 loss: 0.0005576563300564886
batch 190 loss: 0.0005576727096922695
batch 195 loss: 0.0005578078213147819
batch 200 loss: 0.0005576859577558935
batch 205 loss: 0.0005577366799116134
batch 210 loss: 0.0005578288109973073
batch 215 loss: 0.0005577379721216858
batch 220 loss: 0.0005575725110247732
batch 225 loss: 0.0005578233394771814
batch 230 loss: 0.0005577327450737357
batch 235 loss: 0.0005577139789238572
batch 240 loss: 0.0005576238385401666
Training Loss: 0.0005577281352695233
Validation Loss: 0.0005576934093066181
Epoch 15:
batch 5 loss: 0.0005577194155193865
batch 10 loss: 0.0005576572846621275
batch 15 loss: 0.0005576860392466187
batch 20 loss: 0.0005576807539910078
batch 25 loss: 0.0005577428033575416
batch 30 loss: 0.0005576819414272905
batch 35 loss: 0.0005577077972702682
batch 40 loss: 0.0005576456780545414
batch 45 loss: 0.0005577469943091274
batch 50 loss: 0.000557700649369508
batch 55 loss: 0.0005576614756137132
batch 60 loss: 0.0005576800205744803
batch 65 loss: 0.000557675352320075
batch 70 loss: 0.0005576241412200034
batch 75 loss: 0.000557605316862464
batch 80 loss: 0.0005576691939495504
batch 85 loss: 0.0005576588562689721
batch 90 loss: 0.0005576627794653177
batch 95 loss: 0.0005576330004259944
batch 100 loss: 0.0005575806484557688
batch 105 loss: 0.0005577495787292719
batch 110 loss: 0.0005577439907938242
batch 115 loss: 0.0005577208008617163
batch 120 loss: 0.0005577766220085323
batch 125 loss: 0.0005575798568315804
batch 130 loss: 0.0005577532108873128
batch 135 loss: 0.0005576183553785086
batch 140 loss: 0.0005577053525485098
batch 145 loss: 0.0005576378665864467
batch 150 loss: 0.000557691021822393
batch 155 loss: 0.0005577343283221126
batch 160 loss: 0.0005577471223659813
batch 165 loss: 0.000557698612101376
batch 170 loss: 0.0005576195893809199
batch 175 loss: 0.0005576019990257919
batch 180 loss: 0.0005578182288445532
batch 185 loss: 0.0005577299278229475
batch 190 loss: 0.0005578232230618596
batch 195 loss: 0.0005577170173637569
batch 200 loss: 0.0005577024538069963
batch 205 loss: 0.000557639286853373
batch 210 loss: 0.0005575375282205642
batch 215 loss: 0.0005577183212153614
batch 220 loss: 0.0005577212781645357
batch 225 loss: 0.0005576291703619062
batch 230 loss: 0.0005576881696470082
batch 235 loss: 0.0005576485302299262
batch 240 loss: 0.0005576304509304463
Training Loss: 0.0005576833757610681
Validation Loss: 0.0005576933996053413
Epoch 16:
batch 5 loss: 0.0005577336181886494
batch 10 loss: 0.000557647948153317
batch 15 loss: 0.0005576291587203741
batch 20 loss: 0.0005576261668466031
batch 25 loss: 0.0005577122676186264
batch 30 loss: 0.0005575007409788669
batch 35 loss: 0.0005576027557253838
batch 40 loss: 0.0005576610215939582
batch 45 loss: 0.0005576943745836616
batch 50 loss: 0.0005576753639616073
batch 55 loss: 0.0005577119998633861
batch 60 loss: 0.0005577572039328516
batch 65 loss: 0.0005576959811151028
batch 70 loss: 0.0005576738156378269
batch 75 loss: 0.0005576488212682307
batch 80 loss: 0.0005576312192715705
batch 85 loss: 0.0005576706840656698
batch 90 loss: 0.0005576451541855932
batch 95 loss: 0.0005577066331170499
batch 100 loss: 0.0005576851079240441
batch 105 loss: 0.0005576460505835712
batch 110 loss: 0.0005577494390308857
batch 115 loss: 0.0005575172486715019
batch 120 loss: 0.0005576669704169035
batch 125 loss: 0.0005576971801929176
batch 130 loss: 0.0005577019532211125
batch 135 loss: 0.00055775634245947
batch 140 loss: 0.0005577048286795616
batch 145 loss: 0.000557822163682431
batch 150 loss: 0.0005577487638220191
batch 155 loss: 0.0005575685645453632
batch 160 loss: 0.0005575697054155171
batch 165 loss: 0.0005576427560299635
batch 170 loss: 0.0005576778436079622
batch 175 loss: 0.0005577041418291628
batch 180 loss: 0.0005578013020567596
batch 185 loss: 0.000557709252461791
batch 190 loss: 0.000557873270008713
batch 195 loss: 0.0005577700096182526
batch 200 loss: 0.0005576878320425749
batch 205 loss: 0.0005576307768933475
batch 210 loss: 0.0005576707655563951
batch 215 loss: 0.0005577527801506222
batch 220 loss: 0.0005576576921157538
batch 225 loss: 0.000557664898224175
batch 230 loss: 0.0005576527444645762
batch 235 loss: 0.0005577577278017998
batch 240 loss: 0.0005576892290264368
Training Loss: 0.0005576833806117065
Validation Loss: 0.0005576934025157243
Epoch 17:
batch 5 loss: 0.0005576408817432821
batch 10 loss: 0.000557755003683269
batch 15 loss: 0.0005577320349402726
batch 20 loss: 0.000557753792963922
batch 25 loss: 0.0005576183204539121
batch 30 loss: 0.0005577136296778917
batch 35 loss: 0.0005576527328230441
batch 40 loss: 0.0005576335708610714
batch 45 loss: 0.0005576858413405717
batch 50 loss: 0.0005576947587542236
batch 55 loss: 0.0005577548174187541
batch 60 loss: 0.0005577319534495473
batch 65 loss: 0.0005577489035204052
batch 70 loss: 0.0005575551418587566
batch 75 loss: 0.0005576929426752031
batch 80 loss: 0.0005576690891757607
batch 85 loss: 0.0005577123141847551
batch 90 loss: 0.0005577064002864063
batch 95 loss: 0.0005576127674430609
batch 100 loss: 0.0005576761672273278
batch 105 loss: 0.000557719450443983
batch 110 loss: 0.0005577021045610308
batch 115 loss: 0.0005577475181780756
batch 120 loss: 0.0005576409865170717
batch 125 loss: 0.0005576077965088189
batch 130 loss: 0.0005576423252932727
batch 135 loss: 0.0005577726755291223
batch 140 loss: 0.0005578066571615637
batch 145 loss: 0.0005575347342528403
batch 150 loss: 0.0005576917668804526
batch 155 loss: 0.0005576950847171247
batch 160 loss: 0.0005577416624873877
batch 165 loss: 0.0005577381467446685
batch 170 loss: 0.0005576126626692712
batch 175 loss: 0.0005577192991040647
batch 180 loss: 0.0005576480063609779
batch 185 loss: 0.0005575614864937961
batch 190 loss: 0.000557749334257096
batch 195 loss: 0.0005576695431955159
batch 200 loss: 0.0005576414638198912
batch 205 loss: 0.0005576291587203741
batch 210 loss: 0.0005576940719038248
batch 215 loss: 0.0005577686009928584
batch 220 loss: 0.0005576795781962573
batch 225 loss: 0.0005577750969678163
batch 230 loss: 0.0005576380295678973
batch 235 loss: 0.000557696761097759
batch 240 loss: 0.0005575370159931481
Training Loss: 0.0005576833767311958
Validation Loss: 0.0005576934063962351
Epoch 18:
batch 5 loss: 0.0005577318952418864
batch 10 loss: 0.0005576224997639656
batch 15 loss: 0.0005577128147706389
batch 20 loss: 0.0005576880881562829
batch 25 loss: 0.0005577171570621431
batch 30 loss: 0.0005577106727287173
batch 35 loss: 0.0005577288451604545
batch 40 loss: 0.0005576962721534073
batch 45 loss: 0.0005576013936661184
batch 50 loss: 0.0005576360155828298
batch 55 loss: 0.0005575964343734086
batch 60 loss: 0.0005576273193582893
batch 65 loss: 0.000557728495914489
batch 70 loss: 0.0005576282390393316
batch 75 loss: 0.0005576803931035101
batch 80 loss: 0.0005576818948611617
batch 85 loss: 0.0005575907067395747
batch 90 loss: 0.0005577738746069371
batch 95 loss: 0.0005577276111580432
batch 100 loss: 0.0005576670868322253
batch 105 loss: 0.000557777809444815
batch 110 loss: 0.000557716703042388
batch 115 loss: 0.000557640555780381
batch 120 loss: 0.0005575820337980986
batch 125 loss: 0.0005576702067628502
batch 130 loss: 0.0005575735121965408
batch 135 loss: 0.0005575673654675484
batch 140 loss: 0.0005576626397669315
batch 145 loss: 0.0005577021394856275
batch 150 loss: 0.0005577213945798576
batch 155 loss: 0.0005576144554652274
batch 160 loss: 0.0005576983792707324
batch 165 loss: 0.0005576428724452853
batch 170 loss: 0.0005576614639721811
batch 175 loss: 0.0005577786359935999
batch 180 loss: 0.0005577236879616976
batch 185 loss: 0.0005576038267463446
batch 190 loss: 0.0005577616975642741
batch 195 loss: 0.0005578415701165795
batch 200 loss: 0.000557655340526253
batch 205 loss: 0.0005575866554863751
batch 210 loss: 0.0005576478084549308
batch 215 loss: 0.0005576688214205206
batch 220 loss: 0.0005577142583206296
batch 225 loss: 0.0005577030591666698
batch 230 loss: 0.0005577906500548125
batch 235 loss: 0.0005578402080573142
batch 240 loss: 0.0005577066796831787
Training Loss: 0.0005576833779438554
Validation Loss: 0.0005576933996053413
Epoch 19:
batch 5 loss: 0.0005576429190114141
batch 10 loss: 0.0005575955845415592
batch 15 loss: 0.0005578075302764774
batch 20 loss: 0.0005575936404056848
batch 25 loss: 0.0005576258525252342
batch 30 loss: 0.0005577088217251003
batch 35 loss: 0.0005576977622695267
batch 40 loss: 0.0005576620227657258
batch 45 loss: 0.0005576322204433381
batch 50 loss: 0.0005576904863119125
batch 55 loss: 0.0005576613475568593
batch 60 loss: 0.0005576092866249382
batch 65 loss: 0.0005577652249485254
batch 70 loss: 0.0005576474126428365
batch 75 loss: 0.0005576879833824932
batch 80 loss: 0.0005577666219323873
batch 85 loss: 0.000557626923546195
batch 90 loss: 0.0005576468887738883
batch 95 loss: 0.0005576837691478431
batch 100 loss: 0.0005576849915087223
batch 105 loss: 0.0005577210104092956
batch 110 loss: 0.0005577921401709318
batch 115 loss: 0.0005576955853030085
batch 120 loss: 0.0005577161558903754
batch 125 loss: 0.0005577059928327799
batch 130 loss: 0.000557560648303479
batch 135 loss: 0.0005576522438786924
batch 140 loss: 0.0005576430587098003
batch 145 loss: 0.000557669484987855
batch 150 loss: 0.0005576288676820695
batch 155 loss: 0.000557635584846139
batch 160 loss: 0.0005577770178206265
batch 165 loss: 0.0005576786585152149
batch 170 loss: 0.0005576566443778574
batch 175 loss: 0.0005577359232120215
batch 180 loss: 0.000557756470516324
batch 185 loss: 0.0005576651659794152
batch 190 loss: 0.0005576695781201124
batch 195 loss: 0.0005578231415711343
batch 200 loss: 0.000557601056061685
batch 205 loss: 0.0005577003350481391
batch 210 loss: 0.0005577286356128752
batch 215 loss: 0.0005577501840889453
batch 220 loss: 0.0005576431169174612
batch 225 loss: 0.0005576340365223587
batch 230 loss: 0.000557743979152292
batch 235 loss: 0.000557641254272312
batch 240 loss: 0.0005577391129918396
Training Loss: 0.0005576833827944938
Validation Loss: 0.0005576933976650859
Epoch 20:
batch 5 loss: 0.0005576859577558935
batch 10 loss: 0.0005577570293098689
batch 15 loss: 0.0005575776216574013
batch 20 loss: 0.0005577366915531457
batch 25 loss: 0.0005575598683208227
batch 30 loss: 0.0005577005911618471
batch 35 loss: 0.0005576921626925469
batch 40 loss: 0.0005576964933425188
batch 45 loss: 0.0005577202769927681
batch 50 loss: 0.0005576795432716608
batch 55 loss: 0.0005577174248173833
batch 60 loss: 0.0005577109870500863
batch 65 loss: 0.0005577435484156012
batch 70 loss: 0.0005576394032686949
batch 75 loss: 0.0005576171679422259
batch 80 loss: 0.0005577134317718447
batch 85 loss: 0.0005577269126661122
batch 90 loss: 0.0005577642237767577
batch 95 loss: 0.0005577224539592863
batch 100 loss: 0.0005576727562583983
batch 105 loss: 0.000557682744693011
batch 110 loss: 0.000557727087289095
batch 115 loss: 0.0005577084724791348
batch 120 loss: 0.0005576989147812128
batch 125 loss: 0.0005577531643211842
batch 130 loss: 0.0005577177391387523
batch 135 loss: 0.0005576688563451171
batch 140 loss: 0.0005575613235123455
batch 145 loss: 0.0005576261901296675
batch 150 loss: 0.0005576744792051613
batch 155 loss: 0.0005577007541432977
batch 160 loss: 0.0005576422205194831
batch 165 loss: 0.0005576963303610682
batch 170 loss: 0.000557806808501482
batch 175 loss: 0.0005576802184805274
batch 180 loss: 0.0005577046307735145
batch 185 loss: 0.0005577541887760162
batch 190 loss: 0.0005576448864303529
batch 195 loss: 0.0005575402756221592
batch 200 loss: 0.0005576501018367708
batch 205 loss: 0.000557559565640986
batch 210 loss: 0.0005575958057306707
batch 215 loss: 0.0005577385076321661
batch 220 loss: 0.0005576623603701592
batch 225 loss: 0.0005576723953709007
batch 230 loss: 0.0005577557254582644
batch 235 loss: 0.0005577020929194987
batch 240 loss: 0.0005576418363489211
Training Loss: 0.0005576833796415788
Validation Loss: 0.0005576934112468734
Epoch 21:
batch 5 loss: 0.0005576943629421293
batch 10 loss: 0.000557748565915972
batch 15 loss: 0.0005575976683758199
batch 20 loss: 0.0005576742929406464
batch 25 loss: 0.0005576396477408707
batch 30 loss: 0.0005576794152148068
batch 35 loss: 0.0005577951087616384
batch 40 loss: 0.0005575818126089871
batch 45 loss: 0.0005577996373176575
batch 50 loss: 0.0005577044910751283
batch 55 loss: 0.0005576948053203524
batch 60 loss: 0.0005576536757871508
batch 65 loss: 0.000557568424846977
batch 70 loss: 0.0005577152129262686
batch 75 loss: 0.0005576832569204271
batch 80 loss: 0.0005576913594268262
batch 85 loss: 0.0005576552357524633
batch 90 loss: 0.0005577204748988152
batch 95 loss: 0.0005576755269430578
batch 100 loss: 0.0005577482283115387
batch 105 loss: 0.0005576911731623113
batch 110 loss: 0.0005575987393967808
batch 115 loss: 0.0005577188334427774
batch 120 loss: 0.0005577034433372318
batch 125 loss: 0.0005576494033448399
batch 130 loss: 0.000557695934548974
batch 135 loss: 0.0005577560164965689
batch 140 loss: 0.0005575866438448429
batch 145 loss: 0.0005576360737904907
batch 150 loss: 0.0005577186471782625
batch 155 loss: 0.0005576666095294059
batch 160 loss: 0.0005577279953286052
batch 165 loss: 0.0005575557588599622
batch 170 loss: 0.000557649414986372
batch 175 loss: 0.0005577984731644392
batch 180 loss: 0.0005576866562478245
batch 185 loss: 0.0005577408359386027
batch 190 loss: 0.0005576725699938834
batch 195 loss: 0.0005574955604970456
batch 200 loss: 0.0005577286239713431
batch 205 loss: 0.0005576421972364187
batch 210 loss: 0.0005577296251431108
batch 215 loss: 0.0005578105919994414
batch 220 loss: 0.0005575525923632085
batch 225 loss: 0.000557708868291229
batch 230 loss: 0.0005576871684752404
batch 235 loss: 0.0005576974828727544
batch 240 loss: 0.0005577750038355589
Training Loss: 0.0005576833779438554
Validation Loss: 0.0005576934044559796
Epoch 22:
batch 5 loss: 0.0005577058997005224
batch 10 loss: 0.0005576314404606819
batch 15 loss: 0.0005577504285611212
batch 20 loss: 0.0005576300667598843
batch 25 loss: 0.0005575736169703305
batch 30 loss: 0.0005577482399530709
batch 35 loss: 0.0005578017327934504
batch 40 loss: 0.0005576882744207978
batch 45 loss: 0.0005576166091486812
batch 50 loss: 0.0005577291478402913
batch 55 loss: 0.0005577406613156199
batch 60 loss: 0.000557669554837048
batch 65 loss: 0.0005576122552156448
batch 70 loss: 0.0005576245603151619
batch 75 loss: 0.0005577419186010956
batch 80 loss: 0.0005577852600254119
batch 85 loss: 0.000557684397790581
batch 90 loss: 0.00055767489830032
batch 95 loss: 0.0005578555748797954
batch 100 loss: 0.0005577714298851788
batch 105 loss: 0.0005576838855631649
batch 110 loss: 0.0005577041767537594
batch 115 loss: 0.0005577125935815275
batch 120 loss: 0.0005575553514063358
batch 125 loss: 0.0005576446070335806
batch 130 loss: 0.0005576253752224147
batch 135 loss: 0.0005576527211815119
batch 140 loss: 0.0005575853167101741
batch 145 loss: 0.0005577330477535725
batch 150 loss: 0.0005576190887950361
batch 155 loss: 0.0005577217787504196
batch 160 loss: 0.0005576360039412975
batch 165 loss: 0.0005576680181547999
batch 170 loss: 0.0005576163181103766
batch 175 loss: 0.0005576097639277578
batch 180 loss: 0.0005577070871368051
batch 185 loss: 0.000557530636433512
batch 190 loss: 0.0005577085190452636
batch 195 loss: 0.0005576209863647818
batch 200 loss: 0.0005577269359491766
batch 205 loss: 0.0005576677271164953
batch 210 loss: 0.0005577554809860885
batch 215 loss: 0.000557768892031163
batch 220 loss: 0.0005577148171141743
batch 225 loss: 0.0005576489609666168
batch 230 loss: 0.0005576292518526315
batch 235 loss: 0.0005577541771344841
batch 240 loss: 0.0005577646661549807
Training Loss: 0.0005576833781863873
Validation Loss: 0.0005576934063962351
Epoch 23:
batch 5 loss: 0.0005577843869104982
batch 10 loss: 0.0005575171206146478
batch 15 loss: 0.000557764177210629
batch 20 loss: 0.0005577880539931357
batch 25 loss: 0.0005577434785664082
batch 30 loss: 0.0005575446179136634
batch 35 loss: 0.000557800114620477
batch 40 loss: 0.0005576285417191684
batch 45 loss: 0.0005577127099968493
batch 50 loss: 0.0005576460505835712
batch 55 loss: 0.0005576857132837176
batch 60 loss: 0.0005577441770583391
batch 65 loss: 0.00055761041585356
batch 70 loss: 0.0005576589726842939
batch 75 loss: 0.000557608634699136
batch 80 loss: 0.0005577644449658691
batch 85 loss: 0.0005577592412009836
batch 90 loss: 0.0005576533847488462
batch 95 loss: 0.0005577062256634235
batch 100 loss: 0.0005576938041485846
batch 105 loss: 0.0005575508577749134
batch 110 loss: 0.0005576112773269415
batch 115 loss: 0.0005576640833169222
batch 120 loss: 0.0005576494964770973
batch 125 loss: 0.0005576789262704551
batch 130 loss: 0.0005576399038545787
batch 135 loss: 0.0005577468080446124
batch 140 loss: 0.0005576659808866679
batch 145 loss: 0.000557764305267483
batch 150 loss: 0.0005576089723035693
batch 155 loss: 0.000557726644910872
batch 160 loss: 0.0005576725350692868
batch 165 loss: 0.0005577263422310352
batch 170 loss: 0.0005577849107794464
batch 175 loss: 0.0005575934075750411
batch 180 loss: 0.0005575635121203959
batch 185 loss: 0.000557713396847248
batch 190 loss: 0.0005577228148467839
batch 195 loss: 0.0005577990552410484
batch 200 loss: 0.0005577126867137849
batch 205 loss: 0.0005577115691266954
batch 210 loss: 0.000557570019736886
batch 215 loss: 0.0005575802992098033
batch 220 loss: 0.0005577495321631432
batch 225 loss: 0.0005577336647547782
batch 230 loss: 0.00055768305901438
batch 235 loss: 0.0005576438386924564
batch 240 loss: 0.0005577199975959956
Training Loss: 0.0005576833784289193
Validation Loss: 0.0005576934122170011
Epoch 24:
batch 5 loss: 0.0005576723371632398
batch 10 loss: 0.0005576813709922135
batch 15 loss: 0.0005577235133387148
batch 20 loss: 0.0005576066090725362
batch 25 loss: 0.0005576969939284027
batch 30 loss: 0.0005576125578954816
batch 35 loss: 0.0005576206487603486
batch 40 loss: 0.0005576609866693616
batch 45 loss: 0.0005577260861173272
batch 50 loss: 0.0005576596013270319
batch 55 loss: 0.0005577369360253215
batch 60 loss: 0.0005576579016633332
batch 65 loss: 0.0005577718373388052
batch 70 loss: 0.0005577112664468586
batch 75 loss: 0.000557774433400482
batch 80 loss: 0.0005576361902058125
batch 85 loss: 0.0005577097646892071
batch 90 loss: 0.0005576493218541146
batch 95 loss: 0.0005576508119702339
batch 100 loss: 0.0005576758529059588
batch 105 loss: 0.0005576291703619062
batch 110 loss: 0.000557677261531353
batch 115 loss: 0.0005577285541221499
batch 120 loss: 0.0005576798575930297
batch 125 loss: 0.0005575623596087098
batch 130 loss: 0.0005576693220064044
batch 135 loss: 0.0005576219875365496
batch 140 loss: 0.0005575800780206919
batch 145 loss: 0.000557708740234375
batch 150 loss: 0.0005576782277785242
batch 155 loss: 0.0005576552706770599
batch 160 loss: 0.0005577520583756268
batch 165 loss: 0.0005576350726187229
batch 170 loss: 0.000557753595057875
batch 175 loss: 0.000557687331456691
batch 180 loss: 0.000557679554913193
batch 185 loss: 0.000557690067216754
batch 190 loss: 0.0005576680763624608
batch 195 loss: 0.0005576392984949052
batch 200 loss: 0.0005576320923864842
batch 205 loss: 0.0005577506846748293
batch 210 loss: 0.0005577026051469148
batch 215 loss: 0.0005577596370130777
batch 220 loss: 0.0005576749099418521
batch 225 loss: 0.0005577183910645545
batch 230 loss: 0.0005577660282142461
batch 235 loss: 0.0005577530129812658
batch 240 loss: 0.0005577140138484538
Training Loss: 0.0005576833808542385
Validation Loss: 0.0005576934063962351
Epoch 25:
batch 5 loss: 0.000557638646569103
batch 10 loss: 0.0005576750729233027
batch 15 loss: 0.0005576643976382911
batch 20 loss: 0.0005577367264777422
batch 25 loss: 0.0005578255048021674
batch 30 loss: 0.0005577666102908552
batch 35 loss: 0.0005576605792157352
batch 40 loss: 0.000557591940741986
batch 45 loss: 0.0005576663534156978
batch 50 loss: 0.0005577820003964007
batch 55 loss: 0.000557651708368212
batch 60 loss: 0.0005578039796091616
batch 65 loss: 0.0005576999159529805
batch 70 loss: 0.0005576920113526285
batch 75 loss: 0.0005576786352321506
batch 80 loss: 0.0005576582159847021
batch 85 loss: 0.000557578424923122
batch 90 loss: 0.0005576879717409611
batch 95 loss: 0.000557734863832593
batch 100 loss: 0.0005577412783168256
batch 105 loss: 0.0005577057716436684
batch 110 loss: 0.0005576747935265303
batch 115 loss: 0.0005576721043325961
batch 120 loss: 0.0005575219984166324
batch 125 loss: 0.0005575855378992855
batch 130 loss: 0.0005576350609771908
batch 135 loss: 0.0005576446419581771
batch 140 loss: 0.0005576417432166636
batch 145 loss: 0.0005577507545240223
batch 150 loss: 0.000557605957146734
batch 155 loss: 0.0005578086362220347
batch 160 loss: 0.000557608692906797
batch 165 loss: 0.0005577283911406994
batch 170 loss: 0.0005578026408329606
batch 175 loss: 0.0005576265277341009
batch 180 loss: 0.0005577255506068468
batch 185 loss: 0.0005576349911279976
batch 190 loss: 0.0005576921626925469
batch 195 loss: 0.0005575852235779166
batch 200 loss: 0.0005577608360908925
batch 205 loss: 0.0005576346069574356
batch 210 loss: 0.0005578066920861602
batch 215 loss: 0.0005576912430115044
batch 220 loss: 0.0005576249794103205
batch 225 loss: 0.0005576781113632023
batch 230 loss: 0.0005577682633884252
batch 235 loss: 0.0005575544899329543
batch 240 loss: 0.0005576970521360636
Training Loss: 0.0005576833810967704
Validation Loss: 0.0005576934364701932
Epoch 26:
batch 5 loss: 0.0005576898925937712
batch 10 loss: 0.0005577889387495816
batch 15 loss: 0.0005576183088123798
batch 20 loss: 0.0005577152012847364
batch 25 loss: 0.0005576955270953476
batch 30 loss: 0.0005575723247602582
batch 35 loss: 0.0005575832561589778
batch 40 loss: 0.0005576651077717542
batch 45 loss: 0.0005577215692028403
batch 50 loss: 0.0005577010335400701
batch 55 loss: 0.0005577633390203118
batch 60 loss: 0.0005576816969551146
batch 65 loss: 0.0005576482624746859
batch 70 loss: 0.0005576555617153644
batch 75 loss: 0.0005575899383984506
batch 80 loss: 0.0005576495663262903
batch 85 loss: 0.0005577223841100931
batch 90 loss: 0.0005576863652095199
batch 95 loss: 0.0005577262956649065
batch 100 loss: 0.0005576625932008028
batch 105 loss: 0.0005577396368607879
batch 110 loss: 0.0005576957250013947
batch 115 loss: 0.0005576547235250473
batch 120 loss: 0.0005577203817665577
batch 125 loss: 0.0005577123840339482
batch 130 loss: 0.0005577853415161371
batch 135 loss: 0.0005578871932812035
batch 140 loss: 0.0005576918250881135
batch 145 loss: 0.0005577063304372132
batch 150 loss: 0.0005577275180257857
batch 155 loss: 0.000557688728440553
batch 160 loss: 0.0005577014177106321
batch 165 loss: 0.0005576308001764118
batch 170 loss: 0.000557702558580786
batch 175 loss: 0.0005577188218012452
batch 180 loss: 0.0005576977273449302
batch 185 loss: 0.0005575697752647102
batch 190 loss: 0.0005576621973887086
batch 195 loss: 0.0005577290896326304
batch 200 loss: 0.0005577148636803031
batch 205 loss: 0.0005576567491516471
batch 210 loss: 0.0005575965391471982
batch 215 loss: 0.0005577774252742528
batch 220 loss: 0.0005576699739322066
batch 225 loss: 0.0005576454568654299
batch 230 loss: 0.0005575779476203024
batch 235 loss: 0.0005576557130552828
batch 240 loss: 0.0005575485760346055
Training Loss: 0.0005576833871600683
Validation Loss: 0.0005576934131871288
Epoch 27:
batch 5 loss: 0.0005577751318924129
batch 10 loss: 0.0005577878444455564
batch 15 loss: 0.000557676237076521
batch 20 loss: 0.0005577155738137662
batch 25 loss: 0.0005576646770350635
batch 30 loss: 0.0005577656789682806
batch 35 loss: 0.0005576859111897647
batch 40 loss: 0.0005576033378019929
batch 45 loss: 0.0005577710922807455
batch 50 loss: 0.0005577087518759072
batch 55 loss: 0.0005576857947744429
batch 60 loss: 0.0005576331517659127
batch 65 loss: 0.0005576219060458243
batch 70 loss: 0.0005577527452260255
batch 75 loss: 0.0005576843279413879
batch 80 loss: 0.0005577180534601211
batch 85 loss: 0.0005575823481194675
batch 90 loss: 0.0005576173425652087
batch 95 loss: 0.000557778961956501
batch 100 loss: 0.0005576760158874094
batch 105 loss: 0.0005577241536229849
batch 110 loss: 0.0005576963187195361
batch 115 loss: 0.0005577844800427556
batch 120 loss: 0.000557681336067617
batch 125 loss: 0.0005575706483796239
batch 130 loss: 0.0005578095326200128
batch 135 loss: 0.0005575985764153301
batch 140 loss: 0.0005577079020440579
batch 145 loss: 0.0005577308591455221
batch 150 loss: 0.0005576082621701062
batch 155 loss: 0.0005576832103542983
batch 160 loss: 0.000557673373259604
batch 165 loss: 0.0005577574949711562
batch 170 loss: 0.0005577436066232621
batch 175 loss: 0.0005576897761784494
batch 180 loss: 0.0005577137577347458
batch 185 loss: 0.0005576217197813093
batch 190 loss: 0.0005575170624069869
batch 195 loss: 0.0005576134426519275
batch 200 loss: 0.0005576462834142148
batch 205 loss: 0.0005575588787905872
batch 210 loss: 0.0005576844443567098
batch 215 loss: 0.0005576979019679129
batch 220 loss: 0.0005576738389208913
batch 225 loss: 0.0005576063646003604
batch 230 loss: 0.0005576548632234335
batch 235 loss: 0.0005577373201958835
batch 240 loss: 0.0005577122792601585
Training Loss: 0.0005576833869175364
Validation Loss: 0.000557693403485852
Epoch 28:
batch 5 loss: 0.0005575971910730005
batch 10 loss: 0.0005576643394306302
batch 15 loss: 0.0005577484727837146
batch 20 loss: 0.0005576359340921045
batch 25 loss: 0.0005577448988333345
batch 30 loss: 0.000557661836501211
batch 35 loss: 0.0005575658869929612
batch 40 loss: 0.0005577368661761284
batch 45 loss: 0.0005577538628131151
batch 50 loss: 0.0005576192052103579
batch 55 loss: 0.0005577163421548903
batch 60 loss: 0.00055766343139112
batch 65 loss: 0.0005576798925176263
batch 70 loss: 0.0005577672971412539
batch 75 loss: 0.0005577257135882974
batch 80 loss: 0.0005575816030614078
batch 85 loss: 0.0005576833384111524
batch 90 loss: 0.0005577331525273621
batch 95 loss: 0.0005575892981141805
batch 100 loss: 0.000557587610092014
batch 105 loss: 0.0005576672963798046
batch 110 loss: 0.0005577216041274368
batch 115 loss: 0.0005576742580160499
batch 120 loss: 0.0005576937808655202
batch 125 loss: 0.0005575926858000458
batch 130 loss: 0.000557675096206367
batch 135 loss: 0.0005576644092798233
batch 140 loss: 0.0005578029667958617
batch 145 loss: 0.0005576690775342286
batch 150 loss: 0.0005576426046900451
batch 155 loss: 0.0005577831994742155
batch 160 loss: 0.000557649985421449
batch 165 loss: 0.0005577815347351134
batch 170 loss: 0.0005576907540671527
batch 175 loss: 0.0005576520808972419
batch 180 loss: 0.0005576855735853314
batch 185 loss: 0.000557651394046843
batch 190 loss: 0.0005576278199441731
batch 195 loss: 0.0005577498581260443
batch 200 loss: 0.0005576217197813093
batch 205 loss: 0.0005576855503022671
batch 210 loss: 0.0005576851312071085
batch 215 loss: 0.0005577526986598969
batch 220 loss: 0.0005576765397563577
batch 225 loss: 0.0005576892523095012
batch 230 loss: 0.0005577280186116696
batch 235 loss: 0.0005576782044954598
batch 240 loss: 0.0005577532458119095
Training Loss: 0.0005576833857048769
Validation Loss: 0.0005576934335598101
Epoch 29:
batch 5 loss: 0.0005576264928095042
batch 10 loss: 0.0005576775525696576
batch 15 loss: 0.0005576523602940142
batch 20 loss: 0.0005577477160841227
batch 25 loss: 0.0005577625590376556
batch 30 loss: 0.000557632208801806
batch 35 loss: 0.0005577346426434814
batch 40 loss: 0.0005577781004831195
batch 45 loss: 0.0005577478790655732
batch 50 loss: 0.0005575786111876368
batch 55 loss: 0.0005577437928877771
batch 60 loss: 0.0005576659808866679
batch 65 loss: 0.000557716132607311
batch 70 loss: 0.0005576862720772624
batch 75 loss: 0.000557674013543874
batch 80 loss: 0.0005576390540227294
batch 85 loss: 0.000557751301676035
batch 90 loss: 0.0005577299511060118
batch 95 loss: 0.0005576559691689908
batch 100 loss: 0.0005576222436502575
batch 105 loss: 0.0005577066564001143
batch 110 loss: 0.0005576294264756143
batch 115 loss: 0.0005577308475039899
batch 120 loss: 0.0005576597061008215
batch 125 loss: 0.0005577117903158068
batch 130 loss: 0.0005576748284511268
batch 135 loss: 0.0005576012423262
batch 140 loss: 0.0005576941533945501
batch 145 loss: 0.0005576672265306115
batch 150 loss: 0.0005576116847805679
batch 155 loss: 0.0005577059462666511
batch 160 loss: 0.0005576744559220969
batch 165 loss: 0.0005576599622145295
batch 170 loss: 0.0005576306139118969
batch 175 loss: 0.0005576362716965377
batch 180 loss: 0.0005576911964453757
batch 185 loss: 0.000557686819229275
batch 190 loss: 0.0005576700554229319
batch 195 loss: 0.000557680893689394
batch 200 loss: 0.0005576841649599373
batch 205 loss: 0.0005576902301982045
batch 210 loss: 0.0005577047821134328
batch 215 loss: 0.0005577545380219817
batch 220 loss: 0.0005577366217039525
batch 225 loss: 0.000557579449377954
batch 230 loss: 0.0005577112431637942
batch 235 loss: 0.0005576298455707729
batch 240 loss: 0.000557765131816268
Training Loss: 0.0005576833878876642
Validation Loss: 0.0005576934190078948
Epoch 30:
batch 5 loss: 0.0005576167721301317
batch 10 loss: 0.0005576048977673053
batch 15 loss: 0.0005578575888648629
batch 20 loss: 0.0005575528251938522
batch 25 loss: 0.000557737797498703
batch 30 loss: 0.0005575936520472169
batch 35 loss: 0.0005576308583840727
batch 40 loss: 0.0005578888580203056
batch 45 loss: 0.0005576284253038466
batch 50 loss: 0.0005576735944487155
batch 55 loss: 0.0005576390889473259
batch 60 loss: 0.0005577712203375995
batch 65 loss: 0.0005576894734986126
batch 70 loss: 0.0005576451891101897
batch 75 loss: 0.0005577060393989087
batch 80 loss: 0.0005576301249675452
batch 85 loss: 0.0005577369011007249
batch 90 loss: 0.0005576729541644454
batch 95 loss: 0.0005577190895564854
batch 100 loss: 0.0005576648632995784
batch 105 loss: 0.0005577868898399175
batch 110 loss: 0.0005577269359491766
batch 115 loss: 0.0005577589734457433
batch 120 loss: 0.0005578577285632492
batch 125 loss: 0.0005577346310019493
batch 130 loss: 0.0005576735944487155
batch 135 loss: 0.0005577152711339295
batch 140 loss: 0.000557636225130409
batch 145 loss: 0.0005575612653046846
batch 150 loss: 0.0005576193449087441
batch 155 loss: 0.000557662546634674
batch 160 loss: 0.000557678029872477
batch 165 loss: 0.0005577537696808577
batch 170 loss: 0.0005576682975515723
batch 175 loss: 0.0005576830124482512
batch 180 loss: 0.0005576328956522047
batch 185 loss: 0.0005577935604378581
batch 190 loss: 0.0005576600669883192
batch 195 loss: 0.0005575860152021051
batch 200 loss: 0.0005577308358624578
batch 205 loss: 0.0005576360737904907
batch 210 loss: 0.0005576610099524259
batch 215 loss: 0.00055758913513273
batch 220 loss: 0.0005578223965130747
batch 225 loss: 0.0005576322670094669
batch 230 loss: 0.0005576361552812159
batch 235 loss: 0.0005576072493568063
batch 240 loss: 0.0005576384137384593
Training Loss: 0.0005576833917681749
Validation Loss: 0.0005576934791558112
Epoch 31:
batch 5 loss: 0.0005576220457442105
batch 10 loss: 0.0005577383679337799
batch 15 loss: 0.000557679426856339
batch 20 loss: 0.0005576784373261034
batch 25 loss: 0.000557686050888151
batch 30 loss: 0.0005577497067861259
batch 35 loss: 0.0005577288335189224
batch 40 loss: 0.0005576544674113393
batch 45 loss: 0.0005576856550760567
batch 50 loss: 0.0005577760399319232
batch 55 loss: 0.0005576959811151028
batch 60 loss: 0.0005576663883402943
batch 65 loss: 0.0005576610215939582
batch 70 loss: 0.0005577535252086818
batch 75 loss: 0.0005576091818511486
batch 80 loss: 0.0005576415453106165
batch 85 loss: 0.0005576805328018963
batch 90 loss: 0.0005576935363933444
batch 95 loss: 0.00055766865843907
batch 100 loss: 0.0005578216747380793
batch 105 loss: 0.0005576396128162741
batch 110 loss: 0.0005576511844992638
batch 115 loss: 0.0005575916031375528
batch 120 loss: 0.0005577622330747544
batch 125 loss: 0.0005577308009378612
batch 130 loss: 0.0005577231757342815
batch 135 loss: 0.0005576379830017686
batch 140 loss: 0.0005576338851824402
batch 145 loss: 0.0005577307427302002
batch 150 loss: 0.0005576325464062392
batch 155 loss: 0.0005576760275289416
batch 160 loss: 0.0005576585885137319
batch 165 loss: 0.0005576990894041956
batch 170 loss: 0.000557817646767944
batch 175 loss: 0.0005577062955126166
batch 180 loss: 0.0005576613824814558
batch 185 loss: 0.0005575956776738167
batch 190 loss: 0.0005576509516686201
batch 195 loss: 0.0005577145842835307
batch 200 loss: 0.0005576430587098003
batch 205 loss: 0.0005575570161454379
batch 210 loss: 0.0005578228970989585
batch 215 loss: 0.000557547714561224
batch 220 loss: 0.0005576911615207791
batch 225 loss: 0.0005577425821684301
batch 230 loss: 0.0005576769821345806
batch 235 loss: 0.0005576938856393099
batch 240 loss: 0.0005576226045377553
Training Loss: 0.0005576833956486856
Validation Loss: 0.0005576934228884057
Epoch 32:
batch 5 loss: 0.0005577396019361913
batch 10 loss: 0.0005576605210080743
batch 15 loss: 0.0005576429539360106
batch 20 loss: 0.000557702430523932
batch 25 loss: 0.0005577255738899112
batch 30 loss: 0.0005575929302722215
batch 35 loss: 0.0005575695773586631
batch 40 loss: 0.0005575182032771409
batch 45 loss: 0.0005576848750934004
batch 50 loss: 0.0005577747710049152
batch 55 loss: 0.000557711289729923
batch 60 loss: 0.0005577152245678007
batch 65 loss: 0.000557708297856152
batch 70 loss: 0.0005576602183282375
batch 75 loss: 0.0005577438161708415
batch 80 loss: 0.0005577108939178288
batch 85 loss: 0.0005576453171670437
batch 90 loss: 0.0005576907191425562
batch 95 loss: 0.0005576637922786176
batch 100 loss: 0.0005577957141213119
batch 105 loss: 0.0005575857707299292
batch 110 loss: 0.0005576784955337643
batch 115 loss: 0.0005577141768299043
batch 120 loss: 0.0005576601484790445
batch 125 loss: 0.0005578450043685734
batch 130 loss: 0.0005577571340836584
batch 135 loss: 0.000557634700089693
batch 140 loss: 0.0005576438619755208
batch 145 loss: 0.0005577095085754991
batch 150 loss: 0.0005577414412982762
batch 155 loss: 0.0005576684954576195
batch 160 loss: 0.0005576707306317985
batch 165 loss: 0.0005576744209975004
batch 170 loss: 0.0005577116971835494
batch 175 loss: 0.0005576600669883192
batch 180 loss: 0.0005577231990173459
batch 185 loss: 0.0005576907657086849
batch 190 loss: 0.0005576358875259757
batch 195 loss: 0.0005576859228312969
batch 200 loss: 0.0005576624884270131
batch 205 loss: 0.0005577235831879079
batch 210 loss: 0.0005577007541432977
batch 215 loss: 0.0005576203810051083
batch 220 loss: 0.0005576549563556909
batch 225 loss: 0.0005577764939516782
batch 230 loss: 0.0005576700437813997
batch 235 loss: 0.0005576042807660997
batch 240 loss: 0.0005576418014243245
Training Loss: 0.000557683394436026
Validation Loss: 0.0005576934141572565
Epoch 33:
batch 5 loss: 0.0005577371339313686
batch 10 loss: 0.0005576621741056442
batch 15 loss: 0.0005575982504524291
batch 20 loss: 0.0005577128962613642
batch 25 loss: 0.0005577393341809512
batch 30 loss: 0.0005577475298196077
batch 35 loss: 0.0005577150383032859
batch 40 loss: 0.0005576816387474537
batch 45 loss: 0.0005575823946855962
batch 50 loss: 0.0005576772382482886
batch 55 loss: 0.0005577397998422385
batch 60 loss: 0.0005575476912781596
batch 65 loss: 0.0005577430594712496
batch 70 loss: 0.0005576516268774867
batch 75 loss: 0.0005576677387580276
batch 80 loss: 0.0005576570401899517
batch 85 loss: 0.0005576867493800819
batch 90 loss: 0.0005575964110903442
batch 95 loss: 0.0005576253635808826
batch 100 loss: 0.0005577430361881852
batch 105 loss: 0.0005577563890255988
batch 110 loss: 0.0005576346418820321
batch 115 loss: 0.0005575951538048685
batch 120 loss: 0.0005576418829150498
batch 125 loss: 0.000557595631107688
batch 130 loss: 0.0005577116156928241
batch 135 loss: 0.0005577257950790226
batch 140 loss: 0.0005577652715146542
batch 145 loss: 0.0005576818482950329
batch 150 loss: 0.0005576740019023419
batch 155 loss: 0.0005577385309152305
batch 160 loss: 0.0005576461204327643
batch 165 loss: 0.0005576166673563421
batch 170 loss: 0.0005576840601861476
batch 175 loss: 0.0005576289491727948
batch 180 loss: 0.0005576965399086475
batch 185 loss: 0.0005578304291702807
batch 190 loss: 0.0005577329779043793
batch 195 loss: 0.0005577562376856803
batch 200 loss: 0.0005576851428486407
batch 205 loss: 0.0005576733965426683
batch 210 loss: 0.0005576924653723836
batch 215 loss: 0.0005577408708631992
batch 220 loss: 0.0005576489027589559
batch 225 loss: 0.0005577477510087192
batch 230 loss: 0.0005577774369157851
batch 235 loss: 0.0005576379713602364
batch 240 loss: 0.0005575744318775833
Training Loss: 0.0005576834012269198
Validation Loss: 0.0005576934452013423
Epoch 34:
batch 5 loss: 0.0005577336065471173
batch 10 loss: 0.0005577639094553888
batch 15 loss: 0.0005575945368036627
batch 20 loss: 0.0005576757714152337
batch 25 loss: 0.0005577053059823811
batch 30 loss: 0.0005575783434323967
batch 35 loss: 0.0005576631287112832
batch 40 loss: 0.0005576291354373097
batch 45 loss: 0.0005576623254455626
batch 50 loss: 0.000557699182536453
batch 55 loss: 0.0005577262956649065
batch 60 loss: 0.0005576294846832752
batch 65 loss: 0.0005576977389864623
batch 70 loss: 0.0005577642819844187
batch 75 loss: 0.0005576108116656542
batch 80 loss: 0.0005577263073064387
batch 85 loss: 0.0005577116156928241
batch 90 loss: 0.0005577121977694332
batch 95 loss: 0.0005576780764386058
batch 100 loss: 0.0005577350500971078
batch 105 loss: 0.00055770268663764
batch 110 loss: 0.0005576843395829201
batch 115 loss: 0.0005577143863774836
batch 120 loss: 0.0005576543044298887
batch 125 loss: 0.0005577057250775397
batch 130 loss: 0.000557730719447136
batch 135 loss: 0.0005576763302087784
batch 140 loss: 0.0005575776798650622
batch 145 loss: 0.0005578360054641962
batch 150 loss: 0.0005576760857366025
batch 155 loss: 0.0005575545597821474
batch 160 loss: 0.0005576652125455439
batch 165 loss: 0.0005578533047810197
batch 170 loss: 0.0005575974239036441
batch 175 loss: 0.0005576978204771876
batch 180 loss: 0.0005576197057962417
batch 185 loss: 0.0005577158415690064
batch 190 loss: 0.0005576981930062175
batch 195 loss: 0.0005576472729444503
batch 200 loss: 0.0005576648167334497
batch 205 loss: 0.000557696761097759
batch 210 loss: 0.0005576089373789727
batch 215 loss: 0.0005576712428592145
batch 220 loss: 0.000557648076210171
batch 225 loss: 0.0005576476221904158
batch 230 loss: 0.0005576751194894314
batch 235 loss: 0.0005577812320552766
batch 240 loss: 0.0005577041185460985
Training Loss: 0.0005576833881301961
Validation Loss: 0.0005576934209481503
Epoch 35:
batch 5 loss: 0.0005576316965743899
batch 10 loss: 0.0005577664589509368
batch 15 loss: 0.0005576426745392382
batch 20 loss: 0.0005576407304033637
batch 25 loss: 0.0005576815223321318
batch 30 loss: 0.000557670125272125
batch 35 loss: 0.0005577292758971453
batch 40 loss: 0.0005577214062213898
batch 45 loss: 0.000557754433248192
batch 50 loss: 0.0005575982737354934
batch 55 loss: 0.0005575704504735768
batch 60 loss: 0.00055761793628335
batch 65 loss: 0.0005577630363404751
batch 70 loss: 0.0005576743860729039
batch 75 loss: 0.0005577096017077565
batch 80 loss: 0.0005577164236456155
batch 85 loss: 0.0005575672490522266
batch 90 loss: 0.0005577418021857738
batch 95 loss: 0.0005577011848799885
batch 100 loss: 0.0005576193681918085
batch 105 loss: 0.0005576516618020832
batch 110 loss: 0.0005575912888161838
batch 115 loss: 0.0005576660856604576
batch 120 loss: 0.000557604362256825
batch 125 loss: 0.0005576756084337831
batch 130 loss: 0.0005576038500294089
batch 135 loss: 0.0005577638046815991
batch 140 loss: 0.0005577299860306084
batch 145 loss: 0.0005576811265200377
batch 150 loss: 0.0005576748051680624
batch 155 loss: 0.0005576105671934783
batch 160 loss: 0.000557743851095438
batch 165 loss: 0.0005577154224738479
batch 170 loss: 0.0005577837349846959
batch 175 loss: 0.0005577607080340385
batch 180 loss: 0.0005576676223427058
batch 185 loss: 0.0005576548166573048
batch 190 loss: 0.0005575946997851133
batch 195 loss: 0.0005576955387368798
batch 200 loss: 0.0005577986827120185
batch 205 loss: 0.00055769057944417
batch 210 loss: 0.0005578155629336834
batch 215 loss: 0.0005576191702857614
batch 220 loss: 0.000557631382253021
batch 225 loss: 0.0005576752475462854
batch 230 loss: 0.0005577842588536441
batch 235 loss: 0.0005576827796176076
batch 240 loss: 0.00055771772749722
Training Loss: 0.0005576833951636218
Validation Loss: 0.0005576934015455966
Epoch 36:
batch 5 loss: 0.0005577450268901884
batch 10 loss: 0.0005576959461905062
batch 15 loss: 0.0005576253985054791
batch 20 loss: 0.0005577244563028216
batch 25 loss: 0.0005576359922997654
batch 30 loss: 0.0005575903924182057
batch 35 loss: 0.0005576830124482512
batch 40 loss: 0.0005576787982136012
batch 45 loss: 0.0005577599746175111
batch 50 loss: 0.0005576830357313156
batch 55 loss: 0.0005577331176027656
batch 60 loss: 0.0005577383912168443
batch 65 loss: 0.0005576667841523886
batch 70 loss: 0.0005576462834142148
batch 75 loss: 0.0005577339674346149
batch 80 loss: 0.000557584292255342
batch 85 loss: 0.0005577413481660187
batch 90 loss: 0.0005576883326284587
batch 95 loss: 0.0005576855037361383
batch 100 loss: 0.00055770791368559
batch 105 loss: 0.0005575592396780849
batch 110 loss: 0.0005578157608397305
batch 115 loss: 0.0005576285300776362
batch 120 loss: 0.0005577697651460767
batch 125 loss: 0.0005577359697781503
batch 130 loss: 0.0005576753290370106
batch 135 loss: 0.0005577716627158224
batch 140 loss: 0.000557769788429141
batch 145 loss: 0.0005576849798671901
batch 150 loss: 0.0005576491705141962
batch 155 loss: 0.000557605130597949
batch 160 loss: 0.0005576923373155296
batch 165 loss: 0.0005576156778261066
batch 170 loss: 0.0005576157243922352
batch 175 loss: 0.0005577644915319979
batch 180 loss: 0.0005576226627454162
batch 185 loss: 0.0005577311501838266
batch 190 loss: 0.0005577553645707666
batch 195 loss: 0.0005575876915827393
batch 200 loss: 0.000557554280385375
batch 205 loss: 0.0005575391696766019
batch 210 loss: 0.0005576634430326521
batch 215 loss: 0.0005577747011557222
batch 220 loss: 0.0005576075753197074
batch 225 loss: 0.0005576958297751844
batch 230 loss: 0.0005578150390647352
batch 235 loss: 0.0005577084724791348
batch 240 loss: 0.0005576473660767079
Training Loss: 0.0005576834223271969
Validation Loss: 0.0005576935179609185
Epoch 37:
batch 5 loss: 0.0005576517898589373
batch 10 loss: 0.0005575945135205985
batch 15 loss: 0.0005577066796831787
batch 20 loss: 0.0005575269227847457
batch 25 loss: 0.0005575641407631338
batch 30 loss: 0.0005576776224188506
batch 35 loss: 0.0005576441646553576
batch 40 loss: 0.0005576898925937712
batch 45 loss: 0.0005577378906309605
batch 50 loss: 0.0005576143506914377
batch 55 loss: 0.0005577652831561863
batch 60 loss: 0.0005576710798777639
batch 65 loss: 0.000557709182612598
batch 70 loss: 0.0005576735129579902
batch 75 loss: 0.0005576811730861664
batch 80 loss: 0.0005576856434345246
batch 85 loss: 0.0005576126510277391
batch 90 loss: 0.0005576347466558218
batch 95 loss: 0.0005577232106588781
batch 100 loss: 0.0005576377036049962
batch 105 loss: 0.0005577736650593579
batch 110 loss: 0.0005576784256845713
batch 115 loss: 0.0005578002659603953
batch 120 loss: 0.0005576024064794183
batch 125 loss: 0.0005576909170486033
batch 130 loss: 0.000557627179659903
batch 135 loss: 0.0005575917428359389
batch 140 loss: 0.0005577234784141183
batch 145 loss: 0.0005577693344093859
batch 150 loss: 0.0005576308933086694
batch 155 loss: 0.0005576755618676543
batch 160 loss: 0.0005576717085205019
batch 165 loss: 0.0005575988674536347
batch 170 loss: 0.0005576947121880948
batch 175 loss: 0.0005577682866714894
batch 180 loss: 0.0005576505325734615
batch 185 loss: 0.0005576677154749632
batch 190 loss: 0.0005578291951678693
batch 195 loss: 0.0005577891832217574
batch 200 loss: 0.0005577157950028777
batch 205 loss: 0.0005577795091085136
batch 210 loss: 0.0005576586001552642
batch 215 loss: 0.0005577002069912851
batch 220 loss: 0.0005577143398113549
batch 225 loss: 0.0005575597868300974
batch 230 loss: 0.0005577312665991485
batch 235 loss: 0.0005577521864324808
batch 240 loss: 0.0005577555857598782
Training Loss: 0.0005576834063200901
Validation Loss: 0.0005576934539324915
Epoch 38:
batch 5 loss: 0.0005575360963121056
batch 10 loss: 0.000557698612101376
batch 15 loss: 0.0005577873322181404
batch 20 loss: 0.0005577092641033232
batch 25 loss: 0.0005577519885264337
batch 30 loss: 0.0005576626979745924
batch 35 loss: 0.0005577134434133768
batch 40 loss: 0.0005576289491727948
batch 45 loss: 0.0005577764939516782
batch 50 loss: 0.0005576389259658754
batch 55 loss: 0.0005577770061790943
batch 60 loss: 0.0005576617899350822
batch 65 loss: 0.0005575412651523948
batch 70 loss: 0.0005577189032919705
batch 75 loss: 0.0005578278796747327
batch 80 loss: 0.0005577542935498059
batch 85 loss: 0.0005577305797487498
batch 90 loss: 0.0005576391937211156
batch 95 loss: 0.000557657575700432
batch 100 loss: 0.0005577527801506222
batch 105 loss: 0.0005577838164754212
batch 110 loss: 0.0005575076560489833
batch 115 loss: 0.0005576602765358984
batch 120 loss: 0.000557738181669265
batch 125 loss: 0.0005576341063715518
batch 130 loss: 0.0005576743744313717
batch 135 loss: 0.000557654828298837
batch 140 loss: 0.0005576491123065353
batch 145 loss: 0.0005577413365244865
batch 150 loss: 0.00055770396720618
batch 155 loss: 0.0005576506955549121
batch 160 loss: 0.0005577449453994631
batch 165 loss: 0.0005576419760473072
batch 170 loss: 0.0005576326511800289
batch 175 loss: 0.0005576892290264368
batch 180 loss: 0.0005576813477091491
batch 185 loss: 0.0005576396593824029
batch 190 loss: 0.0005577946547418832
batch 195 loss: 0.0005575789720751346
batch 200 loss: 0.0005576586467213928
batch 205 loss: 0.0005576589028351009
batch 210 loss: 0.0005577327799983322
batch 215 loss: 0.0005576202413067222
batch 220 loss: 0.0005575764924287796
batch 225 loss: 0.000557820382528007
batch 230 loss: 0.0005575524875894189
batch 235 loss: 0.0005577550618909299
batch 240 loss: 0.0005576620576903224
Training Loss: 0.0005576834148087074
Validation Loss: 0.0005576934209481503
Epoch 39:
batch 5 loss: 0.0005577045609243214
batch 10 loss: 0.0005576812312938273
batch 15 loss: 0.0005577197065576911
batch 20 loss: 0.0005577079020440579
batch 25 loss: 0.0005577046191319823
batch 30 loss: 0.0005578463315032423
batch 35 loss: 0.0005577204981818795
batch 40 loss: 0.0005578032229095698
batch 45 loss: 0.0005577592994086445
batch 50 loss: 0.0005577301257289946
batch 55 loss: 0.0005576260737143457
batch 60 loss: 0.000557638390455395
batch 65 loss: 0.0005576879950240255
batch 70 loss: 0.0005577102187089622
batch 75 loss: 0.0005576440715231001
batch 80 loss: 0.0005577044212259352
batch 85 loss: 0.0005577425588853657
batch 90 loss: 0.0005577007657848298
batch 95 loss: 0.0005575916962698102
batch 100 loss: 0.0005576864001341164
batch 105 loss: 0.0005576754221692682
batch 110 loss: 0.0005576675641350449
batch 115 loss: 0.0005576579133048654
batch 120 loss: 0.0005577337462455035
batch 125 loss: 0.0005575952003709972
batch 130 loss: 0.0005576731404289603
batch 135 loss: 0.000557632523123175
batch 140 loss: 0.0005576952709816397
batch 145 loss: 0.000557641254272312
batch 150 loss: 0.0005577027681283653
batch 155 loss: 0.0005576625582762063
batch 160 loss: 0.0005576624884270131
batch 165 loss: 0.0005575390532612801
batch 170 loss: 0.0005577594041824341
batch 175 loss: 0.0005577349103987217
batch 180 loss: 0.0005577048752456904
batch 185 loss: 0.0005577719071879983
batch 190 loss: 0.0005576350842602551
batch 195 loss: 0.0005577882402576506
batch 200 loss: 0.0005575502756983042
batch 205 loss: 0.000557658460456878
batch 210 loss: 0.0005577138043008744
batch 215 loss: 0.0005576835479587317
batch 220 loss: 0.0005576001130975783
batch 225 loss: 0.000557601626496762
batch 230 loss: 0.0005577121861279011
batch 235 loss: 0.0005575639079324901
batch 240 loss: 0.0005576776806265116
Training Loss: 0.0005576834378492397
Validation Loss: 0.0005576933957248306
Epoch 40:
batch 5 loss: 0.0005578179494477809
batch 10 loss: 0.0005576870287768542
batch 15 loss: 0.0005576953757554292
batch 20 loss: 0.0005576957599259913
batch 25 loss: 0.0005576121038757265
batch 30 loss: 0.0005577115924097597
batch 35 loss: 0.0005578804295510054
batch 40 loss: 0.0005576271563768387
batch 45 loss: 0.0005576182738877833
batch 50 loss: 0.000557577854488045
batch 55 loss: 0.0005576905561611057
batch 60 loss: 0.0005577392759732902
batch 65 loss: 0.0005575995310209692
batch 70 loss: 0.0005576766794547439
batch 75 loss: 0.000557582825422287
batch 80 loss: 0.0005576845724135637
batch 85 loss: 0.0005576783209107816
batch 90 loss: 0.0005576938157901168
batch 95 loss: 0.0005577276111580432
batch 100 loss: 0.0005576436873525381
batch 105 loss: 0.000557695934548974
batch 110 loss: 0.0005576917668804526
batch 115 loss: 0.000557685422245413
batch 120 loss: 0.0005577034200541676
batch 125 loss: 0.000557676050812006
batch 130 loss: 0.0005575824645347894
batch 135 loss: 0.0005576884024776519
batch 140 loss: 0.0005577019299380481
batch 145 loss: 0.0005576043738983571
batch 150 loss: 0.0005576661205850542
batch 155 loss: 0.0005577907897531987
batch 160 loss: 0.0005577638745307923
batch 165 loss: 0.000557680253405124
batch 170 loss: 0.0005576368537731468
batch 175 loss: 0.0005576572846621275
batch 180 loss: 0.0005576537689194084
batch 185 loss: 0.0005576861789450049
batch 190 loss: 0.000557672290597111
batch 195 loss: 0.0005577340838499367
batch 200 loss: 0.0005577074945904315
batch 205 loss: 0.0005576156545430422
batch 210 loss: 0.0005577425821684301
batch 215 loss: 0.0005577456089667976
batch 220 loss: 0.000557632849086076
batch 225 loss: 0.0005577606265433132
batch 230 loss: 0.000557750987354666
batch 235 loss: 0.0005576267256401479
batch 240 loss: 0.0005576097872108221
Training Loss: 0.0005576834162638988
Validation Loss: 0.0005576934422909592
Epoch 41:
batch 5 loss: 0.0005576672381721437
batch 10 loss: 0.0005576686235144734
batch 15 loss: 0.0005576018826104701
batch 20 loss: 0.000557643617503345
batch 25 loss: 0.0005575796822085977
batch 30 loss: 0.0005576126975938677
batch 35 loss: 0.0005575847113505006
batch 40 loss: 0.0005576902884058654
batch 45 loss: 0.000557809416204691
batch 50 loss: 0.0005577165400609374
batch 55 loss: 0.0005576526280492544
batch 60 loss: 0.0005577099160291255
batch 65 loss: 0.0005577089032158256
batch 70 loss: 0.0005577184725552798
batch 75 loss: 0.000557781325187534
batch 80 loss: 0.0005577278207056224
batch 85 loss: 0.0005575333023443818
batch 90 loss: 0.0005576858180575073
batch 95 loss: 0.0005576634663157165
batch 100 loss: 0.0005576680414378643
batch 105 loss: 0.0005576273193582893
batch 110 loss: 0.0005577936884947121
batch 115 loss: 0.0005577424075454473
batch 120 loss: 0.0005577021394856275
batch 125 loss: 0.0005576530704274774
batch 130 loss: 0.0005576972151175141
batch 135 loss: 0.0005576835363171994
batch 140 loss: 0.0005576879600994288
batch 145 loss: 0.0005577554926276207
batch 150 loss: 0.0005576428258791566
batch 155 loss: 0.0005576789379119873
batch 160 loss: 0.000557694782037288
batch 165 loss: 0.0005576374474912882
batch 170 loss: 0.0005576517432928085
batch 175 loss: 0.0005577103118412196
batch 180 loss: 0.0005578315583989024
batch 185 loss: 0.0005577570758759975
batch 190 loss: 0.0005576863535679877
batch 195 loss: 0.0005576986586675048
batch 200 loss: 0.0005575535935349763
batch 205 loss: 0.0005577057949267328
batch 210 loss: 0.0005576660623773932
batch 215 loss: 0.0005577011848799885
batch 220 loss: 0.0005576570867560804
batch 225 loss: 0.0005577307776547969
batch 230 loss: 0.0005576774710789323
batch 235 loss: 0.0005575981340371072
batch 240 loss: 0.0005577571340836584
Training Loss: 0.0005576834199018776
Validation Loss: 0.0005576934228884057
Epoch 42:
batch 5 loss: 0.000557727343402803
batch 10 loss: 0.0005576173542067408
batch 15 loss: 0.0005576875526458025
batch 20 loss: 0.0005577485775575042
batch 25 loss: 0.0005575213814154268
batch 30 loss: 0.0005576720926910639
batch 35 loss: 0.0005575026152655482
batch 40 loss: 0.0005576408817432821
batch 45 loss: 0.000557608448434621
batch 50 loss: 0.0005576476454734802
batch 55 loss: 0.0005576754454523325
batch 60 loss: 0.000557678216136992
batch 65 loss: 0.000557620171457529
batch 70 loss: 0.0005575907533057034
batch 75 loss: 0.0005576239782385528
batch 80 loss: 0.0005576821742579341
batch 85 loss: 0.0005578215466812253
batch 90 loss: 0.0005578033276833593
batch 95 loss: 0.0005576818133704364
batch 100 loss: 0.0005577113362960518
batch 105 loss: 0.0005576611845754087
batch 110 loss: 0.0005575699964538216
batch 115 loss: 0.0005576353287324309
batch 120 loss: 0.0005577700096182526
batch 125 loss: 0.0005577119998633861
batch 130 loss: 0.0005577476578764618
batch 135 loss: 0.0005576765281148255
batch 140 loss: 0.0005575895542278886
batch 145 loss: 0.0005577174015343189
batch 150 loss: 0.0005577489966526628
batch 155 loss: 0.0005577978561632335
batch 160 loss: 0.0005577167612500489
batch 165 loss: 0.0005577608477324248
batch 170 loss: 0.0005577979725785553
batch 175 loss: 0.0005577053525485098
batch 180 loss: 0.0005576985073275864
batch 185 loss: 0.0005577367381192743
batch 190 loss: 0.0005577481235377491
batch 195 loss: 0.0005575952352955937
batch 200 loss: 0.0005577178206294775
batch 205 loss: 0.0005576694384217262
batch 210 loss: 0.0005576988332904875
batch 215 loss: 0.0005576435360126197
batch 220 loss: 0.0005576850031502545
batch 225 loss: 0.000557603791821748
batch 230 loss: 0.0005576421390287578
batch 235 loss: 0.0005577793577685952
batch 240 loss: 0.0005577161791734397
Training Loss: 0.0005576834334836652
Validation Loss: 0.0005576934151273841
Epoch 43:
batch 5 loss: 0.0005576738389208913
batch 10 loss: 0.0005576813942752779
batch 15 loss: 0.0005576758878305555
batch 20 loss: 0.0005575796123594046
batch 25 loss: 0.000557637051679194
batch 30 loss: 0.0005577742704190314
batch 35 loss: 0.0005577652365900577
batch 40 loss: 0.0005576834548264742
batch 45 loss: 0.0005576626630499959
batch 50 loss: 0.0005576289491727948
batch 55 loss: 0.0005575472023338079
batch 60 loss: 0.0005578174372203648
batch 65 loss: 0.0005576033378019929
batch 70 loss: 0.000557762396056205
batch 75 loss: 0.0005577115109190345
batch 80 loss: 0.0005577507428824902
batch 85 loss: 0.0005575725808739662
batch 90 loss: 0.0005575749790295958
batch 95 loss: 0.0005576884490437805
batch 100 loss: 0.0005576763651333749
batch 105 loss: 0.0005576785188168287
batch 110 loss: 0.0005576362018473446
batch 115 loss: 0.0005577095202170312
batch 120 loss: 0.000557733466848731
batch 125 loss: 0.0005576489958912135
batch 130 loss: 0.0005576424649916589
batch 135 loss: 0.0005576786119490862
batch 140 loss: 0.0005577259697020053
batch 145 loss: 0.0005576955270953476
batch 150 loss: 0.0005577465170063078
batch 155 loss: 0.0005576127092354
batch 160 loss: 0.0005577277624979615
batch 165 loss: 0.0005576733965426683
batch 170 loss: 0.0005575763992965221
batch 175 loss: 0.0005576807772740722
batch 180 loss: 0.0005576858413405717
batch 185 loss: 0.0005577040603384376
batch 190 loss: 0.000557775900233537
batch 195 loss: 0.0005576546071097254
batch 200 loss: 0.0005577752483077348
batch 205 loss: 0.0005577541887760162
batch 210 loss: 0.0005576981347985566
batch 215 loss: 0.0005576601484790445
batch 220 loss: 0.0005577732226811349
batch 225 loss: 0.0005576459458097816
batch 230 loss: 0.000557619717437774
batch 235 loss: 0.0005577295552939177
batch 240 loss: 0.0005576948053203524
Training Loss: 0.000557683449490772
Validation Loss: 0.0005576934190078948
Epoch 44:
batch 5 loss: 0.000557769846636802
batch 10 loss: 0.0005577166099101305
batch 15 loss: 0.0005577074829488992
batch 20 loss: 0.0005576597759500146
batch 25 loss: 0.0005577356903813779
batch 30 loss: 0.0005576165975071489
batch 35 loss: 0.0005577559932135046
batch 40 loss: 0.0005577823729254305
batch 45 loss: 0.0005577118019573391
batch 50 loss: 0.0005576730938628316
batch 55 loss: 0.0005576168885454535
batch 60 loss: 0.0005576101713813842
batch 65 loss: 0.0005576745723374188
batch 70 loss: 0.0005577991483733058
batch 75 loss: 0.0005577663192525506
batch 80 loss: 0.0005577312549576163
batch 85 loss: 0.0005577253294177353
batch 90 loss: 0.0005577303003519773
batch 95 loss: 0.0005576987052336336
batch 100 loss: 0.0005576080875471234
batch 105 loss: 0.0005576579365879297
batch 110 loss: 0.0005576524301432074
batch 115 loss: 0.0005577604053542018
batch 120 loss: 0.0005575583316385746
batch 125 loss: 0.0005576194496825337
batch 130 loss: 0.0005576555267907679
batch 135 loss: 0.0005577110568992794
batch 140 loss: 0.000557658914476633
batch 145 loss: 0.0005576343974098563
batch 150 loss: 0.0005575709394179285
batch 155 loss: 0.000557706574909389
batch 160 loss: 0.0005576375988312066
batch 165 loss: 0.0005576520343311131
batch 170 loss: 0.000557736773043871
batch 175 loss: 0.0005576326628215611
batch 180 loss: 0.0005576920928433537
batch 185 loss: 0.0005576586700044572
batch 190 loss: 0.000557708041742444
batch 195 loss: 0.0005576760740950704
batch 200 loss: 0.0005576948635280133
batch 205 loss: 0.0005576843162998557
batch 210 loss: 0.0005576763302087784
batch 215 loss: 0.0005577408126555383
batch 220 loss: 0.0005575970048084855
batch 225 loss: 0.0005575840245001018
batch 230 loss: 0.0005577039206400514
batch 235 loss: 0.000557748565915972
batch 240 loss: 0.0005577052012085915
Training Loss: 0.0005576834373641759
Validation Loss: 0.0005576934743051728
Epoch 45:
batch 5 loss: 0.0005575530463829637
batch 10 loss: 0.0005576050258241594
batch 15 loss: 0.0005575822200626135
batch 20 loss: 0.0005577116273343564
batch 25 loss: 0.0005575955146923661
batch 30 loss: 0.0005576367024332285
batch 35 loss: 0.0005576785770244896
batch 40 loss: 0.0005577753414399922
batch 45 loss: 0.0005576886702328921
batch 50 loss: 0.000557704467792064
batch 55 loss: 0.0005576337687671184
batch 60 loss: 0.0005576118128374219
batch 65 loss: 0.0005576588679105044
batch 70 loss: 0.0005577655741944909
batch 75 loss: 0.0005575848976150155
batch 80 loss: 0.0005576996132731438
batch 85 loss: 0.0005577560747042299
batch 90 loss: 0.0005577581003308296
batch 95 loss: 0.0005577431642450392
batch 100 loss: 0.0005576606839895248
batch 105 loss: 0.0005576341762207448
batch 110 loss: 0.0005577810225076973
batch 115 loss: 0.0005577187752351165
batch 120 loss: 0.0005577392759732902
batch 125 loss: 0.0005577010801061988
batch 130 loss: 0.0005577169009484351
batch 135 loss: 0.0005576326278969645
batch 140 loss: 0.0005576565628871322
batch 145 loss: 0.0005576909403316677
batch 150 loss: 0.0005577229429036378
batch 155 loss: 0.0005576733499765396
batch 160 loss: 0.0005577065981924534
batch 165 loss: 0.0005577244446612895
batch 170 loss: 0.0005576339783146978
batch 175 loss: 0.0005576882045716048
batch 180 loss: 0.0005577338393777609
batch 185 loss: 0.0005576922791078687
batch 190 loss: 0.0005577325238846243
batch 195 loss: 0.0005577929550781846
batch 200 loss: 0.0005576322902925313
batch 205 loss: 0.0005576855503022671
batch 210 loss: 0.0005576769704930485
batch 215 loss: 0.0005577317206189036
batch 220 loss: 0.0005575388087891043
batch 225 loss: 0.0005576643627136946
batch 230 loss: 0.0005577333155088127
batch 235 loss: 0.0005576194846071303
batch 240 loss: 0.0005577464355155826
Training Loss: 0.0005576834410021547
Validation Loss: 0.0005576934849765773
Epoch 46:
batch 5 loss: 0.0005576454917900264
batch 10 loss: 0.0005577167845331133
batch 15 loss: 0.0005577150150202215
batch 20 loss: 0.0005577987874858081
batch 25 loss: 0.000557652476709336
batch 30 loss: 0.0005576818948611617
batch 35 loss: 0.0005576917319558561
batch 40 loss: 0.0005576620344072581
batch 45 loss: 0.0005576687632128597
batch 50 loss: 0.0005577016272582114
batch 55 loss: 0.0005577319534495473
batch 60 loss: 0.0005578129785135388
batch 65 loss: 0.0005576343392021954
batch 70 loss: 0.0005576589843258262
batch 75 loss: 0.0005576783209107816
batch 80 loss: 0.0005576623487286269
batch 85 loss: 0.0005575710092671216
batch 90 loss: 0.0005576790659688413
batch 95 loss: 0.0005577224190346896
batch 100 loss: 0.0005577175412327051
batch 105 loss: 0.0005575794959440828
batch 110 loss: 0.0005576911033131182
batch 115 loss: 0.0005577700561843812
batch 120 loss: 0.0005577322095632553
batch 125 loss: 0.0005576045135967433
batch 130 loss: 0.0005576268071308732
batch 135 loss: 0.0005576463649049401
batch 140 loss: 0.0005576710449531674
batch 145 loss: 0.0005576103227213025
batch 150 loss: 0.0005577440955676138
batch 155 loss: 0.0005576718715019525
batch 160 loss: 0.000557625899091363
batch 165 loss: 0.000557558226864785
batch 170 loss: 0.0005576579598709941
batch 175 loss: 0.0005577429081313312
batch 180 loss: 0.0005576925817877054
batch 185 loss: 0.0005577683681622147
batch 190 loss: 0.0005577306728810072
batch 195 loss: 0.0005577084608376026
batch 200 loss: 0.0005576882744207978
batch 205 loss: 0.0005576962605118752
batch 210 loss: 0.0005576446535997092
batch 215 loss: 0.0005576589494012297
batch 220 loss: 0.0005577062605880201
batch 225 loss: 0.0005576513474807143
batch 230 loss: 0.0005577123258262873
batch 235 loss: 0.0005577401607297361
batch 240 loss: 0.0005576704046688974
Training Loss: 0.0005576834410021547
Validation Loss: 0.0005576935868399839
Epoch 47:
batch 5 loss: 0.0005577360978350043
batch 10 loss: 0.0005576139083132148
batch 15 loss: 0.0005575941409915686
batch 20 loss: 0.0005576387047767639
batch 25 loss: 0.0005578018957749009
batch 30 loss: 0.0005576221854425966
batch 35 loss: 0.0005578111973591149
batch 40 loss: 0.0005575513700023293
batch 45 loss: 0.0005575838731601834
batch 50 loss: 0.0005577314528636635
batch 55 loss: 0.0005576402531005442
batch 60 loss: 0.0005577534670010209
batch 65 loss: 0.0005576848983764648
batch 70 loss: 0.0005577115225605667
batch 75 loss: 0.0005576574243605137
batch 80 loss: 0.0005576900090090931
batch 85 loss: 0.0005577497300691903
batch 90 loss: 0.0005577266216278076
batch 95 loss: 0.000557792722247541
batch 100 loss: 0.0005575949209742248
batch 105 loss: 0.0005577082512900234
batch 110 loss: 0.0005577209871262312
batch 115 loss: 0.0005576919415034354
batch 120 loss: 0.0005577272037044168
batch 125 loss: 0.0005577562493272126
batch 130 loss: 0.0005577284726314246
batch 135 loss: 0.0005576814757660032
batch 140 loss: 0.0005576500203460455
batch 145 loss: 0.0005575437447987497
batch 150 loss: 0.0005576287512667477
batch 155 loss: 0.0005577880423516035
batch 160 loss: 0.0005576701136305928
batch 165 loss: 0.0005576748517341912
batch 170 loss: 0.0005576923256739974
batch 175 loss: 0.0005577541189268232
batch 180 loss: 0.0005576341645792127
batch 185 loss: 0.0005576472263783217
batch 190 loss: 0.000557638646569103
batch 195 loss: 0.000557736842893064
batch 200 loss: 0.000557706702966243
batch 205 loss: 0.0005576212424784899
batch 210 loss: 0.0005576888332143426
batch 215 loss: 0.0005576057475991548
batch 220 loss: 0.000557751301676035
batch 225 loss: 0.0005577114527113736
batch 230 loss: 0.0005576682859100401
batch 235 loss: 0.0005576390889473259
batch 240 loss: 0.0005576544557698071
Training Loss: 0.0005576834778670066
Validation Loss: 0.0005576934054261073
Epoch 48:
batch 5 loss: 0.0005577743635512888
batch 10 loss: 0.0005576892639510333
batch 15 loss: 0.0005576105788350106
batch 20 loss: 0.0005577530013397336
batch 25 loss: 0.0005577316274866462
batch 30 loss: 0.000557805085554719
batch 35 loss: 0.000557679811026901
batch 40 loss: 0.0005577336763963103
batch 45 loss: 0.0005577069125138224
batch 50 loss: 0.0005577390431426466
batch 55 loss: 0.0005577166099101305
batch 60 loss: 0.0005577270174399018
batch 65 loss: 0.0005577775067649781
batch 70 loss: 0.0005576268071308732
batch 75 loss: 0.00055758225498721
batch 80 loss: 0.0005576087511144579
batch 85 loss: 0.0005576125811785459
batch 90 loss: 0.0005575706716626882
batch 95 loss: 0.0005577144329436123
batch 100 loss: 0.0005575468414463103
batch 105 loss: 0.0005577487288974225
batch 110 loss: 0.0005576744792051613
batch 115 loss: 0.0005576586001552642
batch 120 loss: 0.0005577531759627163
batch 125 loss: 0.0005576378200203181
batch 130 loss: 0.000557698612101376
batch 135 loss: 0.0005577335366979241
batch 140 loss: 0.0005577478907071054
batch 145 loss: 0.0005576745024882257
batch 150 loss: 0.0005577121628448367
batch 155 loss: 0.0005576592637225985
batch 160 loss: 0.000557831337209791
batch 165 loss: 0.0005576594499871135
batch 170 loss: 0.0005577218485996127
batch 175 loss: 0.0005577723495662212
batch 180 loss: 0.0005577086471021175
batch 185 loss: 0.0005576650262810289
batch 190 loss: 0.0005576435360126197
batch 195 loss: 0.0005576497642323375
batch 200 loss: 0.0005577186006121337
batch 205 loss: 0.0005575909279286861
batch 210 loss: 0.000557679426856339
batch 215 loss: 0.0005575795541517437
batch 220 loss: 0.0005577049450948834
batch 225 loss: 0.0005575689719989896
batch 230 loss: 0.0005576254799962044
batch 235 loss: 0.0005576626863330603
batch 240 loss: 0.0005576171563006938
Training Loss: 0.0005576834441550697
Validation Loss: 0.0005576934820661942
Epoch 49:
batch 5 loss: 0.0005575564689934253
batch 10 loss: 0.0005577721167355776
batch 15 loss: 0.0005575454677455127
batch 20 loss: 0.0005577336531132459
batch 25 loss: 0.000557698227930814
batch 30 loss: 0.0005577153991907835
batch 35 loss: 0.0005576447467319668
batch 40 loss: 0.0005576669471338391
batch 45 loss: 0.000557705550454557
batch 50 loss: 0.0005576405907049775
batch 55 loss: 0.0005576800089329481
batch 60 loss: 0.0005576158175244928
batch 65 loss: 0.0005577382748015225
batch 70 loss: 0.0005576889030635357
batch 75 loss: 0.0005577110685408115
batch 80 loss: 0.0005576527793891728
batch 85 loss: 0.0005577196716330945
batch 90 loss: 0.0005577639094553888
batch 95 loss: 0.0005576511961407959
batch 100 loss: 0.0005576054099947214
batch 105 loss: 0.0005576997878961265
batch 110 loss: 0.0005576810683123768
batch 115 loss: 0.0005576806957833469
batch 120 loss: 0.0005576895666308701
batch 125 loss: 0.0005576414871029556
batch 130 loss: 0.0005577001953497529
batch 135 loss: 0.0005576902767643332
batch 140 loss: 0.0005577532923780381
batch 145 loss: 0.0005577011383138597
batch 150 loss: 0.0005577371921390295
batch 155 loss: 0.0005576573428697884
batch 160 loss: 0.000557689496781677
batch 165 loss: 0.0005577282863669097
batch 170 loss: 0.0005576460156589746
batch 175 loss: 0.000557746144477278
batch 180 loss: 0.0005577208939939737
batch 185 loss: 0.0005577462143264711
batch 190 loss: 0.0005576997646130621
batch 195 loss: 0.0005576371680945158
batch 200 loss: 0.0005576554918661714
batch 205 loss: 0.0005576659576036036
batch 210 loss: 0.0005577579606324435
batch 215 loss: 0.0005577053409069777
batch 220 loss: 0.0005576716735959053
batch 225 loss: 0.0005576656432822347
batch 230 loss: 0.0005576309165917337
batch 235 loss: 0.0005576600320637226
batch 240 loss: 0.0005576400551944971
Training Loss: 0.0005576834439125378
Validation Loss: 0.0005576934772155558
Epoch 50:
batch 5 loss: 0.0005577827920205891
batch 10 loss: 0.0005576727329753339
batch 15 loss: 0.0005576497758738696
batch 20 loss: 0.000557789031881839
batch 25 loss: 0.0005575829767622053
batch 30 loss: 0.0005576305673457682
batch 35 loss: 0.0005577536765486002
batch 40 loss: 0.0005576272844336927
batch 45 loss: 0.0005576032563112676
batch 50 loss: 0.0005576923140324652
batch 55 loss: 0.0005577084259130061
batch 60 loss: 0.0005576091352850199
batch 65 loss: 0.0005576841183938086
batch 70 loss: 0.0005576855270192027
batch 75 loss: 0.0005576922791078687
batch 80 loss: 0.0005577143630944193
batch 85 loss: 0.0005577045725658536
batch 90 loss: 0.0005577978328801692
batch 95 loss: 0.0005576600320637226
batch 100 loss: 0.0005576433497481049
batch 105 loss: 0.0005577203235588967
batch 110 loss: 0.0005576971685513854
batch 115 loss: 0.0005577357020229101
batch 120 loss: 0.000557573523838073
batch 125 loss: 0.0005577297182753682
batch 130 loss: 0.0005576578085310757
batch 135 loss: 0.0005577293690294027
batch 140 loss: 0.0005576947005465627
batch 145 loss: 0.0005577913951128721
batch 150 loss: 0.0005576447467319668
batch 155 loss: 0.0005577101255767047
batch 160 loss: 0.0005576637689955532
batch 165 loss: 0.0005577233037911356
batch 170 loss: 0.0005577010451816022
batch 175 loss: 0.0005576897645369172
batch 180 loss: 0.0005575697403401136
batch 185 loss: 0.0005575410439632833
batch 190 loss: 0.0005577211850322783
batch 195 loss: 0.0005575770745053887
batch 200 loss: 0.0005576465395279228
batch 205 loss: 0.000557660823687911
batch 210 loss: 0.0005577739444561303
batch 215 loss: 0.0005577401374466717
batch 220 loss: 0.0005576326046139001
batch 225 loss: 0.0005576717783696949
batch 230 loss: 0.0005576712661422789
batch 235 loss: 0.0005577254109084606
batch 240 loss: 0.0005577290779910981
Training Loss: 0.0005576834819900493
Validation Loss: 0.0005576935557958981
Epoch 51:
batch 5 loss: 0.000557701475918293
batch 10 loss: 0.0005576781695708632
batch 15 loss: 0.0005577988573350013
batch 20 loss: 0.0005576325464062392
batch 25 loss: 0.0005576725583523512
batch 30 loss: 0.0005576342344284057
batch 35 loss: 0.0005577303119935096
batch 40 loss: 0.0005575719173066318
batch 45 loss: 0.0005577035248279571
batch 50 loss: 0.0005577171221375465
batch 55 loss: 0.0005576959112659097
batch 60 loss: 0.0005576679250225425
batch 65 loss: 0.0005577653762884438
batch 70 loss: 0.000557635654695332
batch 75 loss: 0.0005576125462539494
batch 80 loss: 0.0005577067728154361
batch 85 loss: 0.0005576675641350449
batch 90 loss: 0.0005576425115577877
batch 95 loss: 0.0005577165400609374
batch 100 loss: 0.0005576350726187229
batch 105 loss: 0.0005577261326834559
batch 110 loss: 0.0005575835704803467
batch 115 loss: 0.0005576828494668007
batch 120 loss: 0.0005576612777076662
batch 125 loss: 0.0005575991119258106
batch 130 loss: 0.000557680381461978
batch 135 loss: 0.0005577399162575603
batch 140 loss: 0.000557536690030247
batch 145 loss: 0.0005578150157816708
batch 150 loss: 0.0005576835596002638
batch 155 loss: 0.0005576321855187416
batch 160 loss: 0.0005576725583523512
batch 165 loss: 0.0005577186937443912
batch 170 loss: 0.0005576541530899704
batch 175 loss: 0.0005576500319875777
batch 180 loss: 0.0005576597526669502
batch 185 loss: 0.0005577030126005412
batch 190 loss: 0.0005576772848144174
batch 195 loss: 0.0005576689960435033
batch 200 loss: 0.0005576812080107629
batch 205 loss: 0.0005576521041803062
batch 210 loss: 0.0005576736875809729
batch 215 loss: 0.0005577115342020988
batch 220 loss: 0.0005576932919211686
batch 225 loss: 0.0005577820003964007
batch 230 loss: 0.0005577779607847333
batch 235 loss: 0.0005577679607085883
batch 240 loss: 0.00055773442145437
Training Loss: 0.0005576834570092615
Validation Loss: 0.0005576934355000655
Epoch 52:
batch 5 loss: 0.0005576237221248448
batch 10 loss: 0.0005576797178946435
batch 15 loss: 0.0005576659343205393
batch 20 loss: 0.0005578538752160966
batch 25 loss: 0.0005577020230703056
batch 30 loss: 0.0005576588911935687
batch 35 loss: 0.0005576208932325244
batch 40 loss: 0.000557684269733727
batch 45 loss: 0.0005577096831984818
batch 50 loss: 0.0005576646421104669
batch 55 loss: 0.0005576882162131369
batch 60 loss: 0.0005576711613684892
batch 65 loss: 0.0005576251307502389
batch 70 loss: 0.0005576678784564137
batch 75 loss: 0.0005576980882324278
batch 80 loss: 0.0005575697752647102
batch 85 loss: 0.0005576065159402788
batch 90 loss: 0.000557660055346787
batch 95 loss: 0.0005577156669460237
batch 100 loss: 0.000557770801242441
batch 105 loss: 0.0005576619878411293
batch 110 loss: 0.0005576427443884313
batch 115 loss: 0.0005576609750278294
batch 120 loss: 0.0005576995783485473
batch 125 loss: 0.0005577726289629937
batch 130 loss: 0.00055773543426767
batch 135 loss: 0.0005577635951340198
batch 140 loss: 0.0005576116382144392
batch 145 loss: 0.0005577871575951576
batch 150 loss: 0.0005576502764597536
batch 155 loss: 0.0005577128264121711
batch 160 loss: 0.000557736202608794
batch 165 loss: 0.0005577190779149533
batch 170 loss: 0.0005576370051130653
batch 175 loss: 0.0005576674593612552
batch 180 loss: 0.0005577459931373596
batch 185 loss: 0.0005576991243287921
batch 190 loss: 0.0005576499272137881
batch 195 loss: 0.0005576414056122303
batch 200 loss: 0.000557674583978951
batch 205 loss: 0.0005576653871685267
batch 210 loss: 0.0005576308583840727
batch 215 loss: 0.0005577402072958649
batch 220 loss: 0.0005576042807660997
batch 225 loss: 0.0005577586125582457
batch 230 loss: 0.0005576386000029742
batch 235 loss: 0.0005576932453550398
batch 240 loss: 0.0005576669005677104
Training Loss: 0.0005576834303307502
Validation Loss: 0.0005576935470647489
Epoch 53:
batch 5 loss: 0.0005575732211582363
batch 10 loss: 0.0005577629315666854
batch 15 loss: 0.0005576627096161247
batch 20 loss: 0.0005576449912041426
batch 25 loss: 0.0005575926741585135
batch 30 loss: 0.0005576683906838298
batch 35 loss: 0.0005576280760578811
batch 40 loss: 0.0005576825351454318
batch 45 loss: 0.0005577526753768325
batch 50 loss: 0.000557709636632353
batch 55 loss: 0.0005576514056883752
batch 60 loss: 0.0005577021860517561
batch 65 loss: 0.0005575789604336023
batch 70 loss: 0.0005576245952397585
batch 75 loss: 0.0005576451774686575
batch 80 loss: 0.0005575994960963726
batch 85 loss: 0.0005577020929194987
batch 90 loss: 0.000557829427998513
batch 95 loss: 0.000557695422321558
batch 100 loss: 0.0005575509159825743
batch 105 loss: 0.0005576338153332472
batch 110 loss: 0.0005577316624112427
batch 115 loss: 0.0005575795425102114
batch 120 loss: 0.0005577630479820072
batch 125 loss: 0.0005577805102802813
batch 130 loss: 0.0005575761781074107
batch 135 loss: 0.0005577631760388613
batch 140 loss: 0.0005578290903940797
batch 145 loss: 0.000557695736642927
batch 150 loss: 0.0005577025935053825
batch 155 loss: 0.0005576592055149376
batch 160 loss: 0.0005576703464612365
batch 165 loss: 0.0005576486117206513
batch 170 loss: 0.0005578382639214397
batch 175 loss: 0.0005577071220614016
batch 180 loss: 0.000557698612101376
batch 185 loss: 0.0005577155621722341
batch 190 loss: 0.0005576080060563981
batch 195 loss: 0.0005576960276812315
batch 200 loss: 0.0005576732335612177
batch 205 loss: 0.0005576898227445781
batch 210 loss: 0.0005576952244155109
batch 215 loss: 0.0005577096482738853
batch 220 loss: 0.0005576596362516284
batch 225 loss: 0.0005577195785008371
batch 230 loss: 0.0005577263422310352
batch 235 loss: 0.0005577467731200158
batch 240 loss: 0.0005576314986683428
Training Loss: 0.0005576834664680064
Validation Loss: 0.0005576934228884057
Epoch 54:
batch 5 loss: 0.0005575892748311162
batch 10 loss: 0.0005577485542744398
batch 15 loss: 0.0005577415111474692
batch 20 loss: 0.0005577774019911886
batch 25 loss: 0.0005577014409936965
batch 30 loss: 0.0005576438386924564
batch 35 loss: 0.0005576576571911573
batch 40 loss: 0.0005577732343226671
batch 45 loss: 0.000557648146059364
batch 50 loss: 0.000557740789372474
batch 55 loss: 0.0005576868425123394
batch 60 loss: 0.0005576657829806208
batch 65 loss: 0.0005576097406446934
batch 70 loss: 0.0005576655617915094
batch 75 loss: 0.0005577478441409767
batch 80 loss: 0.0005576698924414814
batch 85 loss: 0.0005577160976827144
batch 90 loss: 0.0005576835246756673
batch 95 loss: 0.0005577031057327986
batch 100 loss: 0.0005576058872975409
batch 105 loss: 0.0005576761905103922
batch 110 loss: 0.0005577196599915624
batch 115 loss: 0.0005576549796387553
batch 120 loss: 0.0005576989846304059
batch 125 loss: 0.0005576420458965004
batch 130 loss: 0.0005576489260420203
batch 135 loss: 0.0005577522912062705
batch 140 loss: 0.0005577197065576911
batch 145 loss: 0.0005576910567469895
batch 150 loss: 0.0005576640251092613
batch 155 loss: 0.0005577136762440205
batch 160 loss: 0.0005576300667598843
batch 165 loss: 0.0005576795898377896
batch 170 loss: 0.0005577856441959739
batch 175 loss: 0.000557626283261925
batch 180 loss: 0.0005576722440309822
batch 185 loss: 0.0005575079936534167
batch 190 loss: 0.0005576358758844435
batch 195 loss: 0.0005577659932896495
batch 200 loss: 0.0005576792173087597
batch 205 loss: 0.0005577195668593049
batch 210 loss: 0.0005576245253905654
batch 215 loss: 0.0005576424649916589
batch 220 loss: 0.0005576246650889516
batch 225 loss: 0.0005577292293310165
batch 230 loss: 0.0005577187868766487
batch 235 loss: 0.0005577664938755334
batch 240 loss: 0.0005576392984949052
Training Loss: 0.0005576834502183677
Validation Loss: 0.0005576934384104485
Epoch 55:
batch 5 loss: 0.0005575785180553794
batch 10 loss: 0.0005577331990934909
batch 15 loss: 0.0005576124414801598
batch 20 loss: 0.0005577984848059714
batch 25 loss: 0.0005578161217272282
batch 30 loss: 0.0005577205447480083
batch 35 loss: 0.0005576671333983541
batch 40 loss: 0.0005577510222792625
batch 45 loss: 0.0005576985655352473
batch 50 loss: 0.0005576774943619967
batch 55 loss: 0.0005576842580921948
batch 60 loss: 0.0005576456198468805
batch 65 loss: 0.0005576952011324465
batch 70 loss: 0.0005576800554990769
batch 75 loss: 0.0005577265284955502
batch 80 loss: 0.0005577498814091086
batch 85 loss: 0.0005577112431637942
batch 90 loss: 0.000557538156863302
batch 95 loss: 0.0005576936411671341
batch 100 loss: 0.000557775900233537
batch 105 loss: 0.0005576671683229506
batch 110 loss: 0.0005577407660894096
batch 115 loss: 0.0005576844559982419
batch 120 loss: 0.0005576528259553015
batch 125 loss: 0.0005576536175794899
batch 130 loss: 0.0005576601135544479
batch 135 loss: 0.000557734479662031
batch 140 loss: 0.000557698484044522
batch 145 loss: 0.0005577161558903754
batch 150 loss: 0.0005576783325523138
batch 155 loss: 0.0005576778668910265
batch 160 loss: 0.0005576549679972232
batch 165 loss: 0.0005575320567004383
batch 170 loss: 0.0005576365860179066
batch 175 loss: 0.000557650055270642
batch 180 loss: 0.0005576911265961826
batch 185 loss: 0.0005576424300670624
batch 190 loss: 0.0005575899849645793
batch 195 loss: 0.0005576693452894688
batch 200 loss: 0.0005576351657509803
batch 205 loss: 0.0005577648873440922
batch 210 loss: 0.0005576588213443756
batch 215 loss: 0.0005577304284088314
batch 220 loss: 0.0005576784606091678
batch 225 loss: 0.0005576972616836429
batch 230 loss: 0.0005577605799771846
batch 235 loss: 0.0005576378898695111
batch 240 loss: 0.0005576579249463975
Training Loss: 0.0005576834635576233
Validation Loss: 0.0005576937440006683
Epoch 56:
batch 5 loss: 0.000557749718427658
batch 10 loss: 0.0005576874362304806
batch 15 loss: 0.000557851861231029
batch 20 loss: 0.0005576927680522204
batch 25 loss: 0.0005577337229624391
batch 30 loss: 0.0005576686467975378
batch 35 loss: 0.000557706505060196
batch 40 loss: 0.0005576762487180531
batch 45 loss: 0.0005575732095167041
batch 50 loss: 0.0005577700561843812
batch 55 loss: 0.0005575910327024758
batch 60 loss: 0.0005577307427302002
batch 65 loss: 0.0005576660856604576
batch 70 loss: 0.0005577575182542204
batch 75 loss: 0.000557621824555099
batch 80 loss: 0.0005576433497481049
batch 85 loss: 0.0005576901254244149
batch 90 loss: 0.0005576532450504601
batch 95 loss: 0.0005576566210947931
batch 100 loss: 0.0005576203926466406
batch 105 loss: 0.0005577631178312004
batch 110 loss: 0.0005577372852712869
batch 115 loss: 0.0005576657946221531
batch 120 loss: 0.0005576416966505349
batch 125 loss: 0.000557705166283995
batch 130 loss: 0.0005577008938416839
batch 135 loss: 0.0005576524883508683
batch 140 loss: 0.0005576115683652461
batch 145 loss: 0.0005575496819801628
batch 150 loss: 0.0005576979368925094
batch 155 loss: 0.0005577491130679846
batch 160 loss: 0.0005576958763413131
batch 165 loss: 0.0005576858296990394
batch 170 loss: 0.0005577125819399952
batch 175 loss: 0.0005577013129368424
batch 180 loss: 0.0005575919523835183
batch 185 loss: 0.0005576156545430422
batch 190 loss: 0.0005576864699833095
batch 195 loss: 0.0005576334195211529
batch 200 loss: 0.0005577141419053078
batch 205 loss: 0.0005577245261520148
batch 210 loss: 0.000557684013620019
batch 215 loss: 0.0005575388669967652
batch 220 loss: 0.0005577403935603797
batch 225 loss: 0.0005577436415478588
batch 230 loss: 0.0005577653762884438
batch 235 loss: 0.000557695934548974
batch 240 loss: 0.0005576601717621088
Training Loss: 0.000557683458706985
Validation Loss: 0.0005576934296792994
Epoch 57:
batch 5 loss: 0.0005576663883402943
batch 10 loss: 0.0005576614872552455
batch 15 loss: 0.0005575910443440079
batch 20 loss: 0.0005577589268796146
batch 25 loss: 0.0005576395662501455
batch 30 loss: 0.0005576745374128222
batch 35 loss: 0.0005575398448854685
batch 40 loss: 0.0005576740601100028
batch 45 loss: 0.0005576922209002078
batch 50 loss: 0.0005576569586992264
batch 55 loss: 0.0005576922907494008
batch 60 loss: 0.0005577567382715643
batch 65 loss: 0.000557640683837235
batch 70 loss: 0.0005578003241680562
batch 75 loss: 0.0005576123832724989
batch 80 loss: 0.0005577298696152865
batch 85 loss: 0.0005577265634201467
batch 90 loss: 0.000557697843760252
batch 95 loss: 0.0005576616968028247
batch 100 loss: 0.0005576176103204489
batch 105 loss: 0.0005576774477958679
batch 110 loss: 0.0005576268653385341
batch 115 loss: 0.0005575940362177789
batch 120 loss: 0.0005576288094744086
batch 125 loss: 0.0005577390780672431
batch 130 loss: 0.0005577610922046006
batch 135 loss: 0.0005575818242505193
batch 140 loss: 0.0005577341420575976
batch 145 loss: 0.0005576194147579372
batch 150 loss: 0.0005577538860961795
batch 155 loss: 0.0005576239433139563
batch 160 loss: 0.0005576750496402383
batch 165 loss: 0.000557741487864405
batch 170 loss: 0.0005576420342549681
batch 175 loss: 0.000557794701308012
batch 180 loss: 0.0005575808230787515
batch 185 loss: 0.0005578150972723961
batch 190 loss: 0.0005576906143687666
batch 195 loss: 0.0005577272153459489
batch 200 loss: 0.0005577208357863128
batch 205 loss: 0.0005577790085226298
batch 210 loss: 0.0005576401716098189
batch 215 loss: 0.0005577307078056037
batch 220 loss: 0.000557622592896223
batch 225 loss: 0.0005577938514761627
batch 230 loss: 0.0005576317082159222
batch 235 loss: 0.0005577064817771316
batch 240 loss: 0.0005576818715780973
Training Loss: 0.0005576834548264742
Validation Loss: 0.0005576934500519808
Epoch 58:
batch 5 loss: 0.0005576453637331724
batch 10 loss: 0.0005577554460614919
batch 15 loss: 0.000557715876493603
batch 20 loss: 0.0005577609292231501
batch 25 loss: 0.0005577478907071054
batch 30 loss: 0.0005577905802056193
batch 35 loss: 0.0005576154682785273
batch 40 loss: 0.0005576397641561925
batch 45 loss: 0.000557626795489341
batch 50 loss: 0.0005577524774707854
batch 55 loss: 0.0005576152936555446
batch 60 loss: 0.0005577358300797641
batch 65 loss: 0.0005575992981903255
batch 70 loss: 0.0005577102419920266
batch 75 loss: 0.0005577392410486936
batch 80 loss: 0.0005576405324973166
batch 85 loss: 0.0005577036878094077
batch 90 loss: 0.0005576370749622583
batch 95 loss: 0.0005577460047788918
batch 100 loss: 0.0005577360978350043
batch 105 loss: 0.0005576855968683958
batch 110 loss: 0.0005575898103415966
batch 115 loss: 0.0005576985888183117
batch 120 loss: 0.0005576701136305928
batch 125 loss: 0.0005576285882852971
batch 130 loss: 0.0005576684488914907
batch 135 loss: 0.0005577164934948087
batch 140 loss: 0.0005577508243732154
batch 145 loss: 0.0005576943745836616
batch 150 loss: 0.0005577332805842162
batch 155 loss: 0.0005577259697020053
batch 160 loss: 0.0005578023265115917
batch 165 loss: 0.000557672360446304
batch 170 loss: 0.000557530706282705
batch 175 loss: 0.0005577451665885746
batch 180 loss: 0.0005577060277573764
batch 185 loss: 0.0005577713949605822
batch 190 loss: 0.0005575523944571614
batch 195 loss: 0.0005576432449743152
batch 200 loss: 0.0005576897994615138
batch 205 loss: 0.0005577328847721219
batch 210 loss: 0.0005576192401349544
batch 215 loss: 0.0005576286232098937
batch 220 loss: 0.0005575127084739506
batch 225 loss: 0.0005576842464506626
batch 230 loss: 0.0005576637573540211
batch 235 loss: 0.0005576407420448959
batch 240 loss: 0.0005577355506829918
Training Loss: 0.0005576834824751131
Validation Loss: 0.0005576935946010053
Epoch 59:
batch 5 loss: 0.0005576811730861664
batch 10 loss: 0.0005576756782829761
batch 15 loss: 0.0005576426978223026
batch 20 loss: 0.0005576280527748168
batch 25 loss: 0.0005577336763963103
batch 30 loss: 0.0005576801369898022
batch 35 loss: 0.0005575973191298544
batch 40 loss: 0.0005577664356678725
batch 45 loss: 0.0005577343632467091
batch 50 loss: 0.0005576022202149033
batch 55 loss: 0.000557725690305233
batch 60 loss: 0.0005577736883424222
batch 65 loss: 0.0005576277966611087
batch 70 loss: 0.0005575843853875995
batch 75 loss: 0.0005575908697210252
batch 80 loss: 0.0005577158182859421
batch 85 loss: 0.0005576706491410733
batch 90 loss: 0.0005577525356784463
batch 95 loss: 0.0005577450501732528
batch 100 loss: 0.0005577377392910421
batch 105 loss: 0.000557674712035805
batch 110 loss: 0.0005575753632001579
batch 115 loss: 0.0005576895666308701
batch 120 loss: 0.0005576198222115636
batch 125 loss: 0.0005577069241553545
batch 130 loss: 0.0005576083669438959
batch 135 loss: 0.0005576427094638348
batch 140 loss: 0.0005576027906499803
batch 145 loss: 0.000557693827431649
batch 150 loss: 0.0005578552838414908
batch 155 loss: 0.0005576758063398302
batch 160 loss: 0.0005577470292337239
batch 165 loss: 0.000557686691172421
batch 170 loss: 0.0005576375406235456
batch 175 loss: 0.0005576223600655794
batch 180 loss: 0.0005576622788794339
batch 185 loss: 0.0005577034549787641
batch 190 loss: 0.0005576124647632241
batch 195 loss: 0.0005577007890678942
batch 200 loss: 0.0005577490897849202
batch 205 loss: 0.0005576533381827176
batch 210 loss: 0.0005577562144026161
batch 215 loss: 0.0005577386589720845
batch 220 loss: 0.000557725562248379
batch 225 loss: 0.0005576833384111524
batch 230 loss: 0.0005576727795414627
batch 235 loss: 0.0005577518604695797
batch 240 loss: 0.0005576908937655389
Training Loss: 0.0005576834477930485
Validation Loss: 0.0005576934374403209
Epoch 60:
batch 5 loss: 0.000557805213611573
batch 10 loss: 0.0005576764699071646
batch 15 loss: 0.0005576610448770225
batch 20 loss: 0.0005577574716880918
batch 25 loss: 0.000557675096206367
batch 30 loss: 0.0005577620933763683
batch 35 loss: 0.0005575948976911605
batch 40 loss: 0.0005576601717621088
batch 45 loss: 0.000557616853620857
batch 50 loss: 0.0005576258641667664
batch 55 loss: 0.0005577002186328173
batch 60 loss: 0.0005576806259341538
batch 65 loss: 0.0005577723379246891
batch 70 loss: 0.0005576880765147507
batch 75 loss: 0.0005578028969466687
batch 80 loss: 0.000557607680093497
batch 85 loss: 0.0005577021394856275
batch 90 loss: 0.0005576809984631836
batch 95 loss: 0.0005577617208473385
batch 100 loss: 0.0005576587398536503
batch 105 loss: 0.0005577471805736422
batch 110 loss: 0.0005576146184466779
batch 115 loss: 0.0005577976582571865
batch 120 loss: 0.0005577538046054542
batch 125 loss: 0.0005576360621489584
batch 130 loss: 0.0005576181109063327
batch 135 loss: 0.0005576485535129904
batch 140 loss: 0.0005576918367296458
batch 145 loss: 0.0005575957708060742
batch 150 loss: 0.0005576172028668225
batch 155 loss: 0.0005577667616307735
batch 160 loss: 0.0005576397175900638
batch 165 loss: 0.0005576341180130839
batch 170 loss: 0.000557549495715648
batch 175 loss: 0.0005576926982030272
batch 180 loss: 0.0005577044095844031
batch 185 loss: 0.0005576734431087971
batch 190 loss: 0.000557796424254775
batch 195 loss: 0.0005577378324232996
batch 200 loss: 0.0005576866096816957
batch 205 loss: 0.0005577012430876494
batch 210 loss: 0.0005576257477514446
batch 215 loss: 0.0005575479473918677
batch 220 loss: 0.0005576877389103174
batch 225 loss: 0.0005575882853008807
batch 230 loss: 0.000557682674843818
batch 235 loss: 0.0005577575066126883
batch 240 loss: 0.0005577233270742
Training Loss: 0.0005576834873257515
Validation Loss: 0.0005576936110931759
Epoch 61:
batch 5 loss: 0.0005576408933848142
batch 10 loss: 0.0005576428258791566
batch 15 loss: 0.0005577057949267328
batch 20 loss: 0.000557679811026901
batch 25 loss: 0.0005576305906288326
batch 30 loss: 0.0005577096249908209
batch 35 loss: 0.000557665666565299
batch 40 loss: 0.000557784317061305
batch 45 loss: 0.0005576544790528715
batch 50 loss: 0.0005575986462645233
batch 55 loss: 0.0005576797761023044
batch 60 loss: 0.0005577043746598064
batch 65 loss: 0.0005576783325523138
batch 70 loss: 0.0005576214171014726
batch 75 loss: 0.0005576882511377334
batch 80 loss: 0.0005577036994509399
batch 85 loss: 0.0005577322677709162
batch 90 loss: 0.0005577102303504944
batch 95 loss: 0.0005577343632467091
batch 100 loss: 0.0005576566094532609
batch 105 loss: 0.0005576747586019337
batch 110 loss: 0.0005576018011197448
batch 115 loss: 0.0005577071802690625
batch 120 loss: 0.0005577099043875932
batch 125 loss: 0.0005576419644057751
batch 130 loss: 0.000557707843836397
batch 135 loss: 0.0005576048861257732
batch 140 loss: 0.0005576884606853128
batch 145 loss: 0.0005576772382482886
batch 150 loss: 0.000557654770091176
batch 155 loss: 0.0005577391013503075
batch 160 loss: 0.0005577142932452261
batch 165 loss: 0.0005576696363277734
batch 170 loss: 0.0005576913477852941
batch 175 loss: 0.0005576579947955907
batch 180 loss: 0.0005576734780333936
batch 185 loss: 0.0005576594965532422
batch 190 loss: 0.0005576654220931232
batch 195 loss: 0.0005576904746703804
batch 200 loss: 0.0005576807889156044
batch 205 loss: 0.0005577316973358392
batch 210 loss: 0.0005576236289925873
batch 215 loss: 0.0005578181822784245
batch 220 loss: 0.0005576758994720876
batch 225 loss: 0.0005576906143687666
batch 230 loss: 0.0005577571457251907
batch 235 loss: 0.0005576661438681185
batch 240 loss: 0.0005577096599154174
Training Loss: 0.0005576834538563465
Validation Loss: 0.0005576934481117253
Epoch 62:
batch 5 loss: 0.0005577667616307735
batch 10 loss: 0.0005576587514951825
batch 15 loss: 0.000557755772024393
batch 20 loss: 0.0005578017910011113
batch 25 loss: 0.0005576027208007873
batch 30 loss: 0.0005576588562689721
batch 35 loss: 0.000557867088355124
batch 40 loss: 0.0005576577736064791
batch 45 loss: 0.0005576883209869266
batch 50 loss: 0.0005577648640610278
batch 55 loss: 0.0005576777039095759
batch 60 loss: 0.0005576518597081304
batch 65 loss: 0.0005577479489147664
batch 70 loss: 0.0005577662377618253
batch 75 loss: 0.000557602639310062
batch 80 loss: 0.0005576744326390326
batch 85 loss: 0.0005576063646003604
batch 90 loss: 0.000557698612101376
batch 95 loss: 0.000557691918220371
batch 100 loss: 0.0005576228257268667
batch 105 loss: 0.0005576469120569527
batch 110 loss: 0.0005576222436502575
batch 115 loss: 0.0005576361552812159
batch 120 loss: 0.0005576998344622552
batch 125 loss: 0.0005576985655352473
batch 130 loss: 0.0005577115342020988
batch 135 loss: 0.0005576550145633518
batch 140 loss: 0.0005576887167990207
batch 145 loss: 0.0005575391929596663
batch 150 loss: 0.0005575961666181683
batch 155 loss: 0.0005577221512794494
batch 160 loss: 0.0005575926858000458
batch 165 loss: 0.0005577507079578936
batch 170 loss: 0.0005576328840106726
batch 175 loss: 0.0005577101837843657
batch 180 loss: 0.0005577247939072549
batch 185 loss: 0.0005576934316195547
batch 190 loss: 0.0005576944444328546
batch 195 loss: 0.0005576680647209287
batch 200 loss: 0.0005577325471676886
batch 205 loss: 0.0005577258765697479
batch 210 loss: 0.0005576043389737606
batch 215 loss: 0.0005575859802775085
batch 220 loss: 0.0005577103816904128
batch 225 loss: 0.0005576518015004694
batch 230 loss: 0.0005577609059400857
batch 235 loss: 0.0005577273317612707
batch 240 loss: 0.0005576597526669502
Training Loss: 0.0005576834550690061
Validation Loss: 0.0005576934539324915
Epoch 63:
batch 5 loss: 0.0005577272851951421
batch 10 loss: 0.0005576154566369951
batch 15 loss: 0.0005576735595241189
batch 20 loss: 0.0005577058764174581
batch 25 loss: 0.0005576845374889672
batch 30 loss: 0.0005576512427069247
batch 35 loss: 0.00055768599268049
batch 40 loss: 0.0005577635020017624
batch 45 loss: 0.0005575617658905685
batch 50 loss: 0.0005577395437285304
batch 55 loss: 0.0005578076583333313
batch 60 loss: 0.0005576902884058654
batch 65 loss: 0.0005575922899879515
batch 70 loss: 0.0005577737814746797
batch 75 loss: 0.0005576137104071677
batch 80 loss: 0.0005577233619987965
batch 85 loss: 0.0005576369701884687
batch 90 loss: 0.0005577641539275646
batch 95 loss: 0.0005576687166467309
batch 100 loss: 0.0005577800562605262
batch 105 loss: 0.0005576327792368829
batch 110 loss: 0.0005576592986471951
batch 115 loss: 0.0005576605093665421
batch 120 loss: 0.0005576206371188164
batch 125 loss: 0.0005575460148975253
batch 130 loss: 0.0005576366907916963
batch 135 loss: 0.0005576508352532983
batch 140 loss: 0.0005577972624450922
batch 145 loss: 0.0005577823845669627
batch 150 loss: 0.0005575344664976
batch 155 loss: 0.0005576390190981329
batch 160 loss: 0.0005577627569437027
batch 165 loss: 0.0005576328956522047
batch 170 loss: 0.000557764817494899
batch 175 loss: 0.0005576153169386088
batch 180 loss: 0.0005576601484790445
batch 185 loss: 0.0005577397183515132
batch 190 loss: 0.0005576947121880948
batch 195 loss: 0.0005576850846409798
batch 200 loss: 0.0005577328731305898
batch 205 loss: 0.0005577007308602333
batch 210 loss: 0.0005576580995693802
batch 215 loss: 0.0005577999283559621
batch 220 loss: 0.0005576675990596414
batch 225 loss: 0.0005577042582444846
batch 230 loss: 0.0005577121279202402
batch 235 loss: 0.0005576256546191872
batch 240 loss: 0.0005576296127401293
Training Loss: 0.0005576834579793891
Validation Loss: 0.0005576934131871288
Epoch 64:
batch 5 loss: 0.0005576007766649127
batch 10 loss: 0.0005577291129156947
batch 15 loss: 0.0005577590432949364
batch 20 loss: 0.0005576258758082986
batch 25 loss: 0.0005577500676736235
batch 30 loss: 0.0005578086827881634
batch 35 loss: 0.0005577488685958087
batch 40 loss: 0.0005576453753747046
batch 45 loss: 0.0005577345029450953
batch 50 loss: 0.0005577466799877584
batch 55 loss: 0.0005577758303843439
batch 60 loss: 0.0005576238501816988
batch 65 loss: 0.0005576100782491266
batch 70 loss: 0.0005576270632445812
batch 75 loss: 0.0005575212882831693
batch 80 loss: 0.0005577808828093111
batch 85 loss: 0.0005576638388447464
batch 90 loss: 0.0005576717900112272
batch 95 loss: 0.0005576898111030459
batch 100 loss: 0.0005575860850512981
batch 105 loss: 0.0005575642571784556
batch 110 loss: 0.0005577062023803592
batch 115 loss: 0.0005576317431405187
batch 120 loss: 0.0005576241761445999
batch 125 loss: 0.0005577071686275303
batch 130 loss: 0.000557652220595628
batch 135 loss: 0.000557604618370533
batch 140 loss: 0.000557698227930814
batch 145 loss: 0.0005576341180130839
batch 150 loss: 0.0005576228955760598
batch 155 loss: 0.0005576451541855932
batch 160 loss: 0.000557833060156554
batch 165 loss: 0.0005576899740844965
batch 170 loss: 0.000557668786495924
batch 175 loss: 0.0005576262949034572
batch 180 loss: 0.0005577597883529961
batch 185 loss: 0.000557593593839556
batch 190 loss: 0.0005577177740633488
batch 195 loss: 0.00055779431713745
batch 200 loss: 0.0005576633615419268
batch 205 loss: 0.0005577102303504944
batch 210 loss: 0.0005577717674896121
batch 215 loss: 0.0005576641764491796
batch 220 loss: 0.0005577176809310914
batch 225 loss: 0.0005576369469054044
batch 230 loss: 0.0005577562842518091
batch 235 loss: 0.0005577088915742934
batch 240 loss: 0.0005577020579949022
Training Loss: 0.000557683443184942
Validation Loss: 0.0005576935140804077
Epoch 65:
batch 5 loss: 0.0005575266433879734
batch 10 loss: 0.0005577033036388457
batch 15 loss: 0.0005575319519266486
batch 20 loss: 0.0005576304742135107
batch 25 loss: 0.0005577260279096663
batch 30 loss: 0.0005577024654485285
batch 35 loss: 0.0005576230003498494
batch 40 loss: 0.0005577128031291068
batch 45 loss: 0.0005576935829594732
batch 50 loss: 0.0005577370757237077
batch 55 loss: 0.0005575229646638036
batch 60 loss: 0.000557702942751348
batch 65 loss: 0.0005577430594712496
batch 70 loss: 0.0005574882379733026
batch 75 loss: 0.0005577390547841787
batch 80 loss: 0.0005576674826443196
batch 85 loss: 0.0005576672498136759
batch 90 loss: 0.0005576890427619219
batch 95 loss: 0.0005577009753324092
batch 100 loss: 0.0005577318137511611
batch 105 loss: 0.0005577055853791535
batch 110 loss: 0.0005576878553256393
batch 115 loss: 0.0005576952942647039
batch 120 loss: 0.0005577706615440547
batch 125 loss: 0.0005577400093898177
batch 130 loss: 0.0005576387397013604
batch 135 loss: 0.0005575810093432665
batch 140 loss: 0.0005576496827416122
batch 145 loss: 0.0005576474708504975
batch 150 loss: 0.0005577299976721406
batch 155 loss: 0.0005577080999501049
batch 160 loss: 0.0005578379379585386
batch 165 loss: 0.0005576519761234522
batch 170 loss: 0.0005575789604336023
batch 175 loss: 0.0005577328382059931
batch 180 loss: 0.0005577244097366929
batch 185 loss: 0.000557652476709336
batch 190 loss: 0.0005577999982051551
batch 195 loss: 0.0005577435134910047
batch 200 loss: 0.0005578461452387274
batch 205 loss: 0.0005577216041274368
batch 210 loss: 0.0005576706258580089
batch 215 loss: 0.0005576832918450236
batch 220 loss: 0.000557710265275091
batch 225 loss: 0.0005576887167990207
batch 230 loss: 0.0005576432566158473
batch 235 loss: 0.000557657575700432
batch 240 loss: 0.0005576672381721437
Training Loss: 0.0005576834456102613
Validation Loss: 0.0005576934694545343
Epoch 66:
batch 5 loss: 0.0005576484836637974
batch 10 loss: 0.0005576122552156448
batch 15 loss: 0.0005576279480010271
batch 20 loss: 0.0005576436058618128
batch 25 loss: 0.0005577037925831973
batch 30 loss: 0.0005576381227001548
batch 35 loss: 0.0005575837334617972
batch 40 loss: 0.0005575547227635979
batch 45 loss: 0.0005576603813096881
batch 50 loss: 0.0005577707313932478
batch 55 loss: 0.0005576404277235269
batch 60 loss: 0.0005575950141064823
batch 65 loss: 0.0005576094379648566
batch 70 loss: 0.0005576460040174425
batch 75 loss: 0.0005577660980634391
batch 80 loss: 0.0005575979710556566
batch 85 loss: 0.0005576707189902663
batch 90 loss: 0.0005576764582656324
batch 95 loss: 0.0005576386698521674
batch 100 loss: 0.0005577163305133581
batch 105 loss: 0.000557708740234375
batch 110 loss: 0.000557686435058713
batch 115 loss: 0.0005577228148467839
batch 120 loss: 0.0005577285774052143
batch 125 loss: 0.0005577617906965315
batch 130 loss: 0.0005577252013608813
batch 135 loss: 0.0005576471332460642
batch 140 loss: 0.0005577059229835868
batch 145 loss: 0.0005576509400270879
batch 150 loss: 0.0005577000323683023
batch 155 loss: 0.0005576731869950891
batch 160 loss: 0.0005577262258157134
batch 165 loss: 0.0005577862495556474
batch 170 loss: 0.0005576475174166262
batch 175 loss: 0.0005577090079896152
batch 180 loss: 0.0005577535717748106
batch 185 loss: 0.000557673699222505
batch 190 loss: 0.0005577880190685391
batch 195 loss: 0.0005576940020546317
batch 200 loss: 0.0005576273193582893
batch 205 loss: 0.000557739450596273
batch 210 loss: 0.0005577113246545195
batch 215 loss: 0.0005576460971496999
batch 220 loss: 0.0005577538977377117
batch 225 loss: 0.0005577516392804682
batch 230 loss: 0.0005578169715590775
batch 235 loss: 0.0005576662486419081
batch 240 loss: 0.0005576013470999896
Training Loss: 0.0005576834223271969
Validation Loss: 0.0005576934190078948
Epoch 67:
batch 5 loss: 0.0005576819996349514
batch 10 loss: 0.0005577495438046753
batch 15 loss: 0.0005577645963057876
batch 20 loss: 0.0005576870520599186
batch 25 loss: 0.0005577682517468929
batch 30 loss: 0.0005576550960540771
batch 35 loss: 0.0005577556788921356
batch 40 loss: 0.0005576557945460081
batch 45 loss: 0.0005576816154643893
batch 50 loss: 0.0005575633840635419
batch 55 loss: 0.000557646166998893
batch 60 loss: 0.0005576937808655202
batch 65 loss: 0.0005576468538492918
batch 70 loss: 0.0005576444906182587
batch 75 loss: 0.0005577021278440952
batch 80 loss: 0.000557808973826468
batch 85 loss: 0.0005575926159508526
batch 90 loss: 0.0005576187744736671
batch 95 loss: 0.0005576373660005629
batch 100 loss: 0.0005577182513661682
batch 105 loss: 0.0005576034658588469
batch 110 loss: 0.0005576833384111524
batch 115 loss: 0.0005576795199885964
batch 120 loss: 0.0005577244097366929
batch 125 loss: 0.0005576874245889485
batch 130 loss: 0.000557719764765352
batch 135 loss: 0.0005576461902819574
batch 140 loss: 0.0005576133960857987
batch 145 loss: 0.0005577065981924534
batch 150 loss: 0.0005577699746936559
batch 155 loss: 0.0005576455383561552
batch 160 loss: 0.0005576377618126572
batch 165 loss: 0.0005575765040703118
batch 170 loss: 0.0005577663308940828
batch 175 loss: 0.0005577206611633301
batch 180 loss: 0.0005576432216912508
batch 185 loss: 0.0005576462368480861
batch 190 loss: 0.0005576996831223368
batch 195 loss: 0.0005576563999056816
batch 200 loss: 0.0005576899275183678
batch 205 loss: 0.0005577097530476749
batch 210 loss: 0.0005576138966716826
batch 215 loss: 0.0005576306721195579
batch 220 loss: 0.0005576564697548747
batch 225 loss: 0.0005577816511504353
batch 230 loss: 0.000557780486997217
batch 235 loss: 0.0005577684030868113
batch 240 loss: 0.0005576737457886338
Training Loss: 0.0005576834133535158
Validation Loss: 0.0005576934151273841
Epoch 68:
batch 5 loss: 0.0005577679723501206
batch 10 loss: 0.0005576472030952573
batch 15 loss: 0.000557668146211654
batch 20 loss: 0.0005576548748649657
batch 25 loss: 0.0005576516734436155
batch 30 loss: 0.0005577203468419611
batch 35 loss: 0.000557699496857822
batch 40 loss: 0.0005577482748776674
batch 45 loss: 0.0005578126758337021
batch 50 loss: 0.0005575986462645233
batch 55 loss: 0.0005577001837082207
batch 60 loss: 0.0005576818482950329
batch 65 loss: 0.000557537458371371
batch 70 loss: 0.0005576555849984288
batch 75 loss: 0.0005577839445322752
batch 80 loss: 0.0005576899624429643
batch 85 loss: 0.0005576638039201498
batch 90 loss: 0.0005577415460720659
batch 95 loss: 0.0005576354102231562
batch 100 loss: 0.0005576583556830883
batch 105 loss: 0.000557851349003613
batch 110 loss: 0.0005576419294811785
batch 115 loss: 0.0005577969714067876
batch 120 loss: 0.000557697075419128
batch 125 loss: 0.0005577528383582831
batch 130 loss: 0.0005575785413384438
batch 135 loss: 0.0005576451192609965
batch 140 loss: 0.0005576721858233213
batch 145 loss: 0.0005576575174927711
batch 150 loss: 0.0005576526746153831
batch 155 loss: 0.000557658530306071
batch 160 loss: 0.0005577102187089622
batch 165 loss: 0.0005577406496740878
batch 170 loss: 0.0005577032687142491
batch 175 loss: 0.0005576802301220596
batch 180 loss: 0.0005576878087595105
batch 185 loss: 0.0005577313364483416
batch 190 loss: 0.0005576092400588096
batch 195 loss: 0.0005576245370320976
batch 200 loss: 0.0005575636634603143
batch 205 loss: 0.0005576457246206701
batch 210 loss: 0.0005577361793257296
batch 215 loss: 0.0005576988332904875
batch 220 loss: 0.0005576670751906932
batch 225 loss: 0.0005576640833169222
batch 230 loss: 0.0005576868075877428
batch 235 loss: 0.0005576689960435033
batch 240 loss: 0.0005576636060141027
Training Loss: 0.000557683424995048
Validation Loss: 0.0005576934160975119
Epoch 69:
batch 5 loss: 0.0005576778319664299
batch 10 loss: 0.0005576862371526658
batch 15 loss: 0.0005576951312832535
batch 20 loss: 0.0005576613359153271
batch 25 loss: 0.0005576298804953695
batch 30 loss: 0.0005575189949013293
batch 35 loss: 0.0005577089497819543
batch 40 loss: 0.0005576298106461763
batch 45 loss: 0.0005576653406023979
batch 50 loss: 0.0005576575873419643
batch 55 loss: 0.0005577212199568748
batch 60 loss: 0.0005577485426329076
batch 65 loss: 0.0005576663534156978
batch 70 loss: 0.0005576366325840354
batch 75 loss: 0.0005577916745096445
batch 80 loss: 0.0005577550968155265
batch 85 loss: 0.0005577622214332223
batch 90 loss: 0.0005577540025115014
batch 95 loss: 0.000557689880952239
batch 100 loss: 0.0005576741648837924
batch 105 loss: 0.0005576285300776362
batch 110 loss: 0.0005576740368269384
batch 115 loss: 0.0005577646195888519
batch 120 loss: 0.000557631382253021
batch 125 loss: 0.0005576689378358424
batch 130 loss: 0.0005576985073275864
batch 135 loss: 0.0005576108349487185
batch 140 loss: 0.0005576591240242124
batch 145 loss: 0.0005576464696787298
batch 150 loss: 0.0005576020921580493
batch 155 loss: 0.0005576241994276643
batch 160 loss: 0.0005577577743679285
batch 165 loss: 0.0005576720926910639
batch 170 loss: 0.0005577759933657944
batch 175 loss: 0.0005577297997660935
batch 180 loss: 0.0005575871793553234
batch 185 loss: 0.000557656493037939
batch 190 loss: 0.000557593209668994
batch 195 loss: 0.0005577257601544261
batch 200 loss: 0.000557681592181325
batch 205 loss: 0.000557737541384995
batch 210 loss: 0.0005576777504757047
batch 215 loss: 0.0005576438503339887
batch 220 loss: 0.0005577218485996127
batch 225 loss: 0.0005575809627771378
batch 230 loss: 0.0005576876341365278
batch 235 loss: 0.0005578074604272842
batch 240 loss: 0.000557826878502965
Training Loss: 0.0005576834051074305
Validation Loss: 0.0005576934316195547
Epoch 70:
batch 5 loss: 0.0005577170406468213
batch 10 loss: 0.0005576472380198538
batch 15 loss: 0.0005576009280048311
batch 20 loss: 0.000557669613044709
batch 25 loss: 0.0005575816612690687
batch 30 loss: 0.0005578123265877366
batch 35 loss: 0.0005577782052569091
batch 40 loss: 0.0005576359224505722
batch 45 loss: 0.0005577078787609935
batch 50 loss: 0.0005577074945904315
batch 55 loss: 0.0005576845491304993
batch 60 loss: 0.0005576518829911947
batch 65 loss: 0.0005577510455623269
batch 70 loss: 0.0005576320574618876
batch 75 loss: 0.0005576943862251937
batch 80 loss: 0.0005576033843681216
batch 85 loss: 0.0005576395778916776
batch 90 loss: 0.0005576130002737046
batch 95 loss: 0.0005577548872679472
batch 100 loss: 0.0005577471223659813
batch 105 loss: 0.0005577435484156012
batch 110 loss: 0.0005576537922024726
batch 115 loss: 0.0005576631636358797
batch 120 loss: 0.0005576791940256953
batch 125 loss: 0.0005577217089012265
batch 130 loss: 0.0005576984607614577
batch 135 loss: 0.0005576104391366244
batch 140 loss: 0.0005577138043008744
batch 145 loss: 0.0005576209863647818
batch 150 loss: 0.0005576839321292937
batch 155 loss: 0.000557718554046005
batch 160 loss: 0.0005576654104515911
batch 165 loss: 0.000557700521312654
batch 170 loss: 0.0005577128962613642
batch 175 loss: 0.0005576787516474724
batch 180 loss: 0.0005577757372520864
batch 185 loss: 0.0005575722432695329
batch 190 loss: 0.000557555619161576
batch 195 loss: 0.0005576972500421107
batch 200 loss: 0.0005577452364377677
batch 205 loss: 0.0005576628027483821
batch 210 loss: 0.0005577487638220191
batch 215 loss: 0.0005576595896854997
batch 220 loss: 0.0005577375297434628
batch 225 loss: 0.0005576333496719599
batch 230 loss: 0.0005577250034548342
batch 235 loss: 0.0005577760166488588
batch 240 loss: 0.0005576203460805118
Training Loss: 0.0005576834344537928
Validation Loss: 0.0005576934549026191
Epoch 71:
batch 5 loss: 0.0005576546653173863
batch 10 loss: 0.0005577479023486376
batch 15 loss: 0.0005576725932769477
batch 20 loss: 0.0005577700096182526
batch 25 loss: 0.0005577318370342255
batch 30 loss: 0.0005575849791057408
batch 35 loss: 0.0005576394614763558
batch 40 loss: 0.0005577236413955688
batch 45 loss: 0.0005577556439675391
batch 50 loss: 0.000557607423979789
batch 55 loss: 0.0005576170864515006
batch 60 loss: 0.0005577070522122085
batch 65 loss: 0.0005577314412221312
batch 70 loss: 0.0005575472256168723
batch 75 loss: 0.000557685736566782
batch 80 loss: 0.000557670125272125
batch 85 loss: 0.0005576526047661901
batch 90 loss: 0.0005577043164521455
batch 95 loss: 0.0005576775642111897
batch 100 loss: 0.0005576315918006003
batch 105 loss: 0.0005576435127295553
batch 110 loss: 0.0005577603355050087
batch 115 loss: 0.0005577233503572642
batch 120 loss: 0.0005577012547291815
batch 125 loss: 0.0005575467948801816
batch 130 loss: 0.0005577177624218165
batch 135 loss: 0.0005577149800956249
batch 140 loss: 0.0005576056777499616
batch 145 loss: 0.0005577640840783715
batch 150 loss: 0.0005576000781729818
batch 155 loss: 0.0005577579257078468
batch 160 loss: 0.0005576365278102457
batch 165 loss: 0.0005576846306212246
batch 170 loss: 0.0005577730596996843
batch 175 loss: 0.0005576637806370855
batch 180 loss: 0.0005577195202931762
batch 185 loss: 0.0005576878087595105
batch 190 loss: 0.0005576468887738883
batch 195 loss: 0.0005578325944952667
batch 200 loss: 0.000557735376060009
batch 205 loss: 0.0005576022784225642
batch 210 loss: 0.0005576812545768917
batch 215 loss: 0.0005576539901085198
batch 220 loss: 0.0005575843155384064
batch 225 loss: 0.0005577615112997592
batch 230 loss: 0.0005577079253271222
batch 235 loss: 0.0005577311618253589
batch 240 loss: 0.0005576526978984475
Training Loss: 0.0005576834162638988
Validation Loss: 0.000557693534453089
Epoch 72:
batch 5 loss: 0.0005576196359470487
batch 10 loss: 0.000557575048878789
batch 15 loss: 0.000557660823687911
batch 20 loss: 0.0005576619179919363
batch 25 loss: 0.0005577933741733431
batch 30 loss: 0.0005576705909334124
batch 35 loss: 0.0005577597883529961
batch 40 loss: 0.0005577769479714334
batch 45 loss: 0.0005577207077294589
batch 50 loss: 0.0005576799274422228
batch 55 loss: 0.0005576873663812876
batch 60 loss: 0.0005576134310103953
batch 65 loss: 0.0005575920455157757
batch 70 loss: 0.0005576178897172213
batch 75 loss: 0.000557713583111763
batch 80 loss: 0.0005576848052442074
batch 85 loss: 0.0005577244563028216
batch 90 loss: 0.0005577050382271409
batch 95 loss: 0.000557654199656099
batch 100 loss: 0.0005577414063736797
batch 105 loss: 0.0005577645031735301
batch 110 loss: 0.0005577078321948647
batch 115 loss: 0.0005576730822212994
batch 120 loss: 0.0005576885305345058
batch 125 loss: 0.0005577551666647196
batch 130 loss: 0.0005576334544457496
batch 135 loss: 0.000557676621247083
batch 140 loss: 0.0005576601019129157
batch 145 loss: 0.0005577325820922852
batch 150 loss: 0.0005575179471634329
batch 155 loss: 0.0005577641422860324
batch 160 loss: 0.0005575944669544697
batch 165 loss: 0.0005575005430728197
batch 170 loss: 0.00055778386304155
batch 175 loss: 0.0005577726522460579
batch 180 loss: 0.0005577594041824341
batch 185 loss: 0.0005576894036494195
batch 190 loss: 0.0005575729650445282
batch 195 loss: 0.0005576697411015629
batch 200 loss: 0.000557662162464112
batch 205 loss: 0.0005576976458542049
batch 210 loss: 0.0005577552248723805
batch 215 loss: 0.0005576579598709941
batch 220 loss: 0.0005577086354605854
batch 225 loss: 0.0005578039912506938
batch 230 loss: 0.0005576984374783933
batch 235 loss: 0.0005576484953053295
batch 240 loss: 0.0005576013005338609
Training Loss: 0.0005576834133535158
Validation Loss: 0.0005576933957248306
Epoch 73:
batch 5 loss: 0.0005576920579187572
batch 10 loss: 0.0005576556781306863
batch 15 loss: 0.0005577017203904688
batch 20 loss: 0.0005577309988439083
batch 25 loss: 0.0005576706491410733
batch 30 loss: 0.0005576376453973353
batch 35 loss: 0.0005577105563133955
batch 40 loss: 0.0005577007890678942
batch 45 loss: 0.0005577808246016502
batch 50 loss: 0.0005576627678237856
batch 55 loss: 0.000557661650236696
batch 60 loss: 0.0005576592637225985
batch 65 loss: 0.0005577536299824715
batch 70 loss: 0.0005576147348619997
batch 75 loss: 0.0005576909286901354
batch 80 loss: 0.0005576722440309822
batch 85 loss: 0.0005577120115049184
batch 90 loss: 0.0005577575182542204
batch 95 loss: 0.0005575979710556566
batch 100 loss: 0.0005576276453211904
batch 105 loss: 0.0005576171446591615
batch 110 loss: 0.000557705166283995
batch 115 loss: 0.0005577904637902975
batch 120 loss: 0.0005577140138484538
batch 125 loss: 0.0005577304400503635
batch 130 loss: 0.0005577356088906527
batch 135 loss: 0.000557619275059551
batch 140 loss: 0.000557701988145709
batch 145 loss: 0.0005576253985054791
batch 150 loss: 0.0005576584837399424
batch 155 loss: 0.0005577107425779104
batch 160 loss: 0.0005577715346589684
batch 165 loss: 0.0005576333263888955
batch 170 loss: 0.0005575917777605355
batch 175 loss: 0.0005576067953370512
batch 180 loss: 0.0005577034899033606
batch 185 loss: 0.0005577417672611773
batch 190 loss: 0.0005577220348641276
batch 195 loss: 0.0005576640833169222
batch 200 loss: 0.0005576752708293497
batch 205 loss: 0.0005577067844569683
batch 210 loss: 0.000557617878075689
batch 215 loss: 0.0005577356205321848
batch 220 loss: 0.0005577844684012234
batch 225 loss: 0.0005576178431510925
batch 230 loss: 0.0005576417897827923
batch 235 loss: 0.0005576637457124888
batch 240 loss: 0.0005576251889578999
Training Loss: 0.0005576834043798347
Validation Loss: 0.0005576937197474763
Epoch 74:
batch 5 loss: 0.0005577870761044323
batch 10 loss: 0.0005577020114287734
batch 15 loss: 0.0005575868184678256
batch 20 loss: 0.000557743664830923
batch 25 loss: 0.0005577607313171029
batch 30 loss: 0.0005577751668170095
batch 35 loss: 0.0005576879251748323
batch 40 loss: 0.0005577075877226889
batch 45 loss: 0.0005577479605562985
batch 50 loss: 0.000557582953479141
batch 55 loss: 0.0005576298106461763
batch 60 loss: 0.0005576664465479553
batch 65 loss: 0.0005576312192715705
batch 70 loss: 0.0005576362833380699
batch 75 loss: 0.0005576975177973509
batch 80 loss: 0.0005577145027928054
batch 85 loss: 0.000557698798365891
batch 90 loss: 0.0005576272844336927
batch 95 loss: 0.0005577452597208321
batch 100 loss: 0.0005577259114943444
batch 105 loss: 0.0005577113130129874
batch 110 loss: 0.0005577267496846616
batch 115 loss: 0.0005578670999966562
batch 120 loss: 0.0005576403113082051
batch 125 loss: 0.0005576907424256206
batch 130 loss: 0.0005577243049629032
batch 135 loss: 0.0005575931281782687
batch 140 loss: 0.0005577734438702465
batch 145 loss: 0.0005575735121965408
batch 150 loss: 0.000557656236924231
batch 155 loss: 0.0005576400086283684
batch 160 loss: 0.0005576301831752062
batch 165 loss: 0.0005576245603151619
batch 170 loss: 0.0005576324416324496
batch 175 loss: 0.0005577324773184955
batch 180 loss: 0.0005576787865720689
batch 185 loss: 0.0005576585419476033
batch 190 loss: 0.0005576814408414065
batch 195 loss: 0.0005576997413299978
batch 200 loss: 0.0005576647236011922
batch 205 loss: 0.0005576244788244366
batch 210 loss: 0.0005576503230258822
batch 215 loss: 0.0005576560040935874
batch 220 loss: 0.0005576512310653925
batch 225 loss: 0.0005577749805524945
batch 230 loss: 0.0005576202413067222
batch 235 loss: 0.0005577341187745332
batch 240 loss: 0.0005576379597187042
Training Loss: 0.0005576834169914946
Validation Loss: 0.0005576933996053413
Epoch 75:
batch 5 loss: 0.0005577165051363408
batch 10 loss: 0.0005577343399636447
batch 15 loss: 0.0005577497533522547
batch 20 loss: 0.0005577168427407742
batch 25 loss: 0.0005576600786298513
batch 30 loss: 0.000557614176068455
batch 35 loss: 0.0005577057250775397
batch 40 loss: 0.000557637948077172
batch 45 loss: 0.0005575907416641712
batch 50 loss: 0.0005577021860517561
batch 55 loss: 0.0005575926275923848
batch 60 loss: 0.0005576706258580089
batch 65 loss: 0.000557689683046192
batch 70 loss: 0.0005577372037805616
batch 75 loss: 0.0005576347233727574
batch 80 loss: 0.0005576368770562112
batch 85 loss: 0.0005577338626608253
batch 90 loss: 0.0005577441072091461
batch 95 loss: 0.000557553255930543
batch 100 loss: 0.000557814200874418
batch 105 loss: 0.000557638006284833
batch 110 loss: 0.0005576926050707698
batch 115 loss: 0.0005576736410148441
batch 120 loss: 0.0005576574010774493
batch 125 loss: 0.0005576226743869483
batch 130 loss: 0.0005576616851612926
batch 135 loss: 0.000557782338000834
batch 140 loss: 0.0005578236188739539
batch 145 loss: 0.0005576888797804713
batch 150 loss: 0.0005576917203143239
batch 155 loss: 0.0005576821742579341
batch 160 loss: 0.0005577916861511766
batch 165 loss: 0.0005576837109401822
batch 170 loss: 0.0005577602423727512
batch 175 loss: 0.0005576747236773372
batch 180 loss: 0.0005575986579060555
batch 185 loss: 0.0005576507770456374
batch 190 loss: 0.0005577638512477279
batch 195 loss: 0.0005576314870268106
batch 200 loss: 0.0005575845134444535
batch 205 loss: 0.0005576398922130466
batch 210 loss: 0.0005577421165071428
batch 215 loss: 0.0005577277741394937
batch 220 loss: 0.00055765068391338
batch 225 loss: 0.0005575852235779166
batch 230 loss: 0.0005577132338657975
batch 235 loss: 0.0005576641648076475
batch 240 loss: 0.0005576908937655389
Training Loss: 0.0005576834133535158
Validation Loss: 0.0005576934005754689
Epoch 76:
batch 5 loss: 0.0005577200092375279
batch 10 loss: 0.0005577502422966063
batch 15 loss: 0.0005576523835770786
batch 20 loss: 0.0005576962139457464
batch 25 loss: 0.0005576630006544292
batch 30 loss: 0.0005576860159635544
batch 35 loss: 0.0005577860167250038
batch 40 loss: 0.0005576788797043263
batch 45 loss: 0.0005577297299169004
batch 50 loss: 0.0005577614298090339
batch 55 loss: 0.0005577310570515692
batch 60 loss: 0.0005577417789027095
batch 65 loss: 0.0005576931638643145
batch 70 loss: 0.0005577701143920421
batch 75 loss: 0.0005575235350988806
batch 80 loss: 0.0005577604635618628
batch 85 loss: 0.0005576378549449146
batch 90 loss: 0.0005576854455284774
batch 95 loss: 0.0005577172501944006
batch 100 loss: 0.0005576699157245457
batch 105 loss: 0.0005576646188274025
batch 110 loss: 0.0005577044910751283
batch 115 loss: 0.0005577085656113922
batch 120 loss: 0.000557573779951781
batch 125 loss: 0.0005576201132498681
batch 130 loss: 0.0005577283329330385
batch 135 loss: 0.0005576697411015629
batch 140 loss: 0.0005575682502239942
batch 145 loss: 0.0005577164934948087
batch 150 loss: 0.0005575723480433226
batch 155 loss: 0.000557623093482107
batch 160 loss: 0.0005576500087045133
batch 165 loss: 0.0005576441413722933
batch 170 loss: 0.0005576904863119125
batch 175 loss: 0.0005577052710577845
batch 180 loss: 0.000557746400590986
batch 185 loss: 0.0005576149094849824
batch 190 loss: 0.0005576736060902476
batch 195 loss: 0.000557650881819427
batch 200 loss: 0.000557556003332138
batch 205 loss: 0.0005577967618592084
batch 210 loss: 0.0005576439667493105
batch 215 loss: 0.0005576505907811224
batch 220 loss: 0.0005576911033131182
batch 225 loss: 0.0005577776231803
batch 230 loss: 0.0005577111034654081
batch 235 loss: 0.0005577281466685235
batch 240 loss: 0.0005576680414378643
Training Loss: 0.0005576834036522389
Validation Loss: 0.0005576934015455966
Epoch 77:
batch 5 loss: 0.0005576987168751657
batch 10 loss: 0.0005577577860094607
batch 15 loss: 0.0005577203235588967
batch 20 loss: 0.0005576941650360823
batch 25 loss: 0.0005575222079642117
batch 30 loss: 0.0005576305207796395
batch 35 loss: 0.0005576818715780973
batch 40 loss: 0.0005576297640800476
batch 45 loss: 0.0005576933617703617
batch 50 loss: 0.0005576766328886151
batch 55 loss: 0.0005576423020102084
batch 60 loss: 0.0005576090305112302
batch 65 loss: 0.000557680637575686
batch 70 loss: 0.000557774503249675
batch 75 loss: 0.0005576362484134734
batch 80 loss: 0.0005576769355684519
batch 85 loss: 0.0005577043280936778
batch 90 loss: 0.000557721930090338
batch 95 loss: 0.0005577210802584886
batch 100 loss: 0.0005575907882303
batch 105 loss: 0.000557830254547298
batch 110 loss: 0.0005578344338573515
batch 115 loss: 0.0005576641880907119
batch 120 loss: 0.0005578029085882008
batch 125 loss: 0.0005576635710895061
batch 130 loss: 0.0005577662959694862
batch 135 loss: 0.0005577725009061396
batch 140 loss: 0.0005576021503657103
batch 145 loss: 0.0005577357020229101
batch 150 loss: 0.0005576315452344716
batch 155 loss: 0.0005576650495640933
batch 160 loss: 0.0005575944320298732
batch 165 loss: 0.0005576874478720129
batch 170 loss: 0.0005576953524723649
batch 175 loss: 0.000557602010667324
batch 180 loss: 0.0005576142924837768
batch 185 loss: 0.0005577407311648131
batch 190 loss: 0.0005577177973464132
batch 195 loss: 0.0005576223018579185
batch 200 loss: 0.0005576836643740535
batch 205 loss: 0.0005577908013947308
batch 210 loss: 0.0005577667965553701
batch 215 loss: 0.0005576165160164237
batch 220 loss: 0.000557696761097759
batch 225 loss: 0.0005576530704274774
batch 230 loss: 0.0005577170522883534
batch 235 loss: 0.0005575743736699224
batch 240 loss: 0.0005575985764153301
Training Loss: 0.0005576834106856646
Validation Loss: 0.0005576934131871288
Epoch 78:
batch 5 loss: 0.0005577263189479708
batch 10 loss: 0.000557666877284646
batch 15 loss: 0.000557758065406233
batch 20 loss: 0.0005576099501922727
batch 25 loss: 0.0005575281800702214
batch 30 loss: 0.0005576028022915124
batch 35 loss: 0.0005576301249675452
batch 40 loss: 0.000557692430447787
batch 45 loss: 0.0005576809402555227
batch 50 loss: 0.0005577179603278637
batch 55 loss: 0.0005576957599259913
batch 60 loss: 0.0005577025469392539
batch 65 loss: 0.0005575807997956872
batch 70 loss: 0.0005577070172876119
batch 75 loss: 0.0005576613591983914
batch 80 loss: 0.0005578155163675546
batch 85 loss: 0.0005577562609687447
batch 90 loss: 0.0005576285650022328
batch 95 loss: 0.0005576424184255302
batch 100 loss: 0.0005576901254244149
batch 105 loss: 0.0005576500203460455
batch 110 loss: 0.0005576477735303343
batch 115 loss: 0.0005577400210313499
batch 120 loss: 0.0005576706840656698
batch 125 loss: 0.0005576049792580306
batch 130 loss: 0.0005578120355494321
batch 135 loss: 0.000557632464915514
batch 140 loss: 0.0005577003583312034
batch 145 loss: 0.0005577549687586725
batch 150 loss: 0.000557647692039609
batch 155 loss: 0.0005576487048529089
batch 160 loss: 0.0005577030358836054
batch 165 loss: 0.0005576801020652055
batch 170 loss: 0.0005576260737143457
batch 175 loss: 0.0005577380768954753
batch 180 loss: 0.0005577133619226515
batch 185 loss: 0.0005577317206189036
batch 190 loss: 0.0005576262017711997
batch 195 loss: 0.0005576282273977995
batch 200 loss: 0.0005576950148679316
batch 205 loss: 0.0005577390315011143
batch 210 loss: 0.0005577735253609716
batch 215 loss: 0.0005576520459726452
batch 220 loss: 0.0005576533498242497
batch 225 loss: 0.000557686761021614
batch 230 loss: 0.0005576568306423724
batch 235 loss: 0.0005577056785114109
batch 240 loss: 0.0005577906966209411
Training Loss: 0.0005576834053499624
Validation Loss: 0.0005576934510221084
Epoch 79:
batch 5 loss: 0.0005576088908128441
batch 10 loss: 0.0005575929884798825
batch 15 loss: 0.0005576713709160685
batch 20 loss: 0.0005576223134994507
batch 25 loss: 0.0005576918716542423
batch 30 loss: 0.0005575984367169439
batch 35 loss: 0.0005576866562478245
batch 40 loss: 0.0005577232339419424
batch 45 loss: 0.0005577010568231344
batch 50 loss: 0.0005577155738137662
batch 55 loss: 0.0005576943163760007
batch 60 loss: 0.0005576061666943133
batch 65 loss: 0.0005575630231760443
batch 70 loss: 0.0005575274699367583
batch 75 loss: 0.0005576849216595292
batch 80 loss: 0.0005576394964009523
batch 85 loss: 0.0005576130119152367
batch 90 loss: 0.000557734863832593
batch 95 loss: 0.0005577219766564667
batch 100 loss: 0.0005577485891990364
batch 105 loss: 0.0005576564581133425
batch 110 loss: 0.0005576768890023232
batch 115 loss: 0.0005575630930252373
batch 120 loss: 0.0005576582625508308
batch 125 loss: 0.0005577656789682806
batch 130 loss: 0.0005576667492277921
batch 135 loss: 0.0005576310097239912
batch 140 loss: 0.000557709694840014
batch 145 loss: 0.0005576952011324465
batch 150 loss: 0.0005578110576607287
batch 155 loss: 0.0005576768191531301
batch 160 loss: 0.000557627622038126
batch 165 loss: 0.0005577846080996096
batch 170 loss: 0.000557709380518645
batch 175 loss: 0.0005577092873863876
batch 180 loss: 0.0005577777745202183
batch 185 loss: 0.0005577612784691155
batch 190 loss: 0.0005576577386818826
batch 195 loss: 0.0005577969248406589
batch 200 loss: 0.0005577682750299573
batch 205 loss: 0.0005576199037022889
batch 210 loss: 0.0005577711039222777
batch 215 loss: 0.0005577237461693585
batch 220 loss: 0.0005576180410571396
batch 225 loss: 0.0005576210911385715
batch 230 loss: 0.0005577258416451514
batch 235 loss: 0.000557697843760252
batch 240 loss: 0.0005577756324782968
Training Loss: 0.0005576834007418559
Validation Loss: 0.0005576933986352135
Epoch 80:
batch 5 loss: 0.0005577053176239133
batch 10 loss: 0.0005575490649789572
batch 15 loss: 0.0005576831405051053
batch 20 loss: 0.0005576897878199816
batch 25 loss: 0.0005577099625952541
batch 30 loss: 0.0005576577968895435
batch 35 loss: 0.00055762710981071
batch 40 loss: 0.0005576623720116913
batch 45 loss: 0.0005575866787694394
batch 50 loss: 0.0005576857132837176
batch 55 loss: 0.0005577095784246921
batch 60 loss: 0.0005576288560405374
batch 65 loss: 0.0005577185307629407
batch 70 loss: 0.0005576794967055321
batch 75 loss: 0.0005576610215939582
batch 80 loss: 0.0005576532217673958
batch 85 loss: 0.0005577529431320727
batch 90 loss: 0.000557740533258766
batch 95 loss: 0.0005576166324317456
batch 100 loss: 0.0005576551309786737
batch 105 loss: 0.0005576678202487528
batch 110 loss: 0.0005577113362960518
batch 115 loss: 0.0005576764116995037
batch 120 loss: 0.000557714665774256
batch 125 loss: 0.0005577538046054542
batch 130 loss: 0.0005577357369475067
batch 135 loss: 0.0005575287854298949
batch 140 loss: 0.0005577609990723431
batch 145 loss: 0.000557751243468374
batch 150 loss: 0.0005577037925831973
batch 155 loss: 0.0005577324773184955
batch 160 loss: 0.000557668786495924
batch 165 loss: 0.0005577514762990177
batch 170 loss: 0.0005576857831329108
batch 175 loss: 0.0005576713709160685
batch 180 loss: 0.000557731743901968
batch 185 loss: 0.0005576420458965004
batch 190 loss: 0.0005577774834819138
batch 195 loss: 0.0005576167488470674
batch 200 loss: 0.0005576938390731811
batch 205 loss: 0.0005577800096943974
batch 210 loss: 0.0005576002527959645
batch 215 loss: 0.0005577275180257857
batch 220 loss: 0.0005576469702646136
batch 225 loss: 0.000557733466848731
batch 230 loss: 0.0005576558643952012
batch 235 loss: 0.0005575744435191154
batch 240 loss: 0.0005577352712862194
Training Loss: 0.0005576833966188133
Validation Loss: 0.0005576935189310461
Epoch 81:
batch 5 loss: 0.0005577409407123924
batch 10 loss: 0.000557620485778898
batch 15 loss: 0.0005577429779805243
batch 20 loss: 0.0005577047471888363
batch 25 loss: 0.0005576339550316334
batch 30 loss: 0.0005577181582339108
batch 35 loss: 0.0005577768199145794
batch 40 loss: 0.000557624944485724
batch 45 loss: 0.0005577158299274743
batch 50 loss: 0.0005576677154749632
batch 55 loss: 0.000557759334333241
batch 60 loss: 0.0005576657713390887
batch 65 loss: 0.0005576820462010801
batch 70 loss: 0.0005576652591116726
batch 75 loss: 0.0005577267496846616
batch 80 loss: 0.0005576338735409081
batch 85 loss: 0.0005576949915848672
batch 90 loss: 0.0005576797644607723
batch 95 loss: 0.0005576555035077035
batch 100 loss: 0.0005577603005804121
batch 105 loss: 0.0005577749107033014
batch 110 loss: 0.0005577751551754772
batch 115 loss: 0.0005577224190346896
batch 120 loss: 0.0005576800787821413
batch 125 loss: 0.0005577372852712869
batch 130 loss: 0.0005576444673351943
batch 135 loss: 0.0005576127558015286
batch 140 loss: 0.0005577528849244118
batch 145 loss: 0.0005576476454734802
batch 150 loss: 0.0005575397168286145
batch 155 loss: 0.0005577284726314246
batch 160 loss: 0.0005576851079240441
batch 165 loss: 0.0005576920812018216
batch 170 loss: 0.0005576944793574512
batch 175 loss: 0.0005576821276918054
batch 180 loss: 0.0005576046532951295
batch 185 loss: 0.0005576932337135077
batch 190 loss: 0.0005578053183853626
batch 195 loss: 0.0005576328025199473
batch 200 loss: 0.0005576257128268481
batch 205 loss: 0.0005576594965532422
batch 210 loss: 0.000557669042609632
batch 215 loss: 0.0005575025570578873
batch 220 loss: 0.0005577363539487123
batch 225 loss: 0.0005576170631684362
batch 230 loss: 0.0005576436058618128
batch 235 loss: 0.0005576532450504601
batch 240 loss: 0.0005577204516157508
Training Loss: 0.0005576834019545156
Validation Loss: 0.0005576934452013423
Epoch 82:
