****************************************************************
*                      SLURM Batch System                      *
*           IN2P3 Computing Centre, Villeurbanne FR            *
****************************************************************
* Date:                 Sun Jun 30 13:33:35 CEST 2024
* Job name:             1200
* Job id:               17512721
* User:                 lkarda
* Account:              lisaf
* Submit host:          ccahm001
* Partition:            gpu
* Quality of service:   gpu
* Nodelist:             ccwgslurm0115
****************************************************************
no change     /sps/lisaf/lkarda/miniconda3/condabin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda
no change     /sps/lisaf/lkarda/miniconda3/bin/conda-env
no change     /sps/lisaf/lkarda/miniconda3/bin/activate
no change     /sps/lisaf/lkarda/miniconda3/bin/deactivate
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.sh
no change     /sps/lisaf/lkarda/miniconda3/etc/fish/conf.d/conda.fish
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/Conda.psm1
no change     /sps/lisaf/lkarda/miniconda3/shell/condabin/conda-hook.ps1
no change     /sps/lisaf/lkarda/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /sps/lisaf/lkarda/miniconda3/etc/profile.d/conda.csh
no change     /pbs/home/l/lkarda/.bashrc
No action taken.
AE_net
# of epochs: 100
# of samples: 24000
Batchsize: 100
# of batches: 240
# of nodes in bottleneck 800
20240630_133439
Epoch 1:
batch 5 loss: 0.015516287367790937
batch 10 loss: 0.002791228052228689
batch 15 loss: 0.0018754550954326987
batch 20 loss: 0.0014909406192600728
batch 25 loss: 0.0012170430971309542
batch 30 loss: 0.0010332595906220377
batch 35 loss: 0.0009096219087950886
batch 40 loss: 0.0008278627879917622
batch 45 loss: 0.00081832364667207
batch 50 loss: 0.0007465247763320804
batch 55 loss: 0.0006996295414865017
batch 60 loss: 0.0006714551360346377
batch 65 loss: 0.000657657312694937
batch 70 loss: 0.0006961002363823354
batch 75 loss: 0.0006824064534157514
batch 80 loss: 0.0006455625174567103
batch 85 loss: 0.000628309475723654
batch 90 loss: 0.0006225453456863761
batch 95 loss: 0.0006217104732058942
batch 100 loss: 0.0006206812104210258
batch 105 loss: 0.000618996110279113
batch 110 loss: 0.0006164520047605038
batch 115 loss: 0.0008426912594586611
batch 120 loss: 0.0007509087678045035
batch 125 loss: 0.0006318361614830792
batch 130 loss: 0.0006257580243982375
batch 135 loss: 0.0006237721536308527
batch 140 loss: 0.0006233671330846847
batch 145 loss: 0.000623102078679949
batch 150 loss: 0.0006227577337995172
batch 155 loss: 0.0006220174487680197
batch 160 loss: 0.0006212872453033924
batch 165 loss: 0.0006203399156220257
batch 170 loss: 0.0006186522543430329
batch 175 loss: 0.0006161644239909946
batch 180 loss: 0.0006325343274511397
batch 185 loss: 0.0006455131690017879
batch 190 loss: 0.0006309961550869048
batch 195 loss: 0.0006178882904350758
batch 200 loss: 0.0006166820065118372
batch 205 loss: 0.0006159847020171582
batch 210 loss: 0.000615331344306469
batch 215 loss: 0.0006145816645585
batch 220 loss: 0.0006140949670225381
batch 225 loss: 0.0006133271264843643
batch 230 loss: 0.0006125107523985207
batch 235 loss: 0.0006117151002399624
batch 240 loss: 0.0006109444191679358
Training Loss: 0.001072975237184437
Validation Loss: 0.0005983984107539678
Epoch 2:
batch 5 loss: 0.0006102499435655772
batch 10 loss: 0.0006089774658903479
batch 15 loss: 0.0006070384988561273
batch 20 loss: 0.0006051104748621583
batch 25 loss: 0.0007734911167062819
batch 30 loss: 0.0006896036444231868
batch 35 loss: 0.0006244831718504428
batch 40 loss: 0.0006200749310664832
batch 45 loss: 0.0006176935159601272
batch 50 loss: 0.0006165740080177784
batch 55 loss: 0.0006157042225822806
batch 60 loss: 0.0006145934225060046
batch 65 loss: 0.0006131045520305634
batch 70 loss: 0.0006121486541815103
batch 75 loss: 0.0006113860639743507
batch 80 loss: 0.0006107736960984766
batch 85 loss: 0.0006100322352722287
batch 90 loss: 0.0006093410192988813
batch 95 loss: 0.000608358159661293
batch 100 loss: 0.0006069839117117226
batch 105 loss: 0.0006057705380953848
batch 110 loss: 0.000604853534605354
batch 115 loss: 0.0006029213545843959
batch 120 loss: 0.0006020773435011506
batch 125 loss: 0.0005997792934067548
batch 130 loss: 0.000605711922980845
batch 135 loss: 0.0006043744622729718
batch 140 loss: 0.0006037002429366111
batch 145 loss: 0.0006030753720551729
batch 150 loss: 0.0006024961592629551
batch 155 loss: 0.0006019148626364767
batch 160 loss: 0.000601006078068167
batch 165 loss: 0.0006001076195389033
batch 170 loss: 0.0005998198292218148
batch 175 loss: 0.0005990188335999846
batch 180 loss: 0.0005977114080451429
batch 185 loss: 0.0005958862602710724
batch 190 loss: 0.0011950755142606795
batch 195 loss: 0.00210390305146575
batch 200 loss: 0.0012759611010551453
batch 205 loss: 0.0010685392189770937
batch 210 loss: 0.0009228166076354682
batch 215 loss: 0.0008351999917067587
batch 220 loss: 0.0007668988546356559
batch 225 loss: 0.0007138326531276107
batch 230 loss: 0.0006644473760388792
batch 235 loss: 0.000628075748682022
batch 240 loss: 0.000615371426101774
Training Loss: 0.0006980431118184545
Validation Loss: 0.0006079514287800218
Epoch 3:
batch 5 loss: 0.0005976487067528069
batch 10 loss: 0.0005887159146368504
batch 15 loss: 0.000586656469386071
batch 20 loss: 0.0005860773148015141
batch 25 loss: 0.0005856313277035951
batch 30 loss: 0.000584941531997174
batch 35 loss: 0.0005843657767400145
batch 40 loss: 0.0005838879500515759
batch 45 loss: 0.000583536911290139
batch 50 loss: 0.0005830619833432138
batch 55 loss: 0.0005825720261782407
batch 60 loss: 0.0005821537226438523
batch 65 loss: 0.0005819504265673458
batch 70 loss: 0.0005814263480715453
batch 75 loss: 0.0005811106413602829
batch 80 loss: 0.000580709206406027
batch 85 loss: 0.0005804023356176912
batch 90 loss: 0.0005799887119792401
batch 95 loss: 0.0005797237041406334
batch 100 loss: 0.0005794349825009703
batch 105 loss: 0.000579040648881346
batch 110 loss: 0.0005787862348370254
batch 115 loss: 0.0005783922621048987
batch 120 loss: 0.0005781383253633976
batch 125 loss: 0.0005777514190413058
batch 130 loss: 0.0005775726633146406
batch 135 loss: 0.0005771836498752236
batch 140 loss: 0.00057682377519086
batch 145 loss: 0.0005766469286754728
batch 150 loss: 0.0005763667519204319
batch 155 loss: 0.0005760694504715503
batch 160 loss: 0.0005758741870522499
batch 165 loss: 0.0005756364320404827
batch 170 loss: 0.0005754150799475611
batch 175 loss: 0.000575102330185473
batch 180 loss: 0.0005749412928707897
batch 185 loss: 0.0005747369257733225
batch 190 loss: 0.0005744754103943706
batch 195 loss: 0.0005740633234381676
batch 200 loss: 0.0005740031367167831
batch 205 loss: 0.0005738822976127267
batch 210 loss: 0.0005736675346270203
batch 215 loss: 0.000573326286394149
batch 220 loss: 0.000573227321729064
batch 225 loss: 0.0005728805088438093
batch 230 loss: 0.0005727608688175679
batch 235 loss: 0.0005724778748117387
batch 240 loss: 0.0005724372807890177
Training Loss: 0.000578868254039359
Validation Loss: 0.0005710261505252371
Epoch 4:
batch 5 loss: 0.0005721853813156486
batch 10 loss: 0.000576005270704627
batch 15 loss: 0.0005846725427545608
batch 20 loss: 0.0005897680995985866
batch 25 loss: 0.0005731236888095737
batch 30 loss: 0.0005715477047488094
batch 35 loss: 0.0005717952153645456
batch 40 loss: 0.0005713949445635081
batch 45 loss: 0.0005713341175578535
batch 50 loss: 0.0005708627984859049
batch 55 loss: 0.0005706477095372975
batch 60 loss: 0.0005704255774617196
batch 65 loss: 0.0005702301044948399
batch 70 loss: 0.0005698924534954131
batch 75 loss: 0.000569839810486883
batch 80 loss: 0.0005697569111362099
batch 85 loss: 0.000569618260487914
batch 90 loss: 0.0005692787235602736
batch 95 loss: 0.000569368596188724
batch 100 loss: 0.0005692587234079838
batch 105 loss: 0.0005690084537491203
batch 110 loss: 0.0005689895828254521
batch 115 loss: 0.000568796822335571
batch 120 loss: 0.0005687175318598748
batch 125 loss: 0.000568456738255918
batch 130 loss: 0.0005683817667886615
batch 135 loss: 0.0005683857016265392
batch 140 loss: 0.0005684070521965623
batch 145 loss: 0.0005681647919118405
batch 150 loss: 0.0005680329399183392
batch 155 loss: 0.0005679776892066002
batch 160 loss: 0.0005679416586644948
batch 165 loss: 0.0005677144625224173
batch 170 loss: 0.0005676797009073198
batch 175 loss: 0.0005674816202372313
batch 180 loss: 0.0005674963467754424
batch 185 loss: 0.0005672990460880101
batch 190 loss: 0.000567361491266638
batch 195 loss: 0.0005674112355336547
batch 200 loss: 0.0005672167288139462
batch 205 loss: 0.00056733344681561
batch 210 loss: 0.0005671814898960293
batch 215 loss: 0.000567166623659432
batch 220 loss: 0.0005670787766575813
batch 225 loss: 0.0005672437022440135
batch 230 loss: 0.0005669423029758036
batch 235 loss: 0.0005670040962286293
batch 240 loss: 0.000566999486181885
Training Loss: 0.0005697682900063228
Validation Loss: 0.0005665408622007817
Epoch 5:
batch 5 loss: 0.0005670033744536341
batch 10 loss: 0.0005669108359143138
batch 15 loss: 0.0005668508121743799
batch 20 loss: 0.00056671102065593
batch 25 loss: 0.0005666707758791745
batch 30 loss: 0.0005666436161845922
batch 35 loss: 0.0005665207281708717
batch 40 loss: 0.0005667700432240963
batch 45 loss: 0.0017198120127432047
batch 50 loss: 0.0014370378805324436
batch 55 loss: 0.001251435815356672
batch 60 loss: 0.0010550443315878511
batch 65 loss: 0.0008927869377657772
batch 70 loss: 0.0007718269596807658
batch 75 loss: 0.0007087838137522339
batch 80 loss: 0.000661176978610456
batch 85 loss: 0.0006175699993036688
batch 90 loss: 0.0006018282962031663
batch 95 loss: 0.0006027382449246943
batch 100 loss: 0.0005887765088118613
batch 105 loss: 0.0005844140774570406
batch 110 loss: 0.0005835501942783594
batch 115 loss: 0.0005829791422002018
batch 120 loss: 0.0005821381695568561
batch 125 loss: 0.0005806279485113918
batch 130 loss: 0.0005787991802208125
batch 135 loss: 0.0005757996463216841
batch 140 loss: 0.0005717523978091777
batch 145 loss: 0.0005697341402992606
batch 150 loss: 0.0005693203769624233
batch 155 loss: 0.0005692946491762996
batch 160 loss: 0.0005690259044058621
batch 165 loss: 0.0005689629819244146
batch 170 loss: 0.000568834610749036
batch 175 loss: 0.0005686031654477119
batch 180 loss: 0.0005684929667040706
batch 185 loss: 0.0005683641065843403
batch 190 loss: 0.0005682123475708068
batch 195 loss: 0.0005681570735760033
batch 200 loss: 0.0005679441266693175
batch 205 loss: 0.0005678418441675603
batch 210 loss: 0.0005678494577296078
batch 215 loss: 0.0005675201653502881
batch 220 loss: 0.0005674033309333026
batch 225 loss: 0.0005673371953889728
batch 230 loss: 0.0005672812345437706
batch 235 loss: 0.0005671708495356143
batch 240 loss: 0.000567148916888982
Training Loss: 0.0006551553997269366
Validation Loss: 0.0005664283322403208
Epoch 6:
batch 5 loss: 0.000566988019272685
batch 10 loss: 0.0005667520104907453
batch 15 loss: 0.0005667885881848634
batch 20 loss: 0.0005667178425937891
batch 25 loss: 0.0005665447446517646
batch 30 loss: 0.0005666022771038115
batch 35 loss: 0.0005665792617946863
batch 40 loss: 0.0005664012278430164
batch 45 loss: 0.0005663384334184229
batch 50 loss: 0.0005663401214405894
batch 55 loss: 0.0005663388525135815
batch 60 loss: 0.0005661735427565873
batch 65 loss: 0.0005660702707245945
batch 70 loss: 0.0005659231333993376
batch 75 loss: 0.0005659156013280153
batch 80 loss: 0.0005659593618474901
batch 85 loss: 0.0005657278001308441
batch 90 loss: 0.000565813563298434
batch 95 loss: 0.0005656483583152295
batch 100 loss: 0.0005656944937072695
batch 105 loss: 0.0005655250162817538
batch 110 loss: 0.0005654889275319875
batch 115 loss: 0.0005653599044308066
batch 120 loss: 0.0005653447355143726
batch 125 loss: 0.0005652322084642947
batch 130 loss: 0.0005651755258440971
batch 135 loss: 0.0005651089595630765
batch 140 loss: 0.0005652726627886296
batch 145 loss: 0.0005650346865877509
batch 150 loss: 0.0005649755126796663
batch 155 loss: 0.0005650697741657496
batch 160 loss: 0.0005649425787851214
batch 165 loss: 0.0005648845224641263
batch 170 loss: 0.0005648006685078144
batch 175 loss: 0.0005647676647640764
batch 180 loss: 0.000564648606814444
batch 185 loss: 0.0005647278507240116
batch 190 loss: 0.0005646286415867507
batch 195 loss: 0.0005647139740176498
batch 200 loss: 0.0005645300378091633
batch 205 loss: 0.0005644504097290337
batch 210 loss: 0.0005644699558615685
batch 215 loss: 0.0005645152064971626
batch 220 loss: 0.0005643269163556397
batch 225 loss: 0.0005643258569762111
batch 230 loss: 0.0005642685922794044
batch 235 loss: 0.0005642492673359811
batch 240 loss: 0.0005641294061206281
Training Loss: 0.0005654226161520153
Validation Loss: 0.000563723687082529
Epoch 7:
batch 5 loss: 0.0005640818155370653
batch 10 loss: 0.0005641247727908194
batch 15 loss: 0.0005640553659759462
batch 20 loss: 0.0005639621755108237
batch 25 loss: 0.0005640030023641885
batch 30 loss: 0.0005639377748593688
batch 35 loss: 0.0005638902424834668
batch 40 loss: 0.0005638845846988261
batch 45 loss: 0.0005637987982481718
batch 50 loss: 0.0005637819063849747
batch 55 loss: 0.0005637351074256003
batch 60 loss: 0.0005636595189571381
batch 65 loss: 0.0005636198562569916
batch 70 loss: 0.0005636213812977075
batch 75 loss: 0.000563516840338707
batch 80 loss: 0.0005635245353914798
batch 85 loss: 0.0005636441754177212
batch 90 loss: 0.0005635346402414143
batch 95 loss: 0.0005635352339595556
batch 100 loss: 0.0005634247907437384
batch 105 loss: 0.0005635207053273916
batch 110 loss: 0.0005633964901790023
batch 115 loss: 0.0005635586800053716
batch 120 loss: 0.0005634237895719707
batch 125 loss: 0.0005633878405205906
batch 130 loss: 0.0005633569904603064
batch 135 loss: 0.0005633146036416292
batch 140 loss: 0.0005633055581711233
batch 145 loss: 0.0005632369196973741
batch 150 loss: 0.0005631912616081535
batch 155 loss: 0.0005631679319776595
batch 160 loss: 0.0005630972445942461
batch 165 loss: 0.0005630291067063809
batch 170 loss: 0.0005630175233818591
batch 175 loss: 0.0005629854742437602
batch 180 loss: 0.0005630322848446667
batch 185 loss: 0.0005629298859275878
batch 190 loss: 0.0012354234699159861
batch 195 loss: 0.0009612270863726735
batch 200 loss: 0.0008410973707213998
batch 205 loss: 0.0007811143645085394
batch 210 loss: 0.0007135635591112077
batch 215 loss: 0.0008178489399142564
batch 220 loss: 0.0007060066796839237
batch 225 loss: 0.0006786814075894654
batch 230 loss: 0.000660995882935822
batch 235 loss: 0.0006558280671015382
batch 240 loss: 0.0006526130251586437
Training Loss: 0.0006157226804740882
Validation Loss: 0.0006170037765211115
Epoch 8:
batch 5 loss: 0.0006454814574681222
batch 10 loss: 0.0006349961506202817
batch 15 loss: 0.0006265623844228685
batch 20 loss: 0.0006154832663014531
batch 25 loss: 0.0006373716867528855
batch 30 loss: 0.0006765350815840065
batch 35 loss: 0.0006271381280384958
batch 40 loss: 0.0006223429692909122
batch 45 loss: 0.0006186280050314962
batch 50 loss: 0.0006145678809843958
batch 55 loss: 0.0006103688618168234
batch 60 loss: 0.0006033804034814239
batch 65 loss: 0.0005971904844045639
batch 70 loss: 0.0005946614779531956
batch 75 loss: 0.0005912098102271556
batch 80 loss: 0.0005868308595381677
batch 85 loss: 0.0005805124063044786
batch 90 loss: 0.0005731339915655553
batch 95 loss: 0.0005720934714190662
batch 100 loss: 0.0005718372529372572
batch 105 loss: 0.0005715955630876124
batch 110 loss: 0.0005707168485969305
batch 115 loss: 0.0005705660092644394
batch 120 loss: 0.0005695217987522483
batch 125 loss: 0.0005690898280590773
batch 130 loss: 0.0005688226548954845
batch 135 loss: 0.0005687382654286921
batch 140 loss: 0.0005684459349140525
batch 145 loss: 0.0005682623130269348
batch 150 loss: 0.0005677072331309319
batch 155 loss: 0.0005674842395819724
batch 160 loss: 0.0005673658102750778
batch 165 loss: 0.0005672361236065626
batch 170 loss: 0.0005677971756085753
batch 175 loss: 0.000888848677277565
batch 180 loss: 0.0006383081199601293
batch 185 loss: 0.0006123786326497794
batch 190 loss: 0.0005991757265292108
batch 195 loss: 0.00059490135172382
batch 200 loss: 0.000588457693811506
batch 205 loss: 0.0005846332060173154
batch 210 loss: 0.000582289241719991
batch 215 loss: 0.0005797573481686414
batch 220 loss: 0.0005778209189884365
batch 225 loss: 0.0005763319903053343
batch 230 loss: 0.0005755026126280427
batch 235 loss: 0.000575368432328105
batch 240 loss: 0.0005746479262597858
Training Loss: 0.0005981687022237263
Validation Loss: 0.000571331325530385
Epoch 9:
batch 5 loss: 0.000574876507744193
batch 10 loss: 0.0005726922070607543
batch 15 loss: 0.0005708400974981486
batch 20 loss: 0.000569819170050323
batch 25 loss: 0.0005689309095032513
batch 30 loss: 0.0005680882954038679
batch 35 loss: 0.0005682127317413688
batch 40 loss: 0.0005681283655576408
batch 45 loss: 0.0005678707035258413
batch 50 loss: 0.000566865736618638
batch 55 loss: 0.000566989800427109
batch 60 loss: 0.0005664134398102761
batch 65 loss: 0.0005661791539750994
batch 70 loss: 0.000566570635419339
batch 75 loss: 0.0005656866356730461
batch 80 loss: 0.0005654431297443807
batch 85 loss: 0.0005665802047587932
batch 90 loss: 0.0005671142949722707
batch 95 loss: 0.000567105389200151
batch 100 loss: 0.000566607981454581
batch 105 loss: 0.0005661769304424524
batch 110 loss: 0.000565636670216918
batch 115 loss: 0.0005662889452651143
batch 120 loss: 0.0005654737586155533
batch 125 loss: 0.0005649368278682232
batch 130 loss: 0.0005649002734571696
batch 135 loss: 0.0005649166996590793
batch 140 loss: 0.0005644562072120607
batch 145 loss: 0.0005648041376844048
batch 150 loss: 0.0005642675096169114
batch 155 loss: 0.0005643143085762858
batch 160 loss: 0.0005641202325932682
batch 165 loss: 0.000563749810680747
batch 170 loss: 0.0005641004187054932
batch 175 loss: 0.0005651666899211705
batch 180 loss: 0.0005642217118293047
batch 185 loss: 0.0005636426154524088
batch 190 loss: 0.0005635732202790677
batch 195 loss: 0.0005639090086333454
batch 200 loss: 0.0005648781312629581
batch 205 loss: 0.0005645921919494867
batch 210 loss: 0.0005635021603666246
batch 215 loss: 0.0005642083124257624
batch 220 loss: 0.0005640660063363611
batch 225 loss: 0.0005635331966914236
batch 230 loss: 0.0005633110762573778
batch 235 loss: 0.0005632932181470097
batch 240 loss: 0.0005632901331409812
Training Loss: 0.0005659238706963758
Validation Loss: 0.0005622284797330697
Epoch 10:
batch 5 loss: 0.0005630263709463179
batch 10 loss: 0.0005635329871438444
batch 15 loss: 0.0005626359023153781
batch 20 loss: 0.0005625526653602719
batch 25 loss: 0.0005630100378766656
batch 30 loss: 0.0005631078733131289
batch 35 loss: 0.0005631974083371461
batch 40 loss: 0.0005627716542221606
batch 45 loss: 0.0005634237197227776
batch 50 loss: 0.0005623415112495422
batch 55 loss: 0.0005619122181087733
batch 60 loss: 0.0005625407095067203
batch 65 loss: 0.0005625700345262885
batch 70 loss: 0.0005629441817291081
batch 75 loss: 0.0005628114915452898
batch 80 loss: 0.0005627249483950436
batch 85 loss: 0.0005624651093967259
batch 90 loss: 0.0005630672676488757
batch 95 loss: 0.0005630237283185124
batch 100 loss: 0.000562479195650667
batch 105 loss: 0.0005627899197861552
batch 110 loss: 0.0005623693927191198
batch 115 loss: 0.0005622773431241512
batch 120 loss: 0.000562417006585747
batch 125 loss: 0.0005620874697342515
batch 130 loss: 0.0005619788076728583
batch 135 loss: 0.0005619405535981059
batch 140 loss: 0.0005618053488433361
batch 145 loss: 0.0005625564372166991
batch 150 loss: 0.0005625087884254754
batch 155 loss: 0.0005625628749839961
batch 160 loss: 0.0005623119184747338
batch 165 loss: 0.0005621483549475669
batch 170 loss: 0.000561763031873852
batch 175 loss: 0.0005615982227027416
batch 180 loss: 0.000561784312594682
batch 185 loss: 0.0005620036972686648
batch 190 loss: 0.000561807700432837
batch 195 loss: 0.0005615841131657362
batch 200 loss: 0.0005614570225588977
batch 205 loss: 0.0005616956856101752
batch 210 loss: 0.0005623936769552529
batch 215 loss: 0.0005621330114081502
batch 220 loss: 0.0005626244354061782
batch 225 loss: 0.0005621102638542653
batch 230 loss: 0.000562257890123874
batch 235 loss: 0.0005622493918053806
batch 240 loss: 0.0005624650977551938
Training Loss: 0.0005624129330196108
Validation Loss: 0.0005618481343844905
Epoch 11:
batch 5 loss: 0.0005634560831822455
batch 10 loss: 0.0005627677426673471
batch 15 loss: 0.0005621761782094836
batch 20 loss: 0.0005619697854854167
batch 25 loss: 0.0005619241041131317
batch 30 loss: 0.0005617809947580099
batch 35 loss: 0.0005613051820546388
batch 40 loss: 0.0005612142384052276
batch 45 loss: 0.0005610209540463984
batch 50 loss: 0.000561468896921724
batch 55 loss: 0.000561580783687532
batch 60 loss: 0.0005614508641883731
batch 65 loss: 0.0005613448098301888
batch 70 loss: 0.0005614588386379183
batch 75 loss: 0.0005612403620034456
batch 80 loss: 0.0005613653105683625
batch 85 loss: 0.0005613156245090067
batch 90 loss: 0.0005612389766611158
batch 95 loss: 0.0005612785113044083
batch 100 loss: 0.0005611584638245404
batch 105 loss: 0.0005615468602627516
batch 110 loss: 0.0005616139504127204
batch 115 loss: 0.0005625340156257153
batch 120 loss: 0.0005619795992970467
batch 125 loss: 0.0005613832734525203
batch 130 loss: 0.0005613165092654526
batch 135 loss: 0.0005617849878035486
batch 140 loss: 0.0005633521010167897
batch 145 loss: 0.0005704778828658164
batch 150 loss: 0.0005705978837795556
batch 155 loss: 0.0005579504184424877
batch 160 loss: 0.0005577225587330758
batch 165 loss: 0.0005577128380537033
batch 170 loss: 0.0005578005453571677
batch 175 loss: 0.0005576339550316334
batch 180 loss: 0.000557695236057043
batch 185 loss: 0.0005577663192525506
batch 190 loss: 0.0005576669704169035
batch 195 loss: 0.0005575250135734678
batch 200 loss: 0.0005577599862590432
batch 205 loss: 0.0005577965872362256
batch 210 loss: 0.0005576246534474194
batch 215 loss: 0.0005577914649620652
batch 220 loss: 0.0005577084491960704
batch 225 loss: 0.0005575603921897709
batch 230 loss: 0.0005576194496825337
batch 235 loss: 0.0005576650262810289
batch 240 loss: 0.0005576874478720129
Training Loss: 0.0005605789808517633
Validation Loss: 0.0005576934170676395
Epoch 12:
batch 5 loss: 0.000557699496857822
batch 10 loss: 0.0005576509865932167
batch 15 loss: 0.0005577142699621617
batch 20 loss: 0.0005576937226578593
batch 25 loss: 0.0005576839903369546
batch 30 loss: 0.0005576718598604202
batch 35 loss: 0.0005577771924436092
batch 40 loss: 0.00055767489830032
batch 45 loss: 0.0005576570983976126
batch 50 loss: 0.0005576148047111929
batch 55 loss: 0.0005577223608270287
batch 60 loss: 0.0005577225470915437
batch 65 loss: 0.0005576725234277546
batch 70 loss: 0.0005577086936682463
batch 75 loss: 0.0005576343392021954
batch 80 loss: 0.0005576147930696606
batch 85 loss: 0.0005576715804636479
batch 90 loss: 0.0005576429655775427
batch 95 loss: 0.0005577357951551676
batch 100 loss: 0.0005576273193582893
batch 105 loss: 0.0005576342227868736
batch 110 loss: 0.0005576911731623113
batch 115 loss: 0.0005577024654485285
batch 120 loss: 0.0005577416508458555
batch 125 loss: 0.0005576859228312969
batch 130 loss: 0.0005577006377279758
batch 135 loss: 0.0005576651659794152
batch 140 loss: 0.0005576818133704364
batch 145 loss: 0.0005577122792601585
batch 150 loss: 0.0005576705560088157
batch 155 loss: 0.0005577042582444846
batch 160 loss: 0.0005576266557909548
batch 165 loss: 0.0005576349911279976
batch 170 loss: 0.0005576242809183895
batch 175 loss: 0.0005576727213338018
batch 180 loss: 0.0005575831746682525
batch 185 loss: 0.0005577004398219287
batch 190 loss: 0.0005577349802479148
batch 195 loss: 0.0005577255855314433
batch 200 loss: 0.0005576841183938086
batch 205 loss: 0.0005576846189796925
batch 210 loss: 0.0005576983676292002
batch 215 loss: 0.00055771607439965
batch 220 loss: 0.0005576314171776176
batch 225 loss: 0.0005577356205321848
batch 230 loss: 0.000557708612177521
batch 235 loss: 0.0005577487405389548
batch 240 loss: 0.0005577108939178288
Training Loss: 0.0005576833891003237
Validation Loss: 0.0005576934025157243
Epoch 13:
batch 5 loss: 0.0005575051996856928
batch 10 loss: 0.0005574962357059121
batch 15 loss: 0.0005577011965215206
batch 20 loss: 0.0005576436291448772
batch 25 loss: 0.000557761825621128
batch 30 loss: 0.0005576839903369546
batch 35 loss: 0.0005577617092058062
batch 40 loss: 0.0005577072384767235
batch 45 loss: 0.0005577999516390264
batch 50 loss: 0.0005577258300036192
batch 55 loss: 0.0005576832918450236
batch 60 loss: 0.0005575126386247575
batch 65 loss: 0.0005577873671427369
batch 70 loss: 0.0005576662253588438
batch 75 loss: 0.0005577826523222029
batch 80 loss: 0.0005575683084316551
batch 85 loss: 0.0005576330353505909
batch 90 loss: 0.000557579065207392
batch 95 loss: 0.0005577519885264337
batch 100 loss: 0.0005576936295256018
batch 105 loss: 0.0005577761796303094
batch 110 loss: 0.000557625840883702
batch 115 loss: 0.0005576136056333781
batch 120 loss: 0.0005576580995693802
batch 125 loss: 0.0005577607429586351
batch 130 loss: 0.0005576659576036036
batch 135 loss: 0.0005577015690505505
batch 140 loss: 0.0005576754803769291
batch 145 loss: 0.0005576388211920857
batch 150 loss: 0.0005577496369369328
batch 155 loss: 0.0005577485542744398
batch 160 loss: 0.000557728239800781
batch 165 loss: 0.000557644129730761
batch 170 loss: 0.0005577057250775397
batch 175 loss: 0.000557551218662411
batch 180 loss: 0.0005576187046244741
batch 185 loss: 0.000557721417862922
batch 190 loss: 0.0005576928844675422
batch 195 loss: 0.000557779346127063
batch 200 loss: 0.0005576505907811224
batch 205 loss: 0.0005577361211180687
batch 210 loss: 0.0005576187861151994
batch 215 loss: 0.0005576332216151059
batch 220 loss: 0.000557674968149513
batch 225 loss: 0.0005577025120146572
batch 230 loss: 0.0005578744923695922
batch 235 loss: 0.0005577795905992389
batch 240 loss: 0.0005576309165917337
Training Loss: 0.0005576833825519619
Validation Loss: 0.0005576933976650859
Epoch 14:
batch 5 loss: 0.0005577794276177883
batch 10 loss: 0.0005575809627771378
batch 15 loss: 0.0005577812436968089
batch 20 loss: 0.0005577381001785397
batch 25 loss: 0.0005576764699071646
batch 30 loss: 0.000557612394914031
batch 35 loss: 0.0005577016971074044
batch 40 loss: 0.0005576613009907305
batch 45 loss: 0.000557716772891581
batch 50 loss: 0.0005576959461905062
batch 55 loss: 0.0005577235016971827
batch 60 loss: 0.0005578425247222185
batch 65 loss: 0.0005575868301093579
batch 70 loss: 0.0005577206378802657
batch 75 loss: 0.0005577625590376556
batch 80 loss: 0.00055769911268726
batch 85 loss: 0.0005576369701884687
batch 90 loss: 0.0005576279130764305
batch 95 loss: 0.000557675736490637
batch 100 loss: 0.0005576741765253246
batch 105 loss: 0.0005576551659032703
batch 110 loss: 0.0005576416268013417
batch 115 loss: 0.0005577571922913193
batch 120 loss: 0.0005577638046815991
batch 125 loss: 0.0005576936644501984
batch 130 loss: 0.000557716318871826
batch 135 loss: 0.0005576184601522982
batch 140 loss: 0.0005574569455347955
batch 145 loss: 0.0005576430703513324
batch 150 loss: 0.0005576508236117661
batch 155 loss: 0.0005575735005550087
batch 160 loss: 0.0005577551783062518
batch 165 loss: 0.000557821081019938
batch 170 loss: 0.0005577057134360075
batch 175 loss: 0.0005576588562689721
batch 180 loss: 0.000557594932615757
batch 185 loss: 0.000557715620379895
batch 190 loss: 0.0005576461320742965
batch 195 loss: 0.0005577044328674674
batch 200 loss: 0.0005576164345256984
batch 205 loss: 0.0005576408235356212
batch 210 loss: 0.000557623035274446
batch 215 loss: 0.0005577490897849202
batch 220 loss: 0.0005577115807682276
batch 225 loss: 0.0005577847710810602
batch 230 loss: 0.000557709182612598
batch 235 loss: 0.0005576711380854249
batch 240 loss: 0.000557629531249404
Training Loss: 0.0005576833830370257
Validation Loss: 0.0005576934267689164
Epoch 15:
batch 5 loss: 0.0005577750154770911
batch 10 loss: 0.0005576132447458803
batch 15 loss: 0.0005577016388997435
batch 20 loss: 0.0005576904863119125
batch 25 loss: 0.0005576330586336553
batch 30 loss: 0.0005577092058956623
batch 35 loss: 0.000557872373610735
batch 40 loss: 0.0005576882162131369
batch 45 loss: 0.0005576484836637974
batch 50 loss: 0.0005577179254032671
batch 55 loss: 0.0005576640483923257
batch 60 loss: 0.0005576624651439488
batch 65 loss: 0.0005577027681283653
batch 70 loss: 0.0005576316267251969
batch 75 loss: 0.000557601684704423
batch 80 loss: 0.0005576282041147351
batch 85 loss: 0.0005576782510615885
batch 90 loss: 0.0005577561096288264
batch 95 loss: 0.0005576182156801224
batch 100 loss: 0.0005576175753958523
batch 105 loss: 0.0005576974013820291
batch 110 loss: 0.0005576457595452666
batch 115 loss: 0.0005577777395956218
batch 120 loss: 0.0005576015450060367
batch 125 loss: 0.0005575640825554729
batch 130 loss: 0.0005576352356001735
batch 135 loss: 0.000557654700241983
batch 140 loss: 0.0005576646421104669
batch 145 loss: 0.0005576621973887086
batch 150 loss: 0.0005576294264756143
batch 155 loss: 0.0005576856434345246
batch 160 loss: 0.0005577779840677977
batch 165 loss: 0.000557649985421449
batch 170 loss: 0.0005576785420998931
batch 175 loss: 0.0005576580530032516
batch 180 loss: 0.0005576970404945313
batch 185 loss: 0.0005576816736720502
batch 190 loss: 0.000557593023404479
batch 195 loss: 0.0005577347823418677
batch 200 loss: 0.0005576885072514415
batch 205 loss: 0.0005577718839049339
batch 210 loss: 0.0005578643409535289
batch 215 loss: 0.0005577361793257296
batch 220 loss: 0.0005577938281930983
batch 225 loss: 0.0005576580995693802
batch 230 loss: 0.0005576932919211686
batch 235 loss: 0.0005577577976509929
batch 240 loss: 0.0005575381801463664
Training Loss: 0.0005576833784289193
Validation Loss: 0.0005576933957248306
Epoch 16:
batch 5 loss: 0.0005577399977482855
batch 10 loss: 0.0005576312309131026
batch 15 loss: 0.0005576059105806052
batch 20 loss: 0.0005576147814281285
batch 25 loss: 0.0005577356088906527
batch 30 loss: 0.0005576593568548561
batch 35 loss: 0.0005577468778938055
batch 40 loss: 0.0005576348397880792
batch 45 loss: 0.0005577169358730316
batch 50 loss: 0.0005577241419814527
batch 55 loss: 0.0005576953873969615
batch 60 loss: 0.0005577513133175671
batch 65 loss: 0.0005577330710366368
batch 70 loss: 0.0005577284377068281
batch 75 loss: 0.0005577716277912259
batch 80 loss: 0.0005575946997851133
batch 85 loss: 0.0005576964584179222
batch 90 loss: 0.0005577897885814309
batch 95 loss: 0.0005576567957177759
batch 100 loss: 0.0005575911491177976
batch 105 loss: 0.0005577217089012265
batch 110 loss: 0.0005577164702117443
batch 115 loss: 0.0005576612195000053
batch 120 loss: 0.0005576655268669129
batch 125 loss: 0.0005577110452577472
batch 130 loss: 0.0005576746072620154
batch 135 loss: 0.0005577604286372661
batch 140 loss: 0.0005576722789555788
batch 145 loss: 0.0005576270399615168
batch 150 loss: 0.0005576693918555975
batch 155 loss: 0.0005576834548264742
batch 160 loss: 0.0005576692055910826
batch 165 loss: 0.0005577220697887241
batch 170 loss: 0.000557614688295871
batch 175 loss: 0.0005577015341259539
batch 180 loss: 0.0005577082862146199
batch 185 loss: 0.0005576706491410733
batch 190 loss: 0.0005576002295129001
batch 195 loss: 0.0005577250965870916
batch 200 loss: 0.0005576888215728104
batch 205 loss: 0.0005576297058723867
batch 210 loss: 0.0005576154217123985
batch 215 loss: 0.0005577401374466717
batch 220 loss: 0.0005576682975515723
batch 225 loss: 0.0005576317431405187
batch 230 loss: 0.000557687389664352
batch 235 loss: 0.0005576751311309636
batch 240 loss: 0.000557672290597111
Training Loss: 0.0005576833808542385
Validation Loss: 0.0005576934500519808
Epoch 17:
batch 5 loss: 0.0005577052826993168
batch 10 loss: 0.0005577166099101305
batch 15 loss: 0.0005577212781645357
batch 20 loss: 0.0005577004048973322
batch 25 loss: 0.0005577598582021892
batch 30 loss: 0.0005576825584284961
batch 35 loss: 0.0005577657604590059
batch 40 loss: 0.000557627878151834
batch 45 loss: 0.0005578162265010179
batch 50 loss: 0.0005576522438786924
batch 55 loss: 0.0005576433497481049
batch 60 loss: 0.0005577136995270848
batch 65 loss: 0.0005576914874836803
batch 70 loss: 0.0005576178897172213
batch 75 loss: 0.0005576030351221561
batch 80 loss: 0.000557790300808847
batch 85 loss: 0.0005576555267907679
batch 90 loss: 0.0005576611845754087
batch 95 loss: 0.0005576634663157165
batch 100 loss: 0.0005576060153543949
batch 105 loss: 0.0005576831754297018
batch 110 loss: 0.0005576548050157726
batch 115 loss: 0.0005576876690611243
batch 120 loss: 0.0005576865863986313
batch 125 loss: 0.0005575840943492949
batch 130 loss: 0.0005576345487497747
batch 135 loss: 0.0005577028263360262
batch 140 loss: 0.0005576478666625917
batch 145 loss: 0.0005576355732046067
batch 150 loss: 0.0005577557953074574
batch 155 loss: 0.0005576755502261221
batch 160 loss: 0.0005577543168328702
batch 165 loss: 0.0005577255506068468
batch 170 loss: 0.0005576218594796955
batch 175 loss: 0.0005576735828071832
batch 180 loss: 0.0005578190204687417
batch 185 loss: 0.0005576586583629251
batch 190 loss: 0.0005577523959800601
batch 195 loss: 0.0005576206487603486
batch 200 loss: 0.0005577153177000582
batch 205 loss: 0.0005576707189902663
batch 210 loss: 0.0005576006253249944
batch 215 loss: 0.0005576589843258262
batch 220 loss: 0.0005576361552812159
batch 225 loss: 0.0005577519303187727
batch 230 loss: 0.0005576256196945905
batch 235 loss: 0.0005577376810833812
batch 240 loss: 0.0005576366791501641
Training Loss: 0.0005576833810967704
Validation Loss: 0.0005576934063962351
Epoch 18:
batch 5 loss: 0.0005576623487286269
batch 10 loss: 0.000557743280660361
batch 15 loss: 0.0005577079253271222
batch 20 loss: 0.0005577105330303311
batch 25 loss: 0.000557726202532649
batch 30 loss: 0.0005576950963586569
batch 35 loss: 0.0005575579940341413
batch 40 loss: 0.000557770871091634
batch 45 loss: 0.0005576355732046067
batch 50 loss: 0.0005576643510721624
batch 55 loss: 0.000557549751829356
batch 60 loss: 0.0005577195785008371
batch 65 loss: 0.0005576391471549868
batch 70 loss: 0.0005576639669016004
batch 75 loss: 0.000557690323330462
batch 80 loss: 0.0005576691124588251
batch 85 loss: 0.0005577359115704894
batch 90 loss: 0.0005576177034527063
batch 95 loss: 0.0005576899740844965
batch 100 loss: 0.0005577864474616944
batch 105 loss: 0.0005576893803663552
batch 110 loss: 0.0005577180767431855
batch 115 loss: 0.0005575558287091553
batch 120 loss: 0.0005576568772085011
batch 125 loss: 0.0005577344330959022
batch 130 loss: 0.0005577696720138192
batch 135 loss: 0.0005577036645263433
batch 140 loss: 0.0005576285999268294
batch 145 loss: 0.000557793234474957
batch 150 loss: 0.0005575819639489054
batch 155 loss: 0.0005577030126005412
batch 160 loss: 0.000557552243117243
batch 165 loss: 0.0005577139439992606
batch 170 loss: 0.0005577016505412758
batch 175 loss: 0.0005576878553256393
batch 180 loss: 0.0005577293690294027
batch 185 loss: 0.0005577218602411449
batch 190 loss: 0.000557777239009738
batch 195 loss: 0.0005576582043431699
batch 200 loss: 0.0005576849333010613
batch 205 loss: 0.0005576914292760194
batch 210 loss: 0.0005576263996772468
batch 215 loss: 0.0005575494840741157
batch 220 loss: 0.0005577184725552798
batch 225 loss: 0.0005577428033575416
batch 230 loss: 0.0005577097646892071
batch 235 loss: 0.0005576874711550773
batch 240 loss: 0.0005576781579293311
Training Loss: 0.0005576833774587916
Validation Loss: 0.0005576934219182779
Epoch 19:
batch 5 loss: 0.0005577604752033949
batch 10 loss: 0.0005575454444624484
batch 15 loss: 0.0005575985298492015
batch 20 loss: 0.0005576164810918271
batch 25 loss: 0.0005576374824158847
batch 30 loss: 0.0005577519885264337
batch 35 loss: 0.0005576462601311505
batch 40 loss: 0.0005576048512011767
batch 45 loss: 0.0005576538620516658
batch 50 loss: 0.0005576113821007311
batch 55 loss: 0.0005576264928095042
batch 60 loss: 0.0005576400784775615
batch 65 loss: 0.0005577025352977216
batch 70 loss: 0.0005575788323767483
batch 75 loss: 0.0005576533381827176
batch 80 loss: 0.0005576463416218757
batch 85 loss: 0.000557757611386478
batch 90 loss: 0.0005576714058406651
batch 95 loss: 0.0005577153759077191
batch 100 loss: 0.0005577290547080338
batch 105 loss: 0.0005576664349064231
batch 110 loss: 0.000557703513186425
batch 115 loss: 0.0005577440140768885
batch 120 loss: 0.0005577622097916902
batch 125 loss: 0.000557793362531811
batch 130 loss: 0.0005576620460487902
batch 135 loss: 0.0005578045500442385
batch 140 loss: 0.0005577681236900389
batch 145 loss: 0.0005576517782174051
batch 150 loss: 0.0005576138966716826
batch 155 loss: 0.000557731359731406
batch 160 loss: 0.0005576714407652617
batch 165 loss: 0.0005578064708970487
batch 170 loss: 0.0005576521158218384
batch 175 loss: 0.0005576404742896557
batch 180 loss: 0.0005576733383350074
batch 185 loss: 0.0005576983094215393
batch 190 loss: 0.0005577409523539245
batch 195 loss: 0.0005577375879511238
batch 200 loss: 0.0005575480288825929
batch 205 loss: 0.0005576942930929363
batch 210 loss: 0.0005576572846621275
batch 215 loss: 0.0005577238858677447
batch 220 loss: 0.0005576394032686949
batch 225 loss: 0.0005577079718932509
batch 230 loss: 0.0005576332681812346
batch 235 loss: 0.0005577547592110932
batch 240 loss: 0.0005577735952101648
Training Loss: 0.0005576833810967704
Validation Loss: 0.0005576934005754689
Epoch 20:
batch 5 loss: 0.0005576737923547626
batch 10 loss: 0.0005576123017817736
batch 15 loss: 0.0005577164585702122
batch 20 loss: 0.0005576332099735737
batch 25 loss: 0.000557672861032188
batch 30 loss: 0.0005576409865170717
batch 35 loss: 0.0005577153293415904
batch 40 loss: 0.0005577758769504726
batch 45 loss: 0.0005576306139118969
batch 50 loss: 0.0005576198920607567
batch 55 loss: 0.0005577576928772032
batch 60 loss: 0.0005575942457653582
batch 65 loss: 0.0005577841424383223
batch 70 loss: 0.000557737797498703
batch 75 loss: 0.000557715236209333
batch 80 loss: 0.0005578480428084731
batch 85 loss: 0.0005577619536779821
batch 90 loss: 0.0005576996132731438
batch 95 loss: 0.0005576394498348237
batch 100 loss: 0.0005575727671384811
batch 105 loss: 0.0005577109521254897
batch 110 loss: 0.0005577790667302907
batch 115 loss: 0.0005575716844759881
batch 120 loss: 0.0005576664232648909
batch 125 loss: 0.0005576689261943101
batch 130 loss: 0.000557670823764056
batch 135 loss: 0.000557690451387316
batch 140 loss: 0.000557726772967726
batch 145 loss: 0.0005577192059718072
batch 150 loss: 0.0005577312433160842
batch 155 loss: 0.0005577479605562985
batch 160 loss: 0.0005576569586992264
batch 165 loss: 0.0005577135132625699
batch 170 loss: 0.000557563325855881
batch 175 loss: 0.0005575620103627444
batch 180 loss: 0.0005576532916165888
batch 185 loss: 0.0005576093564741314
batch 190 loss: 0.0005576233030296862
batch 195 loss: 0.0005577897187322378
batch 200 loss: 0.0005576140363700687
batch 205 loss: 0.0005576476105488837
batch 210 loss: 0.0005577310919761657
batch 215 loss: 0.0005576505791395903
batch 220 loss: 0.0005575890652835369
batch 225 loss: 0.0005576869938522577
batch 230 loss: 0.0005578167503699661
batch 235 loss: 0.0005577279720455408
batch 240 loss: 0.0005576810915954411
Training Loss: 0.0005576833842496854
Validation Loss: 0.0005576934025157243
Epoch 21:
batch 5 loss: 0.0005576387164182961
batch 10 loss: 0.000557669484987855
batch 15 loss: 0.0005576825700700283
batch 20 loss: 0.0005576994153670967
batch 25 loss: 0.0005576702766120434
batch 30 loss: 0.0005575150717049837
batch 35 loss: 0.0005577782168984414
batch 40 loss: 0.0005577630712650716
batch 45 loss: 0.0005577317089773715
batch 50 loss: 0.0005576324765570462
batch 55 loss: 0.0005577927688136697
batch 60 loss: 0.0005576627794653177
batch 65 loss: 0.0005576097639277578
batch 70 loss: 0.0005576592171564698
batch 75 loss: 0.0005576992174610495
batch 80 loss: 0.0005577069241553545
batch 85 loss: 0.0005576702300459146
batch 90 loss: 0.0005577706033363938
batch 95 loss: 0.0005576183437369763
batch 100 loss: 0.0005576561903581023
batch 105 loss: 0.0005575731978751719
batch 110 loss: 0.0005576302064582705
batch 115 loss: 0.0005576660507358611
batch 120 loss: 0.0005576181691139936
batch 125 loss: 0.0005577938631176948
batch 130 loss: 0.0005576862487941981
batch 135 loss: 0.0005576660856604576
batch 140 loss: 0.0005576859228312969
batch 145 loss: 0.000557737413328141
batch 150 loss: 0.000557881232816726
batch 155 loss: 0.0005576960975304246
batch 160 loss: 0.0005576713825576008
batch 165 loss: 0.000557573011610657
batch 170 loss: 0.0005575818591751158
batch 175 loss: 0.0005577871692366898
batch 180 loss: 0.0005577115807682276
batch 185 loss: 0.0005577342701144517
batch 190 loss: 0.0005576520459726452
batch 195 loss: 0.0005576704512350261
batch 200 loss: 0.0005576833849772811
batch 205 loss: 0.0005577744916081429
batch 210 loss: 0.0005576462717726827
batch 215 loss: 0.0005577080650255084
batch 220 loss: 0.0005576836410909891
batch 225 loss: 0.0005576705560088157
batch 230 loss: 0.0005576655734330416
batch 235 loss: 0.0005577168078161776
batch 240 loss: 0.000557610287796706
Training Loss: 0.0005576833830370257
Validation Loss: 0.0005576933957248306
Epoch 22:
batch 5 loss: 0.0005576371098868549
batch 10 loss: 0.0005578219308517874
batch 15 loss: 0.000557688798289746
batch 20 loss: 0.0005577011383138597
batch 25 loss: 0.0005576068069785833
batch 30 loss: 0.0005576902069151402
batch 35 loss: 0.0005576111143454909
batch 40 loss: 0.0005576019990257919
batch 45 loss: 0.0005577424424700439
batch 50 loss: 0.0005576511961407959
batch 55 loss: 0.0005576845374889672
batch 60 loss: 0.000557662092614919
batch 65 loss: 0.0005575947347097099
batch 70 loss: 0.0005576696596108377
batch 75 loss: 0.0005576707888394594
batch 80 loss: 0.0005577131640166044
batch 85 loss: 0.0005577649106271565
batch 90 loss: 0.0005576647236011922
batch 95 loss: 0.0005576281226240098
batch 100 loss: 0.0005576453055255115
batch 105 loss: 0.000557623861823231
batch 110 loss: 0.0005577504751272499
batch 115 loss: 0.0005576908704824745
batch 120 loss: 0.0005576773313805461
batch 125 loss: 0.0005576579482294619
batch 130 loss: 0.0005576198687776923
batch 135 loss: 0.0005576654803007841
batch 140 loss: 0.0005577329313382507
batch 145 loss: 0.0005577962961979211
batch 150 loss: 0.0005577047355473042
batch 155 loss: 0.0005577427335083484
batch 160 loss: 0.0005576800554990769
batch 165 loss: 0.0005577749107033014
batch 170 loss: 0.0005577491596341133
batch 175 loss: 0.0005576874245889485
batch 180 loss: 0.0005576986819505692
batch 185 loss: 0.0005576316965743899
batch 190 loss: 0.0005576562951318919
batch 195 loss: 0.0005576973431743682
batch 200 loss: 0.0005576601601205766
batch 205 loss: 0.0005577282281592488
batch 210 loss: 0.0005575817893259227
batch 215 loss: 0.0005577658186666668
batch 220 loss: 0.0005577038391493261
batch 225 loss: 0.0005576126161031425
batch 230 loss: 0.0005577004048973322
batch 235 loss: 0.0005576594034209847
batch 240 loss: 0.0005577016156166792
Training Loss: 0.0005576833907980471
Validation Loss: 0.0005576933937845752
Epoch 23:
batch 5 loss: 0.000557710079010576
batch 10 loss: 0.0005575945717282594
batch 15 loss: 0.0005576997646130621
batch 20 loss: 0.000557621952611953
batch 25 loss: 0.0005576282390393316
batch 30 loss: 0.0005577834672294557
batch 35 loss: 0.0005577141884714365
batch 40 loss: 0.0005576365278102457
batch 45 loss: 0.0005576936877332628
batch 50 loss: 0.0005576733383350074
batch 55 loss: 0.0005576503230258822
batch 60 loss: 0.0005578156909905374
batch 65 loss: 0.0005575812305323779
batch 70 loss: 0.0005577079602517188
batch 75 loss: 0.0005576301482506096
batch 80 loss: 0.0005576501251198351
batch 85 loss: 0.0005577269359491766
batch 90 loss: 0.000557620357722044
batch 95 loss: 0.0005576249561272562
batch 100 loss: 0.0005576251889578999
batch 105 loss: 0.0005576522438786924
batch 110 loss: 0.000557602324988693
batch 115 loss: 0.0005576987634412945
batch 120 loss: 0.0005577146890573204
batch 125 loss: 0.0005577319650910794
batch 130 loss: 0.0005576374824158847
batch 135 loss: 0.0005576709168963135
batch 140 loss: 0.0005576180526986718
batch 145 loss: 0.0005576343042775989
batch 150 loss: 0.0005577038857154549
batch 155 loss: 0.0005576890660449862
batch 160 loss: 0.0005575982038863003
batch 165 loss: 0.000557700137142092
batch 170 loss: 0.0005576786585152149
batch 175 loss: 0.0005576804745942354
batch 180 loss: 0.0005576892639510333
batch 185 loss: 0.0005576924653723836
batch 190 loss: 0.000557779218070209
batch 195 loss: 0.0005576868657954037
batch 200 loss: 0.0005576635478064418
batch 205 loss: 0.0005576637806370855
batch 210 loss: 0.0005578386830165982
batch 215 loss: 0.0005578570649959147
batch 220 loss: 0.000557690707501024
batch 225 loss: 0.000557759846560657
batch 230 loss: 0.0005576756782829761
batch 235 loss: 0.0005576956085860729
batch 240 loss: 0.000557710335124284
Training Loss: 0.0005576833951636218
Validation Loss: 0.0005576935111700247
Epoch 24:
batch 5 loss: 0.0005576591356657445
batch 10 loss: 0.0005576088908128441
batch 15 loss: 0.0005576907889917493
batch 20 loss: 0.000557672989089042
batch 25 loss: 0.0005576506373472512
batch 30 loss: 0.0005576126161031425
batch 35 loss: 0.0005577154573984444
batch 40 loss: 0.0005577621981501579
batch 45 loss: 0.0005576977389864623
batch 50 loss: 0.0005577191594056785
batch 55 loss: 0.0005576950265094638
batch 60 loss: 0.0005576772964559495
batch 65 loss: 0.0005577925476245582
batch 70 loss: 0.0005576682277023792
batch 75 loss: 0.0005576992407441139
batch 80 loss: 0.0005576521856710315
batch 85 loss: 0.0005576511844992638
batch 90 loss: 0.0005577422678470611
batch 95 loss: 0.0005576107068918645
batch 100 loss: 0.0005577079486101866
batch 105 loss: 0.0005578143871389329
batch 110 loss: 0.000557592825498432
batch 115 loss: 0.0005577490548603237
batch 120 loss: 0.0005576754105277359
batch 125 loss: 0.0005578048527240753
batch 130 loss: 0.0005576630705036223
batch 135 loss: 0.0005578037002123892
batch 140 loss: 0.0005576531169936061
batch 145 loss: 0.0005574907758273184
batch 150 loss: 0.0005576947121880948
batch 155 loss: 0.0005576107301749289
batch 160 loss: 0.0005576918716542423
batch 165 loss: 0.000557740789372474
batch 170 loss: 0.0005576943280175328
batch 175 loss: 0.0005575964576564729
batch 180 loss: 0.0005576999159529805
batch 185 loss: 0.0005576185300014913
batch 190 loss: 0.0005577729665674269
batch 195 loss: 0.0005577320349402726
batch 200 loss: 0.0005576096009463072
batch 205 loss: 0.0005576940951868891
batch 210 loss: 0.0005575992166996002
batch 215 loss: 0.0005576285300776362
batch 220 loss: 0.0005577275180257857
batch 225 loss: 0.0005576590658165515
batch 230 loss: 0.0005577470757998527
batch 235 loss: 0.0005576706607826054
batch 240 loss: 0.0005576810566708445
Training Loss: 0.0005576833874026003
Validation Loss: 0.0005576933937845752
Epoch 25:
batch 5 loss: 0.0005577597883529961
batch 10 loss: 0.0005576625699177384
batch 15 loss: 0.0005576825235038996
batch 20 loss: 0.0005576229188591242
batch 25 loss: 0.0005575750372372567
batch 30 loss: 0.0005576348747126758
batch 35 loss: 0.000557627878151834
batch 40 loss: 0.0005576106836088002
batch 45 loss: 0.0005576276336796582
batch 50 loss: 0.0005576506722718477
batch 55 loss: 0.0005576795199885964
batch 60 loss: 0.0005577563657425344
batch 65 loss: 0.0005577005678787828
batch 70 loss: 0.0005577408126555383
batch 75 loss: 0.0005577323725447058
batch 80 loss: 0.000557740218937397
batch 85 loss: 0.0005577146774157881
batch 90 loss: 0.0005577018251642585
batch 95 loss: 0.0005577308475039899
batch 100 loss: 0.0005577426054514945
batch 105 loss: 0.0005577136529609561
batch 110 loss: 0.0005577060510404408
batch 115 loss: 0.0005576739902608096
batch 120 loss: 0.0005577023490332067
batch 125 loss: 0.0005576717318035662
batch 130 loss: 0.0005576570751145482
batch 135 loss: 0.0005577443400397897
batch 140 loss: 0.0005577384144999087
batch 145 loss: 0.0005576321273110807
batch 150 loss: 0.0005576075636781752
batch 155 loss: 0.0005576513591222465
batch 160 loss: 0.0005576214869506657
batch 165 loss: 0.0005577105563133955
batch 170 loss: 0.0005576200899668037
batch 175 loss: 0.000557715876493603
batch 180 loss: 0.0005577594274654984
batch 185 loss: 0.0005576241528615355
batch 190 loss: 0.0005576811265200377
batch 195 loss: 0.0005576536525040865
batch 200 loss: 0.0005577416042797268
batch 205 loss: 0.0005577498115599156
batch 210 loss: 0.0005576148047111929
batch 215 loss: 0.0005576301948167384
batch 220 loss: 0.0005575914983637631
batch 225 loss: 0.000557715620379895
batch 230 loss: 0.000557749648578465
batch 235 loss: 0.0005577184376306832
batch 240 loss: 0.0005577116273343564
Training Loss: 0.0005576833888577918
Validation Loss: 0.0005576934461714699
Epoch 26:
batch 5 loss: 0.0005576676339842379
batch 10 loss: 0.0005577925592660903
batch 15 loss: 0.0005575960618443787
batch 20 loss: 0.00055768828606233
batch 25 loss: 0.0005576615687459708
batch 30 loss: 0.0005576711846515536
batch 35 loss: 0.0005577432457357645
batch 40 loss: 0.0005576803698204458
batch 45 loss: 0.0005577198229730129
batch 50 loss: 0.000557635584846139
batch 55 loss: 0.000557758763898164
batch 60 loss: 0.0005577202886343002
batch 65 loss: 0.0005577208241447807
batch 70 loss: 0.0005576263298280537
batch 75 loss: 0.000557723268866539
batch 80 loss: 0.0005576053052209317
batch 85 loss: 0.0005577090661972762
batch 90 loss: 0.0005576742812991142
batch 95 loss: 0.0005576595896854997
batch 100 loss: 0.0005576900439336896
batch 105 loss: 0.0005576494615525008
batch 110 loss: 0.0005576720926910639
batch 115 loss: 0.0005577080650255084
batch 120 loss: 0.0005576576571911573
batch 125 loss: 0.0005577107891440392
batch 130 loss: 0.0005577117553912103
batch 135 loss: 0.0005575820105150342
batch 140 loss: 0.0005577137460932135
batch 145 loss: 0.0005576415802352131
batch 150 loss: 0.0005576650961302221
batch 155 loss: 0.0005576700903475284
batch 160 loss: 0.0005576477153226734
batch 165 loss: 0.000557609018869698
batch 170 loss: 0.0005577823962084949
batch 175 loss: 0.0005576314637437463
batch 180 loss: 0.0005577862844802439
batch 185 loss: 0.0005576441064476967
batch 190 loss: 0.0005577265401370823
batch 195 loss: 0.0005576615571044385
batch 200 loss: 0.0005576631287112832
batch 205 loss: 0.0005577475880272686
batch 210 loss: 0.000557635584846139
batch 215 loss: 0.000557658460456878
batch 220 loss: 0.0005577509757131339
batch 225 loss: 0.0005576689378358424
batch 230 loss: 0.0005576514638960361
batch 235 loss: 0.0005577395088039339
batch 240 loss: 0.0005576719995588064
Training Loss: 0.0005576833990441325
Validation Loss: 0.0005576934190078948
Epoch 27:
batch 5 loss: 0.0005576659459620714
batch 10 loss: 0.0005576769122853875
batch 15 loss: 0.0005576163879595697
batch 20 loss: 0.0005576895084232092
batch 25 loss: 0.0005577539908699691
batch 30 loss: 0.0005576954456046224
batch 35 loss: 0.0005577043280936778
batch 40 loss: 0.0005577638745307923
batch 45 loss: 0.0005576493800617755
batch 50 loss: 0.0005575875751674175
batch 55 loss: 0.0005576450261287391
batch 60 loss: 0.000557599903549999
batch 65 loss: 0.0005576677271164953
batch 70 loss: 0.000557705934625119
batch 75 loss: 0.0005576535942964256
batch 80 loss: 0.000557609589304775
batch 85 loss: 0.0005577066447585821
batch 90 loss: 0.000557595188729465
batch 95 loss: 0.000557630870025605
batch 100 loss: 0.0005574772716499865
batch 105 loss: 0.000557560718152672
batch 110 loss: 0.000557685608509928
batch 115 loss: 0.0005576826981268824
batch 120 loss: 0.0005577576695941389
batch 125 loss: 0.0005577859235927462
batch 130 loss: 0.0005576176336035132
batch 135 loss: 0.0005576462368480861
batch 140 loss: 0.0005577178206294775
batch 145 loss: 0.0005577043863013387
batch 150 loss: 0.000557674269657582
batch 155 loss: 0.0005576925002969801
batch 160 loss: 0.00055775111541152
batch 165 loss: 0.0005577786709181964
batch 170 loss: 0.0005577218835242093
batch 175 loss: 0.0005577298579737544
batch 180 loss: 0.0005576809751801192
batch 185 loss: 0.000557752710301429
batch 190 loss: 0.0005577423726208508
batch 195 loss: 0.0005576614639721811
batch 200 loss: 0.0005576820112764835
batch 205 loss: 0.0005577102303504944
batch 210 loss: 0.0005577218835242093
batch 215 loss: 0.0005576824187301099
batch 220 loss: 0.0005576966796070337
batch 225 loss: 0.0005577464937232435
batch 230 loss: 0.0005576474242843688
batch 235 loss: 0.0005577612319029868
batch 240 loss: 0.0005577150266617536
Training Loss: 0.0005576833961337494
Validation Loss: 0.0005576934083364904
Epoch 28:
batch 5 loss: 0.000557695236057043
batch 10 loss: 0.0005576514755375684
batch 15 loss: 0.0005576538387686014
batch 20 loss: 0.0005577444680966437
batch 25 loss: 0.0005576004623435438
batch 30 loss: 0.0005576844792813063
batch 35 loss: 0.0005577330477535725
batch 40 loss: 0.0005577489617280662
batch 45 loss: 0.0005578040727414191
batch 50 loss: 0.0005576755851507187
batch 55 loss: 0.0005576788680627942
batch 60 loss: 0.0005577085190452636
batch 65 loss: 0.00055777532979846
batch 70 loss: 0.0005575243732891977
batch 75 loss: 0.0005575613351538778
batch 80 loss: 0.0005577064817771316
batch 85 loss: 0.0005577718722634018
batch 90 loss: 0.0005576733034104109
batch 95 loss: 0.0005577692412771284
batch 100 loss: 0.000557703897356987
batch 105 loss: 0.0005575924413278698
batch 110 loss: 0.0005576620111241937
batch 115 loss: 0.000557809416204691
batch 120 loss: 0.0005577401025220752
batch 125 loss: 0.0005575981922447681
batch 130 loss: 0.0005577083444222808
batch 135 loss: 0.0005576331051997841
batch 140 loss: 0.0005577021976932884
batch 145 loss: 0.0005576764582656324
batch 150 loss: 0.0005576663534156978
batch 155 loss: 0.0005576441064476967
batch 160 loss: 0.00055758620146662
batch 165 loss: 0.0005576718831434846
batch 170 loss: 0.0005576998111791909
batch 175 loss: 0.0005577566917054355
batch 180 loss: 0.0005577018018811941
batch 185 loss: 0.0005577046307735145
batch 190 loss: 0.0005577602190896868
batch 195 loss: 0.0005576607654802501
batch 200 loss: 0.0005575517774559557
batch 205 loss: 0.0005577829666435719
batch 210 loss: 0.0005577872274443507
batch 215 loss: 0.0005576320807449519
batch 220 loss: 0.0005577844218350947
batch 225 loss: 0.0005576051655225456
batch 230 loss: 0.0005575804854743183
batch 235 loss: 0.0005576176452450454
batch 240 loss: 0.0005576215218752623
Training Loss: 0.0005576833932233664
Validation Loss: 0.0005576934141572565
Epoch 29:
batch 5 loss: 0.0005576331051997841
batch 10 loss: 0.0005576117895543575
batch 15 loss: 0.0005576753872446715
batch 20 loss: 0.0005576553288847208
batch 25 loss: 0.0005576872732490301
batch 30 loss: 0.0005576408584602177
batch 35 loss: 0.0005576924188062549
batch 40 loss: 0.0005577024421654642
batch 45 loss: 0.0005576510448008776
batch 50 loss: 0.0005577118718065321
batch 55 loss: 0.0005576084135100245
batch 60 loss: 0.0005577182280831039
batch 65 loss: 0.000557684013620019
batch 70 loss: 0.000557739834766835
batch 75 loss: 0.0005576277035288513
batch 80 loss: 0.0005577788804657757
batch 85 loss: 0.0005576075636781752
batch 90 loss: 0.0005576333031058311
batch 95 loss: 0.0005576975061558187
batch 100 loss: 0.0005577259580604732
batch 105 loss: 0.000557653361465782
batch 110 loss: 0.0005576959112659097
batch 115 loss: 0.0005577470059506595
batch 120 loss: 0.0005576619994826614
batch 125 loss: 0.0005576967261731625
batch 130 loss: 0.0005577106843702495
batch 135 loss: 0.0005576790543273091
batch 140 loss: 0.0005576074705459178
batch 145 loss: 0.0005576904164627195
batch 150 loss: 0.0005576312774792314
batch 155 loss: 0.0005576359457336366
batch 160 loss: 0.0005576486932113766
batch 165 loss: 0.0005576965981163084
batch 170 loss: 0.0005576793919317424
batch 175 loss: 0.0005576581694185734
batch 180 loss: 0.0005576941650360823
batch 185 loss: 0.0005576909636147321
batch 190 loss: 0.0005576167954131961
batch 195 loss: 0.0005577287171036005
batch 200 loss: 0.0005576540599577129
batch 205 loss: 0.0005577643285505473
batch 210 loss: 0.000557757809292525
batch 215 loss: 0.0005577123141847551
batch 220 loss: 0.0005577145959250629
batch 225 loss: 0.0005577908363193273
batch 230 loss: 0.0005577485309913755
batch 235 loss: 0.0005577013711445033
batch 240 loss: 0.0005576531169936061
Training Loss: 0.0005576834007418559
Validation Loss: 0.0005576934267689164
Epoch 30:
batch 5 loss: 0.0005576086696237326
batch 10 loss: 0.0005576562718488276
batch 15 loss: 0.0005577063770033419
batch 20 loss: 0.000557628832757473
batch 25 loss: 0.000557641068007797
batch 30 loss: 0.0005576096358709037
batch 35 loss: 0.0005577525706030428
batch 40 loss: 0.000557685480453074
batch 45 loss: 0.0005576191702857614
batch 50 loss: 0.0005577387870289386
batch 55 loss: 0.0005576679715886713
batch 60 loss: 0.0005577682866714894
batch 65 loss: 0.0005576903000473976
batch 70 loss: 0.00055763756390661
batch 75 loss: 0.0005577366799116134
batch 80 loss: 0.0005576521740294993
batch 85 loss: 0.0005575652467086911
batch 90 loss: 0.0005575945135205985
batch 95 loss: 0.000557687075342983
batch 100 loss: 0.00055773543426767
batch 105 loss: 0.0005577448639087379
batch 110 loss: 0.0005576392053626478
batch 115 loss: 0.000557628448586911
batch 120 loss: 0.0005576675059273839
batch 125 loss: 0.0005576606956310571
batch 130 loss: 0.0005576145485974848
batch 135 loss: 0.0005577155621722341
batch 140 loss: 0.0005577237461693585
batch 145 loss: 0.0005577031173743307
batch 150 loss: 0.000557752454187721
batch 155 loss: 0.0005576891358941794
batch 160 loss: 0.0005576141295023263
batch 165 loss: 0.0005577117903158068
batch 170 loss: 0.0005577213247306645
batch 175 loss: 0.0005577191477641463
batch 180 loss: 0.0005577239440754056
batch 185 loss: 0.0005578329320997
batch 190 loss: 0.0005577547708526254
batch 195 loss: 0.0005576048628427088
batch 200 loss: 0.0005576888564974069
batch 205 loss: 0.0005575665622018277
batch 210 loss: 0.0005576725001446903
batch 215 loss: 0.0005576992873102427
batch 220 loss: 0.0005577115225605667
batch 225 loss: 0.0005577010684646666
batch 230 loss: 0.0005576692637987435
batch 235 loss: 0.0005577814416028559
batch 240 loss: 0.0005577080068178475
Training Loss: 0.0005576833917681749
Validation Loss: 0.0005576934713947897
Epoch 31:
batch 5 loss: 0.0005577064002864063
batch 10 loss: 0.0005575755843892693
batch 15 loss: 0.0005576790077611804
batch 20 loss: 0.0005576854106038809
batch 25 loss: 0.0005576518480665982
batch 30 loss: 0.000557692174334079
batch 35 loss: 0.000557692360598594
batch 40 loss: 0.0005577002419158816
batch 45 loss: 0.0005576139781624078
batch 50 loss: 0.0005576881463639438
batch 55 loss: 0.0005576935480348765
batch 60 loss: 0.0005577515694312751
batch 65 loss: 0.0005577704403549432
batch 70 loss: 0.000557682930957526
batch 75 loss: 0.0005577485426329076
batch 80 loss: 0.0005577070522122085
batch 85 loss: 0.0005576230818405747
batch 90 loss: 0.0005576446652412415
batch 95 loss: 0.000557652022689581
batch 100 loss: 0.0005577626288868487
batch 105 loss: 0.0005576039548031986
batch 110 loss: 0.0005576492752879858
batch 115 loss: 0.0005577049916610122
batch 120 loss: 0.0005576791823841632
batch 125 loss: 0.0005576604628004134
batch 130 loss: 0.0005576543044298887
batch 135 loss: 0.0005576321273110807
batch 140 loss: 0.000557701603975147
batch 145 loss: 0.0005576802184805274
batch 150 loss: 0.0005576313007622958
batch 155 loss: 0.0005577801144681871
batch 160 loss: 0.0005578233278356493
batch 165 loss: 0.0005576407420448959
batch 170 loss: 0.0005577710107900202
batch 175 loss: 0.0005576378549449146
batch 180 loss: 0.0005577290896326304
batch 185 loss: 0.0005576919997110963
batch 190 loss: 0.0005577233736403286
batch 195 loss: 0.0005577131872996688
batch 200 loss: 0.0005577138741500676
batch 205 loss: 0.0005576441762968898
batch 210 loss: 0.0005577566451393067
batch 215 loss: 0.0005576402181759477
batch 220 loss: 0.0005576730007305741
batch 225 loss: 0.0005576464813202619
batch 230 loss: 0.000557604362256825
batch 235 loss: 0.0005575703456997871
batch 240 loss: 0.0005577263655140996
Training Loss: 0.0005576834422148144
Validation Loss: 0.0005576933986352135
Epoch 32:
batch 5 loss: 0.0005578352138400078
batch 10 loss: 0.0005576930707320571
batch 15 loss: 0.0005576270865276456
batch 20 loss: 0.0005577356787398458
batch 25 loss: 0.0005577358650043606
batch 30 loss: 0.0005577092641033232
batch 35 loss: 0.0005576790543273091
batch 40 loss: 0.0005576216732151807
batch 45 loss: 0.000557756784837693
batch 50 loss: 0.0005576307303272188
batch 55 loss: 0.000557650183327496
batch 60 loss: 0.0005576557014137506
batch 65 loss: 0.0005576156312599778
batch 70 loss: 0.0005577136063948274
batch 75 loss: 0.0005576803931035101
batch 80 loss: 0.0005576428258791566
batch 85 loss: 0.0005577706266194582
batch 90 loss: 0.0005577414063736797
batch 95 loss: 0.0005577936302870512
batch 100 loss: 0.0005576579016633332
batch 105 loss: 0.000557627307716757
batch 110 loss: 0.0005577208939939737
batch 115 loss: 0.0005576530005782842
batch 120 loss: 0.0005575702874921262
batch 125 loss: 0.0005576632684096694
batch 130 loss: 0.0005575970280915499
batch 135 loss: 0.0005577089847065509
batch 140 loss: 0.0005576860625296831
batch 145 loss: 0.0005576435942202806
batch 150 loss: 0.0005576081108301878
batch 155 loss: 0.0005577138043008744
batch 160 loss: 0.0005576421157456934
batch 165 loss: 0.0005577097646892071
batch 170 loss: 0.0005577191128395498
batch 175 loss: 0.0005576308933086694
batch 180 loss: 0.0005576480994932354
batch 185 loss: 0.0005577220814302564
batch 190 loss: 0.0005576684488914907
batch 195 loss: 0.0005576155730523169
batch 200 loss: 0.0005576783092692495
batch 205 loss: 0.0005576791707426309
batch 210 loss: 0.0005576279480010271
batch 215 loss: 0.0005577457719482482
batch 220 loss: 0.0005578274372965098
batch 225 loss: 0.0005576362600550056
batch 230 loss: 0.0005576533963903785
batch 235 loss: 0.0005578070878982544
batch 240 loss: 0.0005576535826548934
Training Loss: 0.0005576834109281965
Validation Loss: 0.0005576934558727468
Epoch 33:
batch 5 loss: 0.0005576300318352878
batch 10 loss: 0.0005577490315772593
batch 15 loss: 0.0005575829069130123
batch 20 loss: 0.0005576717318035662
batch 25 loss: 0.0005576735828071832
batch 30 loss: 0.0005577206495217979
batch 35 loss: 0.0005577131407335401
batch 40 loss: 0.0005576905095949769
batch 45 loss: 0.0005576008697971702
batch 50 loss: 0.0005576466792263091
batch 55 loss: 0.0005576308583840727
batch 60 loss: 0.0005577155738137662
batch 65 loss: 0.0005576032563112676
batch 70 loss: 0.0005577220232225955
batch 75 loss: 0.000557710905559361
batch 80 loss: 0.0005577013595029712
batch 85 loss: 0.0005577274831011891
batch 90 loss: 0.0005576445953920483
batch 95 loss: 0.000557593209668994
batch 100 loss: 0.0005577371804974974
batch 105 loss: 0.0005576403462328016
batch 110 loss: 0.0005577139207161963
batch 115 loss: 0.000557721359655261
batch 120 loss: 0.000557681336067617
batch 125 loss: 0.0005577838979661465
batch 130 loss: 0.0005576191819272935
batch 135 loss: 0.0005576969357207418
batch 140 loss: 0.0005577873205766081
batch 145 loss: 0.0005576195428147912
batch 150 loss: 0.000557631382253021
batch 155 loss: 0.0005576657829806208
batch 160 loss: 0.0005577402072958649
batch 165 loss: 0.0005576872965320945
batch 170 loss: 0.0005576593452133238
batch 175 loss: 0.0005576827330514789
batch 180 loss: 0.0005576625582762063
batch 185 loss: 0.0005576636642217637
batch 190 loss: 0.0005577472620643675
batch 195 loss: 0.0005576798226684332
batch 200 loss: 0.0005577374016866088
batch 205 loss: 0.0005576523486524821
batch 210 loss: 0.0005576352356001735
batch 215 loss: 0.0005577227799221874
batch 220 loss: 0.0005577443283982575
batch 225 loss: 0.0005576974363066256
batch 230 loss: 0.0005577565170824528
batch 235 loss: 0.0005576193099841476
batch 240 loss: 0.0005576889729127288
Training Loss: 0.00055768341262592
Validation Loss: 0.0005576934199780226
Epoch 34:
batch 5 loss: 0.000557721289806068
batch 10 loss: 0.0005577176110818982
batch 15 loss: 0.0005576547584496439
batch 20 loss: 0.000557724735699594
batch 25 loss: 0.0005576752359047532
batch 30 loss: 0.0005576579016633332
batch 35 loss: 0.000557710335124284
batch 40 loss: 0.0005576105904765427
batch 45 loss: 0.0005575593211688101
batch 50 loss: 0.0005577538628131151
batch 55 loss: 0.0005577319650910794
batch 60 loss: 0.0005577760981395841
batch 65 loss: 0.0005576952593401074
batch 70 loss: 0.0005576634663157165
batch 75 loss: 0.0005575599265284836
batch 80 loss: 0.000557603023480624
batch 85 loss: 0.0005577041883952916
batch 90 loss: 0.0005576930707320571
batch 95 loss: 0.0005576923140324652
batch 100 loss: 0.0005576305207796395
batch 105 loss: 0.0005577070754952729
batch 110 loss: 0.0005578202195465565
batch 115 loss: 0.0005576884024776519
batch 120 loss: 0.0005577157135121524
batch 125 loss: 0.0005576992174610495
batch 130 loss: 0.0005576734431087971
batch 135 loss: 0.0005577123374678195
batch 140 loss: 0.0005577968899160623
batch 145 loss: 0.0005576541530899704
batch 150 loss: 0.0005576780647970736
batch 155 loss: 0.0005576311494223773
batch 160 loss: 0.0005576461553573608
batch 165 loss: 0.0005577318021096289
batch 170 loss: 0.000557783548720181
batch 175 loss: 0.0005576342227868736
batch 180 loss: 0.0005576747586019337
batch 185 loss: 0.0005575933028012514
batch 190 loss: 0.0005576730240136385
batch 195 loss: 0.0005576053285039961
batch 200 loss: 0.0005577512551099062
batch 205 loss: 0.0005575897404924036
batch 210 loss: 0.0005576552939601243
batch 215 loss: 0.0005576745374128222
batch 220 loss: 0.0005577578558586538
batch 225 loss: 0.0005576567142270506
batch 230 loss: 0.0005577009869739413
batch 235 loss: 0.0005576974013820291
batch 240 loss: 0.0005576667026616633
Training Loss: 0.0005576834327560694
Validation Loss: 0.0005576934549026191
Epoch 35:
batch 5 loss: 0.0005577355157583952
batch 10 loss: 0.0005576738971285522
batch 15 loss: 0.0005577652947977186
batch 20 loss: 0.0005577113130129874
batch 25 loss: 0.000557777110952884
batch 30 loss: 0.0005575626157224179
batch 35 loss: 0.0005575782852247357
batch 40 loss: 0.0005577355972491205
batch 45 loss: 0.0005577402072958649
batch 50 loss: 0.000557705806568265
batch 55 loss: 0.0005577389267273248
batch 60 loss: 0.0005577094620093703
batch 65 loss: 0.0005576452007517218
batch 70 loss: 0.0005577078787609935
batch 75 loss: 0.0005576659343205393
batch 80 loss: 0.0005576034309342504
batch 85 loss: 0.0005577296135015786
batch 90 loss: 0.0005576651310548186
batch 95 loss: 0.000557707401458174
batch 100 loss: 0.0005575981922447681
batch 105 loss: 0.0005577174597419798
batch 110 loss: 0.000557716644834727
batch 115 loss: 0.0005576089024543762
batch 120 loss: 0.0005576173891313374
batch 125 loss: 0.0005576790077611804
batch 130 loss: 0.0005577294854447245
batch 135 loss: 0.000557638006284833
batch 140 loss: 0.0005577106028795242
batch 145 loss: 0.0005577846430242061
batch 150 loss: 0.0005576855852268636
batch 155 loss: 0.0005577170988544822
batch 160 loss: 0.0005576894618570805
batch 165 loss: 0.0005575505085289479
batch 170 loss: 0.0005576714756898582
batch 175 loss: 0.0005576835363171994
batch 180 loss: 0.0005576782510615885
batch 185 loss: 0.0005575669347308576
batch 190 loss: 0.0005577589967288077
batch 195 loss: 0.0005576668307185173
batch 200 loss: 0.0005576052935793996
batch 205 loss: 0.0005576516268774867
batch 210 loss: 0.0005577298230491579
batch 215 loss: 0.0005577535252086818
batch 220 loss: 0.0005576434778049588
batch 225 loss: 0.0005576705909334124
batch 230 loss: 0.0005576401716098189
batch 235 loss: 0.0005577611504122615
batch 240 loss: 0.0005577207659371198
Training Loss: 0.0005576834179616222
Validation Loss: 0.0005576934393805762
Epoch 36:
batch 5 loss: 0.0005576349212788045
batch 10 loss: 0.0005577013595029712
batch 15 loss: 0.0005577667499892414
batch 20 loss: 0.0005576043506152928
batch 25 loss: 0.0005577236297540367
batch 30 loss: 0.0005577158415690064
batch 35 loss: 0.0005578039214015007
batch 40 loss: 0.0005578105221502483
batch 45 loss: 0.0005576605908572674
batch 50 loss: 0.0005576924537308514
batch 55 loss: 0.0005575782153755427
batch 60 loss: 0.0005576118477620184
batch 65 loss: 0.0005576914176344871
batch 70 loss: 0.0005577162490226328
batch 75 loss: 0.0005576818017289043
batch 80 loss: 0.0005576278315857052
batch 85 loss: 0.0005576427211053669
batch 90 loss: 0.0005576377152465284
batch 95 loss: 0.0005576756317168474
batch 100 loss: 0.0005577072384767235
batch 105 loss: 0.0005577101022936404
batch 110 loss: 0.0005575342103838921
batch 115 loss: 0.0005577747942879796
batch 120 loss: 0.0005576923140324652
batch 125 loss: 0.0005577459814958274
batch 130 loss: 0.0005575911025516688
batch 135 loss: 0.0005576514988206327
batch 140 loss: 0.000557718297932297
batch 145 loss: 0.0005577877978794277
batch 150 loss: 0.0005577027681283653
batch 155 loss: 0.0005576859693974257
batch 160 loss: 0.0005576398456469178
batch 165 loss: 0.0005576365278102457
batch 170 loss: 0.0005576184834353626
batch 175 loss: 0.0005577037460170686
batch 180 loss: 0.0005576575873419643
batch 185 loss: 0.0005576653173193335
batch 190 loss: 0.0005577635834924877
batch 195 loss: 0.0005576833267696202
batch 200 loss: 0.0005576556781306863
batch 205 loss: 0.0005576981115154922
batch 210 loss: 0.0005577011615969241
batch 215 loss: 0.0005577445728704334
batch 220 loss: 0.0005576157825998961
batch 225 loss: 0.0005577410222031177
batch 230 loss: 0.0005576644442044198
batch 235 loss: 0.0005576151073910296
batch 240 loss: 0.0005577210220508277
Training Loss: 0.0005576834410021547
Validation Loss: 0.0005576935334829613
Epoch 37:
batch 5 loss: 0.0005577467265538872
batch 10 loss: 0.000557603279594332
batch 15 loss: 0.0005577962612733245
batch 20 loss: 0.0005577405798248946
batch 25 loss: 0.0005577158299274743
batch 30 loss: 0.000557659415062517
batch 35 loss: 0.000557706004474312
batch 40 loss: 0.0005578025593422353
batch 45 loss: 0.0005576882511377334
batch 50 loss: 0.0005576085532084107
batch 55 loss: 0.000557660823687911
batch 60 loss: 0.0005576775176450611
batch 65 loss: 0.0005578013835474849
batch 70 loss: 0.0005576844559982419
batch 75 loss: 0.0005577463889494538
batch 80 loss: 0.0005575557588599622
batch 85 loss: 0.0005576274474151432
batch 90 loss: 0.0005577171570621431
batch 95 loss: 0.0005576912430115044
batch 100 loss: 0.000557681021746248
batch 105 loss: 0.0005576840951107443
batch 110 loss: 0.0005576206254772842
batch 115 loss: 0.0005576970870606601
batch 120 loss: 0.0005576515104621649
batch 125 loss: 0.0005576945841312408
batch 130 loss: 0.000557652220595628
batch 135 loss: 0.0005577239091508091
batch 140 loss: 0.0005576241877861321
batch 145 loss: 0.0005576879484578967
batch 150 loss: 0.0005576476221904158
batch 155 loss: 0.0005575193674303592
batch 160 loss: 0.0005577181698754429
batch 165 loss: 0.0005576633149757982
batch 170 loss: 0.0005577603238634765
batch 175 loss: 0.0005576599971391261
batch 180 loss: 0.0005577354691922665
batch 185 loss: 0.0005577982752583921
batch 190 loss: 0.0005576511030085385
batch 195 loss: 0.0005576268653385341
batch 200 loss: 0.0005576457362622023
batch 205 loss: 0.0005577472387813031
batch 210 loss: 0.0005576443509198725
batch 215 loss: 0.0005577353411354124
batch 220 loss: 0.000557627808302641
batch 225 loss: 0.0005576171795837581
batch 230 loss: 0.0005577247706241905
batch 235 loss: 0.0005577521049417555
batch 240 loss: 0.0005575836752541363
Training Loss: 0.0005576834487631761
Validation Loss: 0.0005576934607233852
Epoch 38:
batch 5 loss: 0.0005576184135861695
batch 10 loss: 0.0005577201372943819
batch 15 loss: 0.0005576437572017312
batch 20 loss: 0.0005576949799433351
batch 25 loss: 0.0005576955969445408
batch 30 loss: 0.0005577091360464692
batch 35 loss: 0.0005576446768827736
batch 40 loss: 0.0005578181007876992
batch 45 loss: 0.0005576226045377553
batch 50 loss: 0.0005575825343839824
batch 55 loss: 0.0005576570052653551
batch 60 loss: 0.0005576387862674892
batch 65 loss: 0.0005577653995715081
batch 70 loss: 0.000557629147078842
batch 75 loss: 0.0005577392876148224
batch 80 loss: 0.0005576854106038809
batch 85 loss: 0.0005577312898822129
batch 90 loss: 0.0005575922201387584
batch 95 loss: 0.0005578711512498558
batch 100 loss: 0.0005576308583840727
batch 105 loss: 0.0005577309406362474
batch 110 loss: 0.0005575543735176324
batch 115 loss: 0.0005576888564974069
batch 120 loss: 0.0005577289382927119
batch 125 loss: 0.0005577947478741408
batch 130 loss: 0.0005576636176556349
batch 135 loss: 0.0005576024879701436
batch 140 loss: 0.0005577401840128005
batch 145 loss: 0.0005577715113759041
batch 150 loss: 0.0005577389150857925
batch 155 loss: 0.0005577118368819356
batch 160 loss: 0.000557581486646086
batch 165 loss: 0.0005576316849328578
batch 170 loss: 0.0005577115225605667
batch 175 loss: 0.0005576240364462137
batch 180 loss: 0.0005577124888077378
batch 185 loss: 0.0005575998569838702
batch 190 loss: 0.0005576262599788606
batch 195 loss: 0.0005577511037699878
batch 200 loss: 0.0005576523137278855
batch 205 loss: 0.0005577322212047875
batch 210 loss: 0.0005576970404945313
batch 215 loss: 0.0005576501251198351
batch 220 loss: 0.0005576928495429456
batch 225 loss: 0.0005577139556407929
batch 230 loss: 0.0005576689727604389
batch 235 loss: 0.000557697145268321
batch 240 loss: 0.0005576466443017125
Training Loss: 0.0005576834710761129
Validation Loss: 0.0005576934335598101
Epoch 39:
batch 5 loss: 0.0005577684380114079
batch 10 loss: 0.000557723781093955
batch 15 loss: 0.0005577988806180656
batch 20 loss: 0.0005576578201726079
batch 25 loss: 0.000557692104484886
batch 30 loss: 0.000557836820371449
batch 35 loss: 0.0005576086230576038
batch 40 loss: 0.0005576377152465284
batch 45 loss: 0.0005576220108196139
batch 50 loss: 0.0005576908821240068
batch 55 loss: 0.0005577186704613269
batch 60 loss: 0.0005576582974754274
batch 65 loss: 0.0005576143972575665
batch 70 loss: 0.0005576177383773029
batch 75 loss: 0.0005577026400715113
batch 80 loss: 0.0005575649905949831
batch 85 loss: 0.0005575712304562331
batch 90 loss: 0.0005575609160587191
batch 95 loss: 0.0005577386706136167
batch 100 loss: 0.0005577404284849763
batch 105 loss: 0.0005577597534283995
batch 110 loss: 0.0005575783434323967
batch 115 loss: 0.0005577043048106134
batch 120 loss: 0.0005577045842073858
batch 125 loss: 0.0005576114053837955
batch 130 loss: 0.0005577362491749227
batch 135 loss: 0.0005576721043325961
batch 140 loss: 0.0005576103576458991
batch 145 loss: 0.0005577654112130404
batch 150 loss: 0.0005576900322921575
batch 155 loss: 0.000557735119946301
batch 160 loss: 0.000557670183479786
batch 165 loss: 0.0005576211377047002
batch 170 loss: 0.0005577860167250038
batch 175 loss: 0.0005577116971835494
batch 180 loss: 0.0005576816620305181
batch 185 loss: 0.0005577536183409392
batch 190 loss: 0.0005577340605668723
batch 195 loss: 0.0005577687988989055
batch 200 loss: 0.0005575847229920327
batch 205 loss: 0.0005576631869189441
batch 210 loss: 0.0005576601019129157
batch 215 loss: 0.0005576950148679316
batch 220 loss: 0.0005577619653195143
batch 225 loss: 0.00055762646952644
batch 230 loss: 0.0005576943629421293
batch 235 loss: 0.0005577278789132833
batch 240 loss: 0.0005575711838901043
Training Loss: 0.0005576834329986014
Validation Loss: 0.0005576934801259388
Epoch 40:
batch 5 loss: 0.0005577195785008371
batch 10 loss: 0.0005577733856625855
batch 15 loss: 0.000557682930957526
batch 20 loss: 0.0005575898569077254
batch 25 loss: 0.0005576730356551707
batch 30 loss: 0.0005576711846515536
batch 35 loss: 0.0005577066913247108
batch 40 loss: 0.0005576424649916589
batch 45 loss: 0.0005577519885264337
batch 50 loss: 0.0005576192284934223
batch 55 loss: 0.0005576384020969272
batch 60 loss: 0.0005576932104304433
batch 65 loss: 0.0005576036870479584
batch 70 loss: 0.0005576374242082238
batch 75 loss: 0.0005577964475378394
batch 80 loss: 0.0005577220697887241
batch 85 loss: 0.0005576022434979677
batch 90 loss: 0.0005577150266617536
batch 95 loss: 0.0005576633149757982
batch 100 loss: 0.0005578159354627133
batch 105 loss: 0.0005577638861723244
batch 110 loss: 0.0005577076459303498
batch 115 loss: 0.0005575943505391479
batch 120 loss: 0.0005577403586357832
batch 125 loss: 0.0005576698807999491
batch 130 loss: 0.0005575103568844497
batch 135 loss: 0.0005576901137828826
batch 140 loss: 0.000557653047144413
batch 145 loss: 0.0005577780422754586
batch 150 loss: 0.0005575916031375528
batch 155 loss: 0.0005576704046688974
batch 160 loss: 0.0005576221039518714
batch 165 loss: 0.0005576759809628129
batch 170 loss: 0.0005576381459832192
batch 175 loss: 0.0005576949799433351
batch 180 loss: 0.0005577254574745894
batch 185 loss: 0.0005576692405156791
batch 190 loss: 0.0005576884257607162
batch 195 loss: 0.0005577313480898738
batch 200 loss: 0.0005577811854891479
batch 205 loss: 0.0005576259805820883
batch 210 loss: 0.0005576669587753713
batch 215 loss: 0.0005576054682023823
batch 220 loss: 0.0005576066323556006
batch 225 loss: 0.0005576969240792095
batch 230 loss: 0.0005577026517130434
batch 235 loss: 0.0005577416624873877
batch 240 loss: 0.0005578462383709848
Training Loss: 0.000557683482960177
Validation Loss: 0.0005576936663904537
Epoch 41:
batch 5 loss: 0.0005577106727287173
batch 10 loss: 0.000557782338000834
batch 15 loss: 0.0005577764357440173
batch 20 loss: 0.0005575886112637818
batch 25 loss: 0.0005576634546741843
batch 30 loss: 0.0005576533847488462
batch 35 loss: 0.0005577509640716016
batch 40 loss: 0.0005576552241109312
batch 45 loss: 0.0005576246418058872
batch 50 loss: 0.0005577079718932509
batch 55 loss: 0.0005576865747570992
batch 60 loss: 0.0005577352712862194
batch 65 loss: 0.0005577084026299417
batch 70 loss: 0.0005577180301770568
batch 75 loss: 0.0005575775518082082
batch 80 loss: 0.0005576363066211343
batch 85 loss: 0.0005577016854658723
batch 90 loss: 0.0005576398223638534
batch 95 loss: 0.0005576532683335244
batch 100 loss: 0.0005576837575063109
batch 105 loss: 0.0005576670053415
batch 110 loss: 0.0005577623727731406
batch 115 loss: 0.0005578417563810945
batch 120 loss: 0.0005576396710239351
batch 125 loss: 0.0005576798808760941
batch 130 loss: 0.0005576998344622552
batch 135 loss: 0.0005575816147029399
batch 140 loss: 0.0005577167612500489
batch 145 loss: 0.0005576760857366025
batch 150 loss: 0.0005578064476139844
batch 155 loss: 0.0005577623727731406
batch 160 loss: 0.0005576721276156604
batch 165 loss: 0.0005576951080001891
batch 170 loss: 0.0005576290888711811
batch 175 loss: 0.0005576837342232466
batch 180 loss: 0.0005577021162025631
batch 185 loss: 0.0005576868425123394
batch 190 loss: 0.0005576639086939394
batch 195 loss: 0.0005576864816248417
batch 200 loss: 0.0005576021736487746
batch 205 loss: 0.0005576867959462106
batch 210 loss: 0.000557690067216754
batch 215 loss: 0.000557758251670748
batch 220 loss: 0.0005576337338425219
batch 225 loss: 0.0005576746189035475
batch 230 loss: 0.0005576386116445065
batch 235 loss: 0.0005576182040385902
batch 240 loss: 0.0005575958988629281
Training Loss: 0.0005576834570092615
Validation Loss: 0.0005576936489281555
Epoch 42:
batch 5 loss: 0.0005577003466896713
batch 10 loss: 0.0005577308125793934
batch 15 loss: 0.0005576983909122646
batch 20 loss: 0.0005575702292844653
batch 25 loss: 0.000557685480453074
batch 30 loss: 0.0005577098694629967
batch 35 loss: 0.0005576681345701217
batch 40 loss: 0.0005576390540227294
batch 45 loss: 0.000557707529515028
batch 50 loss: 0.0005576375289820135
batch 55 loss: 0.0005577895091846586
batch 60 loss: 0.0005576897412538529
batch 65 loss: 0.0005576453637331724
batch 70 loss: 0.0005577710922807455
batch 75 loss: 0.0005577828153036535
batch 80 loss: 0.0005576714989729226
batch 85 loss: 0.0005576330353505909
batch 90 loss: 0.0005576115683652461
batch 95 loss: 0.0005577375530265271
batch 100 loss: 0.0005576949799433351
batch 105 loss: 0.0005576902534812689
batch 110 loss: 0.0005576772964559495
batch 115 loss: 0.0005576460622251034
batch 120 loss: 0.0005577138275839388
batch 125 loss: 0.0005576789728365839
batch 130 loss: 0.0005576289142481983
batch 135 loss: 0.0005576653522439301
batch 140 loss: 0.0005575960618443787
batch 145 loss: 0.0005577071919105947
batch 150 loss: 0.0005575991352088749
batch 155 loss: 0.0005576574592851102
batch 160 loss: 0.0005575556540861725
batch 165 loss: 0.000557779986411333
batch 170 loss: 0.0005576536175794899
batch 175 loss: 0.0005576995201408864
batch 180 loss: 0.000557657121680677
batch 185 loss: 0.0005576875642873347
batch 190 loss: 0.0005577409057877958
batch 195 loss: 0.0005576686700806022
batch 200 loss: 0.0005577604286372661
batch 205 loss: 0.0005577739444561303
batch 210 loss: 0.0005576863419264555
batch 215 loss: 0.0005576215917244554
batch 220 loss: 0.000557657890021801
batch 225 loss: 0.0005577330361120403
batch 230 loss: 0.0005576705443672836
batch 235 loss: 0.0005576932104304433
batch 240 loss: 0.0005577329080551863
Training Loss: 0.0005576834999374114
Validation Loss: 0.0005576934510221084
Epoch 43:
batch 5 loss: 0.0005577161442488432
batch 10 loss: 0.0005576745257712901
batch 15 loss: 0.0005576236406341195
batch 20 loss: 0.0005576298222877085
batch 25 loss: 0.0005576474824920297
batch 30 loss: 0.0005576257128268481
batch 35 loss: 0.0005576950614340603
batch 40 loss: 0.0005577237461693585
batch 45 loss: 0.0005576603231020272
batch 50 loss: 0.0005577930947765708
batch 55 loss: 0.0005576342344284057
batch 60 loss: 0.0005576903000473976
batch 65 loss: 0.0005576670286245644
batch 70 loss: 0.000557714351452887
batch 75 loss: 0.0005576694966293871
batch 80 loss: 0.0005577054340392351
batch 85 loss: 0.0005577104049734772
batch 90 loss: 0.0005577640025876462
batch 95 loss: 0.0005576701019890607
batch 100 loss: 0.0005577319185249508
batch 105 loss: 0.0005576844909228384
batch 110 loss: 0.000557750416919589
batch 115 loss: 0.0005575760267674923
batch 120 loss: 0.0005576702649705112
batch 125 loss: 0.000557677773758769
batch 130 loss: 0.0005576718947850167
batch 135 loss: 0.0005578087060712278
batch 140 loss: 0.0005577328498475254
batch 145 loss: 0.0005576377152465284
batch 150 loss: 0.0005576199386268854
batch 155 loss: 0.0005576479947194457
batch 160 loss: 0.0005576800322160125
batch 165 loss: 0.0005577948875725269
batch 170 loss: 0.0005575948744080961
batch 175 loss: 0.0005577741656452417
batch 180 loss: 0.0005576503463089466
batch 185 loss: 0.0005576104507781565
batch 190 loss: 0.0005576824070885778
batch 195 loss: 0.0005576820112764835
batch 200 loss: 0.0005576190887950361
batch 205 loss: 0.0005576873663812876
batch 210 loss: 0.0005577497417107224
batch 215 loss: 0.0005577261676080525
batch 220 loss: 0.000557671522255987
batch 225 loss: 0.0005576831987127662
batch 230 loss: 0.0005576109397225082
batch 235 loss: 0.0005577420815825462
batch 240 loss: 0.0005576238967478275
Training Loss: 0.0005576835016351348
Validation Loss: 0.0005576934636337683
Epoch 44:
batch 5 loss: 0.0005576904746703804
batch 10 loss: 0.0005577582749538123
batch 15 loss: 0.0005577427451498807
batch 20 loss: 0.0005576713476330042
batch 25 loss: 0.0005575894843786955
batch 30 loss: 0.0005576200201176107
batch 35 loss: 0.000557646353263408
batch 40 loss: 0.0005577025469392539
batch 45 loss: 0.000557494768872857
batch 50 loss: 0.000557652860879898
batch 55 loss: 0.0005577009287662804
batch 60 loss: 0.0005577734438702465
batch 65 loss: 0.0005578160984441638
batch 70 loss: 0.0005576972384005785
batch 75 loss: 0.0005576796480454505
batch 80 loss: 0.0005577406147494913
batch 85 loss: 0.0005577541072852909
batch 90 loss: 0.0005576634546741843
batch 95 loss: 0.0005576716037467122
batch 100 loss: 0.0005577107658609748
batch 105 loss: 0.0005577237694524229
batch 110 loss: 0.0005576714756898582
batch 115 loss: 0.000557708041742444
batch 120 loss: 0.0005576915107667446
batch 125 loss: 0.0005576845142059028
batch 130 loss: 0.0005576098687015474
batch 135 loss: 0.0005576650728471577
batch 140 loss: 0.0005575752584263682
batch 145 loss: 0.000557579379528761
batch 150 loss: 0.0005576750496402383
batch 155 loss: 0.0005577574833296239
batch 160 loss: 0.0005576873198151588
batch 165 loss: 0.0005576523719355464
batch 170 loss: 0.0005577641888521612
batch 175 loss: 0.0005576024181209504
batch 180 loss: 0.0005576845607720316
batch 185 loss: 0.0005577208590693772
batch 190 loss: 0.0005575579823926091
batch 195 loss: 0.0005577723612077534
batch 200 loss: 0.0005576595547609031
batch 205 loss: 0.0005575370509177446
batch 210 loss: 0.0005577341769821941
batch 215 loss: 0.0005576936295256018
batch 220 loss: 0.0005577374831773341
batch 225 loss: 0.0005577223375439644
batch 230 loss: 0.0005577583448030055
batch 235 loss: 0.000557739194482565
batch 240 loss: 0.00055766343139112
Training Loss: 0.0005576834473079846
Validation Loss: 0.000557693424828661
Epoch 45:
batch 5 loss: 0.0005577540141530335
batch 10 loss: 0.0005577220232225955
batch 15 loss: 0.0005576491355895996
batch 20 loss: 0.0005578334094025195
batch 25 loss: 0.0005577329080551863
batch 30 loss: 0.0005576790310442448
batch 35 loss: 0.0005576516152359546
batch 40 loss: 0.0005577096715569496
batch 45 loss: 0.0005577279254794121
batch 50 loss: 0.000557531334925443
batch 55 loss: 0.0005576443276368081
batch 60 loss: 0.0005576520692557096
batch 65 loss: 0.0005576590541750192
batch 70 loss: 0.0005576831172220409
batch 75 loss: 0.0005577121977694332
batch 80 loss: 0.0005576428840868175
batch 85 loss: 0.0005577217205427587
batch 90 loss: 0.0005578371463343501
batch 95 loss: 0.0005577424308285117
batch 100 loss: 0.0005577108473517001
batch 105 loss: 0.0005576824652962387
batch 110 loss: 0.0005577769712544978
batch 115 loss: 0.0005576973431743682
batch 120 loss: 0.000557641324121505
batch 125 loss: 0.0005576087511144579
batch 130 loss: 0.0005576610215939582
batch 135 loss: 0.0005575965973548591
batch 140 loss: 0.0005576104624196887
batch 145 loss: 0.0005576289840973914
batch 150 loss: 0.000557578494772315
batch 155 loss: 0.0005577677162364125
batch 160 loss: 0.0005575743154622615
batch 165 loss: 0.0005577079718932509
batch 170 loss: 0.0005576592404395342
batch 175 loss: 0.0005576372146606446
batch 180 loss: 0.0005576108465902508
batch 185 loss: 0.0005576563882641495
batch 190 loss: 0.0005577723146416247
batch 195 loss: 0.0005576286115683615
batch 200 loss: 0.0005578427808359265
batch 205 loss: 0.0005576990311965347
batch 210 loss: 0.0005577524658292532
batch 215 loss: 0.0005576791241765022
batch 220 loss: 0.000557746016420424
batch 225 loss: 0.0005578077398240566
batch 230 loss: 0.000557618064340204
batch 235 loss: 0.0005575949675403535
batch 240 loss: 0.000557571358513087
Training Loss: 0.0005576834468229208
Validation Loss: 0.0005576934616935129
Epoch 46:
batch 5 loss: 0.00055768700549379
batch 10 loss: 0.0005577027681283653
batch 15 loss: 0.0005577888572588563
batch 20 loss: 0.0005576572846621275
batch 25 loss: 0.0005576035706326366
batch 30 loss: 0.0005576300784014166
batch 35 loss: 0.0005577210220508277
batch 40 loss: 0.0005577118834480643
batch 45 loss: 0.0005576019641011954
batch 50 loss: 0.000557715306058526
batch 55 loss: 0.0005577109754085541
batch 60 loss: 0.0005575845134444535
batch 65 loss: 0.0005577724659815431
batch 70 loss: 0.0005576162948273122
batch 75 loss: 0.000557702174410224
batch 80 loss: 0.0005576426745392382
batch 85 loss: 0.000557677960023284
batch 90 loss: 0.0005576353520154953
batch 95 loss: 0.0005577151663601398
batch 100 loss: 0.0005576742347329855
batch 105 loss: 0.000557791453320533
batch 110 loss: 0.0005577921983785927
batch 115 loss: 0.0005577040719799697
batch 120 loss: 0.0005576822790317237
batch 125 loss: 0.0005577877745963633
batch 130 loss: 0.0005576837807893753
batch 135 loss: 0.0005576370400376618
batch 140 loss: 0.0005577388335950672
batch 145 loss: 0.0005577747011557222
batch 150 loss: 0.0005577933276072145
batch 155 loss: 0.0005577097530476749
batch 160 loss: 0.0005577044328674674
batch 165 loss: 0.000557637622114271
batch 170 loss: 0.0005576432566158473
batch 175 loss: 0.000557614432182163
batch 180 loss: 0.0005576078197918833
batch 185 loss: 0.0005575278890319168
batch 190 loss: 0.0005577494041062892
batch 195 loss: 0.0005577604053542018
batch 200 loss: 0.0005576227675192058
batch 205 loss: 0.0005576434428803623
batch 210 loss: 0.0005576610332354903
batch 215 loss: 0.0005576744326390326
batch 220 loss: 0.0005576970521360636
batch 225 loss: 0.0005576611729338765
batch 230 loss: 0.000557568995282054
batch 235 loss: 0.0005577062023803592
batch 240 loss: 0.0005576801602728664
Training Loss: 0.0005576834851429642
Validation Loss: 0.000557693912802885
Epoch 47:
batch 5 loss: 0.0005575801944360137
batch 10 loss: 0.0005576067022047936
batch 15 loss: 0.0005576735362410546
batch 20 loss: 0.0005577857373282313
batch 25 loss: 0.0005576590192504227
batch 30 loss: 0.0005577928735874593
batch 35 loss: 0.0005577270523644984
batch 40 loss: 0.0005576834082603455
batch 45 loss: 0.0005575423012487591
batch 50 loss: 0.000557684781961143
batch 55 loss: 0.0005577087053097784
batch 60 loss: 0.000557631824631244
batch 65 loss: 0.0005575348157435655
batch 70 loss: 0.0005576913245022297
batch 75 loss: 0.0005577046424150467
batch 80 loss: 0.0005578163429163397
batch 85 loss: 0.0005576845724135637
batch 90 loss: 0.0005577339208684862
batch 95 loss: 0.0005575835006311536
batch 100 loss: 0.0005576292867772281
batch 105 loss: 0.0005577740026637912
batch 110 loss: 0.0005577358766458929
batch 115 loss: 0.0005576407304033637
batch 120 loss: 0.0005577721050940454
batch 125 loss: 0.0005577370407991111
batch 130 loss: 0.000557737797498703
batch 135 loss: 0.0005577198462560773
batch 140 loss: 0.0005576991243287921
batch 145 loss: 0.0005577455158345401
batch 150 loss: 0.000557678914628923
batch 155 loss: 0.0005577358766458929
batch 160 loss: 0.0005576353403739631
batch 165 loss: 0.0005576285184361041
batch 170 loss: 0.0005576550378464162
batch 175 loss: 0.0005577434087172151
batch 180 loss: 0.0005576746421866119
batch 185 loss: 0.0005576959229074419
batch 190 loss: 0.0005577355506829918
batch 195 loss: 0.0005577481468208134
batch 200 loss: 0.0005575524410232902
batch 205 loss: 0.0005576909985393286
batch 210 loss: 0.0005576115800067782
batch 215 loss: 0.0005576441530138254
batch 220 loss: 0.0005576535244472325
batch 225 loss: 0.0005576737807132303
batch 230 loss: 0.0005576632334850729
batch 235 loss: 0.0005577478674240411
batch 240 loss: 0.0005576235358603299
Training Loss: 0.0005576835220078162
Validation Loss: 0.0005576934306494271
Epoch 48:
batch 5 loss: 0.000557698798365891
batch 10 loss: 0.0005577234202064574
batch 15 loss: 0.0005576031049713493
batch 20 loss: 0.0005576721276156604
batch 25 loss: 0.0005576994037255645
batch 30 loss: 0.0005576169933192432
batch 35 loss: 0.0005577103001996875
batch 40 loss: 0.0005575803807005286
batch 45 loss: 0.0005577150033786893
batch 50 loss: 0.0005577194853685797
batch 55 loss: 0.0005577047821134328
batch 60 loss: 0.0005577138043008744
batch 65 loss: 0.0005575866554863751
batch 70 loss: 0.0005577823729254305
batch 75 loss: 0.0005576716386713088
batch 80 loss: 0.0005576238501816988
batch 85 loss: 0.0005577466101385653
batch 90 loss: 0.0005577057483606041
batch 95 loss: 0.0005576292634941638
batch 100 loss: 0.0005575615214183927
batch 105 loss: 0.0005576902884058654
batch 110 loss: 0.0005576557363383472
batch 115 loss: 0.0005576522788032889
batch 120 loss: 0.0005576713127084076
batch 125 loss: 0.0005577834905125201
batch 130 loss: 0.0005577982985414565
batch 135 loss: 0.0005577046074904501
batch 140 loss: 0.0005576884490437805
batch 145 loss: 0.0005576548050157726
batch 150 loss: 0.0005576275405474007
batch 155 loss: 0.0005577660049311817
batch 160 loss: 0.0005576415685936808
batch 165 loss: 0.0005577848642133176
batch 170 loss: 0.0005576349212788045
batch 175 loss: 0.0005578230251558125
batch 180 loss: 0.0005576865165494383
batch 185 loss: 0.0005577386007644236
batch 190 loss: 0.0005577111500315368
batch 195 loss: 0.0005576421623118222
batch 200 loss: 0.0005577012663707138
batch 205 loss: 0.000557741685770452
batch 210 loss: 0.000557640683837235
batch 215 loss: 0.0005577874951995909
batch 220 loss: 0.0005576063878834248
batch 225 loss: 0.0005576260155066848
batch 230 loss: 0.0005576213239692152
batch 235 loss: 0.0005576988565735519
batch 240 loss: 0.0005575641407631338
Training Loss: 0.0005576835154594543
Validation Loss: 0.0005576934500519808
Epoch 49:
batch 5 loss: 0.0005577588453888893
batch 10 loss: 0.0005576846771873534
batch 15 loss: 0.0005576680996455252
batch 20 loss: 0.0005575272138230502
batch 25 loss: 0.0005576408584602177
batch 30 loss: 0.0005576859461143613
batch 35 loss: 0.0005576963187195361
batch 40 loss: 0.0005576975992880762
batch 45 loss: 0.0005578134907409549
batch 50 loss: 0.0005576612427830696
batch 55 loss: 0.0005576949450187385
batch 60 loss: 0.0005576498690061272
batch 65 loss: 0.0005578185548074544
batch 70 loss: 0.0005577023490332067
batch 75 loss: 0.0005576808238402009
batch 80 loss: 0.000557627808302641
batch 85 loss: 0.0005577433388680219
batch 90 loss: 0.0005576701601967216
batch 95 loss: 0.0005576221738010645
batch 100 loss: 0.000557624886278063
batch 105 loss: 0.0005576484370976686
batch 110 loss: 0.0005576783092692495
batch 115 loss: 0.0005576647934503853
batch 120 loss: 0.0005577155505307018
batch 125 loss: 0.0005576307303272188
batch 130 loss: 0.0005577283911406994
batch 135 loss: 0.0005576176219619811
batch 140 loss: 0.0005577347124926746
batch 145 loss: 0.0005576762254349887
batch 150 loss: 0.000557678344193846
batch 155 loss: 0.0005576497409492731
batch 160 loss: 0.0005576534895226359
batch 165 loss: 0.0005576583091169596
batch 170 loss: 0.0005577439093030989
batch 175 loss: 0.0005576871568337083
batch 180 loss: 0.0005577313364483416
batch 185 loss: 0.0005578004056587815
batch 190 loss: 0.0005576198338530958
batch 195 loss: 0.0005576758529059588
batch 200 loss: 0.0005576592520810664
batch 205 loss: 0.0005577123840339482
batch 210 loss: 0.000557628192473203
batch 215 loss: 0.0005577035015448927
batch 220 loss: 0.0005576159805059433
batch 225 loss: 0.0005576656549237669
batch 230 loss: 0.0005577068775892257
batch 235 loss: 0.0005577846430242061
batch 240 loss: 0.0005576679715886713
Training Loss: 0.0005576834751991555
Validation Loss: 0.0005576935674374302
Epoch 50:
batch 5 loss: 0.0005576472845859826
batch 10 loss: 0.0005577780655585229
batch 15 loss: 0.0005576200201176107
batch 20 loss: 0.0005577079835347831
batch 25 loss: 0.0005577085074037313
batch 30 loss: 0.0005577335134148598
batch 35 loss: 0.0005576386698521674
batch 40 loss: 0.0005576221039518714
batch 45 loss: 0.0005577477742917836
batch 50 loss: 0.0005577196832746267
batch 55 loss: 0.0005576257593929767
batch 60 loss: 0.0005576848750934004
batch 65 loss: 0.0005576954572461545
batch 70 loss: 0.0005577479023486376
batch 75 loss: 0.0005576327908784151
batch 80 loss: 0.0005577990552410484
batch 85 loss: 0.0005576962023042143
batch 90 loss: 0.000557712372392416
batch 95 loss: 0.0005576082156039774
batch 100 loss: 0.0005575228016823531
batch 105 loss: 0.0005576467840000987
batch 110 loss: 0.0005576297640800476
batch 115 loss: 0.0005576508701778948
batch 120 loss: 0.0005576868308708072
batch 125 loss: 0.0005576289142481983
batch 130 loss: 0.0005577912903390825
batch 135 loss: 0.0005577701376751065
batch 140 loss: 0.0005576100549660624
batch 145 loss: 0.0005576176336035132
batch 150 loss: 0.0005577437579631806
batch 155 loss: 0.0005575554678216577
batch 160 loss: 0.0005576317198574543
batch 165 loss: 0.0005576776922680438
batch 170 loss: 0.0005577294155955315
batch 175 loss: 0.000557618762832135
batch 180 loss: 0.0005577136878855526
batch 185 loss: 0.0005577471223659813
batch 190 loss: 0.0005576790776103735
batch 195 loss: 0.0005576990311965347
batch 200 loss: 0.000557720335200429
batch 205 loss: 0.0005577088566496968
batch 210 loss: 0.0005576310562901199
batch 215 loss: 0.0005576666444540024
batch 220 loss: 0.0005576996714808047
batch 225 loss: 0.0005577308125793934
batch 230 loss: 0.0005577204981818795
batch 235 loss: 0.0005576509283855557
batch 240 loss: 0.00055780018446967
Training Loss: 0.0005576834591920488
Validation Loss: 0.0005576935460946212
Epoch 51:
batch 5 loss: 0.0005578088574111461
batch 10 loss: 0.0005575873772613704
batch 15 loss: 0.0005575992399826646
batch 20 loss: 0.0005576492752879858
batch 25 loss: 0.0005577262840233743
batch 30 loss: 0.0005577490199357271
batch 35 loss: 0.0005576869007200003
batch 40 loss: 0.0005578111857175827
batch 45 loss: 0.0005576705792918801
batch 50 loss: 0.0005575815099291504
batch 55 loss: 0.0005576523253694177
batch 60 loss: 0.0005577047588303686
batch 65 loss: 0.0005576969357207418
batch 70 loss: 0.0005575855844654143
batch 75 loss: 0.0005576774477958679
batch 80 loss: 0.0005577251897193492
batch 85 loss: 0.0005576993455179036
batch 90 loss: 0.0005575356422923506
batch 95 loss: 0.0005577911506406963
batch 100 loss: 0.0005576498690061272
batch 105 loss: 0.0005576333380304277
batch 110 loss: 0.0005577724543400109
batch 115 loss: 0.0005576518247835338
batch 120 loss: 0.0005576600902713835
batch 125 loss: 0.0005576597759500146
batch 130 loss: 0.0005576851079240441
batch 135 loss: 0.0005576431751251221
batch 140 loss: 0.0005576472380198538
batch 145 loss: 0.0005575601360760629
batch 150 loss: 0.0005577679607085883
batch 155 loss: 0.0005576935480348765
batch 160 loss: 0.0005577568779699504
batch 165 loss: 0.0005576644907705486
batch 170 loss: 0.0005577113828621804
batch 175 loss: 0.0005576219060458243
batch 180 loss: 0.0005576917203143239
batch 185 loss: 0.000557686691172421
batch 190 loss: 0.0005576896015554667
batch 195 loss: 0.0005575851770117879
batch 200 loss: 0.0005577306845225393
batch 205 loss: 0.0005577987409196794
batch 210 loss: 0.0005576858413405717
batch 215 loss: 0.0005577569594606757
batch 220 loss: 0.0005577268544584513
batch 225 loss: 0.0005576978204771876
batch 230 loss: 0.0005576351308263838
batch 235 loss: 0.0005576039548031986
batch 240 loss: 0.0005578003358095885
Training Loss: 0.0005576834853854961
Validation Loss: 0.0005576938002680739
Epoch 52:
batch 5 loss: 0.0005577236646786332
batch 10 loss: 0.000557615770958364
batch 15 loss: 0.0005576833384111524
batch 20 loss: 0.0005576032330282032
batch 25 loss: 0.0005576906725764275
batch 30 loss: 0.0005576440831646323
batch 35 loss: 0.0005577472387813031
batch 40 loss: 0.0005576408933848142
batch 45 loss: 0.0005577837349846959
batch 50 loss: 0.0005576976691372693
batch 55 loss: 0.0005576864467002451
batch 60 loss: 0.0005576329655013979
batch 65 loss: 0.0005576520343311131
batch 70 loss: 0.0005576998344622552
batch 75 loss: 0.0005575414281338454
batch 80 loss: 0.0005576808005571365
batch 85 loss: 0.0005577581934630871
batch 90 loss: 0.0005576115800067782
batch 95 loss: 0.0005576494615525008
batch 100 loss: 0.0005576587864197791
batch 105 loss: 0.0005576913245022297
batch 110 loss: 0.0005576681694947183
batch 115 loss: 0.0005577712785452604
batch 120 loss: 0.0005577209522016346
batch 125 loss: 0.0005576285533607006
batch 130 loss: 0.0005576577153988182
batch 135 loss: 0.0005577090312726795
batch 140 loss: 0.000557646807283163
batch 145 loss: 0.0005576728493906557
batch 150 loss: 0.000557703897356987
batch 155 loss: 0.0005576322553679347
batch 160 loss: 0.0005577055388130247
batch 165 loss: 0.0005575804272666574
batch 170 loss: 0.0005576524650678039
batch 175 loss: 0.0005576878204010427
batch 180 loss: 0.000557667890097946
batch 185 loss: 0.0005576735828071832
batch 190 loss: 0.0005576722673140466
batch 195 loss: 0.0005576842813752592
batch 200 loss: 0.000557739962823689
batch 205 loss: 0.0005577463773079217
batch 210 loss: 0.0005576242925599218
batch 215 loss: 0.0005577643751166761
batch 220 loss: 0.0005577187985181808
batch 225 loss: 0.0005577721633017064
batch 230 loss: 0.0005576882045716048
batch 235 loss: 0.0005577553412877024
batch 240 loss: 0.000557768507860601
Training Loss: 0.0005576834783520705
Validation Loss: 0.0005576934054261073
Epoch 53:
batch 5 loss: 0.0005575595539994537
batch 10 loss: 0.0005576822790317237
batch 15 loss: 0.00055768764577806
batch 20 loss: 0.0005576615571044385
batch 25 loss: 0.0005577346193604172
batch 30 loss: 0.0005576218361966311
batch 35 loss: 0.0005576727562583983
batch 40 loss: 0.0005577748408541084
batch 45 loss: 0.000557661592029035
batch 50 loss: 0.0005576229305006564
batch 55 loss: 0.0005575744202360511
batch 60 loss: 0.0005576517200097441
batch 65 loss: 0.000557758950162679
batch 70 loss: 0.0005577155738137662
batch 75 loss: 0.0005576297640800476
batch 80 loss: 0.000557717215269804
batch 85 loss: 0.0005576424417085945
batch 90 loss: 0.0005576525698415935
batch 95 loss: 0.0005577459931373596
batch 100 loss: 0.0005577229079790413
batch 105 loss: 0.0005577213014476001
batch 110 loss: 0.0005576604977250099
batch 115 loss: 0.0005577132105827332
batch 120 loss: 0.0005576019058935345
batch 125 loss: 0.0005576377385295928
batch 130 loss: 0.0005576831870712339
batch 135 loss: 0.0005577587522566319
batch 140 loss: 0.000557752640452236
batch 145 loss: 0.0005576469819061458
batch 150 loss: 0.0005577898817136883
batch 155 loss: 0.0005576202413067222
batch 160 loss: 0.0005577552481554448
batch 165 loss: 0.0005575487739406526
batch 170 loss: 0.0005576903698965907
batch 175 loss: 0.0005577763891778887
batch 180 loss: 0.0005576409166678786
batch 185 loss: 0.0005577965988777578
batch 190 loss: 0.00055769809987396
batch 195 loss: 0.0005576879368163646
batch 200 loss: 0.0005577508709393442
batch 205 loss: 0.0005576363415457309
batch 210 loss: 0.0005577336414717138
batch 215 loss: 0.0005576915689744055
batch 220 loss: 0.0005577335599809885
batch 225 loss: 0.0005577072151936591
batch 230 loss: 0.0005576070165261626
batch 235 loss: 0.0005576863186433911
batch 240 loss: 0.0005575876915827393
Training Loss: 0.0005576834596771126
Validation Loss: 0.0005576935693776856
Epoch 54:
batch 5 loss: 0.0005576779833063484
batch 10 loss: 0.00055763985728845
batch 15 loss: 0.0005576471681706608
batch 20 loss: 0.0005576083436608315
batch 25 loss: 0.0005577745498158038
batch 30 loss: 0.0005576202529482543
batch 35 loss: 0.0005577906616963446
batch 40 loss: 0.0005576388095505536
batch 45 loss: 0.0005577848060056567
batch 50 loss: 0.0005577825824730099
batch 55 loss: 0.0005577112780883909
batch 60 loss: 0.000557661906350404
batch 65 loss: 0.0005577301140874624
batch 70 loss: 0.0005576814757660032
batch 75 loss: 0.0005575135350227356
batch 80 loss: 0.0005576107883825898
batch 85 loss: 0.0005576482741162181
batch 90 loss: 0.000557755003683269
batch 95 loss: 0.0005576930823735892
batch 100 loss: 0.0005576556315645576
batch 105 loss: 0.0005576817668043077
batch 110 loss: 0.0005576724419370293
batch 115 loss: 0.0005578785436227918
batch 120 loss: 0.0005577199975959956
batch 125 loss: 0.00055760812247172
batch 130 loss: 0.0005575777729973197
batch 135 loss: 0.0005575670977123082
batch 140 loss: 0.0005577344330959022
batch 145 loss: 0.0005577327217906713
batch 150 loss: 0.0005576336290687323
batch 155 loss: 0.0005576580995693802
batch 160 loss: 0.0005575613933615387
batch 165 loss: 0.000557706318795681
batch 170 loss: 0.000557678088080138
batch 175 loss: 0.0005577537580393255
batch 180 loss: 0.0005576632916927338
batch 185 loss: 0.0005577663076110184
batch 190 loss: 0.0005576181109063327
batch 195 loss: 0.0005576186231337488
batch 200 loss: 0.0005577242118306458
batch 205 loss: 0.0005576937925070524
batch 210 loss: 0.00055777010275051
batch 215 loss: 0.000557814771309495
batch 220 loss: 0.0005576074589043855
batch 225 loss: 0.0005576525116339325
batch 230 loss: 0.0005576664232648909
batch 235 loss: 0.0005578171228989958
batch 240 loss: 0.0005576040828600525
Training Loss: 0.0005576834812624535
Validation Loss: 0.000557693552885515
Epoch 55:
batch 5 loss: 0.0005577634670771659
batch 10 loss: 0.0005577498348429799
batch 15 loss: 0.0005576671799644827
batch 20 loss: 0.0005576235824264586
batch 25 loss: 0.0005576679366640746
batch 30 loss: 0.0005576425464823842
batch 35 loss: 0.0005576623254455626
batch 40 loss: 0.0005577074945904315
batch 45 loss: 0.0005576613591983914
batch 50 loss: 0.0005575890769250691
batch 55 loss: 0.000557666621170938
batch 60 loss: 0.0005576664814725518
batch 65 loss: 0.0005576857016421855
batch 70 loss: 0.0005575377028435469
batch 75 loss: 0.000557733525056392
batch 80 loss: 0.0005578180542215704
batch 85 loss: 0.0005577129311859607
batch 90 loss: 0.0005577460397034883
batch 95 loss: 0.0005577354575507342
batch 100 loss: 0.0005576990079134703
batch 105 loss: 0.0005576384952291846
batch 110 loss: 0.0005576383089646697
batch 115 loss: 0.0005576125578954816
batch 120 loss: 0.0005576737690716982
batch 125 loss: 0.0005576762254349887
batch 130 loss: 0.0005576731753535568
batch 135 loss: 0.0005576809402555227
batch 140 loss: 0.0005576033261604607
batch 145 loss: 0.0005577165866270661
batch 150 loss: 0.0005576480529271066
batch 155 loss: 0.0005576733965426683
batch 160 loss: 0.0005577741772867739
batch 165 loss: 0.0005576193099841476
batch 170 loss: 0.0005577036878094077
batch 175 loss: 0.0005575250135734678
batch 180 loss: 0.000557699950877577
batch 185 loss: 0.0005577150615863502
batch 190 loss: 0.0005576960626058281
batch 195 loss: 0.0005578035139478743
batch 200 loss: 0.0005577695206739009
batch 205 loss: 0.0005576940719038248
batch 210 loss: 0.0005576245603151619
batch 215 loss: 0.0005577438045293093
batch 220 loss: 0.0005576535244472325
batch 225 loss: 0.0005576989147812128
batch 230 loss: 0.000557660753838718
batch 235 loss: 0.0005577772739343345
batch 240 loss: 0.0005576762952841819
Training Loss: 0.0005576834720462405
Validation Loss: 0.0005576934102767458
Epoch 56:
batch 5 loss: 0.0005576653289608658
batch 10 loss: 0.0005576680530793964
batch 15 loss: 0.0005576670635491609
batch 20 loss: 0.0005576180177740752
batch 25 loss: 0.0005577273899689317
batch 30 loss: 0.0005577079835347831
batch 35 loss: 0.0005576839204877615
batch 40 loss: 0.0005577842821367085
batch 45 loss: 0.0005575831281021237
batch 50 loss: 0.0005575571558438241
batch 55 loss: 0.0005575783434323967
batch 60 loss: 0.0005576436291448772
batch 65 loss: 0.0005577256786637008
batch 70 loss: 0.0005576541181653738
batch 75 loss: 0.0005577580071985721
batch 80 loss: 0.0005576414288952947
batch 85 loss: 0.0005576813942752779
batch 90 loss: 0.0005577718839049339
batch 95 loss: 0.0005577123141847551
batch 100 loss: 0.0005576584720984101
batch 105 loss: 0.0005576972267590463
batch 110 loss: 0.0005577133386395871
batch 115 loss: 0.0005577169009484351
batch 120 loss: 0.0005577580770477653
batch 125 loss: 0.0005578220705501735
batch 130 loss: 0.0005576653056778014
batch 135 loss: 0.0005577190779149533
batch 140 loss: 0.0005576774012297392
batch 145 loss: 0.0005576997180469334
batch 150 loss: 0.0005576413008384406
batch 155 loss: 0.0005577333271503448
batch 160 loss: 0.0005576498806476593
batch 165 loss: 0.0005575262475758791
batch 170 loss: 0.0005575960269197822
batch 175 loss: 0.0005576629075221718
batch 180 loss: 0.0005576403927989304
batch 185 loss: 0.0005576938157901168
batch 190 loss: 0.0005576842930167913
batch 195 loss: 0.0005576412775553762
batch 200 loss: 0.0005578162497840821
batch 205 loss: 0.0005577072966843844
batch 210 loss: 0.0005577252362854779
batch 215 loss: 0.0005576445953920483
batch 220 loss: 0.0005576769821345806
batch 225 loss: 0.0005576053983531893
batch 230 loss: 0.0005578540614806116
batch 235 loss: 0.0005575777031481266
batch 240 loss: 0.0005577713251113892
Training Loss: 0.0005576834380917716
Validation Loss: 0.0005576934791558112
Epoch 57:
batch 5 loss: 0.0005577304633334279
batch 10 loss: 0.0005576658761128784
batch 15 loss: 0.0005576707189902663
batch 20 loss: 0.0005576809868216515
batch 25 loss: 0.0005576497642323375
batch 30 loss: 0.0005576271330937743
batch 35 loss: 0.0005577168310992419
batch 40 loss: 0.0005576444789767265
batch 45 loss: 0.0005576231982558965
batch 50 loss: 0.0005576641531661152
batch 55 loss: 0.0005576855619437993
batch 60 loss: 0.0005575865507125854
batch 65 loss: 0.0005576693336479365
batch 70 loss: 0.0005577008705586195
batch 75 loss: 0.0005576992174610495
batch 80 loss: 0.0005577405449002981
batch 85 loss: 0.0005576416617259384
batch 90 loss: 0.0005576150841079652
batch 95 loss: 0.000557644828222692
batch 100 loss: 0.0005575765157118439
batch 105 loss: 0.0005576612777076662
batch 110 loss: 0.0005577471456490457
batch 115 loss: 0.0005576305789873004
batch 120 loss: 0.0005576638854108751
batch 125 loss: 0.0005577067262493074
batch 130 loss: 0.0005578370648436249
batch 135 loss: 0.0005577067262493074
batch 140 loss: 0.0005576705560088157
batch 145 loss: 0.0005577023141086102
batch 150 loss: 0.0005576129769906402
batch 155 loss: 0.0005576307885348797
batch 160 loss: 0.0005576580646447837
batch 165 loss: 0.0005576466675847769
batch 170 loss: 0.0005576718947850167
batch 175 loss: 0.000557763478718698
batch 180 loss: 0.000557682488579303
batch 185 loss: 0.0005576931405812501
batch 190 loss: 0.0005576533032581211
batch 195 loss: 0.0005577387288212776
batch 200 loss: 0.0005578125012107194
batch 205 loss: 0.0005576850031502545
batch 210 loss: 0.000557704467792064
batch 215 loss: 0.0005577741074375808
batch 220 loss: 0.0005576437572017312
batch 225 loss: 0.0005578603711910545
batch 230 loss: 0.0005576076917350292
batch 235 loss: 0.0005577026749961078
batch 240 loss: 0.0005577045842073858
Training Loss: 0.000557683473743964
Validation Loss: 0.000557693427739044
Epoch 58:
batch 5 loss: 0.0005576769937761128
batch 10 loss: 0.0005577441188506782
batch 15 loss: 0.0005575893912464381
batch 20 loss: 0.0005576608469709754
batch 25 loss: 0.000557710265275091
batch 30 loss: 0.0005576587514951825
batch 35 loss: 0.0005576244555413723
batch 40 loss: 0.000557658530306071
batch 45 loss: 0.0005577231873758137
batch 50 loss: 0.0005576089723035693
batch 55 loss: 0.0005577824427746237
batch 60 loss: 0.0005575667950324714
batch 65 loss: 0.0005578093463554978
batch 70 loss: 0.0005576286930590868
batch 75 loss: 0.0005576878204010427
batch 80 loss: 0.0005576959811151028
batch 85 loss: 0.0005576513009145856
batch 90 loss: 0.0005576595896854997
batch 95 loss: 0.0005576836760155856
batch 100 loss: 0.0005576828261837363
batch 105 loss: 0.0005577099742367864
batch 110 loss: 0.000557717471383512
batch 115 loss: 0.0005576906492933631
batch 120 loss: 0.0005577074596658349
batch 125 loss: 0.0005577339557930827
batch 130 loss: 0.0005576430819928646
batch 135 loss: 0.0005575963645242154
batch 140 loss: 0.0005577345378696919
batch 145 loss: 0.0005576673429459333
batch 150 loss: 0.0005575797287747264
batch 155 loss: 0.0005577196599915624
batch 160 loss: 0.0005577284493483603
batch 165 loss: 0.000557707843836397
batch 170 loss: 0.0005575487157329917
batch 175 loss: 0.0005576483672484756
batch 180 loss: 0.0005577804055064917
batch 185 loss: 0.0005577653762884438
batch 190 loss: 0.0005576239665970207
batch 195 loss: 0.0005577134317718447
batch 200 loss: 0.0005578148760832846
batch 205 loss: 0.000557622592896223
batch 210 loss: 0.0005577864707447588
batch 215 loss: 0.0005576774943619967
batch 220 loss: 0.0005576228606514633
batch 225 loss: 0.0005576822790317237
batch 230 loss: 0.0005576934083364904
batch 235 loss: 0.0005576935946010053
batch 240 loss: 0.0005576911964453757
Training Loss: 0.0005576834487631761
Validation Loss: 0.0005576935490050043
Epoch 59:
batch 5 loss: 0.0005576853640377521
batch 10 loss: 0.0005577090429142118
batch 15 loss: 0.0005576641415245831
batch 20 loss: 0.0005576506373472512
batch 25 loss: 0.0005576693918555975
batch 30 loss: 0.0005577096133492887
batch 35 loss: 0.0005576686235144734
batch 40 loss: 0.0005576489144004882
batch 45 loss: 0.0005576444556936622
batch 50 loss: 0.0005577696836553514
batch 55 loss: 0.0005578186712227762
batch 60 loss: 0.0005576853640377521
batch 65 loss: 0.0005576877621933818
batch 70 loss: 0.0005576558876782655
batch 75 loss: 0.0005577017203904688
batch 80 loss: 0.0005578113486990333
batch 85 loss: 0.0005577572039328516
batch 90 loss: 0.0005577085423283279
batch 95 loss: 0.0005577477160841227
batch 100 loss: 0.0005577005445957184
batch 105 loss: 0.000557765772100538
batch 110 loss: 0.0005576442810706795
batch 115 loss: 0.0005576643627136946
batch 120 loss: 0.0005576940369792282
batch 125 loss: 0.0005577172851189971
batch 130 loss: 0.0005576157127507031
batch 135 loss: 0.0005576477269642055
batch 140 loss: 0.0005576031398959458
batch 145 loss: 0.0005576806142926216
batch 150 loss: 0.0005576712312176824
batch 155 loss: 0.0005576485185883939
batch 160 loss: 0.0005576228839345276
batch 165 loss: 0.0005578383570536971
batch 170 loss: 0.0005576338851824402
batch 175 loss: 0.0005575648741796613
batch 180 loss: 0.0005576679250225425
batch 185 loss: 0.0005577331176027656
batch 190 loss: 0.0005577030242420733
batch 195 loss: 0.000557574036065489
batch 200 loss: 0.0005576444673351943
batch 205 loss: 0.00055759436218068
batch 210 loss: 0.000557758379727602
batch 215 loss: 0.0005577423609793187
batch 220 loss: 0.0005576137686148286
batch 225 loss: 0.0005576644674874842
batch 230 loss: 0.0005576656083576381
batch 235 loss: 0.0005576497409492731
batch 240 loss: 0.0005576873780228197
Training Loss: 0.0005576834572517934
Validation Loss: 0.0005576934151273841
Epoch 60:
batch 5 loss: 0.0005575952818617225
batch 10 loss: 0.0005576178431510925
batch 15 loss: 0.0005577049101702869
batch 20 loss: 0.0005576045950874686
batch 25 loss: 0.0005577161558903754
batch 30 loss: 0.0005577104864642024
batch 35 loss: 0.0005577898235060274
batch 40 loss: 0.0005577077972702682
batch 45 loss: 0.0005577111034654081
batch 50 loss: 0.0005577584262937308
batch 55 loss: 0.0005577635718509554
batch 60 loss: 0.000557574664708227
batch 65 loss: 0.0005576582392677665
batch 70 loss: 0.0005576023366302251
batch 75 loss: 0.0005577824194915593
batch 80 loss: 0.0005576168885454535
batch 85 loss: 0.0005577199277468026
batch 90 loss: 0.0005575553979724646
batch 95 loss: 0.0005575785529799759
batch 100 loss: 0.0005576434545218944
batch 105 loss: 0.0005576938157901168
batch 110 loss: 0.0005576685420237481
batch 115 loss: 0.0005576661322265863
batch 120 loss: 0.0005577034549787641
batch 125 loss: 0.0005578257143497467
batch 130 loss: 0.0005576759809628129
batch 135 loss: 0.0005576870986260474
batch 140 loss: 0.0005576911382377148
batch 145 loss: 0.0005576539435423911
batch 150 loss: 0.0005577251897193492
batch 155 loss: 0.0005577433272264898
batch 160 loss: 0.0005575099727138877
batch 165 loss: 0.0005577273084782064
batch 170 loss: 0.0005577282980084419
batch 175 loss: 0.0005577493691816926
batch 180 loss: 0.0005577447242103517
batch 185 loss: 0.0005576805328018963
batch 190 loss: 0.0005576530355028808
batch 195 loss: 0.000557688542176038
batch 200 loss: 0.000557673629373312
batch 205 loss: 0.0005576978903263808
batch 210 loss: 0.0005576919997110963
batch 215 loss: 0.0005576669587753713
batch 220 loss: 0.000557739904616028
batch 225 loss: 0.0005577069357968866
batch 230 loss: 0.0005576301948167384
batch 235 loss: 0.0005577144911512733
batch 240 loss: 0.0005576542345806957
Training Loss: 0.000557683421599601
Validation Loss: 0.000557693464603896
Epoch 61:
batch 5 loss: 0.0005577890202403069
batch 10 loss: 0.0005577511852607131
batch 15 loss: 0.0005575482966378332
batch 20 loss: 0.0005576395778916776
batch 25 loss: 0.000557739962823689
batch 30 loss: 0.0005577349918894469
batch 35 loss: 0.0005576635827310384
batch 40 loss: 0.0005577315459959209
batch 45 loss: 0.000557673501316458
batch 50 loss: 0.0005576814757660032
batch 55 loss: 0.0005577724543400109
batch 60 loss: 0.0005577434087172151
batch 65 loss: 0.0005576999159529805
batch 70 loss: 0.0005575767485424876
batch 75 loss: 0.0005577682284638285
batch 80 loss: 0.0005576286115683615
batch 85 loss: 0.0005576747353188694
batch 90 loss: 0.0005577473435550928
batch 95 loss: 0.0005576534080319106
batch 100 loss: 0.0005576037685386837
batch 105 loss: 0.0005577507428824902
batch 110 loss: 0.0005576320691034198
batch 115 loss: 0.0005576895317062735
batch 120 loss: 0.000557578238658607
batch 125 loss: 0.0005576887866482139
batch 130 loss: 0.0005576585768721998
batch 135 loss: 0.0005576838040724397
batch 140 loss: 0.0005575885414145886
batch 145 loss: 0.0005576088093221188
batch 150 loss: 0.0005576924188062549
batch 155 loss: 0.0005576573661528528
batch 160 loss: 0.000557617680169642
batch 165 loss: 0.000557509483769536
batch 170 loss: 0.0005576423020102084
batch 175 loss: 0.0005576529190875589
batch 180 loss: 0.0005576723371632398
batch 185 loss: 0.0005577884032391012
batch 190 loss: 0.0005576609401032329
batch 195 loss: 0.0005577701842412353
batch 200 loss: 0.0005575622199103236
batch 205 loss: 0.0005578525713644921
batch 210 loss: 0.0005576893221586942
batch 215 loss: 0.0005577146308496595
batch 220 loss: 0.0005577090661972762
batch 225 loss: 0.0005577515228651464
batch 230 loss: 0.0005576742929406464
batch 235 loss: 0.0005576484487392009
batch 240 loss: 0.0005578381009399891
Training Loss: 0.0005576834390618994
Validation Loss: 0.000557693552885515
Epoch 62:
batch 5 loss: 0.0005576599738560617
batch 10 loss: 0.0005577523610554635
batch 15 loss: 0.0005576128838583827
batch 20 loss: 0.000557613477576524
batch 25 loss: 0.0005576829425990582
batch 30 loss: 0.0005577492062002421
batch 35 loss: 0.0005576297757215798
batch 40 loss: 0.0005576999275945127
batch 45 loss: 0.0005576720577664673
batch 50 loss: 0.0005576113122515381
batch 55 loss: 0.000557645014487207
batch 60 loss: 0.0005577203119173646
batch 65 loss: 0.0005576423835009336
batch 70 loss: 0.0005576628143899142
batch 75 loss: 0.0005576195078901946
batch 80 loss: 0.0005576928379014134
batch 85 loss: 0.000557693000882864
batch 90 loss: 0.000557692488655448
batch 95 loss: 0.0005576275289058685
batch 100 loss: 0.000557622138876468
batch 105 loss: 0.0005577889387495816
batch 110 loss: 0.0005576836178079247
batch 115 loss: 0.0005577142001129687
batch 120 loss: 0.0005577263538725675
batch 125 loss: 0.0005576700437813997
batch 130 loss: 0.0005576871102675795
batch 135 loss: 0.000557685806415975
batch 140 loss: 0.0005575545481406152
batch 145 loss: 0.0005577523843385279
batch 150 loss: 0.0005576821393333375
batch 155 loss: 0.0005576872732490301
batch 160 loss: 0.0005576433846727014
batch 165 loss: 0.0005577106378041208
batch 170 loss: 0.0005575585761107505
batch 175 loss: 0.0005577604984864593
batch 180 loss: 0.0005577513366006315
batch 185 loss: 0.0005576800322160125
batch 190 loss: 0.0005575705552473664
batch 195 loss: 0.00055763921700418
batch 200 loss: 0.0005577091127634048
batch 205 loss: 0.0005576751427724957
batch 210 loss: 0.0005577433155849576
batch 215 loss: 0.000557789416052401
batch 220 loss: 0.0005576217430643737
batch 225 loss: 0.0005578139214776456
batch 230 loss: 0.0005576847121119499
batch 235 loss: 0.000557779660448432
batch 240 loss: 0.000557741872034967
Training Loss: 0.0005576834895085389
Validation Loss: 0.0005576934267689164
Epoch 63:
batch 5 loss: 0.0005576804163865745
batch 10 loss: 0.0005576993920840323
batch 15 loss: 0.0005576994619332254
batch 20 loss: 0.0005577993695624172
batch 25 loss: 0.0005576141411438584
batch 30 loss: 0.0005576911149546504
batch 35 loss: 0.0005576633731834591
batch 40 loss: 0.0005576328840106726
batch 45 loss: 0.0005576757015660405
batch 50 loss: 0.0005575872026383876
batch 55 loss: 0.0005576714756898582
batch 60 loss: 0.000557641254272312
batch 65 loss: 0.0005576112889684737
batch 70 loss: 0.0005576721276156604
batch 75 loss: 0.0005577271920628845
batch 80 loss: 0.0005576479132287204
batch 85 loss: 0.0005577307543717325
batch 90 loss: 0.0005577349220402539
batch 95 loss: 0.000557684397790581
batch 100 loss: 0.0005576260387897492
batch 105 loss: 0.0005578422918915748
batch 110 loss: 0.000557801965624094
batch 115 loss: 0.0005577051895670592
batch 120 loss: 0.0005576633149757982
batch 125 loss: 0.0005576493102125823
batch 130 loss: 0.0005577795905992389
batch 135 loss: 0.0005575107876211405
batch 140 loss: 0.0005576797295361758
batch 145 loss: 0.0005576816853135824
batch 150 loss: 0.0005576346302405
batch 155 loss: 0.0005577313946560025
batch 160 loss: 0.000557770486921072
batch 165 loss: 0.0005577808595262467
batch 170 loss: 0.0005576551891863346
batch 175 loss: 0.0005577146075665951
batch 180 loss: 0.0005576954106800258
batch 185 loss: 0.0005576446652412415
batch 190 loss: 0.0005576171213760972
batch 195 loss: 0.0005575759103521704
batch 200 loss: 0.0005576750147156418
batch 205 loss: 0.0005576518247835338
batch 210 loss: 0.0005577163654379546
batch 215 loss: 0.0005577679490670562
batch 220 loss: 0.0005576531402766705
batch 225 loss: 0.0005576348979957402
batch 230 loss: 0.0005576082039624453
batch 235 loss: 0.000557758507784456
batch 240 loss: 0.0005577157600782812
Training Loss: 0.0005576834630725595
Validation Loss: 0.0005576934578130022
Epoch 64:
batch 5 loss: 0.0005575539311394095
batch 10 loss: 0.0005576547118835151
batch 15 loss: 0.0005576972267590463
batch 20 loss: 0.0005575966439209879
batch 25 loss: 0.0005576353287324309
batch 30 loss: 0.0005576334544457496
batch 35 loss: 0.0005576409283094108
batch 40 loss: 0.0005575897288508713
batch 45 loss: 0.0005577955627813936
batch 50 loss: 0.0005577411968261004
batch 55 loss: 0.0005575147806666791
batch 60 loss: 0.000557654700241983
batch 65 loss: 0.0005578072858043015
batch 70 loss: 0.000557614688295871
batch 75 loss: 0.0005577209056355059
batch 80 loss: 0.0005577070754952729
batch 85 loss: 0.0005576801719143987
batch 90 loss: 0.0005576633266173303
batch 95 loss: 0.0005577404401265085
batch 100 loss: 0.0005577828967943787
batch 105 loss: 0.0005576562602072954
batch 110 loss: 0.0005576900904998184
batch 115 loss: 0.0005577113945037127
batch 120 loss: 0.0005577380652539432
batch 125 loss: 0.0005576824652962387
batch 130 loss: 0.0005579301156103611
batch 135 loss: 0.000557595060672611
batch 140 loss: 0.0005575017421506345
batch 145 loss: 0.0005576895782724023
batch 150 loss: 0.0005575355840846896
batch 155 loss: 0.0005576912313699723
batch 160 loss: 0.0005577163072302937
batch 165 loss: 0.0005577354226261377
batch 170 loss: 0.0005577953183092177
batch 175 loss: 0.0005577019182965159
batch 180 loss: 0.0005576506373472512
batch 185 loss: 0.0005577508243732154
batch 190 loss: 0.0005577098461799323
batch 195 loss: 0.000557758507784456
batch 200 loss: 0.0005576509516686201
batch 205 loss: 0.0005575917311944067
batch 210 loss: 0.0005576407420448959
batch 215 loss: 0.0005577211384661495
batch 220 loss: 0.0005578042939305306
batch 225 loss: 0.0005576026160269976
batch 230 loss: 0.0005576938507147134
batch 235 loss: 0.0005577422096394002
batch 240 loss: 0.0005576935829594732
Training Loss: 0.0005576834681657298
Validation Loss: 0.0005576937352695192
Epoch 65:
batch 5 loss: 0.0005576499155722558
batch 10 loss: 0.0005575861316174269
batch 15 loss: 0.0005576982744969428
batch 20 loss: 0.0005575827439315617
batch 25 loss: 0.0005576304392889142
batch 30 loss: 0.0005577862611971796
batch 35 loss: 0.0005577718839049339
batch 40 loss: 0.0005577316391281783
batch 45 loss: 0.0005578440381214022
batch 50 loss: 0.0005576508934609592
batch 55 loss: 0.0005577199277468026
batch 60 loss: 0.0005577052244916559
batch 65 loss: 0.0005576988914981484
batch 70 loss: 0.0005576172145083547
batch 75 loss: 0.0005576943745836616
batch 80 loss: 0.0005577538628131151
batch 85 loss: 0.0005576958297751844
batch 90 loss: 0.0005576951080001891
batch 95 loss: 0.0005577067960985005
batch 100 loss: 0.0005576343392021954
batch 105 loss: 0.0005576225579716265
batch 110 loss: 0.0005576857714913785
batch 115 loss: 0.000557719508651644
batch 120 loss: 0.0005577739677391947
batch 125 loss: 0.0005576717434450984
batch 130 loss: 0.0005577023257501423
batch 135 loss: 0.0005577194737270474
batch 140 loss: 0.0005576583324000239
batch 145 loss: 0.0005576186464168132
batch 150 loss: 0.0005575975636020302
batch 155 loss: 0.0005576411378569901
batch 160 loss: 0.0005575884482823312
batch 165 loss: 0.0005577556905336678
batch 170 loss: 0.0005576787400059402
batch 175 loss: 0.0005577028729021549
batch 180 loss: 0.0005577693693339825
batch 185 loss: 0.0005576287629082799
batch 190 loss: 0.0005575750954449176
batch 195 loss: 0.0005577340489253402
batch 200 loss: 0.0005576336290687323
batch 205 loss: 0.0005575702409259975
batch 210 loss: 0.0005576978204771876
batch 215 loss: 0.0005576637107878923
batch 220 loss: 0.0005576348747126758
batch 225 loss: 0.0005577299627475441
batch 230 loss: 0.0005577320698648691
batch 235 loss: 0.0005576665513217449
batch 240 loss: 0.0005577789968810975
Training Loss: 0.000557683452158623
Validation Loss: 0.0005576934355000655
Epoch 66:
batch 5 loss: 0.0005576638621278107
batch 10 loss: 0.0005576519877649843
batch 15 loss: 0.0005576712312176824
batch 20 loss: 0.0005576028837822377
batch 25 loss: 0.0005576696945354343
batch 30 loss: 0.0005576085764914751
batch 35 loss: 0.0005576819647103548
batch 40 loss: 0.0005576852709054947
batch 45 loss: 0.0005577479605562985
batch 50 loss: 0.0005577359581366182
batch 55 loss: 0.0005578354932367802
batch 60 loss: 0.0005576994153670967
batch 65 loss: 0.000557590601965785
batch 70 loss: 0.000557749776635319
batch 75 loss: 0.0005576164810918271
batch 80 loss: 0.000557694595772773
batch 85 loss: 0.0005575278191827238
batch 90 loss: 0.0005576598923653364
batch 95 loss: 0.0005577285424806178
batch 100 loss: 0.0005577021511271596
batch 105 loss: 0.0005576170980930329
batch 110 loss: 0.0005577225703746081
batch 115 loss: 0.0005577247473411262
batch 120 loss: 0.0005576157243922352
batch 125 loss: 0.0005575582850724459
batch 130 loss: 0.0005576761323027313
batch 135 loss: 0.0005576864816248417
batch 140 loss: 0.0005576796364039182
batch 145 loss: 0.0005577202304266393
batch 150 loss: 0.0005577399511821568
batch 155 loss: 0.0005576964351348579
batch 160 loss: 0.0005576466792263091
batch 165 loss: 0.0005576565861701965
batch 170 loss: 0.0005576523137278855
batch 175 loss: 0.0005577270057983696
batch 180 loss: 0.000557633233256638
batch 185 loss: 0.0005576795083470643
batch 190 loss: 0.0005576528725214303
batch 195 loss: 0.0005576554569415749
batch 200 loss: 0.000557736773043871
batch 205 loss: 0.0005577854695729912
batch 210 loss: 0.0005576746072620154
batch 215 loss: 0.0005577438627369702
batch 220 loss: 0.0005576074589043855
batch 225 loss: 0.000557668914552778
batch 230 loss: 0.0005578095093369484
batch 235 loss: 0.0005577819421887398
batch 240 loss: 0.0005577310221269727
Training Loss: 0.0005576834305732821
Validation Loss: 0.0005576934597532575
Epoch 67:
batch 5 loss: 0.0005577191943302751
batch 10 loss: 0.0005577509407885372
batch 15 loss: 0.0005575965624302626
batch 20 loss: 0.0005577289150096477
batch 25 loss: 0.0005576620693318546
batch 30 loss: 0.0005577424890361726
batch 35 loss: 0.0005577014526352286
batch 40 loss: 0.0005576263996772468
batch 45 loss: 0.0005577309639193118
batch 50 loss: 0.0005575931398198008
batch 55 loss: 0.000557649799156934
batch 60 loss: 0.0005578042822889983
batch 65 loss: 0.0005576803581789136
batch 70 loss: 0.0005577055620960891
batch 75 loss: 0.0005576627445407212
batch 80 loss: 0.0005576341645792127
batch 85 loss: 0.0005576650495640933
batch 90 loss: 0.0005576533265411853
batch 95 loss: 0.0005577531293965877
batch 100 loss: 0.0005576980533078313
batch 105 loss: 0.0005576240946538746
batch 110 loss: 0.0005577435251325369
batch 115 loss: 0.0005575507995672524
batch 120 loss: 0.0005577248055487871
batch 125 loss: 0.0005576463066972792
batch 130 loss: 0.0005576999392360449
batch 135 loss: 0.0005576375289820135
batch 140 loss: 0.0005576942698098719
batch 145 loss: 0.0005577557254582644
batch 150 loss: 0.0005576357827521861
batch 155 loss: 0.0005577110918238759
batch 160 loss: 0.00055770956678316
batch 165 loss: 0.0005576904281042516
batch 170 loss: 0.0005577008822001516
batch 175 loss: 0.0005575889023020864
batch 180 loss: 0.0005577575881034136
batch 185 loss: 0.0005576929659582674
batch 190 loss: 0.000557750416919589
batch 195 loss: 0.000557727087289095
batch 200 loss: 0.0005577084375545382
batch 205 loss: 0.0005576981464400887
batch 210 loss: 0.0005576470750384033
batch 215 loss: 0.000557631824631244
batch 220 loss: 0.0005576844210736454
batch 225 loss: 0.0005576048977673053
batch 230 loss: 0.0005577569478191436
batch 235 loss: 0.0005576492054387927
batch 240 loss: 0.000557623477652669
Training Loss: 0.0005576834320284737
Validation Loss: 0.0005576934849765773
Epoch 68:
batch 5 loss: 0.0005576600786298513
batch 10 loss: 0.0005577067844569683
batch 15 loss: 0.0005577819072641432
batch 20 loss: 0.0005575625109486282
batch 25 loss: 0.0005577107542194426
batch 30 loss: 0.0005575695890001952
batch 35 loss: 0.0005576722323894501
batch 40 loss: 0.0005577326170168817
batch 45 loss: 0.0005576513591222465
batch 50 loss: 0.0005575855495408178
batch 55 loss: 0.000557622394990176
batch 60 loss: 0.0005577689036726952
batch 65 loss: 0.0005576577153988182
batch 70 loss: 0.0005576488096266985
batch 75 loss: 0.000557592639233917
batch 80 loss: 0.0005577890435233713
batch 85 loss: 0.0005576477735303343
batch 90 loss: 0.0005578012671321631
batch 95 loss: 0.0005576860858127475
batch 100 loss: 0.0005576493800617755
batch 105 loss: 0.0005576175870373845
batch 110 loss: 0.0005576696363277734
batch 115 loss: 0.0005577028729021549
batch 120 loss: 0.0005577761330641806
batch 125 loss: 0.000557686307001859
batch 130 loss: 0.0005576221272349357
batch 135 loss: 0.0005575945950113236
batch 140 loss: 0.000557749264407903
batch 145 loss: 0.0005576539435423911
batch 150 loss: 0.00055773442145437
batch 155 loss: 0.0005576532566919923
batch 160 loss: 0.0005577272269874812
batch 165 loss: 0.0005575239309109747
batch 170 loss: 0.0005577887059189379
batch 175 loss: 0.0005576163064688444
batch 180 loss: 0.0005577389150857925
batch 185 loss: 0.0005577864358201623
batch 190 loss: 0.0005575534887611866
batch 195 loss: 0.0005577534437179565
batch 200 loss: 0.0005577238858677447
batch 205 loss: 0.0005577291129156947
batch 210 loss: 0.0005577767500653863
batch 215 loss: 0.0005577217205427587
batch 220 loss: 0.0005577250849455595
batch 225 loss: 0.000557504512835294
batch 230 loss: 0.0005576449795626104
batch 235 loss: 0.0005578041775152088
batch 240 loss: 0.0005577286938205361
Training Loss: 0.0005576834356664525
Validation Loss: 0.0005576935412439828
Epoch 69:
batch 5 loss: 0.0005576403229497374
batch 10 loss: 0.0005576223018579185
batch 15 loss: 0.0005578021635301411
batch 20 loss: 0.0005575742456130683
batch 25 loss: 0.00055769911268726
batch 30 loss: 0.0005577549221925437
batch 35 loss: 0.0005577032919973135
batch 40 loss: 0.0005577926291152835
batch 45 loss: 0.0005577221978455782
batch 50 loss: 0.0005576549097895622
batch 55 loss: 0.0005577374715358019
batch 60 loss: 0.0005576907424256206
batch 65 loss: 0.0005577120464295149
batch 70 loss: 0.0005575887626037002
batch 75 loss: 0.0005576485767960548
batch 80 loss: 0.0005576741881668567
batch 85 loss: 0.0005576934898272156
batch 90 loss: 0.0005576989613473415
batch 95 loss: 0.0005576899391598999
batch 100 loss: 0.0005576866446062922
batch 105 loss: 0.0005576567142270506
batch 110 loss: 0.000557682232465595
batch 115 loss: 0.0005578086944296956
batch 120 loss: 0.0005576143856160342
batch 125 loss: 0.0005576326977461577
batch 130 loss: 0.0005576646421104669
batch 135 loss: 0.0005575499031692744
batch 140 loss: 0.0005576420109719038
batch 145 loss: 0.0005577004631049931
batch 150 loss: 0.0005576448631472886
batch 155 loss: 0.0005576415220275521
batch 160 loss: 0.0005576663883402943
batch 165 loss: 0.0005576869007200003
batch 170 loss: 0.0005576928495429456
batch 175 loss: 0.0005577029543928802
batch 180 loss: 0.000557674397714436
batch 185 loss: 0.0005576901836320758
batch 190 loss: 0.0005576733849011362
batch 195 loss: 0.0005576270166784524
batch 200 loss: 0.0005577412783168256
batch 205 loss: 0.0005577424075454473
batch 210 loss: 0.0005577726755291223
batch 215 loss: 0.0005576960975304246
batch 220 loss: 0.000557655154261738
batch 225 loss: 0.0005577454459853471
batch 230 loss: 0.0005575642688199877
batch 235 loss: 0.0005577369011007249
batch 240 loss: 0.0005577117670327425
Training Loss: 0.000557683440032027
Validation Loss: 0.0005576934500519808
Epoch 70:
batch 5 loss: 0.0005577514413744212
batch 10 loss: 0.0005575560382567347
batch 15 loss: 0.0005576818366535008
batch 20 loss: 0.0005576401250436902
batch 25 loss: 0.0005576955387368798
batch 30 loss: 0.0005577368778176605
batch 35 loss: 0.0005576571449637413
batch 40 loss: 0.000557751557789743
batch 45 loss: 0.0005576707888394594
batch 50 loss: 0.0005576946423389018
batch 55 loss: 0.0005576563533395528
batch 60 loss: 0.0005575821385718882
batch 65 loss: 0.0005577137810178101
batch 70 loss: 0.0005577011383138597
batch 75 loss: 0.0005577016738243401
batch 80 loss: 0.0005575944436714053
batch 85 loss: 0.0005577156087383627
batch 90 loss: 0.0005577352829277515
batch 95 loss: 0.0005575909395702183
batch 100 loss: 0.0005577372037805616
batch 105 loss: 0.0005577461677603423
batch 110 loss: 0.0005576758761890232
batch 115 loss: 0.0005577613017521798
batch 120 loss: 0.000557628006208688
batch 125 loss: 0.0005576724302954972
batch 130 loss: 0.0005577240139245987
batch 135 loss: 0.0005576547351665795
batch 140 loss: 0.0005576975177973509
batch 145 loss: 0.0005577358300797641
batch 150 loss: 0.0005577908712439239
batch 155 loss: 0.000557751627638936
batch 160 loss: 0.0005576669587753713
batch 165 loss: 0.0005576750496402383
batch 170 loss: 0.0005576708354055882
batch 175 loss: 0.0005577297881245614
batch 180 loss: 0.0005576655152253806
batch 185 loss: 0.0005576781579293311
batch 190 loss: 0.0005576268769800663
batch 195 loss: 0.0005577110568992794
batch 200 loss: 0.0005576568888500333
batch 205 loss: 0.0005576947587542236
batch 210 loss: 0.0005577597534283995
batch 215 loss: 0.0005577063653618098
batch 220 loss: 0.0005576700670644641
batch 225 loss: 0.0005575877381488681
batch 230 loss: 0.0005577330011874437
batch 235 loss: 0.0005575680523179471
batch 240 loss: 0.0005576008697971702
Training Loss: 0.0005576834305732821
Validation Loss: 0.0005576933976650859
Epoch 71:
batch 5 loss: 0.0005576942814514041
batch 10 loss: 0.000557679997291416
batch 15 loss: 0.0005575965158641338
batch 20 loss: 0.0005577830830588937
batch 25 loss: 0.0005577199743129313
batch 30 loss: 0.0005576157127507031
batch 35 loss: 0.0005576163646765053
batch 40 loss: 0.0005576847703196109
batch 45 loss: 0.0005577856441959739
batch 50 loss: 0.0005577879957854748
batch 55 loss: 0.0005578091950155795
batch 60 loss: 0.0005578061449341476
batch 65 loss: 0.0005577848060056567
batch 70 loss: 0.0005576862604357302
batch 75 loss: 0.0005576830822974444
batch 80 loss: 0.000557678216136992
batch 85 loss: 0.0005576706840656698
batch 90 loss: 0.0005576528259553015
batch 95 loss: 0.0005576572730205953
batch 100 loss: 0.0005576472613029182
batch 105 loss: 0.0005575313465669751
batch 110 loss: 0.000557623861823231
batch 115 loss: 0.0005577487987466157
batch 120 loss: 0.0005576280644163489
batch 125 loss: 0.0005576453171670437
batch 130 loss: 0.0005575762595981359
batch 135 loss: 0.0005577098345384002
batch 140 loss: 0.0005576786934398115
batch 145 loss: 0.0005576213239692152
batch 150 loss: 0.0005576698458753526
batch 155 loss: 0.0005576489027589559
batch 160 loss: 0.0005577333038672804
batch 165 loss: 0.0005576653638854623
batch 170 loss: 0.000557702628429979
batch 175 loss: 0.0005575927556492388
batch 180 loss: 0.0005578038515523076
batch 185 loss: 0.0005576492170803249
batch 190 loss: 0.0005576986703090369
batch 195 loss: 0.0005576488212682307
batch 200 loss: 0.0005576251773163676
batch 205 loss: 0.0005576483439654112
batch 210 loss: 0.0005577711039222777
batch 215 loss: 0.0005576998344622552
batch 220 loss: 0.0005577180185355246
batch 225 loss: 0.0005576100433245301
batch 230 loss: 0.0005576287978328765
batch 235 loss: 0.0005577578442171216
batch 240 loss: 0.0005577286472544074
Training Loss: 0.0005576834325135375
Validation Loss: 0.0005576934568428745
Epoch 72:
batch 5 loss: 0.000557558226864785
batch 10 loss: 0.0005576464114710689
batch 15 loss: 0.0005577104864642024
batch 20 loss: 0.0005577753065153957
batch 25 loss: 0.0005576430936343968
batch 30 loss: 0.0005576698924414814
batch 35 loss: 0.0005576163996011019
batch 40 loss: 0.0005577343981713057
batch 45 loss: 0.000557613861747086
batch 50 loss: 0.0005577319301664829
batch 55 loss: 0.0005577445263043046
batch 60 loss: 0.0005575853865593672
batch 65 loss: 0.0005577422445639968
batch 70 loss: 0.0005577354459092021
batch 75 loss: 0.0005576040362939239
batch 80 loss: 0.0005578054930083454
batch 85 loss: 0.0005575835821218789
batch 90 loss: 0.0005575808696448803
batch 95 loss: 0.0005577444680966437
batch 100 loss: 0.0005576651776209474
batch 105 loss: 0.0005576504277996719
batch 110 loss: 0.0005577095085754991
batch 115 loss: 0.0005577242001891137
batch 120 loss: 0.0005577001138590276
batch 125 loss: 0.000557672989089042
batch 130 loss: 0.0005576330586336553
batch 135 loss: 0.0005577332223765552
batch 140 loss: 0.0005577568779699504
batch 145 loss: 0.0005577442585490644
batch 150 loss: 0.000557684968225658
batch 155 loss: 0.0005577486241236329
batch 160 loss: 0.0005577064002864063
batch 165 loss: 0.0005576270748861134
batch 170 loss: 0.0005576661322265863
batch 175 loss: 0.0005576344090513885
batch 180 loss: 0.0005577290081419051
batch 185 loss: 0.0005576631636358797
batch 190 loss: 0.0005576285999268294
batch 195 loss: 0.0005576489376835525
batch 200 loss: 0.0005576803465373814
batch 205 loss: 0.0005576798459514976
batch 210 loss: 0.0005577075527980924
batch 215 loss: 0.000557696633040905
batch 220 loss: 0.0005577062955126166
batch 225 loss: 0.0005576751776970923
batch 230 loss: 0.0005576758761890232
batch 235 loss: 0.0005578090669587255
batch 240 loss: 0.0005576195893809199
Training Loss: 0.0005576834082603455
Validation Loss: 0.0005576934287091717
Epoch 73:
batch 5 loss: 0.0005577594507485628
batch 10 loss: 0.000557678600307554
batch 15 loss: 0.000557761883828789
batch 20 loss: 0.0005577898235060274
batch 25 loss: 0.0005577138275839388
batch 30 loss: 0.0005576648516580462
batch 35 loss: 0.0005576404742896557
batch 40 loss: 0.0005576880997978151
batch 45 loss: 0.0005577781703323126
batch 50 loss: 0.0005577512201853096
batch 55 loss: 0.0005577906966209411
batch 60 loss: 0.0005575881456024945
batch 65 loss: 0.000557620485778898
batch 70 loss: 0.0005576543044298887
batch 75 loss: 0.0005576317780651152
batch 80 loss: 0.0005576549330726266
batch 85 loss: 0.0005576139199547469
batch 90 loss: 0.0005577152129262686
batch 95 loss: 0.0005577433388680219
batch 100 loss: 0.0005576375289820135
batch 105 loss: 0.0005576317897066474
batch 110 loss: 0.0005576349678449333
batch 115 loss: 0.000557663943618536
batch 120 loss: 0.0005576133145950735
batch 125 loss: 0.0005576957482844591
batch 130 loss: 0.0005576808121986687
batch 135 loss: 0.0005576422438025475
batch 140 loss: 0.000557764049153775
batch 145 loss: 0.000557661592029035
batch 150 loss: 0.0005577496252954006
batch 155 loss: 0.0005577179486863315
batch 160 loss: 0.0005578263895586133
batch 165 loss: 0.0005577314528636635
batch 170 loss: 0.0005576852592639626
batch 175 loss: 0.0005577776115387678
batch 180 loss: 0.0005577998934313654
batch 185 loss: 0.0005577170057222247
batch 190 loss: 0.0005575946415774524
batch 195 loss: 0.0005577160860411823
batch 200 loss: 0.0005576671101152897
batch 205 loss: 0.0005576042807660997
batch 210 loss: 0.000557567272335291
batch 215 loss: 0.0005576158408075571
batch 220 loss: 0.000557579577434808
batch 225 loss: 0.000557719636708498
batch 230 loss: 0.0005577040719799697
batch 235 loss: 0.0005576554569415749
batch 240 loss: 0.0005575093673542142
Training Loss: 0.0005576834111707285
Validation Loss: 0.0005576934063962351
Epoch 74:
batch 5 loss: 0.0005577201955020428
batch 10 loss: 0.0005575933959335089
batch 15 loss: 0.0005577175295911729
batch 20 loss: 0.0005576753057539463
batch 25 loss: 0.0005575885414145886
batch 30 loss: 0.0005576572613790632
batch 35 loss: 0.0005577063770033419
batch 40 loss: 0.0005577189498580992
batch 45 loss: 0.000557672418653965
batch 50 loss: 0.000557692814618349
batch 55 loss: 0.000557735760230571
batch 60 loss: 0.0005577195552177727
batch 65 loss: 0.0005576927564106881
batch 70 loss: 0.0005575978080742061
batch 75 loss: 0.0005577491596341133
batch 80 loss: 0.0005578026757575572
batch 85 loss: 0.0005576446186751127
batch 90 loss: 0.0005577388685196639
batch 95 loss: 0.0005576188559643924
batch 100 loss: 0.0005576527561061084
batch 105 loss: 0.0005575816729106009
batch 110 loss: 0.000557786924764514
batch 115 loss: 0.0005576611030846835
batch 120 loss: 0.0005575585062615573
batch 125 loss: 0.0005576531984843314
batch 130 loss: 0.000557672290597111
batch 135 loss: 0.0005576146650128067
batch 140 loss: 0.0005577180651016534
batch 145 loss: 0.0005577178671956062
batch 150 loss: 0.0005577401025220752
batch 155 loss: 0.0005575986113399267
batch 160 loss: 0.0005576884956099093
batch 165 loss: 0.0005576368421316147
batch 170 loss: 0.000557628832757473
batch 175 loss: 0.0005577805102802813
batch 180 loss: 0.0005577377858571708
batch 185 loss: 0.0005576841067522764
batch 190 loss: 0.0005576624185778201
batch 195 loss: 0.0005576931987889111
batch 200 loss: 0.0005577832576818764
batch 205 loss: 0.0005576645489782095
batch 210 loss: 0.0005576863186433911
batch 215 loss: 0.0005577211733907462
batch 220 loss: 0.0005576438270509243
batch 225 loss: 0.000557638006284833
batch 230 loss: 0.0005577023955993354
batch 235 loss: 0.0005576069466769695
batch 240 loss: 0.0005578471696935594
Training Loss: 0.0005576834259651757
Validation Loss: 0.000557693403485852
Epoch 75:
batch 5 loss: 0.0005577279604040086
batch 10 loss: 0.0005576261086389422
batch 15 loss: 0.0005577641539275646
batch 20 loss: 0.0005576955387368798
batch 25 loss: 0.0005577627220191061
batch 30 loss: 0.0005576094496063888
batch 35 loss: 0.0005576893221586942
batch 40 loss: 0.0005577373201958835
batch 45 loss: 0.000557619275059551
batch 50 loss: 0.0005577142932452261
batch 55 loss: 0.0005576446419581771
batch 60 loss: 0.0005577306961640716
batch 65 loss: 0.0005575815332122148
batch 70 loss: 0.0005577667616307735
batch 75 loss: 0.0005576223833486438
batch 80 loss: 0.0005576536874286831
batch 85 loss: 0.0005577694159001112
batch 90 loss: 0.0005577739211730659
batch 95 loss: 0.0005576691590249538
batch 100 loss: 0.0005576408933848142
batch 105 loss: 0.0005577436881139874
batch 110 loss: 0.0005577840260230005
batch 115 loss: 0.0005577235016971827
batch 120 loss: 0.0005576754920184612
batch 125 loss: 0.0005576875759288668
batch 130 loss: 0.0005575918592512607
batch 135 loss: 0.0005576242692768574
batch 140 loss: 0.0005577566218562425
batch 145 loss: 0.0005576990661211312
batch 150 loss: 0.0005576092517003417
batch 155 loss: 0.0005576965515501798
batch 160 loss: 0.0005576943163760007
batch 165 loss: 0.0005575682152993977
batch 170 loss: 0.0005576495779678226
batch 175 loss: 0.000557564536575228
batch 180 loss: 0.0005577374366112053
batch 185 loss: 0.0005577484262175858
batch 190 loss: 0.000557706505060196
batch 195 loss: 0.0005576494382694364
batch 200 loss: 0.0005577211733907462
batch 205 loss: 0.0005577327916398645
batch 210 loss: 0.0005577726871706546
batch 215 loss: 0.0005576540832407772
batch 220 loss: 0.0005576438619755208
batch 225 loss: 0.0005576968425884843
batch 230 loss: 0.0005575603689067065
batch 235 loss: 0.0005576422903686762
batch 240 loss: 0.0005576696014031768
Training Loss: 0.0005576834019545156
Validation Loss: 0.0005576933966949582
Epoch 76:
batch 5 loss: 0.0005577196017839015
batch 10 loss: 0.0005577287054620683
batch 15 loss: 0.0005576510215178132
batch 20 loss: 0.0005577385192736984
batch 25 loss: 0.0005576342809945345
batch 30 loss: 0.0005576600437052548
batch 35 loss: 0.0005577789735980332
batch 40 loss: 0.0005576272960752249
batch 45 loss: 0.0005577006842941046
batch 50 loss: 0.0005577009171247483
batch 55 loss: 0.0005576901137828826
batch 60 loss: 0.000557568995282054
batch 65 loss: 0.0005577046773396433
batch 70 loss: 0.0005576469819061458
batch 75 loss: 0.0005577028729021549
batch 80 loss: 0.0005576223018579185
batch 85 loss: 0.0005575347924605012
batch 90 loss: 0.0005577577278017998
batch 95 loss: 0.0005576959694735706
batch 100 loss: 0.0005577535135671497
batch 105 loss: 0.0005578103591687977
batch 110 loss: 0.0005575554212555289
batch 115 loss: 0.0005576931755058468
batch 120 loss: 0.0005576815223321318
batch 125 loss: 0.000557643745560199
batch 130 loss: 0.0005577649222686887
batch 135 loss: 0.0005577086471021175
batch 140 loss: 0.0005576845956966281
batch 145 loss: 0.000557584420312196
batch 150 loss: 0.0005576237221248448
batch 155 loss: 0.0005576512077823282
batch 160 loss: 0.0005577365518547595
batch 165 loss: 0.0005576266790740192
batch 170 loss: 0.0005576561903581023
batch 175 loss: 0.0005577190895564854
batch 180 loss: 0.0005576658761128784
batch 185 loss: 0.0005576910451054573
batch 190 loss: 0.0005577418487519026
batch 195 loss: 0.0005576572730205953
batch 200 loss: 0.0005576599389314652
batch 205 loss: 0.0005577080184593797
batch 210 loss: 0.0005578629672527314
batch 215 loss: 0.0005577352130785584
batch 220 loss: 0.0005576624069362878
batch 225 loss: 0.0005577102303504944
batch 230 loss: 0.0005576845374889672
batch 235 loss: 0.00055765719152987
batch 240 loss: 0.0005576097290031611
Training Loss: 0.0005576834274203672
Validation Loss: 0.0005576934228884057
Epoch 77:
batch 5 loss: 0.0005578175885602831
batch 10 loss: 0.0005577713251113892
batch 15 loss: 0.0005575891816988587
batch 20 loss: 0.0005574988899752498
batch 25 loss: 0.0005577379837632179
batch 30 loss: 0.0005576829193159938
batch 35 loss: 0.0005576967727392912
batch 40 loss: 0.0005577185889706016
batch 45 loss: 0.0005576818133704364
batch 50 loss: 0.0005576178897172213
batch 55 loss: 0.0005576544092036784
batch 60 loss: 0.0005576837691478431
batch 65 loss: 0.0005577210104092956
batch 70 loss: 0.000557631510309875
batch 75 loss: 0.00055772167397663
batch 80 loss: 0.0005577146774157881
batch 85 loss: 0.0005576440249569714
batch 90 loss: 0.0005577246076427401
batch 95 loss: 0.0005576540366746485
batch 100 loss: 0.000557630485855043
batch 105 loss: 0.0005577078787609935
batch 110 loss: 0.0005577492760494351
batch 115 loss: 0.000557775073684752
batch 120 loss: 0.0005576602183282375
batch 125 loss: 0.0005576301482506096
batch 130 loss: 0.0005577142583206296
batch 135 loss: 0.0005577374016866088
batch 140 loss: 0.0005576388211920857
batch 145 loss: 0.0005576611845754087
batch 150 loss: 0.0005577864474616944
batch 155 loss: 0.0005577433854341507
batch 160 loss: 0.0005576121038757265
batch 165 loss: 0.0005576380877755583
batch 170 loss: 0.00055766177829355
batch 175 loss: 0.0005577375995926559
batch 180 loss: 0.0005576000898145139
batch 185 loss: 0.0005576017778366804
batch 190 loss: 0.0005576846422627568
batch 195 loss: 0.0005576700204983354
batch 200 loss: 0.0005576006602495909
batch 205 loss: 0.0005576469819061458
batch 210 loss: 0.0005578428856097162
batch 215 loss: 0.0005576884956099093
batch 220 loss: 0.0005576909636147321
batch 225 loss: 0.0005577772040851414
batch 230 loss: 0.000557728752028197
batch 235 loss: 0.0005575829651206732
batch 240 loss: 0.0005576411727815867
Training Loss: 0.0005576834048648986
Validation Loss: 0.0005576934025157243
Epoch 78:
batch 5 loss: 0.0005577528267167508
batch 10 loss: 0.0005576196592301131
batch 15 loss: 0.0005576603929512203
batch 20 loss: 0.0005576939787715673
batch 25 loss: 0.0005577473202720284
batch 30 loss: 0.0005576387280598283
batch 35 loss: 0.0005577057250775397
batch 40 loss: 0.0005576825118623674
batch 45 loss: 0.000557752326130867
batch 50 loss: 0.0005575302755460143
batch 55 loss: 0.0005577663774602115
batch 60 loss: 0.0005577678442932665
batch 65 loss: 0.0005576618714258075
batch 70 loss: 0.0005576812895014882
batch 75 loss: 0.0005577366799116134
batch 80 loss: 0.0005576590425334871
batch 85 loss: 0.0005577185773290693
batch 90 loss: 0.0005575973307713866
batch 95 loss: 0.0005576860741712153
batch 100 loss: 0.0005577008472755551
batch 105 loss: 0.0005576629075221718
batch 110 loss: 0.0005577747477218508
batch 115 loss: 0.0005575791466981173
batch 120 loss: 0.0005577461677603423
batch 125 loss: 0.0005576227209530771
batch 130 loss: 0.0005576958181336522
batch 135 loss: 0.0005576069466769695
batch 140 loss: 0.0005576742580160499
batch 145 loss: 0.0005576946772634983
batch 150 loss: 0.0005576619645580649
batch 155 loss: 0.0005577113945037127
batch 160 loss: 0.0005577176460064947
batch 165 loss: 0.0005577267031185329
batch 170 loss: 0.0005577368079684675
batch 175 loss: 0.000557664327789098
batch 180 loss: 0.0005576803698204458
batch 185 loss: 0.0005577157833613455
batch 190 loss: 0.0005576043506152928
batch 195 loss: 0.0005576235242187976
batch 200 loss: 0.0005576532217673958
batch 205 loss: 0.0005576725583523512
batch 210 loss: 0.000557705934625119
batch 215 loss: 0.0005576612777076662
batch 220 loss: 0.0005577369360253215
batch 225 loss: 0.0005577165982685983
batch 230 loss: 0.0005577049450948834
batch 235 loss: 0.0005576467840000987
batch 240 loss: 0.0005576452240347862
Training Loss: 0.0005576834046223667
Validation Loss: 0.0005576934044559796
Epoch 79:
batch 5 loss: 0.0005577777279540897
batch 10 loss: 0.0005577382980845869
batch 15 loss: 0.0005575861083343625
batch 20 loss: 0.0005576885305345058
batch 25 loss: 0.0005577331525273621
batch 30 loss: 0.0005576842930167913
batch 35 loss: 0.000557669613044709
batch 40 loss: 0.0005577167845331133
batch 45 loss: 0.0005575809744186699
batch 50 loss: 0.0005577250383794307
batch 55 loss: 0.0005576678202487528
batch 60 loss: 0.0005577120464295149
batch 65 loss: 0.0005577536998316645
batch 70 loss: 0.0005576640600338578
batch 75 loss: 0.0005576884374022484
batch 80 loss: 0.000557604874484241
batch 85 loss: 0.0005576730472967029
batch 90 loss: 0.0005577245960012078
batch 95 loss: 0.0005577085772529245
batch 100 loss: 0.0005576669936999679
batch 105 loss: 0.0005576811730861664
batch 110 loss: 0.0005575964925810695
batch 115 loss: 0.0005577277741394937
batch 120 loss: 0.0005576124298386276
batch 125 loss: 0.0005577240488491952
batch 130 loss: 0.0005575728137046099
batch 135 loss: 0.0005577903124503791
batch 140 loss: 0.0005576955736614764
batch 145 loss: 0.0005576873547397554
batch 150 loss: 0.0005576600669883192
batch 155 loss: 0.0005576907657086849
batch 160 loss: 0.0005577352712862194
batch 165 loss: 0.0005578231764957308
batch 170 loss: 0.0005577720352448523
batch 175 loss: 0.0005577389616519213
batch 180 loss: 0.0005576256779022514
batch 185 loss: 0.0005576566676609218
batch 190 loss: 0.0005576605908572674
batch 195 loss: 0.00055761105613783
batch 200 loss: 0.0005576448747888207
batch 205 loss: 0.0005577011150307953
batch 210 loss: 0.0005576627794653177
batch 215 loss: 0.0005576796364039182
batch 220 loss: 0.0005576956784352661
batch 225 loss: 0.0005577185074798763
batch 230 loss: 0.0005576262949034572
batch 235 loss: 0.0005576177034527063
batch 240 loss: 0.0005576293100602925
Training Loss: 0.0005576833920107068
Validation Loss: 0.0005576934209481503
Epoch 80:
batch 5 loss: 0.0005576106137596071
batch 10 loss: 0.0005576292518526315
batch 15 loss: 0.0005576067371293903
batch 20 loss: 0.000557653815485537
batch 25 loss: 0.0005577393108978868
batch 30 loss: 0.0005576148745603859
batch 35 loss: 0.000557750347070396
batch 40 loss: 0.0005576530587859452
batch 45 loss: 0.0005576968425884843
batch 50 loss: 0.0005577233270742
batch 55 loss: 0.0005577851203270257
batch 60 loss: 0.0005576496245339513
batch 65 loss: 0.0005576511728577315
batch 70 loss: 0.0005577195552177727
batch 75 loss: 0.0005576693802140653
batch 80 loss: 0.000557703769300133
batch 85 loss: 0.0005576785188168287
batch 90 loss: 0.0005576983094215393
batch 95 loss: 0.0005576935713179409
batch 100 loss: 0.0005576783791184425
batch 105 loss: 0.000557631510309875
batch 110 loss: 0.0005576921394094825
batch 115 loss: 0.0005575656541623175
batch 120 loss: 0.0005576785420998931
batch 125 loss: 0.0005577106378041208
batch 130 loss: 0.0005576397757977247
batch 135 loss: 0.0005576828261837363
batch 140 loss: 0.0005577387986704707
batch 145 loss: 0.0005576682044193149
batch 150 loss: 0.0005577252362854779
batch 155 loss: 0.0005576511262916029
batch 160 loss: 0.0005577085888944566
batch 165 loss: 0.0005577235366217792
batch 170 loss: 0.0005577238043770194
batch 175 loss: 0.0005577159230597317
batch 180 loss: 0.0005577249219641089
batch 185 loss: 0.0005577216041274368
batch 190 loss: 0.0005577354226261377
batch 195 loss: 0.0005576771800406277
batch 200 loss: 0.0005576961790211499
batch 205 loss: 0.0005577232455834747
batch 210 loss: 0.0005576869705691933
batch 215 loss: 0.0005576663534156978
batch 220 loss: 0.0005576622090302408
batch 225 loss: 0.0005577083909884095
batch 230 loss: 0.0005576826282776892
batch 235 loss: 0.0005576777271926403
batch 240 loss: 0.0005575779941864312
Training Loss: 0.0005576833898279195
Validation Loss: 0.0005576934131871288
Epoch 81:
batch 5 loss: 0.0005577831529080868
batch 10 loss: 0.0005576897296123206
batch 15 loss: 0.0005577080184593797
batch 20 loss: 0.0005576980183832347
batch 25 loss: 0.0005576689844019711
batch 30 loss: 0.0005577499745413661
batch 35 loss: 0.0005575865041464567
batch 40 loss: 0.0005577224306762219
batch 45 loss: 0.0005576403345912695
batch 50 loss: 0.0005576622672379017
batch 55 loss: 0.0005576612777076662
batch 60 loss: 0.0005576700903475284
batch 65 loss: 0.0005575729301199317
batch 70 loss: 0.0005578074371442199
batch 75 loss: 0.0005576423485763371
batch 80 loss: 0.0005576845025643707
batch 85 loss: 0.0005576415569521487
batch 90 loss: 0.0005575859802775085
batch 95 loss: 0.0005576681112870574
batch 100 loss: 0.0005576623603701592
batch 105 loss: 0.0005577568779699504
batch 110 loss: 0.0005577195319347083
batch 115 loss: 0.0005576650844886899
batch 120 loss: 0.0005577079253271222
batch 125 loss: 0.0005577423027716577
batch 130 loss: 0.0005577395553700626
batch 135 loss: 0.000557754933834076
batch 140 loss: 0.0005577269941568375
batch 145 loss: 0.0005577566800639034
batch 150 loss: 0.0005577353993430733
batch 155 loss: 0.0005576853756792844
batch 160 loss: 0.0005577006959356368
batch 165 loss: 0.000557732512243092
batch 170 loss: 0.0005575856543146074
batch 175 loss: 0.0005575975519604981
batch 180 loss: 0.0005577082862146199
batch 185 loss: 0.0005577250383794307
batch 190 loss: 0.0005578228854574263
batch 195 loss: 0.0005576028022915124
batch 200 loss: 0.0005576015217229723
batch 205 loss: 0.0005576130351983011
batch 210 loss: 0.0005577300093136728
batch 215 loss: 0.0005576267722062767
batch 220 loss: 0.0005576753057539463
batch 225 loss: 0.0005576081341132522
batch 230 loss: 0.0005577034782618285
batch 235 loss: 0.000557619787286967
batch 240 loss: 0.0005576533847488462
Training Loss: 0.0005576834068051539
Validation Loss: 0.0005576934558727468
Epoch 82:
batch 5 loss: 0.0005577520816586912
batch 10 loss: 0.0005576598225161434
batch 15 loss: 0.0005576228722929955
batch 20 loss: 0.0005576938157901168
batch 25 loss: 0.0005578011041507125
batch 30 loss: 0.0005576719995588064
batch 35 loss: 0.0005576730123721063
batch 40 loss: 0.0005577276227995753
batch 45 loss: 0.0005577160278335214
batch 50 loss: 0.0005576398340053857
batch 55 loss: 0.0005578240961767733
batch 60 loss: 0.0005576545489020645
batch 65 loss: 0.0005576958996243775
batch 70 loss: 0.0005576020455919206
batch 75 loss: 0.0005576893920078874
batch 80 loss: 0.0005576027208007873
batch 85 loss: 0.0005576849333010613
batch 90 loss: 0.0005578153533861042
batch 95 loss: 0.0005576392053626478
batch 100 loss: 0.0005577040719799697
batch 105 loss: 0.000557691534049809
batch 110 loss: 0.0005576980882324278
batch 115 loss: 0.0005576476454734802
batch 120 loss: 0.0005576633149757982
batch 125 loss: 0.0005576555384323
batch 130 loss: 0.0005577368661761284
batch 135 loss: 0.0005576886585913598
batch 140 loss: 0.0005575977265834808
batch 145 loss: 0.0005576103576458991
batch 150 loss: 0.0005577016505412758
batch 155 loss: 0.0005577204283326864
batch 160 loss: 0.0005576856434345246
batch 165 loss: 0.000557597610168159
batch 170 loss: 0.0005577545845881105
batch 175 loss: 0.0005576762603595852
batch 180 loss: 0.000557674269657582
batch 185 loss: 0.0005575722898356616
batch 190 loss: 0.0005577767267823219
batch 195 loss: 0.0005575611372478307
batch 200 loss: 0.0005576410098001361
batch 205 loss: 0.0005577327683568001
batch 210 loss: 0.0005577131407335401
batch 215 loss: 0.0005576740484684705
batch 220 loss: 0.0005577137460932135
batch 225 loss: 0.0005575754214078188
batch 230 loss: 0.0005577182746492326
batch 235 loss: 0.0005577082862146199
batch 240 loss: 0.0005577456206083298
Training Loss: 0.0005576833980740048
Validation Loss: 0.0005576933996053413
Epoch 83:
batch 5 loss: 0.0005577573203481734
batch 10 loss: 0.0005576747702434659
batch 15 loss: 0.0005576853291131556
batch 20 loss: 0.0005576405674219131
batch 25 loss: 0.0005577454809099436
batch 30 loss: 0.0005576279596425593
batch 35 loss: 0.0005576579133048654
batch 40 loss: 0.0005576825235038996
batch 45 loss: 0.0005576777854003012
batch 50 loss: 0.0005577021976932884
batch 55 loss: 0.0005575995077379048
batch 60 loss: 0.0005576793570071459
batch 65 loss: 0.0005577438394539058
batch 70 loss: 0.0005577076924964785
batch 75 loss: 0.0005576486000791192
batch 80 loss: 0.0005575909046456218
batch 85 loss: 0.0005576645489782095
batch 90 loss: 0.0005576533148996532
batch 95 loss: 0.0005577015923336148
batch 100 loss: 0.000557636539451778
batch 105 loss: 0.0005577062256634235
batch 110 loss: 0.0005576888332143426
batch 115 loss: 0.0005576368886977435
batch 120 loss: 0.0005577020347118378
batch 125 loss: 0.0005576292052865029
batch 130 loss: 0.0005576439434662461
batch 135 loss: 0.0005577884614467621
batch 140 loss: 0.0005576975527219474
batch 145 loss: 0.0005577185773290693
batch 150 loss: 0.0005576671683229506
batch 155 loss: 0.0005577452597208321
batch 160 loss: 0.0005577230593189597
batch 165 loss: 0.00055788429453969
batch 170 loss: 0.000557670823764056
batch 175 loss: 0.0005576407420448959
batch 180 loss: 0.0005578142357990145
batch 185 loss: 0.0005576832219958306
batch 190 loss: 0.0005576772266067565
batch 195 loss: 0.0005576519994065166
batch 200 loss: 0.0005576769239269197
batch 205 loss: 0.0005576750729233027
batch 210 loss: 0.0005576191120781005
batch 215 loss: 0.0005576396593824029
batch 220 loss: 0.0005577160161919891
batch 225 loss: 0.0005575177259743213
batch 230 loss: 0.000557752256281674
batch 235 loss: 0.0005577531992457807
batch 240 loss: 0.0005576057243160904
Training Loss: 0.0005576833997717282
Validation Loss: 0.0005576934083364904
Epoch 84:
batch 5 loss: 0.0005576446070335806
batch 10 loss: 0.0005577478092163801
batch 15 loss: 0.0005577058647759259
batch 20 loss: 0.0005577286588959396
batch 25 loss: 0.0005576718365773558
batch 30 loss: 0.0005575905088335276
batch 35 loss: 0.0005577307310886681
batch 40 loss: 0.0005576691241003573
batch 45 loss: 0.0005576836643740535
batch 50 loss: 0.0005576497642323375
batch 55 loss: 0.0005576586583629251
batch 60 loss: 0.0005576013470999896
batch 65 loss: 0.0005576900788582861
batch 70 loss: 0.0005577397416345776
batch 75 loss: 0.0005576035473495722
batch 80 loss: 0.0005575900548137724
batch 85 loss: 0.0005576949566602707
batch 90 loss: 0.0005576193681918085
batch 95 loss: 0.0005577789852395654
batch 100 loss: 0.000557663559447974
batch 105 loss: 0.0005575845134444535
batch 110 loss: 0.0005576979951001704
batch 115 loss: 0.000557676434982568
batch 120 loss: 0.0005576986703090369
batch 125 loss: 0.0005577099975198508
batch 130 loss: 0.0005576477851718664
batch 135 loss: 0.0005577592062763869
batch 140 loss: 0.0005577131872996688
batch 145 loss: 0.0005577298928983509
batch 150 loss: 0.0005575970862992108
batch 155 loss: 0.0005577117204666138
batch 160 loss: 0.0005576476105488837
batch 165 loss: 0.0005576877971179784
batch 170 loss: 0.0005576621857471764
batch 175 loss: 0.0005577403702773154
batch 180 loss: 0.000557768065482378
batch 185 loss: 0.0005575910327024758
batch 190 loss: 0.000557653175201267
batch 195 loss: 0.0005576323601417243
batch 200 loss: 0.0005577843636274338
batch 205 loss: 0.0005577820702455938
batch 210 loss: 0.0005577666684985161
batch 215 loss: 0.0005577050964348018
batch 220 loss: 0.0005577243631705642
batch 225 loss: 0.0005576992058195174
batch 230 loss: 0.0005576308816671371
batch 235 loss: 0.000557636993471533
batch 240 loss: 0.0005577014875598252
Training Loss: 0.0005576833975889409
Validation Loss: 0.0005576934180377672
Epoch 85:
batch 5 loss: 0.0005576784955337643
batch 10 loss: 0.0005576259805820883
batch 15 loss: 0.0005576639086939394
batch 20 loss: 0.000557760987430811
batch 25 loss: 0.0005577419768087565
batch 30 loss: 0.0005576740368269384
batch 35 loss: 0.0005577932693995536
batch 40 loss: 0.000557660881895572
batch 45 loss: 0.000557792093604803
batch 50 loss: 0.0005576108349487185
batch 55 loss: 0.0005575973191298544
batch 60 loss: 0.0005576040945015848
batch 65 loss: 0.0005577085888944566
batch 70 loss: 0.0005575840943492949
batch 75 loss: 0.0005577202187851072
batch 80 loss: 0.000557638774625957
batch 85 loss: 0.0005576439900323749
batch 90 loss: 0.0005576897645369172
batch 95 loss: 0.0005577051546424627
batch 100 loss: 0.0005576716852374375
batch 105 loss: 0.000557613221462816
batch 110 loss: 0.0005576160387136042
batch 115 loss: 0.0005577501608058811
batch 120 loss: 0.0005576887750066816
batch 125 loss: 0.0005578046431764961
batch 130 loss: 0.000557656423188746
batch 135 loss: 0.0005576489609666168
batch 140 loss: 0.0005576217779889702
batch 145 loss: 0.0005576886585913598
batch 150 loss: 0.0005576541414484381
batch 155 loss: 0.0005577275063842535
batch 160 loss: 0.0005577368778176605
batch 165 loss: 0.0005577048170380295
batch 170 loss: 0.0005576968425884843
batch 175 loss: 0.0005577049334533513
batch 180 loss: 0.000557645398657769
batch 185 loss: 0.000557641324121505
batch 190 loss: 0.0005577116389758885
batch 195 loss: 0.0005576140247285366
batch 200 loss: 0.000557635200675577
batch 205 loss: 0.0005578135373070836
batch 210 loss: 0.0005575905088335276
batch 215 loss: 0.000557608308736235
batch 220 loss: 0.0005577612319029868
batch 225 loss: 0.0005576374591328203
batch 230 loss: 0.0005578909534960985
batch 235 loss: 0.0005576331983320415
batch 240 loss: 0.0005577397067099809
Training Loss: 0.0005576833837646215
Validation Loss: 0.0005576933986352135
Epoch 86:
batch 5 loss: 0.0005577556439675391
batch 10 loss: 0.0005577099975198508
batch 15 loss: 0.0005576138850301504
batch 20 loss: 0.0005576735245995223
batch 25 loss: 0.0005576903349719942
batch 30 loss: 0.0005577681702561676
batch 35 loss: 0.0005577331059612334
batch 40 loss: 0.00055769186001271
batch 45 loss: 0.0005577555624768138
batch 50 loss: 0.0005576737457886338
batch 55 loss: 0.0005576453637331724
batch 60 loss: 0.0005575477611273527
batch 65 loss: 0.0005577685195021331
batch 70 loss: 0.0005577121628448367
batch 75 loss: 0.0005576058058068157
batch 80 loss: 0.0005576358060352504
batch 85 loss: 0.0005577223608270287
batch 90 loss: 0.0005576570052653551
batch 95 loss: 0.0005576889961957932
batch 100 loss: 0.0005576614290475846
batch 105 loss: 0.0005577329779043793
batch 110 loss: 0.0005576131865382194
batch 115 loss: 0.0005576829076744616
batch 120 loss: 0.0005577092641033232
batch 125 loss: 0.0005576709751039744
batch 130 loss: 0.0005576907540671527
batch 135 loss: 0.0005576801835559308
batch 140 loss: 0.000557702430523932
batch 145 loss: 0.0005577226751483977
batch 150 loss: 0.0005575883900746703
batch 155 loss: 0.0005577190429903566
batch 160 loss: 0.0005576560972258449
batch 165 loss: 0.0005577324889600277
batch 170 loss: 0.0005577153293415904
batch 175 loss: 0.000557737157214433
batch 180 loss: 0.0005577997188083827
batch 185 loss: 0.0005576463066972792
batch 190 loss: 0.0005577900446951389
batch 195 loss: 0.0005576396593824029
batch 200 loss: 0.0005575120565481484
batch 205 loss: 0.0005576692870818079
batch 210 loss: 0.0005575913470238447
batch 215 loss: 0.0005577245145104825
batch 220 loss: 0.0005576073075644672
batch 225 loss: 0.0005576658761128784
batch 230 loss: 0.0005577371455729007
batch 235 loss: 0.0005576503579504787
batch 240 loss: 0.0005577038740739226
Training Loss: 0.0005576833832795576
Validation Loss: 0.0005576934170676395
Epoch 87:
batch 5 loss: 0.0005576056777499616
batch 10 loss: 0.0005575936171226204
batch 15 loss: 0.0005577691830694675
batch 20 loss: 0.0005576451309025288
batch 25 loss: 0.0005576323485001922
batch 30 loss: 0.0005576321389526128
batch 35 loss: 0.0005576348165050149
batch 40 loss: 0.000557564350310713
batch 45 loss: 0.0005577702540904284
batch 50 loss: 0.00055765489814803
batch 55 loss: 0.0005576871568337083
batch 60 loss: 0.0005577441188506782
batch 65 loss: 0.0005577110103331506
batch 70 loss: 0.000557700137142092
batch 75 loss: 0.0005577030009590089
batch 80 loss: 0.0005576922791078687
batch 85 loss: 0.000557686435058713
batch 90 loss: 0.0005575891584157944
batch 95 loss: 0.000557663245126605
batch 100 loss: 0.0005577391013503075
batch 105 loss: 0.0005576238152571022
batch 110 loss: 0.0005577574484050274
batch 115 loss: 0.0005576721974648535
batch 120 loss: 0.0005577203468419611
batch 125 loss: 0.0005576522089540958
batch 130 loss: 0.0005577126052230596
batch 135 loss: 0.00055766407167539
batch 140 loss: 0.0005576375522650778
batch 145 loss: 0.000557713012676686
batch 150 loss: 0.0005576669471338391
batch 155 loss: 0.0005576194496825337
batch 160 loss: 0.0005577020812779665
batch 165 loss: 0.000557609845418483
batch 170 loss: 0.000557647761888802
batch 175 loss: 0.0005577680072747171
batch 180 loss: 0.0005578125128522515
batch 185 loss: 0.0005576102179475128
batch 190 loss: 0.0005577425239607692
batch 195 loss: 0.0005576228257268667
batch 200 loss: 0.0005577630479820072
batch 205 loss: 0.0005576157942414284
batch 210 loss: 0.0005577827687375247
batch 215 loss: 0.0005577698699198663
batch 220 loss: 0.0005576807307079434
batch 225 loss: 0.0005577884265221655
batch 230 loss: 0.0005577152245678007
batch 235 loss: 0.0005576018826104701
batch 240 loss: 0.0005577112780883909
Training Loss: 0.0005576833857048769
Validation Loss: 0.000557693424828661
Epoch 88:
batch 5 loss: 0.0005576641182415187
batch 10 loss: 0.0005576557014137506
batch 15 loss: 0.0005576615454629064
batch 20 loss: 0.0005576475756242872
batch 25 loss: 0.0005576179479248822
batch 30 loss: 0.0005577788571827114
batch 35 loss: 0.0005576772382482886
batch 40 loss: 0.0005576820462010801
batch 45 loss: 0.0005578700569458306
batch 50 loss: 0.0005575963179580868
batch 55 loss: 0.0005576785188168287
batch 60 loss: 0.0005577127798460424
batch 65 loss: 0.0005576657480560243
batch 70 loss: 0.0005577508709393442
batch 75 loss: 0.0005576909985393286
batch 80 loss: 0.0005577560164965689
batch 85 loss: 0.0005576968775130809
batch 90 loss: 0.0005576617550104856
batch 95 loss: 0.0005577274598181248
batch 100 loss: 0.0005576721974648535
batch 105 loss: 0.0005576057941652834
batch 110 loss: 0.0005576590308919549
batch 115 loss: 0.000557847076561302
batch 120 loss: 0.0005576431285589933
batch 125 loss: 0.0005577187985181808
batch 130 loss: 0.0005576545489020645
batch 135 loss: 0.0005575272836722434
batch 140 loss: 0.000557617424055934
batch 145 loss: 0.0005576279247179628
batch 150 loss: 0.0005576523835770786
batch 155 loss: 0.0005577568896114826
batch 160 loss: 0.0005576055147685111
batch 165 loss: 0.0005576451309025288
batch 170 loss: 0.0005577022209763526
batch 175 loss: 0.0005577941890805959
batch 180 loss: 0.0005576295079663396
batch 185 loss: 0.0005577679490670562
batch 190 loss: 0.0005577486823312938
batch 195 loss: 0.0005576212774030864
batch 200 loss: 0.0005577450385317207
batch 205 loss: 0.0005576133378781378
batch 210 loss: 0.0005577182513661682
batch 215 loss: 0.0005575902410782874
batch 220 loss: 0.000557779788505286
batch 225 loss: 0.0005576940136961638
batch 230 loss: 0.0005575733375735581
batch 235 loss: 0.0005576820694841444
batch 240 loss: 0.0005577173084020615
Training Loss: 0.0005576833910405792
Validation Loss: 0.0005576933937845752
Epoch 89:
batch 5 loss: 0.000557697587646544
batch 10 loss: 0.0005577035364694894
batch 15 loss: 0.0005577225354500114
batch 20 loss: 0.0005575727787800133
batch 25 loss: 0.0005576723837293684
batch 30 loss: 0.0005576503346674145
batch 35 loss: 0.000557708041742444
batch 40 loss: 0.0005575557472184301
batch 45 loss: 0.000557626609224826
batch 50 loss: 0.0005576913827098906
batch 55 loss: 0.0005577521515078843
batch 60 loss: 0.0005576895433478058
batch 65 loss: 0.0005575955379754305
batch 70 loss: 0.0005577197414822876
batch 75 loss: 0.0005577311152592301
batch 80 loss: 0.0005577539326623082
batch 85 loss: 0.0005577039322815835
batch 90 loss: 0.000557786540593952
batch 95 loss: 0.0005577164120040834
batch 100 loss: 0.0005576172959990799
batch 105 loss: 0.0005577691015787422
batch 110 loss: 0.0005577791831456125
batch 115 loss: 0.0005575907649472356
batch 120 loss: 0.0005576291820034385
batch 125 loss: 0.0005575832561589778
batch 130 loss: 0.0005577494390308857
batch 135 loss: 0.0005576038034632802
batch 140 loss: 0.0005576681112870574
batch 145 loss: 0.0005576646071858704
batch 150 loss: 0.000557655340526253
batch 155 loss: 0.0005577214644290507
batch 160 loss: 0.000557723268866539
batch 165 loss: 0.0005577665288001299
batch 170 loss: 0.0005576819181442261
batch 175 loss: 0.0005576484487392009
batch 180 loss: 0.0005576245603151619
batch 185 loss: 0.0005576736875809729
batch 190 loss: 0.000557780172675848
batch 195 loss: 0.0005577140371315181
batch 200 loss: 0.0005576581694185734
batch 205 loss: 0.0005576726980507374
batch 210 loss: 0.0005577616160735488
batch 215 loss: 0.0005576359573751688
batch 220 loss: 0.0005577806034125388
batch 225 loss: 0.0005576358758844435
batch 230 loss: 0.0005577411153353751
batch 235 loss: 0.0005576925352215767
batch 240 loss: 0.0005575301824137568
Training Loss: 0.0005576833910405792
Validation Loss: 0.0005576934694545343
Epoch 90:
batch 5 loss: 0.0005577347590588033
batch 10 loss: 0.000557775900233537
batch 15 loss: 0.0005576158873736858
batch 20 loss: 0.0005577109521254897
batch 25 loss: 0.000557643105275929
batch 30 loss: 0.0005576380528509617
batch 35 loss: 0.000557695236057043
batch 40 loss: 0.0005576927913352847
batch 45 loss: 0.0005576538620516658
batch 50 loss: 0.0005576562136411667
batch 55 loss: 0.0005576907424256206
batch 60 loss: 0.000557643105275929
batch 65 loss: 0.0005576464580371975
batch 70 loss: 0.0005576339550316334
batch 75 loss: 0.000557649414986372
batch 80 loss: 0.0005575837218202651
batch 85 loss: 0.0005576887167990207
batch 90 loss: 0.0005577075178734958
batch 95 loss: 0.0005575944785960018
batch 100 loss: 0.0005577018018811941
batch 105 loss: 0.0005576686933636665
batch 110 loss: 0.000557731103617698
batch 115 loss: 0.0005576804396696389
batch 120 loss: 0.0005577277741394937
batch 125 loss: 0.0005576504045166075
batch 130 loss: 0.0005577501258812844
batch 135 loss: 0.0005577153526246548
batch 140 loss: 0.0005576847237534821
batch 145 loss: 0.0005576817085966468
batch 150 loss: 0.0005577147821895778
batch 155 loss: 0.0005576059338636696
batch 160 loss: 0.0005576928611844778
batch 165 loss: 0.0005578161682933569
batch 170 loss: 0.0005577157018706203
batch 175 loss: 0.0005577191244810819
batch 180 loss: 0.0005576439900323749
batch 185 loss: 0.0005576543277129531
batch 190 loss: 0.0005577339208684862
batch 195 loss: 0.0005578010343015194
batch 200 loss: 0.0005577209289185703
batch 205 loss: 0.0005576746189035475
batch 210 loss: 0.0005576985306106508
batch 215 loss: 0.0005577728850767016
batch 220 loss: 0.0005576509865932167
batch 225 loss: 0.0005576949217356742
batch 230 loss: 0.0005576851428486407
batch 235 loss: 0.0005576272029429674
batch 240 loss: 0.0005575323128141462
Training Loss: 0.0005576833827944938
Validation Loss: 0.000557693403485852
Epoch 91:
batch 5 loss: 0.0005576482508331537
batch 10 loss: 0.000557699881028384
batch 15 loss: 0.0005576936877332628
batch 20 loss: 0.0005576475290581584
batch 25 loss: 0.0005577841075137258
batch 30 loss: 0.0005576371564529836
batch 35 loss: 0.0005577876814641059
batch 40 loss: 0.0005575859686359763
batch 45 loss: 0.0005577850854024291
batch 50 loss: 0.0005576172843575478
batch 55 loss: 0.0005577042582444846
batch 60 loss: 0.0005576789728365839
batch 65 loss: 0.0005576416617259384
batch 70 loss: 0.0005576947936788201
batch 75 loss: 0.0005577783216722309
batch 80 loss: 0.0005577003699727357
batch 85 loss: 0.0005577357718721032
batch 90 loss: 0.0005576582858338952
batch 95 loss: 0.0005577469361014664
batch 100 loss: 0.0005575760849751532
batch 105 loss: 0.0005576282157562674
batch 110 loss: 0.0005577389616519213
batch 115 loss: 0.0005576925119385123
batch 120 loss: 0.0005576506722718477
batch 125 loss: 0.0005576718365773558
batch 130 loss: 0.0005576895317062735
batch 135 loss: 0.0005577053059823811
batch 140 loss: 0.0005576417897827923
batch 145 loss: 0.0005576703581027687
batch 150 loss: 0.0005577695206739009
batch 155 loss: 0.0005577301722951234
batch 160 loss: 0.0005576309282332659
batch 165 loss: 0.0005576896015554667
batch 170 loss: 0.000557619717437774
batch 175 loss: 0.000557585887145251
batch 180 loss: 0.0005577520350925624
batch 185 loss: 0.0005575995659455657
batch 190 loss: 0.0005576286814175547
batch 195 loss: 0.0005577727220952511
batch 200 loss: 0.0005577464587986469
batch 205 loss: 0.0005577413481660187
batch 210 loss: 0.0005576409865170717
batch 215 loss: 0.0005576771451160312
batch 220 loss: 0.0005576927796937525
batch 225 loss: 0.0005577198578976094
batch 230 loss: 0.0005576605559326709
batch 235 loss: 0.0005576628376729786
batch 240 loss: 0.000557590089738369
Training Loss: 0.0005576833784289193
Validation Loss: 0.0005576934073663627
Epoch 92:
batch 5 loss: 0.0005577056901529432
batch 10 loss: 0.0005577432340942324
batch 15 loss: 0.0005577967036515475
batch 20 loss: 0.0005577110801823437
batch 25 loss: 0.0005576763185672462
batch 30 loss: 0.0005577248171903193
batch 35 loss: 0.0005577360629104078
batch 40 loss: 0.0005576799623668194
batch 45 loss: 0.0005576059687882662
batch 50 loss: 0.0005577039322815835
batch 55 loss: 0.000557706190738827
batch 60 loss: 0.0005576316616497934
batch 65 loss: 0.000557647307869047
batch 70 loss: 0.0005577786127105355
batch 75 loss: 0.0005575002403929829
batch 80 loss: 0.0005576432798989117
batch 85 loss: 0.0005576532450504601
batch 90 loss: 0.0005576801486313343
batch 95 loss: 0.000557830254547298
batch 100 loss: 0.0005577297881245614
batch 105 loss: 0.0005576530704274774
batch 110 loss: 0.0005578415933996439
batch 115 loss: 0.0005576030816882849
batch 120 loss: 0.0005576599156484008
batch 125 loss: 0.0005576146650128067
batch 130 loss: 0.0005576455849222838
batch 135 loss: 0.0005576625932008028
batch 140 loss: 0.0005577188916504383
batch 145 loss: 0.0005576407304033637
batch 150 loss: 0.0005577764357440173
batch 155 loss: 0.0005576428840868175
batch 160 loss: 0.0005577056552283465
batch 165 loss: 0.0005576323717832565
batch 170 loss: 0.0005576526513323188
batch 175 loss: 0.0005577873322181404
batch 180 loss: 0.0005576177150942385
batch 185 loss: 0.000557764363475144
batch 190 loss: 0.0005577248055487871
batch 195 loss: 0.0005576521740294993
batch 200 loss: 0.0005576622439548373
batch 205 loss: 0.0005575202289037406
batch 210 loss: 0.0005577008938416839
batch 215 loss: 0.0005577147239819169
batch 220 loss: 0.0005576929077506065
batch 225 loss: 0.0005578118143603206
batch 230 loss: 0.0005576416500844061
batch 235 loss: 0.0005575840594246983
batch 240 loss: 0.0005575926625169814
Training Loss: 0.000557683379156515
Validation Loss: 0.0005576933976650859
Epoch 93:
batch 5 loss: 0.000557728111743927
batch 10 loss: 0.00055776780936867
batch 15 loss: 0.0005576783092692495
batch 20 loss: 0.0005575710325501859
batch 25 loss: 0.0005576708121225238
batch 30 loss: 0.0005577489151619375
batch 35 loss: 0.0005577050265856087
batch 40 loss: 0.0005576926399953663
batch 45 loss: 0.0005576917435973882
batch 50 loss: 0.0005576945026405155
batch 55 loss: 0.0005576946306973696
batch 60 loss: 0.0005577567382715643
batch 65 loss: 0.000557738624047488
batch 70 loss: 0.0005574961425736547
batch 75 loss: 0.0005576685769483447
batch 80 loss: 0.0005576521973125637
batch 85 loss: 0.0005576282157562674
batch 90 loss: 0.0005577570875175298
batch 95 loss: 0.0005577214993536473
batch 100 loss: 0.000557624117936939
batch 105 loss: 0.0005577271338552236
batch 110 loss: 0.000557638646569103
batch 115 loss: 0.0005577597068622709
batch 120 loss: 0.000557789602316916
batch 125 loss: 0.0005576511262916029
batch 130 loss: 0.0005575770745053887
batch 135 loss: 0.0005577142699621617
batch 140 loss: 0.0005575786111876368
batch 145 loss: 0.0005576960160396993
batch 150 loss: 0.0005576479714363813
batch 155 loss: 0.0005576419644057751
batch 160 loss: 0.0005576873430982232
batch 165 loss: 0.0005576886585913598
batch 170 loss: 0.0005576508701778948
batch 175 loss: 0.0005577526637353003
batch 180 loss: 0.0005576786468736827
batch 185 loss: 0.0005577574484050274
batch 190 loss: 0.0005577570525929331
batch 195 loss: 0.0005577155272476376
batch 200 loss: 0.0005577531177550554
batch 205 loss: 0.0005575203569605947
batch 210 loss: 0.0005575547809712588
batch 215 loss: 0.0005577170522883534
batch 220 loss: 0.00055766279110685
batch 225 loss: 0.0005576590308919549
batch 230 loss: 0.000557662476785481
batch 235 loss: 0.000557677005417645
batch 240 loss: 0.000557798333466053
Training Loss: 0.0005576833752760043
Validation Loss: 0.0005576934112468734
Epoch 94:
batch 5 loss: 0.0005577284027822316
batch 10 loss: 0.0005576055031269789
batch 15 loss: 0.000557650055270642
batch 20 loss: 0.0005577905103564262
batch 25 loss: 0.0005576204508543015
batch 30 loss: 0.0005576735478825867
batch 35 loss: 0.0005577215808443726
batch 40 loss: 0.0005576547118835151
batch 45 loss: 0.0005575947579927742
batch 50 loss: 0.0005576898111030459
batch 55 loss: 0.0005577210104092956
batch 60 loss: 0.0005577618372626603
batch 65 loss: 0.00055766407167539
batch 70 loss: 0.000557693128939718
batch 75 loss: 0.0005576963303610682
batch 80 loss: 0.0005575210554525257
batch 85 loss: 0.0005576515686698258
batch 90 loss: 0.0005576260853558778
batch 95 loss: 0.0005577240604907274
batch 100 loss: 0.0005576559924520552
batch 105 loss: 0.0005576606490649283
batch 110 loss: 0.0005577925359830261
batch 115 loss: 0.0005576991592533886
batch 120 loss: 0.0005577341536991299
batch 125 loss: 0.0005577239440754056
batch 130 loss: 0.000557736458722502
batch 135 loss: 0.0005576269351877273
batch 140 loss: 0.000557660753838718
batch 145 loss: 0.0005576593452133238
batch 150 loss: 0.0005575807415880262
batch 155 loss: 0.0005576990311965347
batch 160 loss: 0.0005577099393121899
batch 165 loss: 0.0005576706957072019
batch 170 loss: 0.00055764967110008
batch 175 loss: 0.0005577724543400109
batch 180 loss: 0.0005575682735070586
batch 185 loss: 0.0005576602765358984
batch 190 loss: 0.0005576895433478058
batch 195 loss: 0.0005577223259024322
batch 200 loss: 0.0005577245145104825
batch 205 loss: 0.0005576325347647071
batch 210 loss: 0.000557587225921452
batch 215 loss: 0.0005577250616624951
batch 220 loss: 0.0005578435841016472
batch 225 loss: 0.0005576035473495722
batch 230 loss: 0.0005577390664257109
batch 235 loss: 0.0005577650968916714
batch 240 loss: 0.0005577202071435749
Training Loss: 0.000557683379156515
Validation Loss: 0.0005576933986352135
Epoch 95:
batch 5 loss: 0.0005576674942858517
batch 10 loss: 0.000557579577434808
batch 15 loss: 0.0005577770876698196
batch 20 loss: 0.0005576464929617942
batch 25 loss: 0.0005577084259130061
batch 30 loss: 0.0005577657488174736
batch 35 loss: 0.0005576355964876711
batch 40 loss: 0.0005576815223321318
batch 45 loss: 0.0005576481344178319
batch 50 loss: 0.000557781825773418
batch 55 loss: 0.0005576826515607536
batch 60 loss: 0.00055775340879336
batch 65 loss: 0.0005576781695708632
batch 70 loss: 0.0005576455383561552
batch 75 loss: 0.0005577010568231344
batch 80 loss: 0.0005576240364462137
batch 85 loss: 0.0005576797295361758
batch 90 loss: 0.0005576713010668755
batch 95 loss: 0.0005577432224527001
batch 100 loss: 0.0005576679948717355
batch 105 loss: 0.0005577904055826366
batch 110 loss: 0.000557618064340204
batch 115 loss: 0.0005576405208557844
batch 120 loss: 0.0005577351548708975
batch 125 loss: 0.000557640299666673
batch 130 loss: 0.0005576764931902289
batch 135 loss: 0.0005577195435762406
batch 140 loss: 0.0005575233022682369
batch 145 loss: 0.0005575889372266829
batch 150 loss: 0.000557736586779356
batch 155 loss: 0.0005577007541432977
batch 160 loss: 0.0005577121395617723
batch 165 loss: 0.0005576825700700283
batch 170 loss: 0.0005578175885602831
batch 175 loss: 0.0005577534204348922
batch 180 loss: 0.0005577196017839015
batch 185 loss: 0.0005577660049311817
batch 190 loss: 0.0005577728967182338
batch 195 loss: 0.0005576532683335244
batch 200 loss: 0.0005575661780312657
batch 205 loss: 0.0005576804163865745
batch 210 loss: 0.0005576514638960361
batch 215 loss: 0.0005576933152042329
batch 220 loss: 0.000557660881895572
batch 225 loss: 0.0005576728377491236
batch 230 loss: 0.0005575036397203803
batch 235 loss: 0.0005577179486863315
batch 240 loss: 0.0005576689960435033
Training Loss: 0.0005576833801266427
Validation Loss: 0.0005576934005754689
Epoch 96:
batch 5 loss: 0.0005577208707109093
batch 10 loss: 0.0005575736751779914
batch 15 loss: 0.0005577174248173833
batch 20 loss: 0.0005577020696364343
batch 25 loss: 0.0005576224648393691
batch 30 loss: 0.0005577104981057346
batch 35 loss: 0.0005576551426202059
batch 40 loss: 0.000557718810159713
batch 45 loss: 0.0005576948286034166
batch 50 loss: 0.0005576618015766144
batch 55 loss: 0.0005576213472522796
batch 60 loss: 0.0005577064235694707
batch 65 loss: 0.0005577322212047875
batch 70 loss: 0.000557636097073555
batch 75 loss: 0.000557591940741986
batch 80 loss: 0.0005577058182097971
batch 85 loss: 0.0005576753057539463
batch 90 loss: 0.0005576647236011922
batch 95 loss: 0.0005576891009695828
batch 100 loss: 0.0005576880532316864
batch 105 loss: 0.0005577986245043576
batch 110 loss: 0.0005575952236540616
batch 115 loss: 0.0005576507304795086
batch 120 loss: 0.0005576711962930858
batch 125 loss: 0.0005577068543061614
batch 130 loss: 0.0005577021860517561
batch 135 loss: 0.0005577162723056972
batch 140 loss: 0.0005577351665124297
batch 145 loss: 0.0005575807532295585
batch 150 loss: 0.0005576469004154206
batch 155 loss: 0.0005576943047344684
batch 160 loss: 0.0005576309282332659
batch 165 loss: 0.0005577714997343719
batch 170 loss: 0.0005576175171881914
batch 175 loss: 0.0005576708936132491
batch 180 loss: 0.0005577314295805991
batch 185 loss: 0.0005576399387791752
batch 190 loss: 0.0005576085532084107
batch 195 loss: 0.0005577159812673926
batch 200 loss: 0.0005578018142841756
batch 205 loss: 0.0005576782277785242
batch 210 loss: 0.0005577893694862724
batch 215 loss: 0.0005577820935286582
batch 220 loss: 0.0005575749906711281
batch 225 loss: 0.0005576862255111337
batch 230 loss: 0.0005577300558798015
batch 235 loss: 0.0005577192292548716
batch 240 loss: 0.0005576663883402943
Training Loss: 0.0005576833743058766
Validation Loss: 0.0005576934044559796
Epoch 97:
batch 5 loss: 0.0005577586591243744
batch 10 loss: 0.0005576266907155514
batch 15 loss: 0.0005575870629400015
batch 20 loss: 0.0005576992640271783
batch 25 loss: 0.0005576805095188319
batch 30 loss: 0.0005577318836003542
batch 35 loss: 0.0005577203934080899
batch 40 loss: 0.0005575416958890856
batch 45 loss: 0.0005576457129791379
batch 50 loss: 0.0005576864467002451
batch 55 loss: 0.0005577976466156542
batch 60 loss: 0.0005576687748543918
batch 65 loss: 0.0005575678078457713
batch 70 loss: 0.0005576735711656511
batch 75 loss: 0.0005575211835093796
batch 80 loss: 0.0005577603471465409
batch 85 loss: 0.0005578125594183803
batch 90 loss: 0.0005576356081292034
batch 95 loss: 0.0005576540832407772
batch 100 loss: 0.0005577013362199068
batch 105 loss: 0.000557625584769994
batch 110 loss: 0.0005577210686169565
batch 115 loss: 0.0005577118950895965
batch 120 loss: 0.0005577287171036005
batch 125 loss: 0.0005577862728387117
batch 130 loss: 0.000557745574042201
batch 135 loss: 0.0005576855153776705
batch 140 loss: 0.000557596841827035
batch 145 loss: 0.0005577569711022079
batch 150 loss: 0.0005577016388997435
batch 155 loss: 0.0005577296018600464
batch 160 loss: 0.0005577121279202402
batch 165 loss: 0.0005575442803092301
batch 170 loss: 0.0005576404975727201
batch 175 loss: 0.0005577149451710284
batch 180 loss: 0.0005577137344516814
batch 185 loss: 0.0005576214520260691
batch 190 loss: 0.0005575842806138098
batch 195 loss: 0.0005575741757638753
batch 200 loss: 0.0005576724768616259
batch 205 loss: 0.0005577048286795616
batch 210 loss: 0.0005576927796937525
batch 215 loss: 0.0005576890893280506
batch 220 loss: 0.0005576599505729973
batch 225 loss: 0.0005576895549893379
batch 230 loss: 0.0005577338044531644
batch 235 loss: 0.0005577788804657757
batch 240 loss: 0.0005578141426667571
Training Loss: 0.0005576833733357489
Validation Loss: 0.0005576933976650859
Epoch 98:
batch 5 loss: 0.0005576629773713648
batch 10 loss: 0.0005577360396273434
batch 15 loss: 0.0005577305797487498
batch 20 loss: 0.0005577373900450766
batch 25 loss: 0.0005576379946433008
batch 30 loss: 0.0005575580871663988
batch 35 loss: 0.0005576694034971297
batch 40 loss: 0.0005576935363933444
batch 45 loss: 0.0005576675874181091
batch 50 loss: 0.0005576894269324839
batch 55 loss: 0.0005577496602199971
batch 60 loss: 0.0005576743511483073
batch 65 loss: 0.0005577370058745146
batch 70 loss: 0.0005576837225817144
batch 75 loss: 0.0005577249801717698
batch 80 loss: 0.0005576074006967246
batch 85 loss: 0.0005577122676186264
batch 90 loss: 0.0005576733965426683
batch 95 loss: 0.0005576696014031768
batch 100 loss: 0.0005576815688982606
batch 105 loss: 0.0005576324532739818
batch 110 loss: 0.0005576635478064418
batch 115 loss: 0.0005577048636041581
batch 120 loss: 0.0005577291827648878
batch 125 loss: 0.0005578227574005723
batch 130 loss: 0.0005575909395702183
batch 135 loss: 0.0005577009753324092
batch 140 loss: 0.00055776028893888
batch 145 loss: 0.0005576804629527033
batch 150 loss: 0.0005576497758738696
batch 155 loss: 0.0005576615571044385
batch 160 loss: 0.0005577105330303311
batch 165 loss: 0.0005576896946877241
batch 170 loss: 0.000557791325263679
batch 175 loss: 0.0005576738971285522
batch 180 loss: 0.0005576943862251937
batch 185 loss: 0.0005575775867328048
batch 190 loss: 0.0005577248288318515
batch 195 loss: 0.0005578046897426247
batch 200 loss: 0.0005577188567258418
batch 205 loss: 0.0005576131981797517
batch 210 loss: 0.0005576325580477715
batch 215 loss: 0.0005576474010013044
batch 220 loss: 0.0005577099393121899
batch 225 loss: 0.0005577080301009119
batch 230 loss: 0.0005575605318881571
batch 235 loss: 0.0005576449097134173
batch 240 loss: 0.0005576060037128627
Training Loss: 0.0005576833781863873
Validation Loss: 0.0005576934073663627
Epoch 99:
batch 5 loss: 0.0005576388328336179
batch 10 loss: 0.0005578550044447184
batch 15 loss: 0.00055764444405213
batch 20 loss: 0.0005576368654146791
batch 25 loss: 0.0005575603456236422
batch 30 loss: 0.0005576612893491983
batch 35 loss: 0.0005576353287324309
batch 40 loss: 0.0005577522446401417
batch 45 loss: 0.0005576167139224708
batch 50 loss: 0.000557710905559361
batch 55 loss: 0.0005577452480793
batch 60 loss: 0.000557766342535615
batch 65 loss: 0.0005576785886660218
batch 70 loss: 0.0005577502073720097
batch 75 loss: 0.0005577239790000021
batch 80 loss: 0.0005577073781751097
batch 85 loss: 0.0005576033378019929
batch 90 loss: 0.0005576897878199816
batch 95 loss: 0.000557703513186425
batch 100 loss: 0.0005577082862146199
batch 105 loss: 0.0005576756549999118
batch 110 loss: 0.0005577748408541084
batch 115 loss: 0.0005576209980063141
batch 120 loss: 0.000557656493037939
batch 125 loss: 0.0005577265284955502
batch 130 loss: 0.0005576221970841289
batch 135 loss: 0.0005576897645369172
batch 140 loss: 0.0005577805801294744
batch 145 loss: 0.0005577214644290507
batch 150 loss: 0.0005575933028012514
batch 155 loss: 0.0005577192292548716
batch 160 loss: 0.000557715690229088
batch 165 loss: 0.0005576435709372163
batch 170 loss: 0.0005576293333433568
batch 175 loss: 0.0005576869007200003
batch 180 loss: 0.0005576161900535225
batch 185 loss: 0.0005576967727392912
batch 190 loss: 0.0005577250267378986
batch 195 loss: 0.0005576528725214303
batch 200 loss: 0.0005577612086199224
batch 205 loss: 0.0005576199619099498
batch 210 loss: 0.0005577046074904501
batch 215 loss: 0.0005575848394073546
batch 220 loss: 0.0005576324998401106
batch 225 loss: 0.0005576532217673958
batch 230 loss: 0.000557699438650161
batch 235 loss: 0.0005577362608164549
batch 240 loss: 0.0005576738156378269
Training Loss: 0.000557683373093217
Validation Loss: 0.0005576934005754689
Epoch 100:
batch 5 loss: 0.0005576622788794339
batch 10 loss: 0.000557681149803102
batch 15 loss: 0.0005576002295129001
batch 20 loss: 0.000557689182460308
batch 25 loss: 0.0005576371913775802
batch 30 loss: 0.0005576925817877054
batch 35 loss: 0.0005577059695497155
batch 40 loss: 0.0005577255855314433
batch 45 loss: 0.0005577046191319823
batch 50 loss: 0.0005576678435318172
batch 55 loss: 0.0005576196010224522
batch 60 loss: 0.0005576933384872973
batch 65 loss: 0.0005577312316745519
batch 70 loss: 0.0005576772149652242
batch 75 loss: 0.0005577333038672804
batch 80 loss: 0.0005576405092142522
batch 85 loss: 0.0005576749565079809
batch 90 loss: 0.0005577502073720097
batch 95 loss: 0.0005575888557359576
batch 100 loss: 0.0005576934199780226
batch 105 loss: 0.0005577197880484164
batch 110 loss: 0.0005576711613684892
batch 115 loss: 0.000557721487712115
batch 120 loss: 0.0005575610208325088
batch 125 loss: 0.0005576959578320384
batch 130 loss: 0.0005576460156589746
batch 135 loss: 0.0005575264454819262
batch 140 loss: 0.0005576679948717355
batch 145 loss: 0.0005577318021096289
batch 150 loss: 0.0005577103001996875
batch 155 loss: 0.0005576875642873347
batch 160 loss: 0.0005576812196522951
batch 165 loss: 0.0005576617084443569
batch 170 loss: 0.000557662732899189
batch 175 loss: 0.0005576825351454318
batch 180 loss: 0.000557740533258766
batch 185 loss: 0.0005577343283221126
batch 190 loss: 0.0005576365627348423
batch 195 loss: 0.000557662220671773
batch 200 loss: 0.0005577378557063639
batch 205 loss: 0.0005577377509325743
batch 210 loss: 0.000557706574909389
batch 215 loss: 0.0005577420466579497
batch 220 loss: 0.0005577742122113705
batch 225 loss: 0.0005577114061452448
batch 230 loss: 0.0005576665746048093
batch 235 loss: 0.0005576386931352318
batch 240 loss: 0.0005577161209657788
Training Loss: 0.0005576833726081531
Validation Loss: 0.000557693403485852
****************************************************************
*                      SLURM Batch System                      *
*           IN2P3 Computing Centre, Villeurbanne FR            *
****************************************************************
Date: Mon Jul  1 18:39:47 CEST 2024
Job informations can be found using these commands:
Accounting:
sacct -j 17512721
Efficiency:
seff 17512721
****************************************************************
